diff --git a/drivers/net/ethernet/micrel/ksz9897/Kconfig b/drivers/net/ethernet/micrel/ksz9897/Kconfig
new file mode 100644
index 0000000..ba5befd
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/Kconfig
@@ -0,0 +1,116 @@
+
+config KSZ_PTP
+	bool "1588 PTP support"
+	depends on KSZ8462_HLI || HAVE_KSZ8463 || HAVE_KSZ9897
+	default y
+	help
+	  Enable 1588 PTP support.
+
+config KSZ_STP
+	bool "STP support"
+	depends on KSZ8462_HLI || KSZ_SWITCH
+	default y
+	help
+	  Enable STP support.
+
+menuconfig KSZ_SWITCHES
+	tristate "Drivers for Microchip KSZ switches"
+	---help---
+	  Supports Microchip KSZ switches.
+
+if KSZ_SWITCHES
+
+comment "Microchip KSZ switch device drivers"
+
+config KSZ_SWITCH
+	bool
+	default n
+
+config KSZ_DSA
+	bool
+	default n
+
+config KSZ_SWITCH_EMBEDDED
+	bool
+	default n
+	select KSZ_SWITCH
+
+config HAVE_KSZ9897
+	bool
+	default n
+
+config HAVE_SPI_KSZ9897
+	bool
+	default n
+	select HAVE_KSZ9897
+
+config I2C_KSZ9897
+	tristate "I2C driver for Microchip KSZ9897 switch"
+	select KSZ_SWITCH if !NET_DSA_TAG_TAIL
+	select KSZ_DSA if NET_DSA_TAG_TAIL
+	select HAVE_KSZ9897
+	---help---
+	  Supports the Microchip KSZ9897 switch.
+
+config SPI_KSZ9897
+	tristate "SPI driver for Microchip KSZ9897 switch"
+	select KSZ_SWITCH if !NET_DSA_TAG_TAIL
+	select KSZ_DSA if NET_DSA_TAG_TAIL
+	select HAVE_SPI_KSZ9897
+	---help---
+	  Supports the Microchip KSZ9897 switch.
+
+config KSZ9897_EMBEDDED
+	bool "Microchip KSZ9897 switch support in network controller"
+	depends on SPI_KSZ9897 = n && !NET_DSA_TAG_TAIL
+	select KSZ_SWITCH_EMBEDDED
+	select HAVE_SPI_KSZ9897
+	---help---
+	  Supports the Microchip KSZ9897 switch used within a network controller.
+
+config KSZ_IBA_ONLY
+	bool "Microchip KSZ9897 switch with IBA support"
+	depends on SPI_KSZ9897 = n && !NET_DSA_TAG_TAIL
+	select KSZ_SWITCH_EMBEDDED
+	select KSZ_IBA
+	select HAVE_KSZ9897
+	---help---
+	  Supports the Microchip KSZ9897 switch used within a network controller using IBA.
+
+config KSZ_IBA
+	bool "IBA support"
+	depends on HAVE_KSZ9897
+	default y
+	help
+	  Enable IBA support.
+
+config KSZ_MRP
+	bool "MRP support"
+	depends on KSZ_SWITCH
+	default y if (HAVE_KSZ9897)
+	help
+	  Enable MRP support.
+
+config KSZ_MSRP
+	bool "MSRP support"
+	depends on (HAVE_KSZ9897) && KSZ_MRP
+	default y
+	help
+	  Enable MSRP support.
+
+config KSZ_DLR
+	bool "DLR support"
+	depends on (HAVE_KSZ9897 || HAVE_KSZ8795) && KSZ_SWITCH
+	default y if (HAVE_KSZ9897)
+	help
+	  Enable DLR support.
+
+config KSZ_HSR
+	bool "HSR support"
+	depends on (HAVE_KSZ9897) && KSZ_SWITCH
+	default y if (HAVE_KSZ9897)
+	help
+	  Enable HSR support.
+
+endif
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/Makefile b/drivers/net/ethernet/micrel/ksz9897/Makefile
new file mode 100644
index 0000000..59672b0
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/Makefile
@@ -0,0 +1,6 @@
+#
+# Makefile for the Microchip/Micrel network device drivers.
+#
+
+obj-$(CONFIG_I2C_KSZ9897) += i2c-ksz9897.o
+obj-$(CONFIG_SPI_KSZ9897) += spi-ksz9897.o
diff --git a/drivers/net/ethernet/micrel/ksz9897/hsr_main.h b/drivers/net/ethernet/micrel/ksz9897/hsr_main.h
new file mode 100644
index 0000000..1791d23
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/hsr_main.h
@@ -0,0 +1,223 @@
+/* Copyright 2011-2014 Autronica Fire and Security AS
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; either version 2 of the License, or (at your option)
+ * any later version.
+ *
+ * Author(s):
+ *	2011-2014 Arvid Brodin, arvid.brodin@alten.se
+ */
+
+#ifndef __HSR_PRIVATE_H
+#define __HSR_PRIVATE_H
+
+#include <linux/netdevice.h>
+#include <linux/if_vlan.h>
+#include <linux/list.h>
+
+
+#ifdef CONFIG_KSZ_SWITCH
+#define CONFIG_HSRv1
+#endif
+
+
+/* Time constants as specified in the HSR specification (IEC-62439-3 2010)
+ * Table 8.
+ * All values in milliseconds.
+ */
+#define HSR_LIFE_CHECK_INTERVAL		 2000 /* ms */
+#define HSR_NODE_FORGET_TIME		60000 /* ms */
+#define HSR_ANNOUNCE_INTERVAL		  100 /* ms */
+
+#define HSR_ENTRY_FORGET_TIME		  400 /* ms */
+#define HSR_NODE_REBOOT_INTERVAL	  500 /* ms */
+
+
+/* By how much may slave1 and slave2 timestamps of latest received frame from
+ * each node differ before we notify of communication problem?
+ */
+#define MAX_SLAVE_DIFF			 3000 /* ms */
+#define HSR_SEQNR_START			(USHRT_MAX - 1024)
+
+
+/* How often shall we check for broken ring and remove node entries older than
+ * HSR_NODE_FORGET_TIME?
+ */
+#define PRUNE_PERIOD			 3000 /* ms */
+
+
+#ifndef CONFIG_HSRv1
+#define HSR_TLV_ANNOUNCE		   22
+#endif
+#define PRP_TLV_LIFE_CHECK		   20
+#define HSR_TLV_LIFE_CHECK		   23
+#define HSR_TLV_REDBOX			   30
+
+
+/* HSR Tag.
+ * As defined in IEC-62439-3:2010, the HSR tag is really { ethertype = 0x88FB,
+ * path, LSDU_size, sequence Nr }. But we let eth_header() create { h_dest,
+ * h_source, h_proto = 0x88FB }, and add { path, LSDU_size, sequence Nr,
+ * encapsulated protocol } instead.
+ *
+ * Field names as defined in the IEC:2010 standard for HSR.
+ */
+struct hsr_tag {
+	__be16		path_and_LSDU_size;
+	__be16		sequence_nr;
+	__be16		encap_proto;
+} __packed;
+
+#define HSR_HLEN	6
+
+/* The helper functions below assumes that 'path' occupies the 4 most
+ * significant bits of the 16-bit field shared by 'path' and 'LSDU_size' (or
+ * equivalently, the 4 most significant bits of HSR tag byte 14).
+ *
+ * This is unclear in the IEC specification; its definition of MAC addresses
+ * indicates the spec is written with the least significant bit first (to the
+ * left). This, however, would mean that the LSDU field would be split in two
+ * with the path field in-between, which seems strange. I'm guessing the MAC
+ * address definition is in error.
+ */
+static inline u16 get_hsr_tag_path(struct hsr_tag *ht)
+{
+	return ntohs(ht->path_and_LSDU_size) >> 12;
+}
+
+static inline u16 get_hsr_tag_LSDU_size(struct hsr_tag *ht)
+{
+	return ntohs(ht->path_and_LSDU_size) & 0x0FFF;
+}
+
+static inline void set_hsr_tag_path(struct hsr_tag *ht, u16 path)
+{
+	ht->path_and_LSDU_size = htons(
+			(ntohs(ht->path_and_LSDU_size) & 0x0FFF) | (path << 12));
+}
+
+static inline void set_hsr_tag_LSDU_size(struct hsr_tag *ht, u16 LSDU_size)
+{
+	ht->path_and_LSDU_size = htons(
+			(ntohs(ht->path_and_LSDU_size) & 0xF000) |
+			(LSDU_size & 0x0FFF));
+}
+
+struct hsr_ethhdr {
+	struct ethhdr	ethhdr;
+	struct hsr_tag	hsr_tag;
+} __packed;
+
+
+/* HSR Supervision Frame data types.
+ * Field names as defined in the IEC:2010 standard for HSR.
+ */
+#ifdef CONFIG_HSRv1
+struct hsr_sup_tag {
+	__be16		path_and_HSR_Ver;
+	__be16		sequence_nr;
+} __packed;
+
+struct hsr_sup_type {
+	__u8		HSR_TLV_Type;
+	__u8		HSR_TLV_Length;
+} __packed;
+#else
+struct hsr_sup_tag {
+	__be16		path_and_HSR_Ver;
+	__be16		sequence_nr;
+	__u8		HSR_TLV_Type;
+	__u8		HSR_TLV_Length;
+} __packed;
+#endif
+
+struct hsr_sup_payload {
+	unsigned char	MacAddressA[ETH_ALEN];
+} __packed;
+
+static inline u16 get_hsr_stag_path(struct hsr_sup_tag *hst)
+{
+	return get_hsr_tag_path((struct hsr_tag *) hst);
+}
+
+static inline u16 get_hsr_stag_HSR_ver(struct hsr_sup_tag *hst)
+{
+	return get_hsr_tag_LSDU_size((struct hsr_tag *) hst);
+}
+
+static inline void set_hsr_stag_path(struct hsr_sup_tag *hst, u16 path)
+{
+	set_hsr_tag_path((struct hsr_tag *) hst, path);
+}
+
+static inline void set_hsr_stag_HSR_Ver(struct hsr_sup_tag *hst, u16 HSR_Ver)
+{
+	set_hsr_tag_LSDU_size((struct hsr_tag *) hst, HSR_Ver);
+}
+
+struct hsr_ethhdr_sp {
+	struct ethhdr		ethhdr;
+	struct hsr_sup_tag	hsr_sup;
+#ifdef CONFIG_HSRv1
+	struct hsr_sup_type	sup_type;
+#endif
+} __packed;
+
+
+enum hsr_port_type {
+	HSR_PT_NONE = 0,	/* Must be 0, used by framereg */
+	HSR_PT_SLAVE_A,
+	HSR_PT_SLAVE_B,
+	HSR_PT_INTERLINK,
+	HSR_PT_MASTER,
+	HSR_PT_PORTS,	/* This must be the last item in the enum */
+};
+
+struct hsr_port {
+	struct list_head	port_list;
+	struct net_device	*dev;
+	struct hsr_priv		*hsr;
+	enum hsr_port_type	type;
+};
+
+struct hsr_priv {
+	struct rcu_head		rcu_head;
+	struct list_head	ports;
+	struct list_head	node_db;	/* Known HSR nodes */
+	struct list_head	self_node_db;	/* MACs of slaves */
+	struct timer_list	announce_timer;	/* Supervision frame dispatch */
+	struct timer_list	prune_timer;
+	int announce_count;
+	u16 sequence_nr;
+#ifdef CONFIG_HSRv1
+	u16 sup_sequence_nr;
+	u8 net_id;
+	u8 redbox_id;
+	u16 vid;
+#endif
+	spinlock_t seqnr_lock;			/* locking for sequence_nr */
+	unsigned char		sup_multicast_addr[ETH_ALEN];
+};
+
+#define hsr_for_each_port(hsr, port) \
+	list_for_each_entry_rcu((port), &(hsr)->ports, port_list)
+
+#if 0
+static
+struct hsr_port *hsr_port_get_hsr(struct hsr_priv *hsr, enum hsr_port_type pt);
+#endif
+
+/* Caller must ensure skb is a valid HSR frame */
+static inline u16 hsr_get_skb_sequence_nr(struct sk_buff *skb)
+{
+	struct hsr_ethhdr *hsr_ethhdr;
+
+	hsr_ethhdr = (struct hsr_ethhdr *) skb_mac_header(skb);
+	if (hsr_ethhdr->ethhdr.h_proto == htons(ETH_P_8021Q))
+		hsr_ethhdr = (struct hsr_ethhdr *)((u8 *) hsr_ethhdr +
+			VLAN_HLEN);
+	return ntohs(hsr_ethhdr->hsr_tag.sequence_nr);
+}
+
+#endif /*  __HSR_PRIVATE_H */
diff --git a/drivers/net/ethernet/micrel/ksz9897/i2c-ksz9897.c b/drivers/net/ethernet/micrel/ksz9897/i2c-ksz9897.c
new file mode 100644
index 0000000..68a7fefb
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/i2c-ksz9897.c
@@ -0,0 +1,765 @@
+/**
+ * Microchip KSZ9897 I2C driver
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+#include <linux/i2c.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#define MAX_REQUEST_SIZE		80
+
+#include "ksz_cfg_9897.h"
+
+
+#if 1
+#define NO_EEE
+#endif
+
+
+#ifdef CONFIG_KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+#define CONFIG_HAVE_HSR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+
+#define SW_DRV_RELDATE			"Feb 8, 2017"
+#define SW_DRV_VERSION			"1.1.0"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		i2c_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	i2c_wrreg16(ks, reg, val)
+#define HW_R24(ks, reg)		i2c_rdreg24(ks, reg)
+#define HW_W24(ks, reg, val)	i2c_wrreg24(ks, reg, val)
+#define HW_R32(ks, reg)		i2c_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	i2c_wrreg32(ks, reg, val)
+
+#include "ksz_sw_phy.h"
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define contain_reg(addr, len, reg)	\
+	(addr <= (reg) && (reg) <= (addr + len - 1))
+
+static void i2c_chk_regs(struct ksz_sw *sw, u32 addr, u8 *val, size_t txl)
+{
+	int i;
+	u32 port_reg;
+
+	port_reg = REG_PTP_MSG_CONF1 + 1;
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PTP_ENABLE)
+			sw->overrides |= PTP_TAG;
+		else
+			sw->overrides &= ~PTP_TAG;
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+		if (!(sw->features & NEW_CAP) &&
+		    sw->TAIL_TAG_LOOKUP >= 0x100)
+			sw->overrides |= PTP_TAG;
+#endif
+	}
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PORT_TAIL_TAG_ENABLE)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+	}
+}  /* i2c_chk_regs */
+
+/*
+ * I2C register read/write calls.
+ *
+ * All these calls issue I2C transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * i2c_wrreg - issue write register command
+ * @ks:		The switch device structure.
+ * @addr:	The register address.
+ * @val:	The value to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary i2c message(s)
+ * to write data to the register specified in @addr.
+ */
+static void i2c_wrreg(struct sw_priv *ks, u32 addr, void *txb, size_t txl)
+{
+	struct i2c_hw_priv *hw_priv = ks->hw_dev;
+	struct i2c_msg msg;
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("W not locked\n");
+
+	hw_priv->txd[0] = (u8)(addr >> 8);
+	hw_priv->txd[1] = (u8) addr;
+
+	memcpy(&hw_priv->txd[2], txb, txl);
+
+	msg.addr = i2c->addr;
+	msg.flags = 0;
+	msg.len = 2 + txl;
+	msg.buf = hw_priv->txd;
+
+	if (i2c_transfer(adapter, &msg, 1) != 1)
+		pr_alert("i2c_transfer() failed\n");
+	i2c_chk_regs(&ks->sw, addr, txb, txl);
+}
+
+/**
+ * i2c_wrreg32 - write 32bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg32(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 4;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 4);
+}
+
+/**
+ * i2c_wrreg24 - write 24bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg24(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 3;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 3);
+}
+
+/**
+ * i2c_wrreg16 - write 16bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg16(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 2;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 2);
+}
+
+/**
+ * i2c_wrreg8 - write 8bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg8(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 1;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(ks, reg, txb, 1);
+}
+
+/**
+ * i2c_rdreg - issue read register command and return the data
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary i2c message(s)
+ * to read data from the register specified in @op.
+ */
+static void i2c_rdreg(struct sw_priv *ks, u32 addr, void *rxb, size_t rxl)
+{
+	struct i2c_hw_priv *hw_priv = ks->hw_dev;
+	struct i2c_msg msg[2];
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("R not locked\n");
+
+	hw_priv->txd[0] = (u8)(addr >> 8);
+	hw_priv->txd[1] = (u8) addr;
+
+	msg[0].addr = i2c->addr;
+	msg[0].flags = 0;
+	msg[0].len = 2;
+	msg[0].buf = hw_priv->txd;
+
+	msg[1].addr = i2c->addr;
+	msg[1].flags = I2C_M_RD;
+	msg[1].len = rxl;
+	msg[1].buf = rxb;
+
+	if (i2c_transfer(adapter, msg, 2) != 2)
+		pr_alert("i2c_transfer() failed\n");
+}
+
+/**
+ * i2c_rdreg8 - read 8 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 i2c_rdreg8(struct sw_priv *ks, u32 reg)
+{
+	u8 rxb[1];
+
+	i2c_rdreg(ks, reg, rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * i2c_rdreg16 - read 16 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 i2c_rdreg16(struct sw_priv *ks, u32 reg)
+{
+	__le16 rx = 0;
+
+	i2c_rdreg(ks, reg, &rx, 2);
+	return be16_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg24 - read 24 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 24bit register from the chip.
+ */
+static u32 i2c_rdreg24(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(ks, reg, &rx, 3);
+	return be32_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg32 - read 32 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ */
+static u32 i2c_rdreg32(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(ks, reg, &rx, 4);
+	return be32_to_cpu(rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * delay_micro - delay in microsecond
+ * @microsec:	Number of microseconds to delay.
+ *
+ * This routine delays in microseconds.
+ */
+static inline void delay_micro(uint microsec)
+{
+	uint millisec = microsec / 1000;
+
+	microsec %= 1000;
+	if (millisec)
+		mdelay(millisec);
+	if (microsec)
+		udelay(microsec);
+}
+
+/**
+ * delay_milli - delay in millisecond
+ * @millisec:	Number of milliseconds to delay.
+ *
+ * This routine delays in milliseconds.
+ */
+static void delay_milli(uint millisec)
+{
+	unsigned long ticks = millisec * HZ / 1000;
+
+	if (!ticks || in_interrupt())
+		mdelay(millisec);
+	else {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(ticks);
+	}
+}
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+static inline void copy_old_skb(struct sk_buff *old, struct sk_buff *skb)
+{
+	skb->dev = old->dev;
+	skb->sk = old->sk;
+	skb->protocol = old->protocol;
+	skb->ip_summed = old->ip_summed;
+	skb->csum = old->csum;
+	skb_shinfo(skb)->tx_flags = skb_shinfo(old)->tx_flags;
+	skb_set_network_header(skb, ETH_HLEN);
+
+	dev_kfree_skb(old);
+}  /* copy_old_skb */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define KSZSW_REGS_SIZE			0x8000
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x0000, 0x7FFF },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->reglock);
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed\n", __func__);
+		mutex_unlock(sw->reglock);
+		sw->reg->lock(sw);
+	}
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R24(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	i2c_rdreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	i2c_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W24(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+
+static void link_update_work(struct work_struct *work);
+
+static void sw_dis_intr(struct ksz_sw *sw);
+static void sw_ena_intr(struct ksz_sw *sw);
+
+#define USE_DIFF_PORT_PRIORITY
+#include "ksz_sw_9897.c"
+
+
+#define MAX_I2C_DEVICES		1
+
+static int ksz9897_probe(struct i2c_client *i2c,
+	const struct i2c_device_id *i2c_id)
+{
+	struct i2c_hw_priv *hw_priv;
+	struct sw_priv *ks;
+
+	ks = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!ks)
+		return -ENOMEM;
+
+	ks->hw_dev = kzalloc(sizeof(struct i2c_hw_priv), GFP_KERNEL);
+	if (!ks->hw_dev) {
+		kfree(ks);
+		return -ENOMEM;
+	}
+	hw_priv = ks->hw_dev;
+
+	hw_priv->i2cdev = i2c;
+
+	ks->dev = &i2c->dev;
+
+	ks->intr_mode = intr_mode ? IRQF_TRIGGER_FALLING :
+		IRQF_TRIGGER_LOW;
+	ks->irq = i2c->irq;
+
+	return ksz_probe(ks);
+}
+
+static int ksz9897_remove(struct i2c_client *i2c)
+{
+	struct sw_priv *ks = dev_get_drvdata(&i2c->dev);
+
+	return ksz_remove(ks);
+}
+
+#define I2C_SWITCH_NAME			"i2c-ksz9897"
+
+/* Please change the I2C address if necessary. */
+#define I2C_SWITCH_ADDR			0x5F
+
+/* Please provide a system interrupt number here. */
+#define I2C_SWITCH_INTR			-1
+
+static int ksz9897_detect(struct i2c_client *i2c, struct i2c_board_info *info)
+{
+	strncpy(info->type, I2C_SWITCH_NAME, I2C_NAME_SIZE);
+	info->irq = I2C_SWITCH_INTR;
+	return 0;
+}
+
+static unsigned short i2c_address_list[] = {
+	I2C_SWITCH_ADDR,
+
+	I2C_CLIENT_END
+};
+
+static const struct i2c_device_id i2c_id[] = {
+	{ I2C_SWITCH_NAME, 0 },
+	{ },
+};
+
+static struct i2c_driver ksz9897_driver = {
+	.driver.name	= I2C_SWITCH_NAME,
+	.probe		= ksz9897_probe,
+	.remove		= ksz9897_remove,
+	.id_table	= i2c_id,
+
+	/* Big enough to be accepted in all cases. */
+	.class		= 0xffff,
+	.detect		= ksz9897_detect,
+	.address_list	= i2c_address_list,
+};
+
+static int __init ksz9897_init(void)
+{
+	int ret;
+printk("%s \n", __FUNCTION__);
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	ret = i2c_add_driver(&ksz9897_driver);
+printk("%s i2c_add_driver: %d\n", __FUNCTION__, ret);
+	if (ret)
+		return ret;
+
+	/* Probe not called. */
+	if (!sw_device_present) {
+		struct i2c_adapter *adap;
+
+		/* Assume I2C bus starts at 0. */
+		adap = i2c_get_adapter(0);
+printk("%s adap: %p\n", __FUNCTION__, adap);
+		/* I2C master may not be created yet. */
+		if (!adap) {
+#if !defined(CONFIG_I2C_KSZ9897_MODULE)
+			struct i2c_board_info info = {
+				.type	= I2C_SWITCH_NAME,
+				.addr	= I2C_SWITCH_ADDR,
+				.irq	= I2C_SWITCH_INTR,
+			};
+
+			ret = i2c_register_board_info(0, &info, 1);
+#else
+			return -ENODEV;
+#endif
+		} else
+			i2c_put_adapter(adap);
+	}
+printk("%s ret: %d\n", __FUNCTION__, ret);
+	return ret;
+}
+
+static void __exit ksz9897_exit(void)
+{
+	i2c_del_driver(&ksz9897_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+module_init(ksz9897_init);
+module_exit(ksz9897_exit);
+#endif
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+module_param(avb, int, 0);
+module_param(authen, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+MODULE_PARM_DESC(avb, "AVB support");
+MODULE_PARM_DESC(authen, "802.1X Authentication");
+
+#ifdef CONFIG_KSZ_IBA
+module_param(iba, int, 0);
+MODULE_PARM_DESC(iba, "IBA support");
+#endif
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(0=level low, 1=falling)");
+
+module_param(sw_host_port, int, 0);
+MODULE_PARM_DESC(sw_host_port,
+	"Configure switch host port");
+
+module_param(ports, int, 0);
+MODULE_PARM_DESC(ports,
+	"Configure number of ports");
+
+module_param(eth1_ports, int, 0);
+module_param(eth2_ports, int, 0);
+module_param(eth3_ports, int, 0);
+module_param(eth4_ports, int, 0);
+module_param(eth5_ports, int, 0);
+module_param(eth6_ports, int, 0);
+MODULE_PARM_DESC(eth1_ports, "Ports to use on device 1.");
+MODULE_PARM_DESC(eth2_ports, "Ports to use on device 2.");
+MODULE_PARM_DESC(eth3_ports, "Ports to use on device 3.");
+MODULE_PARM_DESC(eth4_ports, "Ports to use on device 4.");
+MODULE_PARM_DESC(eth5_ports, "Ports to use on device 5.");
+MODULE_PARM_DESC(eth6_ports, "Ports to use on device 6.");
+
+module_param(eth1_vlan, int, 0);
+module_param(eth2_vlan, int, 0);
+module_param(eth3_vlan, int, 0);
+module_param(eth4_vlan, int, 0);
+module_param(eth5_vlan, int, 0);
+module_param(eth6_vlan, int, 0);
+MODULE_PARM_DESC(eth1_vlan, "VLAN to use on device 1.");
+MODULE_PARM_DESC(eth2_vlan, "VLAN to use on device 2.");
+MODULE_PARM_DESC(eth3_vlan, "VLAN to use on device 3.");
+MODULE_PARM_DESC(eth4_vlan, "VLAN to use on device 4.");
+MODULE_PARM_DESC(eth5_vlan, "VLAN to use on device 5.");
+MODULE_PARM_DESC(eth6_vlan, "VLAN to use on device 6.");
+
+module_param(eth1_proto, charp, 0);
+module_param(eth2_proto, charp, 0);
+module_param(eth3_proto, charp, 0);
+module_param(eth4_proto, charp, 0);
+module_param(eth5_proto, charp, 0);
+module_param(eth6_proto, charp, 0);
+MODULE_PARM_DESC(eth1_proto, "Protocol to use on device 1.");
+MODULE_PARM_DESC(eth2_proto, "Protocol to use on device 2.");
+MODULE_PARM_DESC(eth3_proto, "Protocol to use on device 3.");
+MODULE_PARM_DESC(eth4_proto, "Protocol to use on device 4.");
+MODULE_PARM_DESC(eth5_proto, "Protocol to use on device 5.");
+MODULE_PARM_DESC(eth6_proto, "Protocol to use on device 6.");
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("Microchip KSZ9897 I2C Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("i2c:ksz9897");
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz9897.h b/drivers/net/ethernet/micrel/ksz9897/ksz9897.h
new file mode 100644
index 0000000..8352651
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz9897.h
@@ -0,0 +1,1719 @@
+/**
+ * Microchip KSZ9897 definition file
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ9897_H
+#define __KSZ9897_H
+
+
+#define KS_PRIO_M			0x7
+#define KS_PRIO_S			4
+
+
+/* 0 - Operation */
+#define REG_CHIP_ID0__1			0x0000
+
+#define REG_CHIP_ID1__1			0x0001
+
+#define FAMILY_ID			0x95
+#define FAMILY_ID_94			0x94
+#define FAMILY_ID_95			0x95
+#define FAMILY_ID_85			0x85
+#define FAMILY_ID_98			0x98
+#define FAMILY_ID_88			0x88
+
+#define REG_CHIP_ID2__1			0x0002
+
+#define CHIP_ID_63			0x63
+#define CHIP_ID_66			0x66
+#define CHIP_ID_67			0x67
+#define CHIP_ID_77			0x77
+#define CHIP_ID_93			0x93
+#define CHIP_ID_96			0x96
+#define CHIP_ID_97			0x97
+
+#define REG_CHIP_ID3__1			0x0003
+
+#define SWITCH_REVISION_M		0x0F
+#define SWITCH_REVISION_S		4
+#define SWITCH_RESET			0x01
+
+#define REG_SW_PME_CTRL			0x0006
+
+#define PME_ENABLE			(1 << 1)
+#define PME_POLARITY			(1 << 0)
+
+#define REG_GLOBAL_OPTIONS		0x000F
+
+/* For KSZ9897 */
+#define SW_GIGABIT_ABLE			(1 << 6)
+#define SW_REDUNDANCY_ABLE		(1 << 5)
+#define SW_AVB_ABLE			(1 << 4)
+#define SW_9567_RL_5_2			0xC
+#define SW_9477_SL_5_2			0xD
+
+#define SW_9896_GL_5_1			0xB
+#define SW_9896_RL_5_1			0x8
+#define SW_9896_SL_5_1			0x9
+
+#define SW_9895_GL_4_1			0x7
+#define SW_9895_RL_4_1			0x4
+#define SW_9895_SL_4_1			0x5
+
+#define SW_9896_RL_4_2			0x6
+
+#define SW_9893_RL_2_1			0x0
+#define SW_9893_SL_2_1			0x1
+#define SW_9893_GL_2_1			0x3
+
+/* For KSZ9893 */
+#define SW_QW_ABLE			(1 << 5)
+#define SW_9893_RN_2_1			0xC
+
+#define REG_SW_INT_STATUS__4		0x0010
+#define REG_SW_INT_MASK__4		0x0014
+
+#define LUE_INT				(1 << 31)
+#define TRIG_TS_INT			(1 << 30)
+#define APB_TIMEOUT_INT			(1 << 29)
+
+#define SWITCH_INT_MASK			\
+	(TRIG_TS_INT | APB_TIMEOUT_INT)
+
+#define REG_SW_PORT_INT_STATUS__4	0x0018
+#define REG_SW_PORT_INT_MASK__4		0x001C
+#define REG_SW_PHY_INT_STATUS		0x0020
+#define REG_SW_PHY_INT_ENABLE		0x0024
+
+/* 1 - Global */
+#define REG_SW_GLOBAL_SERIAL_CTRL_0	0x0100
+#define SW_SPARE_REG_2			(1 << 7)
+#define SW_SPARE_REG_1			(1 << 6)
+#define SW_SPARE_REG_0			(1 << 5)
+#define SW_BIG_ENDIAN			(1 << 4)
+#define SPI_AUTO_EDGE_DETECTION		(1 << 1)
+#define SPI_CLOCK_OUT_RISING_EDGE	(1 << 0)
+
+#define REG_SW_GLOBAL_OUTPUT_CTRL__1	0x0103
+#define SW_ENABLE_REFCLKO		(1 << 1)
+#define SW_REFCLKO_IS_125MHZ		(1 << 0)
+
+#define REG_SW_IBA__4			0x0104
+
+#define SW_IBA_ENABLE			(1 << 31)
+#define SW_IBA_DA_MATCH			(1 << 30)
+#define SW_IBA_INIT			(1 << 29)
+#define SW_IBA_QID_M			0xF
+#define SW_IBA_QID_S			22
+#define SW_IBA_PORT_M			0x2F
+#define SW_IBA_PORT_S			16
+#define SW_IBA_FRAME_TPID_M		0xFFFF
+
+#define REG_SW_APB_TIMEOUT_ADDR__4	0x0108
+
+#define APB_TIMEOUT_ACKNOWLEDGE		(1 << 31)
+
+#define REG_SW_IBA_SYNC__1		0x010C
+
+#define REG_SW_IO_STRENGTH__1		0x010D
+#define SW_DRIVE_STRENGTH_M		0x7
+#define SW_DRIVE_STRENGTH_2MA		0
+#define SW_DRIVE_STRENGTH_4MA		1
+#define SW_DRIVE_STRENGTH_8MA		2
+#define SW_DRIVE_STRENGTH_12MA		3
+#define SW_DRIVE_STRENGTH_16MA		4
+#define SW_DRIVE_STRENGTH_20MA		5
+#define SW_DRIVE_STRENGTH_24MA		6
+#define SW_DRIVE_STRENGTH_28MA		7
+#define SW_HI_SPEED_DRIVE_STRENGTH_S	4
+#define SW_LO_SPEED_DRIVE_STRENGTH_S	0
+
+#define REG_SW_IBA_STATUS__4		0x0110
+
+#define SW_IBA_REQ			(1 << 31)
+#define SW_IBA_RESP			(1 << 30)
+#define SW_IBA_DA_MISMATCH		(1 << 14)
+#define SW_IBA_FMT_MISMATCH		(1 << 13)
+#define SW_IBA_CODE_ERROR		(1 << 12)
+#define SW_IBA_CMD_ERROR		(1 << 11)
+#define SW_IBA_CMD_LOC_M		((1 << 6) - 1)
+
+#define REG_SW_IBA_STATES__4		0x0114
+
+#define SW_IBA_BUF_STATE_S		30
+#define SW_IBA_CMD_STATE_S		28
+#define SW_IBA_RESP_STATE_S		26
+#define SW_IBA_STATE_M			0x3
+#define SW_IBA_PACKET_SIZE_M		0x7F
+#define SW_IBA_PACKET_SIZE_S		16
+#define SW_IBA_FMT_ID_M			0xFFFF
+
+#define REG_SW_IBA_RESULT__4		0x0118
+
+#define SW_IBA_SIZE_S			24
+
+#define SW_IBA_RETRY_CNT_M		((1 << 5) - 1)
+
+
+/* 2 - PHY */
+#define REG_SW_POWER_MANAGEMENT_CTRL	0x0201
+
+#define SW_PLL_POWER_DOWN		(1 << 5)
+#define SW_POWER_DOWN_MODE		0x3
+#define SW_ENERGY_DETECTION		1
+#define SW_SOFT_POWER_DOWN		2
+#define SW_POWER_SAVING			3
+
+/* 3 - Operation Control */
+#define REG_SW_OPERATION		0x0300
+
+#define SW_DOUBLE_TAG			(1 << 7)
+#define SW_RESET			(1 << 1)
+#define SW_START			(1 << 0)
+
+#define REG_SW_MAC_ADDR_0		0x0302
+#define REG_SW_MAC_ADDR_1		0x0303
+#define REG_SW_MAC_ADDR_2		0x0304
+#define REG_SW_MAC_ADDR_3		0x0305
+#define REG_SW_MAC_ADDR_4		0x0306
+#define REG_SW_MAC_ADDR_5		0x0307
+
+#define REG_SW_MTU__2			0x0308
+
+#define REG_SW_ISP_TPID__2		0x030A
+
+#define REG_SW_HSR_TPID__2		0x030C
+
+#define REG_AVB_STRATEGY__2		0x030E
+
+#define SW_SHAPING_CREDIT_ACCT		(1 << 1)
+#define SW_POLICING_CREDIT_ACCT		(1 << 0)
+
+#define REG_SW_LUE_CTRL_0		0x0310
+
+#define SW_VLAN_ENABLE			(1 << 7)
+#define SW_DROP_INVALID_VID		(1 << 6)
+#define SW_AGE_CNT_M			0x7
+#define SW_AGE_CNT_S			3
+#define SW_RESV_MCAST_ENABLE		(1 << 2)
+#define SW_HASH_OPTION_M		0x03
+#define SW_HASH_OPTION_CRC		1
+#define SW_HASH_OPTION_XOR		2
+#define SW_HASH_OPTION_DIRECT		3
+
+#define REG_SW_LUE_CTRL_1		0x0311
+
+#define UNICAST_LEARN_DISABLE		(1 << 7)
+#define SW_SRC_ADDR_FILTER		(1 << 6)
+#define SW_FLUSH_STP_TABLE		(1 << 5)
+#define SW_FLUSH_MSTP_TABLE		(1 << 4)
+#define SW_FWD_MCAST_SRC_ADDR		(1 << 3)
+#define SW_AGING_ENABLE			(1 << 2)
+#define SW_FAST_AGING			(1 << 1)
+#define SW_LINK_AUTO_AGING		(1 << 0)
+
+#define REG_SW_LUE_CTRL_2		0x0312
+
+#define SW_TRAP_DOUBLE_TAG		(1 << 6)
+#define SW_EGRESS_VLAN_FILTER_DYN	(1 << 5)
+#define SW_EGRESS_VLAN_FILTER_STA	(1 << 4)
+#define SW_FLUSH_OPTION_M		0x3
+#define SW_FLUSH_OPTION_S		2
+#define SW_FLUSH_OPTION_DYN_MAC		1
+#define SW_FLUSH_OPTION_STA_MAC		2
+#define SW_FLUSH_OPTION_BOTH		3
+#define SW_PRIO_M			0x3
+#define SW_PRIO_DA			0
+#define SW_PRIO_SA			1
+#define SW_PRIO_HIGHEST_DA_SA		2
+#define SW_PRIO_LOWEST_DA_SA		3
+
+#define REG_SW_LUE_CTRL_3		0x0313
+
+#define REG_SW_LUE_INT_STATUS		0x0314
+#define REG_SW_LUE_INT_ENABLE		0x0315
+
+#define LEARN_FAIL_INT			(1 << 2)
+#define ALMOST_FULL_INT			(1 << 1)
+#define WRITE_FAIL_INT			(1 << 0)
+
+#define REG_SW_LUE_INDEX_0__2		0x0316
+
+#define ENTRY_INDEX_M			0x0FFF
+
+#define REG_SW_LUE_INDEX_1__2		0x0318
+
+#define FAIL_INDEX_M			0x03FF
+
+#define REG_SW_LUE_INDEX_2__2		0x031A
+
+#define REG_SW_LUE_UNK_UCAST_CTRL__4	0x0320
+
+#define SW_UNK_UCAST_ENABLE		(1 << 31)
+
+#define REG_SW_LUE_UNK_MCAST_CTRL__4	0x0324
+
+#define SW_UNK_MCAST_ENABLE		(1 << 31)
+
+#define REG_SW_LUE_UNK_VID_CTRL__4	0x0328
+
+#define SW_UNK_VID_ENABLE		(1 << 31)
+
+#define REG_SW_MAC_CTRL_0		0x0330
+
+#define SW_NEW_BACKOFF			(1 << 7)
+#define SW_CHECK_LENGTH			(1 << 3)
+#define SW_PAUSE_UNH_MODE		(1 << 1)
+#define SW_AGGR_BACKOFF			(1 << 0)
+
+#define REG_SW_MAC_CTRL_1		0x0331
+
+#define MULTICAST_STORM_DISABLE		(1 << 6)
+#define SW_BACK_PRESSURE		(1 << 5)
+#define FAIR_FLOW_CTRL			(1 << 4)
+#define NO_EXC_COLLISION_DROP		(1 << 3)
+#define SW_JUMBO_PACKET			(1 << 2)
+#define SW_LEGAL_PACKET_DISABLE		(1 << 1)
+#define SW_PASS_SHORT_FRAME		(1 << 0)
+
+#define REG_SW_MAC_CTRL_2		0x0332
+
+#define SW_REPLACE_VID			(1 << 3)
+#define BROADCAST_STORM_RATE_HI		0x07
+
+#define REG_SW_MAC_CTRL_3		0x0333
+
+#define BROADCAST_STORM_RATE_LO		0xFF
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SW_MAC_CTRL_4		0x0334
+
+#define SW_PASS_PAUSE			(1 << 3)
+
+#define REG_SW_MAC_CTRL_5		0x0335
+
+#define SW_OUT_RATE_LIMIT_QUEUE_BASED	(1 << 3)
+
+#define REG_SW_MAC_CTRL_6		0x0336
+
+#define SW_MIB_COUNTER_FLUSH		(1 << 7)
+#define SW_MIB_COUNTER_FREEZE		(1 << 6)
+
+#define REG_SW_MAC_802_1P_MAP_0		0x0338
+#define REG_SW_MAC_802_1P_MAP_1		0x0339
+#define REG_SW_MAC_802_1P_MAP_2		0x033A
+#define REG_SW_MAC_802_1P_MAP_3		0x033B
+
+#define SW_802_1P_MAP_M			KS_PRIO_M
+#define SW_802_1P_MAP_S			KS_PRIO_S
+
+#define REG_SW_MAC_ISP_CTRL		0x033C
+
+#define REG_SW_MAC_TOS_CTRL		0x033E
+
+#define SW_TOS_DSCP_REMARK		(1 << 1)
+#define SW_TOS_DSCP_REMAP		(1 << 0)
+
+#define REG_SW_MAC_TOS_PRIO_0		0x0340
+#define REG_SW_MAC_TOS_PRIO_1		0x0341
+#define REG_SW_MAC_TOS_PRIO_2		0x0342
+#define REG_SW_MAC_TOS_PRIO_3		0x0343
+#define REG_SW_MAC_TOS_PRIO_4		0x0344
+#define REG_SW_MAC_TOS_PRIO_5		0x0345
+#define REG_SW_MAC_TOS_PRIO_6		0x0346
+#define REG_SW_MAC_TOS_PRIO_7		0x0347
+#define REG_SW_MAC_TOS_PRIO_8		0x0348
+#define REG_SW_MAC_TOS_PRIO_9		0x0349
+#define REG_SW_MAC_TOS_PRIO_10		0x034A
+#define REG_SW_MAC_TOS_PRIO_11		0x034B
+#define REG_SW_MAC_TOS_PRIO_12		0x034C
+#define REG_SW_MAC_TOS_PRIO_13		0x034D
+#define REG_SW_MAC_TOS_PRIO_14		0x034E
+#define REG_SW_MAC_TOS_PRIO_15		0x034F
+#define REG_SW_MAC_TOS_PRIO_16		0x0350
+#define REG_SW_MAC_TOS_PRIO_17		0x0351
+#define REG_SW_MAC_TOS_PRIO_18		0x0352
+#define REG_SW_MAC_TOS_PRIO_19		0x0353
+#define REG_SW_MAC_TOS_PRIO_20		0x0354
+#define REG_SW_MAC_TOS_PRIO_21		0x0355
+#define REG_SW_MAC_TOS_PRIO_22		0x0356
+#define REG_SW_MAC_TOS_PRIO_23		0x0357
+#define REG_SW_MAC_TOS_PRIO_24		0x0358
+#define REG_SW_MAC_TOS_PRIO_25		0x0359
+#define REG_SW_MAC_TOS_PRIO_26		0x035A
+#define REG_SW_MAC_TOS_PRIO_27		0x035B
+#define REG_SW_MAC_TOS_PRIO_28		0x035C
+#define REG_SW_MAC_TOS_PRIO_29		0x035D
+#define REG_SW_MAC_TOS_PRIO_30		0x035E
+#define REG_SW_MAC_TOS_PRIO_31		0x035F
+
+#define REG_SW_MRI_CTRL_0		0x0370
+
+#define SW_IGMP_SNOOP			(1 << 6)
+#define SW_IPV6_MLD_OPTION		(1 << 3)
+#define SW_IPV6_MLD_SNOOP		(1 << 2)
+#define SW_MIRROR_RX_TX			(1 << 0)
+
+#define REG_SW_CLASS_D_IP_CTRL__4	0x0374
+
+#define SW_CLASS_D_IP_ENABLE		(1 << 31)
+
+#define REG_SW_MRI_CTRL_8		0x0378
+
+#define SW_NO_COLOR_S			6
+#define SW_RED_COLOR_S			4
+#define SW_YELLOW_COLOR_S		2
+#define SW_GREEN_COLOR_S		0
+#define SW_COLOR_M			0x3
+
+#define REG_SW_QM_CTRL__4		0x0390
+
+#define PRIO_SCHEME_SELECT_M		KS_PRIO_M
+#define PRIO_SCHEME_SELECT_S		6
+#define PRIO_MAP_3_HI			0
+#define PRIO_MAP_2_HI			2
+#define PRIO_MAP_0_LO			3
+#define UNICAST_VLAN_BOUNDARY		(1 << 1)
+
+#define REG_SW_EEE_QM_CTRL__2		0x03C0
+
+#define REG_SW_EEE_TXQ_WAIT_TIME__2	0x03C2
+
+/* 4 - */
+#define REG_SW_VLAN_ENTRY__4		0x0400
+
+#define VLAN_VALID			(1 << 31)
+#define VLAN_FORWARD_OPTION		(1 << 27)
+#define VLAN_PRIO_M			KS_PRIO_M
+#define VLAN_PRIO_S			24
+#define VLAN_MSTP_M			0x7
+#define VLAN_MSTP_S			12
+#define VLAN_FID_M			0x7F
+
+#define REG_SW_VLAN_ENTRY_UNTAG__4	0x0404
+#define REG_SW_VLAN_ENTRY_PORTS__4	0x0408
+
+#define REG_SW_VLAN_ENTRY_INDEX__2	0x040C
+
+#define VLAN_INDEX_M			0x0FFF
+
+#define REG_SW_VLAN_CTRL		0x040E
+
+#define VLAN_START			(1 << 7)
+#define VLAN_ACTION			0x3
+#define VLAN_WRITE			1
+#define VLAN_READ			2
+#define VLAN_CLEAR			3
+
+#define REG_SW_ALU_INDEX_0		0x0410
+
+#define ALU_FID_INDEX_S			16
+#define ALU_MAC_ADDR_HI			0xFFFF
+
+#define REG_SW_ALU_INDEX_1		0x0414
+
+#define ALU_DIRECT_INDEX_M		((1 << 12) - 1)
+
+#define REG_SW_ALU_CTRL__4		0x0418
+
+#define ALU_VALID_CNT_M			((1 << 14) - 1)
+#define ALU_VALID_CNT_S			16
+#define ALU_START			(1 << 7)
+#define ALU_VALID			(1 << 6)
+#define ALU_VALID_OR_STOP		(1 << 5)
+#define ALU_DIRECT			(1 << 2)
+#define ALU_ACTION			0x3
+#define ALU_WRITE			1
+#define ALU_READ			2
+#define ALU_SEARCH			3
+
+#define REG_SW_ALU_STAT_CTRL__4		0x041C
+
+#define ALU_STAT_INDEX_M		((1 << 4) - 1)
+#define ALU_STAT_INDEX_S		16
+#define ALU_RESV_MCAST_INDEX_M		((1 << 6) - 1)
+#define ALU_STAT_START			(1 << 7)
+#define ALU_RESV_MCAST_ADDR		(1 << 1)
+#define ALU_STAT_READ			(1 << 0)
+
+#define REG_SW_ALU_VAL_A		0x0420
+
+#define ALU_V_STATIC_VALID		(1 << 31)
+#define ALU_V_SRC_FILTER		(1 << 30)
+#define ALU_V_DST_FILTER		(1 << 29)
+#define ALU_V_PRIO_AGE_CNT_M		((1 << 3) - 1)
+#define ALU_V_PRIO_AGE_CNT_S		26
+#define ALU_V_MSTP_M			0x7
+
+#define REG_SW_ALU_VAL_B		0x0424
+
+#define ALU_V_OVERRIDE			(1 << 31)
+#define ALU_V_USE_FID			(1 << 30)
+#define ALU_V_PORT_MAP			((1 << 24) - 1)
+
+#define REG_SW_ALU_VAL_C		0x0428
+
+#define ALU_V_FID_M			((1 << 16) - 1)
+#define ALU_V_FID_S			16
+#define ALU_V_MAC_ADDR_HI		0xFFFF
+
+#define REG_SW_ALU_VAL_D		0x042C
+
+#define REG_HSR_ALU_INDEX_0		0x0440
+
+#define REG_HSR_ALU_INDEX_1		0x0444
+
+#define HSR_DST_MAC_INDEX_LO_S		16
+#define HSR_SRC_MAC_INDEX_HI		0xFFFF
+
+#define REG_HSR_ALU_INDEX_2		0x0448
+
+#define HSR_INDEX_MAX			(1 << 9)
+#define HSR_DIRECT_INDEX_M		(HSR_INDEX_MAX - 1)
+
+#define REG_HSR_ALU_INDEX_3		0x044C
+
+#define HSR_PATH_INDEX_M		((1 << 4) - 1)
+
+#define REG_HSR_ALU_CTRL__4		0x0450
+
+#define HSR_VALID_CNT_M			((1 << 14) - 1)
+#define HSR_VALID_CNT_S			16
+#define HSR_START			(1 << 7)
+#define HSR_VALID			(1 << 6)
+#define HSR_SEARCH_END			(1 << 5)
+#define HSR_DIRECT			(1 << 2)
+#define HSR_ACTION			0x3
+#define HSR_WRITE			1
+#define HSR_READ			2
+#define HSR_SEARCH			3
+
+#define REG_HSR_ALU_VAL_A		0x0454
+
+#define HSR_V_STATIC_VALID		(1 << 31)
+#define HSR_V_AGE_CNT_M			((1 << 3) - 1)
+#define HSR_V_AGE_CNT_S			26
+#define HSR_V_PATH_ID_M			((1 << 4) - 1)
+
+#define REG_HSR_ALU_VAL_B		0x0458
+
+#define REG_HSR_ALU_VAL_C		0x045C
+
+#define HSR_V_DST_MAC_ADDR_LO_S		16
+#define HSR_V_SRC_MAC_ADDR_HI		0xFFFF
+
+#define REG_HSR_ALU_VAL_D		0x0460
+
+#define REG_HSR_ALU_VAL_E		0x0464
+
+#define HSR_V_START_SEQ_1_S		16
+#define HSR_V_START_SEQ_2_S		0
+
+#define REG_HSR_ALU_VAL_F		0x0468
+
+#define HSR_V_EXP_SEQ_1_S		16
+#define HSR_V_EXP_SEQ_2_S		0
+
+#define REG_HSR_ALU_VAL_G		0x046C
+
+#define HSR_V_SEQ_CNT_1_S		16
+#define HSR_V_SEQ_CNT_2_S		0
+
+#define HSR_V_SEQ_M			((1 << 16) - 1)
+
+
+/* 5 - PTP Clock */
+#define REG_PTP_CLK_CTRL		0x0500
+
+#define PTP_STEP_ADJ			(1 << 6)
+#define PTP_STEP_DIR			(1 << 5)
+#define PTP_READ_TIME			(1 << 4)
+#define PTP_LOAD_TIME			(1 << 3)
+#define PTP_CLK_ADJ_ENABLE		(1 << 2)
+#define PTP_CLK_ENABLE			(1 << 1)
+#define PTP_CLK_RESET			(1 << 0)
+
+#define REG_PTP_RTC_SUB_NANOSEC__2	0x0502
+
+#define PTP_RTC_SUB_NANOSEC_M		0x0007
+
+#define REG_PTP_RTC_NANOSEC		0x0504
+#define REG_PTP_RTC_NANOSEC_H		0x0504
+#define REG_PTP_RTC_NANOSEC_L		0x0506
+
+#define REG_PTP_RTC_SEC			0x0508
+#define REG_PTP_RTC_SEC_H		0x0508
+#define REG_PTP_RTC_SEC_L		0x050A
+
+#define REG_PTP_SUBNANOSEC_RATE		0x050C
+#define REG_PTP_SUBNANOSEC_RATE_H	0x050C
+
+#define PTP_RATE_DIR			(1 << 31)
+#define PTP_TMP_RATE_ENABLE		(1 << 30)
+
+#define REG_PTP_SUBNANOSEC_RATE_L	0x050E
+
+#define REG_PTP_RATE_DURATION		0x0510
+#define REG_PTP_RATE_DURATION_H		0x0510
+#define REG_PTP_RATE_DURATION_L		0x0512
+
+#define REG_PTP_MSG_CONF1		0x0514
+
+#define PTP_802_1AS			(1 << 7)
+#define PTP_ENABLE			(1 << 6)
+#define PTP_ETH_ENABLE			(1 << 5)
+#define PTP_IPV4_UDP_ENABLE		(1 << 4)
+#define PTP_IPV6_UDP_ENABLE		(1 << 3)
+#define PTP_TC_P2P			(1 << 2)
+#define PTP_MASTER			(1 << 1)
+#define PTP_1STEP			(1 << 0)
+
+#define REG_PTP_MSG_CONF2		0x0516
+
+#define PTP_UNICAST_ENABLE		(1 << 12)
+#define PTP_ALTERNATE_MASTER		(1 << 11)
+#define PTP_ALL_HIGH_PRIO		(1 << 10)
+#define PTP_SYNC_CHECK			(1 << 9)
+#define PTP_DELAY_CHECK			(1 << 8)
+#define PTP_PDELAY_CHECK		(1 << 7)
+#define PTP_DROP_SYNC_DELAY_REQ		(1 << 5)
+#define PTP_DOMAIN_CHECK		(1 << 4)
+#define PTP_UDP_CHECKSUM		(1 << 2)
+
+#define REG_PTP_DOMAIN_VERSION		0x0518
+#define PTP_VERSION_M			0xFF00
+#define PTP_DOMAIN_M			0x00FF
+
+#define REG_PTP_UNIT_INDEX__4		0x0520
+
+#define PTP_UNIT_M			0xF
+
+/* 2013-09-10 */
+#define PTP_GPIO_INDEX_S		16
+#define PTP_TSI_INDEX_S			8
+#define PTP_TOU_INDEX_S			0
+
+#define REG_PTP_TRIG_STATUS__4		0x0524
+
+#define TRIG_ERROR_S			16
+#define TRIG_DONE_S			0
+
+#define REG_PTP_INT_STATUS__4		0x0528
+
+#define TRIG_INT_S			16
+#define TS_INT_S			0
+
+#define TRIG_UNIT_M			0x7
+#define TS_UNIT_M			0x3
+
+#define REG_PTP_CTRL_STAT__4		0x052C
+
+#define GPIO_IN				(1 << 7)
+#define GPIO_OUT			(1 << 6)
+#define TS_INT_ENABLE			(1 << 5)
+#define TRIG_ACTIVE			(1 << 4)
+#define TRIG_ENABLE			(1 << 3)
+#define TRIG_RESET			(1 << 2)
+#define TS_ENABLE			(1 << 1)
+#define TS_RESET			(1 << 0)
+
+#define GPIO_CTRL_M			\
+	(GPIO_IN | GPIO_OUT)
+
+#define TRIG_CTRL_M			\
+	(TRIG_ACTIVE | TRIG_ENABLE | TRIG_RESET)
+
+#define TS_CTRL_M			\
+	(TS_INT_ENABLE | TS_ENABLE | TS_RESET)
+
+#define REG_TRIG_TARGET_NANOSEC		0x0530
+#define REG_TRIG_TARGET_SEC		0x0534
+
+#define REG_TRIG_CTRL__4		0x0538
+
+#define TRIG_CASCADE_ENABLE		(1 << 31)
+#define TRIG_CASCADE_TAIL		(1 << 30)
+#define TRIG_CASCADE_UPS_M		0xF
+#define TRIG_CASCADE_UPS_S		26
+#define TRIG_NOW			(1 << 25)
+#define TRIG_NOTIFY			(1 << 24)
+#define TRIG_EDGE			(1 << 23)
+#define TRIG_PATTERN_S			20
+#define TRIG_PATTERN_M			0x7
+#define TRIG_NEG_EDGE			0
+#define TRIG_POS_EDGE			1
+#define TRIG_NEG_PULSE			2
+#define TRIG_POS_PULSE			3
+#define TRIG_NEG_PERIOD			4
+#define TRIG_POS_PERIOD			5
+#define TRIG_REG_OUTPUT			6
+#define TRIG_GPO_S			16
+#define TRIG_GPO_M			0xF
+#define TRIG_CASCADE_ITERATE_CNT_M	0xFFFF
+
+#define REG_TRIG_CYCLE_WIDTH		0x053C
+
+#define REG_TRIG_CYCLE_CNT		0x0540
+
+#define TRIG_CYCLE_CNT_M		0xFFFF
+#define TRIG_CYCLE_CNT_S		16
+#define TRIG_BIT_PATTERN_M		0xFFFF
+
+#define REG_TRIG_ITERATE_TIME		0x0544
+
+#define REG_TRIG_PULSE_WIDTH__4		0x0548
+
+#define TRIG_PULSE_WIDTH_M		0x00FFFFFF
+
+#define REG_TS_CTRL_STAT__4		0x0550
+
+#define TS_EVENT_DETECT_M		0xF
+#define TS_EVENT_DETECT_S		17
+#define TS_EVENT_OVERFLOW		(1 << 16)
+#define TS_GPI_M			0xF
+#define TS_GPI_S			8
+#define TS_DETECT_RISE			(1 << 7)
+#define TS_DETECT_FALL			(1 << 6)
+#define TS_DETECT_S			6
+#define TS_CASCADE_TAIL			(1 << 5)
+#define TS_CASCADE_UPS_M		0xF
+#define TS_CASCADE_UPS_S		1
+#define TS_CASCADE_ENABLE		(1 << 0)
+
+#define DETECT_RISE			(TS_DETECT_RISE >> TS_DETECT_S)
+#define DETECT_FALL			(TS_DETECT_FALL >> TS_DETECT_S)
+
+#define REG_TS_EVENT_0_NANOSEC		0x0554
+#define REG_TS_EVENT_0_SEC		0x0558
+#define REG_TS_EVENT_0_SUB_NANOSEC	0x055C
+
+#define REG_TS_EVENT_1_NANOSEC		0x0560
+#define REG_TS_EVENT_1_SEC		0x0564
+#define REG_TS_EVENT_1_SUB_NANOSEC	0x0568
+
+#define REG_TS_EVENT_2_NANOSEC		0x056C
+#define REG_TS_EVENT_2_SEC		0x0570
+#define REG_TS_EVENT_2_SUB_NANOSEC	0x0574
+
+#define REG_TS_EVENT_3_NANOSEC		0x0578
+#define REG_TS_EVENT_3_SEC		0x057C
+#define REG_TS_EVENT_3_SUB_NANOSEC	0x0580
+
+#define REG_TS_EVENT_4_NANOSEC		0x0584
+#define REG_TS_EVENT_4_SEC		0x0588
+#define REG_TS_EVENT_4_SUB_NANOSEC	0x058C
+
+#define REG_TS_EVENT_5_NANOSEC		0x0590
+#define REG_TS_EVENT_5_SEC		0x0594
+#define REG_TS_EVENT_5_SUB_NANOSEC	0x0598
+
+#define REG_TS_EVENT_6_NANOSEC		0x059C
+#define REG_TS_EVENT_6_SEC		0x05A0
+#define REG_TS_EVENT_6_SUB_NANOSEC	0x05A4
+
+#define REG_TS_EVENT_7_NANOSEC		0x05A8
+#define REG_TS_EVENT_7_SEC		0x05AC
+#define REG_TS_EVENT_7_SUB_NANOSEC	0x05B0
+
+#define TS_EVENT_EDGE_M			0x1
+#define TS_EVENT_EDGE_S			30
+#define TS_EVENT_NANOSEC_M		((1 << 30) - 1)
+
+#define TS_EVENT_SUB_NANOSEC_M		0x7
+
+#define TS_EVENT_SAMPLE			\
+	(REG_TS_EVENT_1_NANOSEC - REG_TS_EVENT_0_NANOSEC)
+
+
+#define PORT_CTRL_ADDR(port, addr)	((addr) | (((port) + 1) << 12))
+
+#define REG_GLOBAL_RR_INDEX__1		0x0600
+
+/* DLR */
+#define REG_DLR_SRC_PORT__4		0x0604
+
+#define DLR_SRC_PORT_UNICAST		(1 << 31)
+#define DLR_SRC_PORT_M			0x3
+#define DLR_SRC_PORT_BOTH		0
+#define DLR_SRC_PORT_EACH		1
+
+#define REG_DLR_IP_ADDR__4		0x0608
+
+#define REG_DLR_CTRL__1			0x0610
+
+#define DLR_RESET_SEQ_ID		(1 << 3)
+#define DLR_BACKUP_AUTO_ON		(1 << 2)
+#define DLR_BEACON_TX_ENABLE		(1 << 1)
+#define DLR_ASSIST_ENABLE		(1 << 0)
+
+#define REG_DLR_STATE__1		0x0611
+
+#define DLR_NODE_STATE_M		0x3
+#define DLR_NODE_STATE_S		1
+#define DLR_NODE_STATE_IDLE		0
+#define DLR_NODE_STATE_FAULT		1
+#define DLR_NODE_STATE_NORMAL		2
+#define DLR_RING_STATE_FAULT		0
+#define DLR_RING_STATE_NORMAL		1
+
+#define REG_DLR_PRECEDENCE__1		0x0612
+
+#define REG_DLR_BEACON_INTERVAL__4	0x0614
+
+#define REG_DLR_BEACON_TIMEOUT__4	0x0618
+
+#define REG_DLR_TIMEOUT_WINDOW__4	0x061C
+
+#define DLR_TIMEOUT_WINDOW_M		((1 << 22) - 1)
+
+#define REG_DLR_VLAN_ID__2		0x0620
+
+#define DLR_VLAN_ID_M			((1 << 12) - 1)
+
+#define REG_DLR_DEST_ADDR_0		0x0622
+#define REG_DLR_DEST_ADDR_1		0x0623
+#define REG_DLR_DEST_ADDR_2		0x0624
+#define REG_DLR_DEST_ADDR_3		0x0625
+#define REG_DLR_DEST_ADDR_4		0x0626
+#define REG_DLR_DEST_ADDR_5		0x0627
+
+#define REG_DLR_PORT_MAP__4		0x0628
+
+#define REG_DLR_CLASS__1		0x062C
+
+#define DLR_FRAME_QID_M			0x3
+
+/* HSR */
+#define REG_HSR_PORT_MAP__4		0x0640
+
+#define REG_HSR_ALU_CTRL_0__1		0x0644
+
+#define HSR_DUPLICATE_DISCARD		(1 << 7)
+#define HSR_NODE_UNICAST		(1 << 6)
+#define HSR_AGE_CNT_DEFAULT_M		0x7
+#define HSR_AGE_CNT_DEFAULT_S		3
+#define HSR_LEARN_MCAST_DISABLE		(1 << 2)
+#define HSR_HASH_OPTION_M		0x3
+#define HSR_HASH_DISABLE		0
+#define HSR_HASH_UPPER_BITS		1
+#define HSR_HASH_LOWER_BITS		2
+#define HSR_HASH_XOR_BOTH_BITS		3
+
+#define REG_HSR_ALU_CTRL_1__1		0x0645
+
+#define HSR_LEARN_UCAST_DISABLE		(1 << 7)
+#define HSR_FLUSH_TABLE			(1 << 5)
+#define HSR_PROC_MCAST_SRC		(1 << 3)
+#define HSR_AGING_ENABLE		(1 << 2)
+
+#define REG_HSR_ALU_CTRL_2__2		0x0646
+
+#define REG_HSR_ALU_AGE_PERIOD__4	0x0648
+
+#define REG_HSR_ALU_INT_STATUS__1	0x064C
+#define REG_HSR_ALU_INT_MASK__1		0x064D
+
+#define HSR_WINDOW_OVERFLOW_INT		(1 << 3)
+#define HSR_LEARN_FAIL_INT		(1 << 2)
+#define HSR_ALMOST_FULL_INT		(1 << 1)
+#define HSR_WRITE_FAIL_INT		(1 << 0)
+
+#define REG_HSR_ALU_ENTRY_0__2		0x0650
+
+#define HSR_ENTRY_INDEX_M		((1 << 10) - 1)
+#define HSR_FAIL_INDEX_M		((1 << 8) - 1)
+
+#define REG_HSR_ALU_ENTRY_1__2		0x0652
+
+#define HSR_FAIL_LEARN_INDEX_M		((1 << 8) - 1)
+
+#define REG_HSR_ALU_ENTRY_3__2		0x0654
+
+#define HSR_CPU_ACCESS_ENTRY_INDEX_M	((1 << 8) - 1)
+
+
+/* 0 - Operation */
+#define REG_PORT_DEFAULT_VID		0x0000
+
+#define REG_PORT_CUSTOM_VID		0x0002
+#define REG_PORT_AVB_SR_1_VID		0x0004
+#define REG_PORT_AVB_SR_2_VID		0x0006
+
+#define REG_PORT_AVB_SR_1_TYPE		0x0008
+#define REG_PORT_AVB_SR_2_TYPE		0x000A
+
+#define REG_PORT_PME_STATUS		0x0013
+#define REG_PORT_PME_CTRL		0x0017
+
+#define PME_WOL_MAGICPKT		(1 << 2)
+#define PME_WOL_LINKUP			(1 << 1)
+#define PME_WOL_ENERGY			(1 << 0)
+
+#define REG_PORT_INT_STATUS		0x001B
+#define REG_PORT_INT_MASK		0x001F
+
+#define PORT_SGMII_INT			(1 << 3)
+#define PORT_PTP_INT			(1 << 2)
+#define PORT_PHY_INT			(1 << 1)
+#define PORT_ACL_INT			(1 << 0)
+
+#define PORT_INT_MASK			\
+	(PORT_SGMII_INT | PORT_PTP_INT | PORT_PHY_INT | PORT_ACL_INT)
+
+#define REG_PORT_CTRL_0			0x0020
+
+#define PORT_MAC_LOOPBACK		(1 << 7)
+#define PORT_FORCE_TX_FLOW_CTRL		(1 << 4)
+#define PORT_FORCE_RX_FLOW_CTRL		(1 << 3)
+#define PORT_TAIL_TAG_ENABLE		(1 << 2)
+#define PORT_QUEUE_SPLIT_ENABLE		0x3
+
+#define REG_PORT_CTRL_1			0x0021
+
+#define PORT_SRP_ENABLE			0x3
+
+#define REG_PORT_STATUS_0		0x0030
+
+#define PORT_INTF_SPEED_M		0x3
+#define PORT_INTF_SPEED_S		3
+#define PORT_INTF_FULL_DUPLEX		(1 << 2)
+#define PORT_TX_FLOW_CTRL		(1 << 1)
+#define PORT_RX_FLOW_CTRL		(1 << 0)
+
+#define REG_PORT_STATUS_1		0x0034
+
+/* 1 - PHY */
+#define REG_PORT_PHY_CTRL		0x0100
+
+#define PORT_PHY_RESET			(1 << 15)
+#define PORT_PHY_LOOPBACK		(1 << 14)
+#define PORT_SPEED_100MBIT		(1 << 13)
+#define PORT_AUTO_NEG_ENABLE		(1 << 12)
+#define PORT_POWER_DOWN			(1 << 11)
+#define PORT_ISOLATE			(1 << 10)
+#define PORT_AUTO_NEG_RESTART		(1 << 9)
+#define PORT_FULL_DUPLEX		(1 << 8)
+#define PORT_COLLISION_TEST		(1 << 7)
+#define PORT_SPEED_1000MBIT		(1 << 6)
+#if 0
+#define PHY_HP_MDIX			(1 << 5)
+#define PHY_REMOTE_FAULT_DISABLE	(1 << 2)
+#define PHY_LED_DISABLE			(1 << 0)
+#endif
+
+#define REG_PORT_PHY_STATUS		0x0102
+
+#define PORT_100BT4_CAPABLE		(1 << 15)
+#define PORT_100BTX_FD_CAPABLE		(1 << 14)
+#define PORT_100BTX_CAPABLE		(1 << 13)
+#define PORT_10BT_FD_CAPABLE		(1 << 12)
+#define PORT_10BT_CAPABLE		(1 << 11)
+#define PORT_EXTENDED_STATUS		(1 << 8)
+#define PORT_MII_SUPPRESS_CAPABLE	(1 << 6)
+#define PORT_AUTO_NEG_ACKNOWLEDGE	(1 << 5)
+#define PORT_REMOTE_FAULT		(1 << 4)
+#define PORT_AUTO_NEG_CAPABLE		(1 << 3)
+#define PORT_LINK_STATUS		(1 << 2)
+#define PORT_JABBER_DETECT		(1 << 1)
+#define PORT_EXTENDED_CAPABILITY	(1 << 0)
+
+#define REG_PORT_PHY_ID_HI		0x0104
+#define REG_PORT_PHY_ID_LO		0x0106
+
+#define KSZ9897_ID_HI			0x0022
+#define KSZ9897_ID_LO			0x1622
+
+#define REG_PORT_PHY_AUTO_NEGOTIATION	0x0108
+
+#define PORT_AUTO_NEG_NEXT_PAGE		(1 << 15)
+#define PORT_AUTO_NEG_REMOTE_FAULT	(1 << 13)
+#define PORT_AUTO_NEG_ASYM_PAUSE	(1 << 11)
+#define PORT_AUTO_NEG_SYM_PAUSE		(1 << 10)
+#define PORT_AUTO_NEG_100BT4		(1 << 9)
+#define PORT_AUTO_NEG_100BTX_FD		(1 << 8)
+#define PORT_AUTO_NEG_100BTX		(1 << 7)
+#define PORT_AUTO_NEG_10BT_FD		(1 << 6)
+#define PORT_AUTO_NEG_10BT		(1 << 5)
+#define PORT_AUTO_NEG_SELECTOR		0x001F
+#define PORT_AUTO_NEG_802_3		0x0001
+
+#define PORT_AUTO_NEG_PAUSE		\
+	(PORT_AUTO_NEG_ASYM_PAUSE | PORT_AUTO_NEG_SYM_PAUSE)
+
+#define REG_PORT_PHY_REMOTE_CAPABILITY	0x010A
+
+#define PORT_REMOTE_NEXT_PAGE		(1 << 15)
+#define PORT_REMOTE_ACKNOWLEDGE		(1 << 14)
+#define PORT_REMOTE_REMOTE_FAULT	(1 << 13)
+#define PORT_REMOTE_ASYM_PAUSE		(1 << 11)
+#define PORT_REMOTE_SYM_PAUSE		(1 << 10)
+#define PORT_REMOTE_100BTX_FD		(1 << 8)
+#define PORT_REMOTE_100BTX		(1 << 7)
+#define PORT_REMOTE_10BT_FD		(1 << 6)
+#define PORT_REMOTE_10BT		(1 << 5)
+
+#define REG_PORT_PHY_1000_CTRL		0x0112
+
+#define PORT_AUTO_NEG_MANUAL		(1 << 12)
+#define PORT_AUTO_NEG_MASTER		(1 << 11)
+#define PORT_AUTO_NEG_MASTER_PREFERRED	(1 << 10)
+#define PORT_AUTO_NEG_1000BT_FD		(1 << 9)
+#define PORT_AUTO_NEG_1000BT		(1 << 8)
+
+#define REG_PORT_PHY_1000_STATUS	0x0114
+
+#define PORT_MASTER_FAULT		(1 << 15)
+#define PORT_LOCAL_MASTER		(1 << 14)
+#define PORT_LOCAL_RX_OK		(1 << 13)
+#define PORT_REMOTE_RX_OK		(1 << 12)
+#define PORT_REMOTE_1000BT_FD		(1 << 11)
+#define PORT_REMOTE_1000BT		(1 << 10)
+#define PORT_REMOTE_IDLE_CNT_M		0x0F
+
+#define PORT_PHY_1000_STATIC_STATUS	\
+	(PORT_LOCAL_RX_OK |		\
+	PORT_REMOTE_RX_OK |		\
+	PORT_REMOTE_1000BT_FD |		\
+	PORT_REMOTE_1000BT)
+
+#define REG_PORT_PHY_MMD_SETUP		0x011A
+
+#define PORT_MMD_OP_MODE_M		0x3
+#define PORT_MMD_OP_MODE_S		14
+#define PORT_MMD_OP_INDEX		0
+#define PORT_MMD_OP_DATA_NO_INCR	1
+#define PORT_MMD_OP_DATA_INCR_RW	2
+#define PORT_MMD_OP_DATA_INCR_W		3
+#define PORT_MMD_DEVICE_ID_M		0x1F
+
+#define MMD_SETUP(mode, dev)		\
+	(((u16) mode << PORT_MMD_OP_MODE_S) | dev)
+
+#define REG_PORT_PHY_MMD_INDEX_DATA	0x011C
+
+#define MMD_DEVICE_ID_DSP		1
+
+#define MMD_DSP_SQI_CHAN_A		0xAC
+#define MMD_DSP_SQI_CHAN_B		0xAD
+#define MMD_DSP_SQI_CHAN_C		0xAE
+#define MMD_DSP_SQI_CHAN_D		0xAF
+
+#define DSP_SQI_ERR_DETECTED		(1 << 15)
+#define DSP_SQI_AVG_ERR			0x7FFF
+
+#define MMD_DEVICE_ID_COMMON		2
+
+#define MMD_DEVICE_ID_EEE_ADV		7
+
+#define MMD_EEE_ADV			0x3C
+#define EEE_ADV_100MBIT			(1 << 1)
+#define EEE_ADV_1GBIT			(1 << 2)
+
+#define MMD_EEE_LP_ADV			0x3D
+#define MMD_EEE_MSG_CODE		0x3F
+
+#define MMD_DEVICE_ID_AFED		0x1C
+
+#define REG_PORT_PHY_EXTENDED_STATUS	0x011E
+
+#define PORT_100BTX_FD_ABLE		(1 << 15)
+#define PORT_100BTX_ABLE		(1 << 14)
+#define PORT_10BT_FD_ABLE		(1 << 13)
+#define PORT_10BT_ABLE			(1 << 12)
+
+#define REG_PORT_SGMII_ADDR__4		0x0200
+#define PORT_SGMII_AUTO_INCR		(1 << 23)
+#define PORT_SGMII_DEVICE_ID_M		0x1F
+#define PORT_SGMII_DEVICE_ID_S		16
+#define PORT_SGMII_ADDR_M		((1 << 21) - 1)
+
+#define REG_PORT_SGMII_DATA__4		0x0204
+#define PORT_SGMII_DATA_M		((1 << 16) - 1)
+
+#define MMD_DEVICE_ID_PMA		0x01
+#define MMD_DEVICE_ID_PCS		0x03
+#define MMD_DEVICE_ID_PHY_XS		0x04
+#define MMD_DEVICE_ID_DTE_XS		0x05
+#define MMD_DEVICE_ID_AN		0x07
+#define MMD_DEVICE_ID_VENDOR_CTRL	0x1E
+#define MMD_DEVICE_ID_VENDOR_MII	0x1F
+
+#define SR_MII				MMD_DEVICE_ID_VENDOR_MII
+
+#define MMD_SR_MII_CTRL			0x0000
+
+#define SR_MII_RESET			(1 << 15)
+#define SR_MII_LOOPBACK			(1 << 14)
+#define SR_MII_SPEED_100MBIT		(1 << 13)
+#define SR_MII_AUTO_NEG_ENABLE		(1 << 12)
+#define SR_MII_POWER_DOWN		(1 << 11)
+#define SR_MII_AUTO_NEG_RESTART		(1 << 9)
+#define SR_MII_FULL_DUPLEX		(1 << 8)
+#define SR_MII_SPEED_1000MBIT		(1 << 6)
+
+#define MMD_SR_MII_STATUS		0x0001
+#define MMD_SR_MII_ID_1			0x0002
+#define MMD_SR_MII_ID_2			0x0003
+#define MMD_SR_MII_AUTO_NEGOTIATION	0x0004
+
+#define SR_MII_AUTO_NEG_NEXT_PAGE	(1 << 15)
+#define SR_MII_AUTO_NEG_REMOTE_FAULT_M	0x3
+#define SR_MII_AUTO_NEG_REMOTE_FAULT_S	12
+#define SR_MII_AUTO_NEG_NO_ERROR	0
+#define SR_MII_AUTO_NEG_OFFLINE		1
+#define SR_MII_AUTO_NEG_LINK_FAILURE	2
+#define SR_MII_AUTO_NEG_ERROR		3
+#define SR_MII_AUTO_NEG_PAUSE_M		0x3
+#define SR_MII_AUTO_NEG_PAUSE_S		7
+#define SR_MII_AUTO_NEG_NO_PAUSE	0
+#define SR_MII_AUTO_NEG_ASYM_PAUSE_TX	1
+#define SR_MII_AUTO_NEG_SYM_PAUSE	2
+#define SR_MII_AUTO_NEG_ASYM_PAUSE_RX	3
+#define SR_MII_AUTO_NEG_HALF_DUPLEX	(1 << 6)
+#define SR_MII_AUTO_NEG_FULL_DUPLEX	(1 << 5)
+
+#define MMD_SR_MII_REMOTE_CAPABILITY	0x0005
+#define MMD_SR_MII_AUTO_NEG_EXP		0x0006
+#define MMD_SR_MII_AUTO_NEG_EXT		0x000F
+
+#define MMD_SR_MII_DIGITAL_CTRL_1	0x8000
+
+
+#define MMD_SR_MII_AUTO_NEG_CTRL	0x8001
+
+#define SR_MII_8_BIT			(1 << 8)
+#define SR_MII_SGMII_LINK_UP		(1 << 4)
+#define SR_MII_TX_CFG_PHY_MASTER	(1 << 3)
+#define SR_MII_PCS_MODE_M		0x3
+#define SR_MII_PCS_MODE_S		1
+#define SR_MII_PCS_SGMII		2
+#define SR_MII_AUTO_NEG_COMPLETE_INTR	(1 << 0)
+
+#define MMD_SR_MII_AUTO_NEG_STATUS	0x8002
+
+#define SR_MII_STAT_LINK_UP		(1 << 4)
+#define SR_MII_STAT_M			0x3
+#define SR_MII_STAT_S			2
+#define SR_MII_STAT_10_MBPS		0
+#define SR_MII_STAT_100_MBPS		1
+#define SR_MII_STAT_1000_MBPS		2
+#define SR_MII_STAT_FULL_DUPLEX		(1 << 1)
+
+#define MMD_SR_MII_PHY_CTRL		0x80A0
+
+#define SR_MII_PHY_LANE_SEL_M		0xF
+#define SR_MII_PHY_LANE_SEL_S		8
+#define SR_MII_PHY_WRITE		(1 << 1)
+#define SR_MII_PHY_START_BUSY		(1 << 0)
+
+#define MMD_SR_MII_PHY_ADDR		0x80A1
+
+#define SR_MII_PHY_ADDR_M		((1 << 16) - 1)
+
+#define MMD_SR_MII_PHY_DATA		0x80A2
+
+#define SR_MII_PHY_DATA_M		((1 << 16) - 1)
+
+#define SR_MII_PHY_JTAG_CHIP_ID_HI	0x000C
+#define SR_MII_PHY_JTAG_CHIP_ID_LO	0x000D
+
+
+/*
+ * 2016-05-31
+ * PHY registers greater than 15 needs to be written in 32-bit.
+ */
+#define REG_PORT_PHY_REMOTE_LB_LED	0x0122
+
+#define PORT_REMOTE_LOOPBACK		(1 << 8)
+#define PORT_LED_SELECT			(3 << 6)
+#define PORT_LED_CTRL			(3 << 4)
+#define PORT_LED_CTRL_TEST		(1 << 3)
+#define PORT_10BT_PREAMBLE		(1 << 2)
+#define PORT_LINK_MD_10BT_ENABLE	(1 << 1)
+#define PORT_LINK_MD_PASS		(1 << 0)
+
+#define REG_PORT_PHY_LINK_MD		0x0124
+
+#define PORT_START_CABLE_DIAG		(1 << 15)
+#define PORT_TX_DISABLE			(1 << 14)
+#define PORT_CABLE_DIAG_PAIR_M		0x3
+#define PORT_CABLE_DIAG_PAIR_S		12
+#define PORT_CABLE_DIAG_SELECT_M	0x3
+#define PORT_CABLE_DIAG_SELECT_S	10
+#define PORT_CABLE_DIAG_RESULT_M	0x3
+#define PORT_CABLE_DIAG_RESULT_S	8
+#define PORT_CABLE_STAT_NORMAL		0
+#define PORT_CABLE_STAT_OPEN		1
+#define PORT_CABLE_STAT_SHORT		2
+#define PORT_CABLE_STAT_FAILED		3
+#if 0
+#define PORT_CABLE_10M_SHORT		(1 << 12)
+#endif
+#define PORT_CABLE_FAULT_COUNTER	0x00FF
+
+#define REG_PORT_PHY_PMA_STATUS		0x0126
+
+#define PORT_1000_LINK_GOOD		(1 << 1)
+#define PORT_100_LINK_GOOD		(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_STATUS	0x0128
+
+#define PORT_LINK_DETECT		(1 << 14)
+#define PORT_SIGNAL_DETECT		(1 << 13)
+#define PORT_PHY_STAT_MDI		(1 << 12)
+#define PORT_PHY_STAT_MASTER		(1 << 11)
+
+#define REG_PORT_PHY_RXER_COUNTER	0x012A
+
+#define REG_PORT_PHY_INT_ENABLE		0x0136
+#define REG_PORT_PHY_INT_STATUS		0x0137
+
+#define JABBER_INT			(1 << 7)
+#define RX_ERR_INT			(1 << 6)
+#define PAGE_RX_INT			(1 << 5)
+#define PARALLEL_DETECT_FAULT_INT	(1 << 4)
+#define LINK_PARTNER_ACK_INT		(1 << 3)
+#define LINK_DOWN_INT			(1 << 2)
+#define REMOTE_FAULT_INT		(1 << 1)
+#define LINK_UP_INT			(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_1	0x0138
+
+#define PORT_REG_CLK_SPEED_25_MHZ	(1 << 14)
+#define PORT_PHY_FORCE_MDI		(1 << 7)
+#define PORT_PHY_AUTO_MDIX_DISABLE	(1 << 6)
+
+/* Same as PORT_PHY_LOOPBACK */
+#define PORT_PHY_PCS_LOOPBACK		(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_2	0x013A
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_3	0x013C
+
+#define PORT_100BT_FIXED_LATENCY	(1 << 15)
+
+#define REG_PORT_PHY_PHY_CTRL		0x013E
+
+#define PORT_INT_PIN_HIGH		(1 << 14)
+#define PORT_ENABLE_JABBER		(1 << 9)
+#define PORT_STAT_SPEED_1000MBIT	(1 << 6)
+#define PORT_STAT_SPEED_100MBIT		(1 << 5)
+#define PORT_STAT_SPEED_10MBIT		(1 << 4)
+#define PORT_STAT_FULL_DUPLEX		(1 << 3)
+
+/* Same as PORT_PHY_STAT_MASTER */
+#define PORT_STAT_MASTER		(1 << 2)
+#define PORT_RESET			(1 << 1)
+#define PORT_LINK_STATUS_FAIL		(1 << 0)
+
+/* 3 - xMII */
+#define REG_PORT_XMII_CTRL_0		0x0300
+
+#define PORT_SGMII_SEL			(1 << 7)
+#define PORT_MII_FULL_DUPLEX		(1 << 6)
+#define PORT_MII_TX_FLOW_CTRL		(1 << 5)
+#define PORT_MII_100MBIT		(1 << 4)
+#define PORT_MII_RX_FLOW_CTRL		(1 << 3)
+#define PORT_GRXC_ENABLE		(1 << 0)
+
+#define REG_PORT_XMII_CTRL_1		0x0301
+
+#define PORT_RMII_CLK_SEL		(1 << 7)
+/* S1 */
+#define PORT_MII_1000MBIT_S1		(1 << 6)
+/* S2 */
+#define PORT_MII_NOT_1GBIT		(1 << 6)
+#define PORT_MII_SEL_EDGE		(1 << 5)
+#define PORT_RGMII_ID_IG_ENABLE		(1 << 4)
+#define PORT_RGMII_ID_EG_ENABLE		(1 << 3)
+#define PORT_MII_MAC_MODE		(1 << 2)
+#define PORT_MII_SEL_M			0x3
+/* S1 */
+#define PORT_MII_SEL_S1			0x0
+#define PORT_RMII_SEL_S1		0x1
+#define PORT_GMII_SEL_S1		0x2
+#define PORT_RGMII_SEL_S1		0x3
+/* S2 */
+#define PORT_RGMII_SEL			0x0
+#define PORT_RMII_SEL			0x1
+#define PORT_GMII_SEL			0x2
+#define PORT_MII_SEL			0x3
+
+/* 4 - MAC */
+#define REG_PORT_MAC_CTRL_0		0x0400
+
+#define PORT_BROADCAST_STORM		(1 << 1)
+#define PORT_JUMBO_FRAME		(1 << 0)
+
+#define REG_PORT_MAC_CTRL_1		0x0401
+
+#define PORT_BACK_PRESSURE		(1 << 3)
+#define PORT_PASS_ALL			(1 << 0)
+
+#define REG_PORT_MAC_CTRL_2		0x0402
+
+#define PORT_100BT_EEE_DISABLE		(1 << 7)
+#define PORT_1000BT_EEE_DISABLE		(1 << 6)
+
+#define REG_PORT_MAC_IN_RATE_LIMIT	0x0403
+
+#define PORT_IN_PORT_BASED_S		6
+#define PORT_RATE_PACKET_BASED_S	5
+#define PORT_IN_FLOW_CTRL_S		4
+#define PORT_COUNT_IFG_S		1
+#define PORT_COUNT_PREAMBLE_S		0
+#define PORT_IN_PORT_BASED		(1 << 6)
+#define PORT_IN_PACKET_BASED		(1 << 5)
+#define PORT_IN_FLOW_CTRL		(1 << 4)
+#define PORT_IN_LIMIT_MODE_M		0x3
+#define PORT_IN_LIMIT_MODE_S		2
+#define PORT_IN_ALL			0
+#define PORT_IN_UNICAST			1
+#define PORT_IN_MULTICAST		2
+#define PORT_IN_BROADCAST		3
+#define PORT_COUNT_IFG			(1 << 1)
+#define PORT_COUNT_PREAMBLE		(1 << 0)
+
+#define REG_PORT_IN_RATE_0		0x0410
+#define REG_PORT_IN_RATE_1		0x0411
+#define REG_PORT_IN_RATE_2		0x0412
+#define REG_PORT_IN_RATE_3		0x0413
+#define REG_PORT_IN_RATE_4		0x0414
+#define REG_PORT_IN_RATE_5		0x0415
+#define REG_PORT_IN_RATE_6		0x0416
+#define REG_PORT_IN_RATE_7		0x0417
+
+#define REG_PORT_OUT_RATE_0		0x0420
+#define REG_PORT_OUT_RATE_1		0x0421
+#define REG_PORT_OUT_RATE_2		0x0422
+#define REG_PORT_OUT_RATE_3		0x0423
+
+#define PORT_RATE_LIMIT_M		((1 << 7) - 1)
+
+/* 5 - MIB Counters */
+#define REG_PORT_MIB_CTRL_STAT__4	0x0500
+
+#define MIB_COUNTER_OVERFLOW		(1 << 31)
+#define MIB_COUNTER_VALID		(1 << 30)
+#define MIB_COUNTER_READ		(1 << 25)
+#define MIB_COUNTER_FLUSH_FREEZE	(1 << 24)
+#define MIB_COUNTER_INDEX_M		((1 << 8) - 1)
+#define MIB_COUNTER_INDEX_S		16
+#define MIB_COUNTER_DATA_HI_M		0xF
+
+#define REG_PORT_MIB_DATA		0x0504
+
+/* 6 - ACL */
+#define REG_PORT_ACL_0			0x0600
+
+#define ACL_FIRST_RULE_M		0xF
+
+#define REG_PORT_ACL_1			0x0601
+
+#define ACL_MODE_M			0x3
+#define ACL_MODE_S			4
+#define ACL_MODE_DISABLE		0
+#define ACL_MODE_LAYER_2		1
+#define ACL_MODE_LAYER_3		2
+#define ACL_MODE_LAYER_4		3
+#define ACL_ENABLE_M			0x3
+#define ACL_ENABLE_S			2
+#define ACL_ENABLE_2_COUNT		0
+#define ACL_ENABLE_2_TYPE		1
+#define ACL_ENABLE_2_MAC		2
+#define ACL_ENABLE_2_BOTH		3
+#define ACL_ENABLE_3_IP			1
+#define ACL_ENABLE_3_SRC_DST_COMP	2
+#define ACL_ENABLE_4_PROTOCOL		0
+#define ACL_ENABLE_4_TCP_PORT_COMP	1
+#define ACL_ENABLE_4_UDP_PORT_COMP	2
+#define ACL_ENABLE_4_TCP_SEQN_COMP	3
+#define ACL_SRC				(1 << 1)
+#define ACL_EQUAL			(1 << 0)
+
+#define REG_PORT_ACL_2			0x0602
+#define REG_PORT_ACL_3			0x0603
+
+#define ACL_MAX_PORT			0xFFFF
+
+#define REG_PORT_ACL_4			0x0604
+#define REG_PORT_ACL_5			0x0605
+
+#define ACL_MIN_PORT			0xFFFF
+#define ACL_IP_ADDR			0xFFFFFFFF
+#define ACL_TCP_SEQNUM			0xFFFFFFFF
+
+#define REG_PORT_ACL_6			0x0606
+
+#define ACL_RESERVED			0xF8
+#define ACL_PORT_MODE_M			0x3
+#define ACL_PORT_MODE_S			1
+#define ACL_PORT_MODE_DISABLE		0
+#define ACL_PORT_MODE_EITHER		1
+#define ACL_PORT_MODE_IN_RANGE		2
+#define ACL_PORT_MODE_OUT_OF_RANGE	3
+
+#define REG_PORT_ACL_7			0x0607
+
+#define ACL_TCP_FLAG_ENABLE		(1 << 0)
+
+#define REG_PORT_ACL_8			0x0608
+
+#define ACL_TCP_FLAG_M			0xFF
+
+#define REG_PORT_ACL_9			0x0609
+
+#define ACL_TCP_FLAG			0xFF
+#define ACL_ETH_TYPE			0xFFFF
+#define ACL_IP_M			0xFFFFFFFF
+
+#define REG_PORT_ACL_A			0x060A
+
+#define ACL_PRIO_MODE_M			0x3
+#define ACL_PRIO_MODE_S			6
+#define ACL_PRIO_MODE_DISABLE		0
+#define ACL_PRIO_MODE_HIGHER		1
+#define ACL_PRIO_MODE_LOWER		2
+#define ACL_PRIO_MODE_REPLACE		3
+#define ACL_PRIO_M			KS_PRIO_M
+#define ACL_PRIO_S			3
+#define ACL_VLAN_PRIO_REPLACE		(1 << 2)
+#define ACL_VLAN_PRIO_M			KS_PRIO_M
+#define ACL_VLAN_PRIO_HI_M		0x3
+
+#define REG_PORT_ACL_B			0x060B
+
+#define ACL_VLAN_PRIO_LO_M		0x8
+#define ACL_VLAN_PRIO_S			7
+#define ACL_MAP_MODE_M			0x3
+#define ACL_MAP_MODE_S			5
+#define ACL_MAP_MODE_DISABLE		0
+#define ACL_MAP_MODE_OR			1
+#define ACL_MAP_MODE_AND		2
+#define ACL_MAP_MODE_REPLACE		3
+
+#define ACL_CNT_M			((1 << 11) - 1)
+#define ACL_CNT_S			5
+
+#define REG_PORT_ACL_C			0x060C
+
+#define REG_PORT_ACL_D			0x060D
+#define ACL_MSEC_UNIT			(1 << 6)
+#define ACL_INTR_MODE			(1 << 5)
+#define ACL_PORT_MAP			0x7F
+
+#define REG_PORT_ACL_E			0x060E
+#define REG_PORT_ACL_F			0x060F
+
+#define REG_PORT_ACL_BYTE_EN_MSB	0x0610
+#define REG_PORT_ACL_BYTE_EN_LSB	0x0611
+
+#define ACL_ACTION_START		0xA
+#define ACL_ACTION_LEN			4
+#define ACL_INTR_CNT_START		0xD
+#define ACL_RULESET_START		0xE
+#define ACL_RULESET_LEN			2
+#define ACL_TABLE_LEN			16
+
+#define ACL_ACTION_ENABLE		0x003C
+#define ACL_MATCH_ENABLE		0x7FC0
+#define ACL_RULESET_ENABLE		0x8003
+#define ACL_BYTE_ENABLE			0xFFFF
+
+#define REG_PORT_ACL_CTRL_0		0x0612
+
+#define PORT_ACL_WRITE_DONE		(1 << 6)
+#define PORT_ACL_READ_DONE		(1 << 5)
+#define PORT_ACL_WRITE			(1 << 4)
+#define PORT_ACL_INDEX_M		0xF
+
+#define REG_PORT_ACL_CTRL_1		0x0613
+
+/* 8 - Classification and Policing */
+#define REG_PORT_MRI_MIRROR_CTRL	0x0800
+
+#define PORT_MIRROR_RX			(1 << 6)
+#define PORT_MIRROR_TX			(1 << 5)
+#define PORT_MIRROR_SNIFFER		(1 << 1)
+
+#define REG_PORT_MRI_PRIO_CTRL		0x0801
+
+#define PORT_HIGHEST_PRIO		(1 << 7)
+#define PORT_OR_PRIO			(1 << 6)
+#define PORT_MAC_PRIO_ENABLE		(1 << 4)
+#define PORT_VLAN_PRIO_ENABLE		(1 << 3)
+#define PORT_802_1P_PRIO_ENABLE		(1 << 2)
+#define PORT_DIFFSERV_PRIO_ENABLE	(1 << 1)
+#define PORT_ACL_PRIO_ENABLE		(1 << 0)
+
+#define REG_PORT_MRI_MAC_CTRL		0x0802
+
+#define PORT_USER_PRIO_CEILING		(1 << 7)
+#define PORT_DROP_NON_VLAN		(1 << 4)
+#define PORT_DROP_TAG			(1 << 3)
+#define PORT_BASED_PRIO_M		KS_PRIO_M
+#define PORT_BASED_PRIO_S		0
+
+#define REG_PORT_MRI_AUTHEN_CTRL	0x0803
+
+#define PORT_ACL_ENABLE			(1 << 2)
+#define PORT_AUTHEN_MODE		0x3
+#define PORT_AUTHEN_NORMAL		0
+#define PORT_AUTHEN_BLOCK		1
+#define PORT_AUTHEN_PASS		2
+#define PORT_AUTHEN_TRAP		3
+
+#define REG_PORT_MRI_INDEX__4		0x0804
+
+#define MRI_INDEX_P_M			0x7
+#define MRI_INDEX_P_S			16
+#define MRI_INDEX_Q_M			0x3
+#define MRI_INDEX_Q_S			0
+
+#define REG_PORT_MRI_TC_MAP__4		0x0808
+
+#define PORT_TC_MAP_M			0xf
+#define PORT_TC_MAP_S			4
+
+#define REG_PORT_MRI_POLICE_CTRL__4	0x080C
+
+#define POLICE_DROP_ALL			(1 << 10)
+#define POLICE_PACKET_TYPE_M		0x3
+#define POLICE_PACKET_TYPE_S		8
+#define POLICE_PACKET_DROPPED		0
+#define POLICE_PACKET_GREEN		1
+#define POLICE_PACKET_YELLOW		2
+#define POLICE_PACKET_RED		3
+#define PORT_BASED_POLICING		(1 << 7)
+#define NON_DSCP_COLOR_M		0x3
+#define NON_DSCP_COLOR_S		5
+#define COLOR_MARK_ENABLE		(1 << 4)
+#define COLOR_REMAP_ENABLE		(1 << 3)
+#define POLICE_DROP_SRP			(1 << 2)
+#define POLICE_COLOR_NOT_AWARE		(1 << 1)
+#define POLICE_ENABLE			(1 << 0)
+
+#define REG_PORT_POLICE_COLOR_0__4	0x0810
+#define REG_PORT_POLICE_COLOR_1__4	0x0814
+#define REG_PORT_POLICE_COLOR_2__4	0x0818
+#define REG_PORT_POLICE_COLOR_3__4	0x081C
+
+#define POLICE_COLOR_MAP_S		2
+#define POLICE_COLOR_MAP_M		((1 << POLICE_COLOR_MAP_S) - 1)
+
+#define REG_PORT_POLICE_RATE__4		0x0820
+
+#define POLICE_CIR_S			16
+#define POLICE_PIR_S			0
+
+#define REG_PORT_POLICE_BURST_SIZE__4	0x0824
+
+#define POLICE_BURST_SIZE_M		0x3FFF
+#define POLICE_CBS_S			16
+#define POLICE_PBS_S			0
+
+#define REG_PORT_WRED_PM_CTRL_0__4	0x0830
+
+#define WRED_PM_CTRL_M			((1 << 11) - 1)
+
+#define WRED_PM_MAX_THRESHOLD_S		16
+#define WRED_PM_MIN_THRESHOLD_S		0
+
+#define REG_PORT_WRED_PM_CTRL_1__4	0x0834
+
+#define WRED_PM_MULTIPLIER_S		16
+#define WRED_PM_AVG_QUEUE_SIZE_S	0
+
+#define REG_PORT_WRED_QUEUE_CTRL_0__4	0x0840
+#define REG_PORT_WRED_QUEUE_CTRL_1__4	0x0844
+
+#define REG_PORT_WRED_QUEUE_PMON__4	0x0848
+
+#define WRED_RANDOM_DROP_ENABLE		(1 << 31)
+#define WRED_PMON_FLUSH			(1 << 30)
+#define WRED_DROP_GYR_DISABLE		(1 << 29)
+#define WRED_DROP_YR_DISABLE		(1 << 28)
+#define WRED_DROP_R_DISABLE		(1 << 27)
+#define WRED_DROP_ALL			(1 << 26)
+#define WRED_PMON_M			((1 << 24) - 1)
+
+/* 9 - Shaping */
+
+#define REG_PORT_MTI_QUEUE_INDEX__4	0x0900
+
+#define REG_PORT_MTI_QUEUE_CTRL_0__4	0x0904
+
+#define MTI_PVID_REPLACE		(1 << 0)
+
+#define REG_PORT_MTI_QUEUE_CTRL_0	0x0914
+
+#define MTI_SCHEDULE_MODE_M		0x3
+#define MTI_SCHEDULE_MODE_S		6
+#define MTI_SCHEDULE_STRICT_PRIO	0
+#define MTI_SCHEDULE_WRR		2
+#define MTI_SHAPING_M			0x3
+#define MTI_SHAPING_S			4
+#define MTI_SHAPING_OFF			0
+#define MTI_SHAPING_SRP			1
+#define MTI_SHAPING_TIME_AWARE		2
+#if 0
+#define MTI_PREEMPT_ENABLE		(1 << 3)
+#endif
+
+#define REG_PORT_MTI_QUEUE_CTRL_1	0x0915
+
+#define MTI_TX_RATIO_M			((1 << 7) - 1)
+
+#define REG_PORT_MTI_QUEUE_CTRL_2__2	0x0916
+#define REG_PORT_MTI_HI_WATER_MARK	0x0916
+#define REG_PORT_MTI_QUEUE_CTRL_3__2	0x0918
+#define REG_PORT_MTI_LO_WATER_MARK	0x0918
+#define REG_PORT_MTI_QUEUE_CTRL_4__2	0x091A
+#define REG_PORT_MTI_CREDIT_INCREMENT	0x091A
+
+/* A - QM */
+
+#define REG_PORT_QM_CTRL__4		0x0A00
+
+#define PORT_QM_DROP_PRIO_M		0x3
+
+#define REG_PORT_VLAN_MEMBERSHIP__4	0x0A04
+
+#define REG_PORT_QM_QUEUE_INDEX__4	0x0A08
+
+#define PORT_QM_QUEUE_INDEX_S		24
+#define PORT_QM_BURST_SIZE_S		16
+#define PORT_QM_MIN_RESV_SPACE_M	((1 << 11) - 1)
+
+#define REG_PORT_QM_WATER_MARK__4	0x0A0C
+
+#define PORT_QM_HI_WATER_MARK_S		16
+#define PORT_QM_LO_WATER_MARK_S		0
+#define PORT_QM_WATER_MARK_M		((1 << 11) - 1)
+
+#define REG_PORT_QM_TX_CNT_0__4		0x0A10
+
+#define PORT_QM_TX_CNT_USED_S		0
+#define PORT_QM_TX_CNT_M		((1 << 11) - 1)
+
+#define REG_PORT_QM_TX_CNT_1__4		0x0A14
+
+#define PORT_QM_TX_CNT_CALCULATED_S	16
+#define PORT_QM_TX_CNT_AVAIL_S		0
+
+/* B - LUE */
+#define REG_PORT_LUE_CTRL		0x0B00
+
+#define PORT_VLAN_LOOKUP_VID_0		(1 << 7)
+#define PORT_INGRESS_FILTER		(1 << 6)
+#define PORT_DISCARD_NON_VID		(1 << 5)
+#define PORT_MAC_BASED_802_1X		(1 << 4)
+#define PORT_SRC_ADDR_FILTER		(1 << 3)
+
+#define REG_PORT_LUE_MSTP_INDEX		0x0B01
+
+#define REG_PORT_LUE_MSTP_STATE		0x0B04
+
+#define PORT_TX_ENABLE			(1 << 2)
+#define PORT_RX_ENABLE			(1 << 1)
+#define PORT_LEARN_DISABLE		(1 << 0)
+
+/* C - PTP */
+
+#define REG_PTP_PORT_RX_DELAY__2	0x0C00
+#define REG_PTP_PORT_TX_DELAY__2	0x0C02
+#define REG_PTP_PORT_ASYM_DELAY__2	0x0C04
+
+#define REG_PTP_PORT_XDELAY_TS		0x0C08
+#define REG_PTP_PORT_XDELAY_TS_H	0x0C08
+#define REG_PTP_PORT_XDELAY_TS_L	0x0C0A
+
+#define REG_PTP_PORT_SYNC_TS		0x0C0C
+#define REG_PTP_PORT_SYNC_TS_H		0x0C0C
+#define REG_PTP_PORT_SYNC_TS_L		0x0C0E
+
+#define REG_PTP_PORT_PDRESP_TS		0x0C10
+#define REG_PTP_PORT_PDRESP_TS_H	0x0C10
+#define REG_PTP_PORT_PDRESP_TS_L	0x0C12
+
+#define REG_PTP_PORT_TX_INT_STATUS__2	0x0C14
+#define REG_PTP_PORT_TX_INT_ENABLE__2	0x0C16
+
+#define PTP_PORT_SYNC_INT		(1 << 15)
+#define PTP_PORT_XDELAY_REQ_INT		(1 << 14)
+#define PTP_PORT_PDELAY_RESP_INT	(1 << 13)
+
+#define REG_PTP_PORT_LINK_DELAY__4	0x0C18
+
+
+/* Default values are used in ksz_sw_9897.h if these are not defined. */
+#define PRIO_QUEUES			4
+#define RX_PRIO_QUEUES			8
+
+#define KS_PRIO_IN_REG			2
+
+#define TOTAL_PORT_NUM			7
+
+#define KSZ9897_COUNTER_NUM		0x20
+#define TOTAL_KSZ9897_COUNTER_NUM	(KSZ9897_COUNTER_NUM + 2 + 2)
+
+#define SWITCH_COUNTER_NUM		KSZ9897_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ9897_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw_9897.c */
+#define SW_D				u8
+#define SW_R(sw, addr)			(sw)->reg->r8(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w8(sw, addr, val)
+#define SW_SIZE				(1)
+#define SW_SIZE_STR			"%02x"
+#define port_r				port_r8
+#define port_w				port_w8
+
+#define P_BCAST_STORM_CTRL		REG_PORT_MAC_CTRL_0
+#define P_PRIO_CTRL			REG_PORT_MRI_PRIO_CTRL
+#define P_MIRROR_CTRL			REG_PORT_MRI_MIRROR_CTRL
+#define P_STP_CTRL			REG_PORT_LUE_MSTP_STATE
+#define P_PHY_CTRL			REG_PORT_PHY_CTRL
+#define P_NEG_RESTART_CTRL		REG_PORT_PHY_CTRL
+#define P_LINK_STATUS			REG_PORT_PHY_STATUS
+#define P_SPEED_STATUS			REG_PORT_PHY_PHY_CTRL
+#define P_RATE_LIMIT_CTRL		REG_PORT_MAC_IN_RATE_LIMIT
+
+#define S_LINK_AGING_CTRL		REG_SW_LUE_CTRL_1
+#define S_MIRROR_CTRL			REG_SW_MRI_CTRL_0
+#define S_REPLACE_VID_CTRL		REG_SW_MAC_CTRL_2
+#define S_802_1P_PRIO_CTRL		REG_SW_MAC_802_1P_MAP_0
+#define S_TOS_PRIO_CTRL			REG_SW_MAC_TOS_PRIO_0
+#define S_FLUSH_TABLE_CTRL		REG_SW_LUE_CTRL_1
+
+#define REG_SWITCH_RESET		REG_RESET_CTRL
+
+#define SW_FLUSH_DYN_MAC_TABLE		SW_FLUSH_MSTP_TABLE
+
+
+#define MAX_TIMESTAMP_UNIT		2
+#define MAX_TRIG_UNIT			3
+#define MAX_TIMESTAMP_EVENT_UNIT	8
+#define MAX_GPIO			4
+
+#define PTP_TRIG_UNIT_M			((1 << MAX_TRIG_UNIT) - 1)
+#define PTP_TS_UNIT_M			((1 << MAX_TIMESTAMP_UNIT) - 1)
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_cfg_9897.h b/drivers/net/ethernet/micrel/ksz9897/ksz_cfg_9897.h
new file mode 100644
index 0000000..ce55aee
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_cfg_9897.h
@@ -0,0 +1,25 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_9897_H
+#define KSZ_CFG_9897_H
+
+#if defined(CONFIG_KSZ_PTP)
+/* Support 1588 PTP. */
+#define CONFIG_1588_PTP
+#endif
+
+
+#include "ksz_common.h"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.h"
+
+#include "ksz9897.h"
+#include "ksz_sw_9897.h"
+
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_common.c b/drivers/net/ethernet/micrel/ksz9897/ksz_common.c
new file mode 100644
index 0000000..9eaa8f4
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_common.c
@@ -0,0 +1,325 @@
+/**
+ * Microchip Ethernet driver common code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2011 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/**
+ * ksz_start_timer - start kernel timer
+ * @info:	Kernel timer information.
+ * @time:	The time tick.
+ *
+ * This routine starts the kernel timer after the specified time tick.
+ */
+static void ksz_start_timer(struct ksz_timer_info *info, int time)
+{
+	info->cnt = 0;
+	info->timer.expires = jiffies + time;
+	add_timer(&info->timer);
+
+	/* infinity */
+	info->max = -1;
+}  /* ksz_start_timer */
+
+/**
+ * ksz_stop_timer - stop kernel timer
+ * @info:	Kernel timer information.
+ *
+ * This routine stops the kernel timer.
+ */
+static void ksz_stop_timer(struct ksz_timer_info *info)
+{
+	if (info->max) {
+		info->max = 0;
+		del_timer_sync(&info->timer);
+	}
+}  /* ksz_stop_timer */
+
+static void ksz_init_timer(struct ksz_timer_info *info, int period,
+	void (*function)(unsigned long), void *data)
+{
+	info->max = 0;
+	info->period = period;
+	setup_timer(&info->timer, function, (unsigned long) data);
+}  /* ksz_init_timer */
+
+static void ksz_update_timer(struct ksz_timer_info *info)
+{
+	++info->cnt;
+	if (info->max > 0) {
+		if (info->cnt < info->max) {
+			info->timer.expires = jiffies + info->period;
+			add_timer(&info->timer);
+		} else
+			info->max = 0;
+	} else if (info->max < 0) {
+		info->timer.expires = jiffies + info->period;
+		add_timer(&info->timer);
+	}
+}  /* ksz_update_timer */
+
+/* -------------------------------------------------------------------------- */
+
+#define DBG_CH  '-'
+
+#ifdef DEBUG_MSG
+
+/* 2 lines buffer. */
+#define DEBUG_MSG_BUF			(80 * 2)
+
+#define PRINT_MSG_SIZE			(80 * 150)
+#define PRINT_INT_SIZE			(80 * 10)
+
+#define DEBUG_MSG_SIZE			(PRINT_MSG_SIZE + PRINT_INT_SIZE + \
+	DEBUG_MSG_BUF * 2)
+
+struct dbg_print {
+	char *dbg_buf;
+	char *int_buf;
+	char *msg;
+	char *int_msg;
+	int msg_cnt;
+	int int_cnt;
+	int last_msg_line;
+	int last_int_line;
+	unsigned long lock;
+
+	struct work_struct dbg_print;
+	struct ksz_timer_info dbg_timer_info;
+};
+
+static struct dbg_print db;
+
+static void print_buf(char *buf, char **msg, int *cnt, int *last)
+{
+	char ch;
+	char *start;
+
+	if (*last)
+		printk(KERN_INFO "%c\n", DBG_CH);
+	*last = 0;
+	if ('\n' == buf[*cnt - 2] && DBG_CH == buf[*cnt - 1]) {
+		buf[*cnt - 1] = '\0';
+		*last = 1;
+	}
+	*msg = buf;
+
+	/* Kernel seems to limit printk buffer to 1024 bytes. */
+	while (strlen(buf) >= 1024) {
+		start = &buf[1020];
+		while (start != buf && *start != '\n')
+			start--;
+		if (start != buf) {
+			start++;
+			ch = *start;
+			*start = '\0';
+			printk(KERN_INFO "%s", buf);
+			*start = ch;
+			buf = start;
+		}
+	}
+	*cnt = 0;
+	printk(KERN_INFO "%s", buf);
+}  /* print_buf */
+
+static void dbg_print_work(struct work_struct *work)
+{
+	if (db.msg != db.dbg_buf)
+		print_buf(db.dbg_buf, &db.msg, &db.msg_cnt,
+			&db.last_msg_line);
+	if (db.int_msg != db.int_buf) {
+		printk(KERN_INFO "---\n");
+		print_buf(db.int_buf, &db.int_msg, &db.int_cnt,
+			&db.last_int_line);
+		printk(KERN_INFO "+++\n");
+	}
+}  /* dbg_print_work */
+
+static void dbg_monitor(unsigned long ptr)
+{
+	struct dbg_print *dbp = (struct dbg_print *) ptr;
+
+	dbg_print_work(&dbp->dbg_print);
+	ksz_update_timer(&dbp->dbg_timer_info);
+}  /* dbg_monitor */
+
+static int init_dbg(void)
+{
+	db.dbg_buf = kmalloc(DEBUG_MSG_SIZE, GFP_KERNEL);
+	if (!db.dbg_buf)
+		return -ENOMEM;
+
+	db.msg = db.dbg_buf;
+	*db.msg = '\0';
+	db.int_buf = db.dbg_buf + PRINT_MSG_SIZE + DEBUG_MSG_BUF;
+	db.int_msg = db.int_buf;
+	*db.int_msg = '\0';
+	db.msg_cnt = db.int_cnt = 0;
+	db.last_msg_line = 1;
+	db.last_int_line = 1;
+	db.lock = 0;
+
+	INIT_WORK(&db.dbg_print, dbg_print_work);
+
+	/* 100 ms timeout */
+	ksz_init_timer(&db.dbg_timer_info, 100 * HZ / 1000, dbg_monitor, &db);
+	ksz_start_timer(&db.dbg_timer_info, db.dbg_timer_info.period);
+
+	return 0;
+}  /* init_dbg */
+
+static void exit_dbg(void)
+{
+	if (db.dbg_buf) {
+		ksz_stop_timer(&db.dbg_timer_info);
+		flush_work(&db.dbg_print);
+
+		if (db.msg != db.dbg_buf)
+			printk(KERN_DEBUG "%s\n", db.dbg_buf);
+		if (db.int_msg != db.int_buf)
+			printk(KERN_DEBUG "%s\n", db.int_buf);
+		kfree(db.dbg_buf);
+		db.dbg_buf = NULL;
+	}
+}  /* exit_dbg */
+#endif
+
+static void dbg_msg(char *fmt, ...)
+{
+#ifdef DEBUG_MSG
+	va_list args;
+	char **msg;
+	int *dbg_cnt;
+	int left;
+	int in_intr = in_interrupt();
+	int n;
+
+	dbg_cnt = &db.msg_cnt;
+	msg = &db.msg;
+	left = PRINT_MSG_SIZE - db.msg_cnt - 1;
+	if (left <= 0) {
+		db.last_msg_line = 1;
+		return;
+	}
+
+	/* Called within interrupt routines. */
+	if (in_intr) {
+		/*
+		 * If not able to get lock then put in the interrupt message
+		 * buffer.
+		 */
+		if (test_bit(1, &db.lock)) {
+			dbg_cnt = &db.int_cnt;
+			msg = &db.int_msg;
+			left = PRINT_INT_SIZE - db.int_cnt - 1;
+			in_intr = 0;
+		}
+	} else
+		set_bit(1, &db.lock);
+	va_start(args, fmt);
+	n = vsnprintf(*msg, left + 1, fmt, args);
+	va_end(args);
+	if (n > 0) {
+		if (left > n)
+			left = n;
+		*dbg_cnt += left;
+		*msg += left;
+	}
+	if (!in_intr)
+		clear_bit(1, &db.lock);
+#endif
+}  /* dbg_msg */
+
+/* -------------------------------------------------------------------------- */
+
+static inline void dbp_mac_addr(u8 *addr)
+{
+	dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x",
+		addr[0], addr[1], addr[2],
+		addr[3], addr[4], addr[5]);
+}  /* dbp_mac_addr */
+
+static inline void dbp_pkt(struct sk_buff *skb, char first, char *msg, int hdr)
+{
+	int i;
+	int len = skb->len;
+	u8 *data = (u8 *) skb->data;
+
+	if (!first || first != data[0]) {
+		if (msg)
+			dbg_msg(msg);
+		if (hdr && len > 0x50)
+			len = 0x50;
+		for (i = 0; i < len; i++) {
+			dbg_msg("%02x ", data[i]);
+			if ((i % 16) == 15)
+				dbg_msg("\n");
+		}
+		if ((i % 16))
+			dbg_msg("\n");
+	}
+}  /* dbp_pkt */
+
+
+#ifdef USE_SHOW_HELP
+enum {
+	SHOW_HELP_NONE,
+	SHOW_HELP_ON_OFF,
+	SHOW_HELP_NUM,
+	SHOW_HELP_HEX,
+	SHOW_HELP_HEX_2,
+	SHOW_HELP_HEX_4,
+	SHOW_HELP_HEX_8,
+	SHOW_HELP_SPECIAL,
+};
+
+static char *help_formats[] = {
+	"",
+	"%d%s\n",
+	"%u%s\n",
+	"0x%x%s\n",
+	"0x%02x%s\n",
+	"0x%04x%s\n",
+	"0x%08x%s\n",
+	"%d%s\n",
+};
+
+static char *display_strs[] = {
+	" (off)",
+	" (on)",
+};
+
+static char *show_on_off(uint on)
+{
+	if (on <= 1)
+		return display_strs[on];
+	return NULL;
+}  /* show_on_off */
+
+static ssize_t sysfs_show(ssize_t len, char *buf, int type, int chk, char *ptr,
+	int verbose)
+{
+	if (type) {
+		if (verbose) {
+			if (SHOW_HELP_ON_OFF == type)
+				ptr = show_on_off(chk);
+		}
+		len += sprintf(buf + len, help_formats[type], chk, ptr);
+	}
+	return len;
+}  /* sysfs_show */
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_common.h b/drivers/net/ethernet/micrel/ksz9897/ksz_common.h
new file mode 100644
index 0000000..86529c3
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_common.h
@@ -0,0 +1,83 @@
+/**
+ * Microchip Ethernet driver common header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2011 Micrel, Inc.
+ *
+ * This file contains shared structure definitions to be used between network
+ * and switch drivers.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_COMMON_H
+#define KSZ_COMMON_H
+
+
+/* Used to indicate type of flow control support. */
+enum {
+	PHY_NO_FLOW_CTRL,
+	PHY_FLOW_CTRL,
+	PHY_TX_ONLY,
+	PHY_RX_ONLY
+};
+
+/* Used to indicate link connection state. */
+enum {
+	media_connected,
+	media_disconnected,
+	media_unknown
+};
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * struct ksz_timer_info - Timer information data structure
+ * @timer:	Kernel timer.
+ * @cnt:	Running timer counter.
+ * @max:	Number of times to run timer; -1 for infinity.
+ * @period:	Timer period in jiffies.
+ */
+struct ksz_timer_info {
+	struct timer_list timer;
+	int cnt;
+	int max;
+	int period;
+};
+
+/**
+ * struct ksz_counter_info - OS dependent counter information data structure
+ * @counter:	Wait queue to wakeup after counters are read.
+ * @time:	Next time in jiffies to read counter.
+ * @read:	Indication of counters read in full or not.
+ */
+struct ksz_counter_info {
+	wait_queue_head_t counter;
+	unsigned long time;
+	int read;
+};
+
+#define DEV_NAME_SIZE			32
+
+/**
+ * struct ksz_dev_attr - Sysfs data structure
+ * @dev_attr:	Device attribute.
+ * @dev_name:	Attribute name.
+ */
+struct ksz_dev_attr {
+	struct device_attribute dev_attr;
+	char dev_name[DEV_NAME_SIZE];
+};
+
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_dlr.c b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr.c
new file mode 100644
index 0000000..ef622e6
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr.c
@@ -0,0 +1,5736 @@
+/**
+ * Microchip DLR code
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 0
+#define DBG_DLR_STATE
+#endif
+
+#if 0
+#define DBG_DLR_BEACON
+#endif
+#if 0
+#define DBG_DLR_OPER
+#endif
+#if 0
+#define DBG_DLR_HW_OPER
+#endif
+#if 0
+#define DBG_DLR_SUPERVISOR
+#endif
+#if 0
+#define DBG_DLR_ANN_SIGNON
+#endif
+#if 0
+#define DBG_DLR_SIGNON
+#endif
+
+
+static u8 MAC_ADDR_BEACON[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x01 };
+static u8 MAC_ADDR_SIGNON[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x02 };
+static u8 MAC_ADDR_ANNOUNCE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x03 };
+static u8 MAC_ADDR_ADVERTISE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x04 };
+static u8 MAC_ADDR_LEARNING_UPDATE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x05 };
+
+
+#define BEACON_TICK			5
+#define BEACON_INTERVAL			(BEACON_TICK * 10)
+
+enum {
+	DLR_ANNOUNCE_NODE,
+	DLR_BEACON_NODE,
+	DLR_SUPERVISOR,
+	DLR_ACTIVE_SUPERVISOR,
+};
+
+enum {
+	DLR_BEGIN,
+
+	DLR_IDLE_STATE,
+	DLR_FAULT_STATE,
+	DLR_NORMAL_STATE,
+	DLR_ACTIVE_STATE,
+	DLR_ACTIVE_FAULT_STATE,
+	DLR_ACTIVE_NORMAL_STATE,
+	DLR_BACKUP_STATE,
+	DLR_PREPARE_STATE,
+	DLR_RESTART_STATE,
+};
+
+static int dbg_leak = 5;
+
+#ifdef DBG_DLR_BEACON
+static int dbg_bcn;
+#endif
+
+#ifdef DBG_DLR_ANN_SIGNON
+static int dbg_ann;
+#endif
+
+#ifdef CONFIG_HAVE_ACL_HW
+
+/* Lower action index has higher precedence. */
+#define DLR_BEACON_DROP_ACL_ACTION	12
+#define DLR_NO_ACL_ACTION		13
+#define DLR_DROP_ACL_ACTION		15
+
+#define DLR_BEACON_DROP_ACL_RULE	12
+#define DLR_BEACON_ACL_RULE		13
+#define DLR_DROP_SELF_ACL_RULE		15
+#define DLR_BEACON_TIMEOUT_ACL_RULE	14
+
+#define DLR_BEACON_DROP_ACL_ENTRY	12
+#define DLR_BEACON_ACL_ENTRY		13
+#define DLR_DROP_SELF_ACL_ENTRY		15
+#define DLR_TIMEOUT_ACL_ENTRY		14
+
+static void setup_acl_beacon(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = DLR_BEACON_DROP_ACL_RULE;
+	int first_rule = i;
+	int ruleset = (3 << i);
+
+	acl = &cfg->acl_info[i];
+	mutex_lock(&sw->acllock);
+	if (!memcmp(acl->mac, info->attrib.active_super_addr.addr, ETH_ALEN) &&
+	    acl->first_rule == first_rule)
+		goto done;
+
+	acl->first_rule = first_rule;
+	acl->ruleset = ruleset;
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 1;
+	memcpy(acl->mac, info->attrib.active_super_addr.addr, ETH_ALEN);
+	memcpy(info->beacon_addr, acl->mac, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	sw_w_acl_rule(sw, port, i, acl);
+
+done:
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_beacon */
+
+static void setup_acl_beacon_timeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+	u8 *addr = MAC_ADDR_BEACON;
+
+	i = DLR_TIMEOUT_ACL_ENTRY;
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->first_rule = DLR_NO_ACL_ACTION;
+	acl->ruleset = (1 << i);
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_COUNT;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, addr, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	if (info->beacon_timeout > ACL_CNT_M) {
+		int cnt = info->beacon_timeout + 500;
+
+		cnt /= 1000;
+		acl->cnt = cnt;
+		acl->msec = 1;
+	} else {
+		acl->cnt = info->beacon_timeout;
+		acl->msec = 0;
+	}
+	acl->intr_mode = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_beacon_timeout */
+
+static void disable_acl_beacon_timeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = DLR_BEACON_TIMEOUT_ACL_RULE;
+
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->ruleset = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* disable_acl_beacon_timeout */
+
+static void setup_acl_beacon_drop(struct ksz_dlr_info *info, int drop)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i = DLR_BEACON_DROP_ACL_ACTION;
+	int p;
+
+	for (p = 0; p < 2; p++)
+		setup_acl_beacon(info, info->ports[p]);
+	mutex_lock(&sw->acllock);
+	for (p = 0; p < 2; p++) {
+		cfg = &sw->info->port_cfg[info->ports[p]];
+
+		acl = &cfg->acl_info[i];
+		acl->map_mode = ACL_MAP_MODE_REPLACE;
+		acl->ports = drop & info->member;
+		if (!drop)
+			acl->map_mode = ACL_MAP_MODE_DISABLE;
+		sw_w_acl_action(sw, info->ports[p], i, acl);
+	}
+	mutex_unlock(&sw->acllock);
+
+	if (drop)
+		return;
+
+	/* Ring state may be changed in the beacon. */
+	memset(&info->beacon_info[0].last, 0, sizeof(struct ksz_dlr_beacon));
+	memset(&info->beacon_info[1].last, 0, sizeof(struct ksz_dlr_beacon));
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 0;
+}  /* setup_acl_beacon_drop */
+
+static void setup_acl_self(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+
+	/* Drop packets sent by self. */
+	i = DLR_DROP_SELF_ACL_ENTRY;
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->data[0] = 0xff;
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_MAC;
+	acl->equal = 1;
+	acl->src = 1;
+	memcpy(acl->mac, info->src_addr, ETH_ALEN);
+	acl->eth_type = 0;
+	acl->first_rule = DLR_DROP_ACL_ACTION;
+	acl->ruleset = (1 << i);
+	acl->map_mode = ACL_MAP_MODE_REPLACE;
+	acl->ports = sw->HOST_MASK;
+	acl->ports = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_self */
+
+static void dlr_setup_acl(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+
+	mutex_lock(&sw->acllock);
+
+	/* Use MAC table for beacon forwarding. */
+	i = DLR_BEACON_ACL_ENTRY;
+	acl = &cfg->acl_info[i];
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, MAC_ADDR_BEACON, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	sw_w_acl_rule(sw, port, i, acl);
+
+	i = DLR_BEACON_ACL_RULE;
+	acl = &cfg->acl_info[i];
+	acl->first_rule = DLR_NO_ACL_ACTION;
+	acl->ruleset = (1 << i);
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	i = DLR_NO_ACL_ACTION;
+	acl = &cfg->acl_info[i];
+	acl->map_mode = ACL_MAP_MODE_DISABLE;
+	acl->ports = 0;
+	sw_w_acl_action(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* dlr_setup_acl */
+#endif
+
+static void setup_vlan_table(struct ksz_dlr_info *info, u16 vid, int set)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	u32 ports = 0;
+
+#ifdef DBG_DLR_OPER
+	dbg_msg(" %s %d %d\n", __func__, vid, set);
+#endif
+
+	/* Do not do anything for VID 0, which is priority tagged frame. */
+	if (1 >= vid)
+		return;
+	if (set)
+		ports = sw->HOST_MASK | info->member;
+	sw->ops->cfg_vlan(sw, 0, vid, 0, ports);
+}  /* setup_vlan_table */
+
+#ifdef CONFIG_HAVE_DLR_HW
+static void dlr_hw_set_state(struct ksz_sw *sw, u8 node, u8 ring)
+{
+	u8 data;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_STATE__1);
+	data &= ~DLR_RING_STATE_NORMAL;
+	data &= ~(DLR_NODE_STATE_M << DLR_NODE_STATE_S);
+	node &= DLR_NODE_STATE_M;
+	node <<= DLR_NODE_STATE_S;
+	ring &= DLR_RING_STATE_NORMAL;
+	data |= node | ring;
+	SW_W(sw, REG_DLR_STATE__1, data);
+}
+
+static void dlr_hw_set_supervisor(struct ksz_sw *sw, u8 super)
+{
+	u8 data;
+	u8 saved;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	saved = data;
+	data &= ~DLR_BEACON_TX_ENABLE;
+	data &= ~DLR_BACKUP_AUTO_ON;
+	if (DLR_ACTIVE_SUPERVISOR == super)
+		data |= DLR_BEACON_TX_ENABLE;
+#if 0
+/*
+ * THa  2016/12/30
+ * Occassionally the ACL beacon timeout no longer is triggered after the
+ * active supervisor is turned off.  The backup supervisor feature in the
+ * switch starts sending beacons, but software has no way to become the
+ * active supervisor.  Turning on/off supervisor does not recover from this
+ * problem.
+ */
+	if (DLR_SUPERVISOR == super) {
+
+		/* Turn off previous automatic start first. */
+		if (saved & DLR_BACKUP_AUTO_ON) {
+			SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+			dbg_msg("  reset backup: %x\n", saved);
+#endif
+		}
+		data |= DLR_BACKUP_AUTO_ON;
+	}
+#endif
+	data |= DLR_ASSIST_ENABLE;
+	SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+	dbg_msg("  %s %x\n", __func__, data);
+#endif
+}
+
+static void dlr_hw_set_dest_addr(struct ksz_sw *sw, u8 *addr)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w(sw, REG_DLR_DEST_ADDR_0, addr, ETH_ALEN);
+}
+
+static void dlr_hw_set_ip_addr(struct ksz_sw *sw, u32 ip_addr)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_IP_ADDR__4, ip_addr);
+}
+
+static void dlr_hw_reset_seq_id(struct ksz_sw *sw)
+{
+	u8 data;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	SW_W(sw, REG_DLR_CTRL__1, data | DLR_RESET_SEQ_ID);
+	SW_W(sw, REG_DLR_CTRL__1, data & ~DLR_RESET_SEQ_ID);
+}
+
+static void dlr_hw_set_port_map(struct ksz_sw *sw, u32 ports)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_PORT_MAP__4, ports);
+}
+
+static void dlr_hw_set_precedence(struct ksz_sw *sw, u8 precedence)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w8(sw, REG_DLR_PRECEDENCE__1, precedence);
+}
+
+static void dlr_hw_set_interval(struct ksz_sw *sw, u16 interval)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_BEACON_INTERVAL__4, interval);
+}
+
+static void dlr_hw_set_timeout(struct ksz_sw *sw, u32 timeout)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_BEACON_TIMEOUT__4, timeout);
+}
+
+static void dlr_hw_set_vlan_id(struct ksz_sw *sw, u16 vid)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w16(sw, REG_DLR_VLAN_ID__2, vid);
+}
+#endif
+
+
+#define DLR_LEARNING_ENTRY		3
+#define DLR_BEACON_ENTRY		4
+#define DLR_ANNOUNCE_ENTRY		5
+#define DLR_SIGNON_ENTRY		6
+#define DLR_SUPERVISOR_ENTRY		7
+#define DLR_SUPERVISOR_DEV_0_ENTRY	8
+
+static void sw_setup_dlr(struct ksz_sw *sw)
+{
+	struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+	/* Enable 802.1p priority to give highest priority to beacon frame. */
+	sw_ena_802_1p(sw, dlr->ports[0]);
+	sw_ena_802_1p(sw, dlr->ports[1]);
+	sw_ena_802_1p(sw, sw->HOST_PORT);
+
+#ifdef CONFIG_HAVE_ACL_HW
+	/* Need to receive beacon frame with changed VID. */
+	sw->ops->fwd_unk_vid(sw);
+#endif
+
+#ifdef CONFIG_HAVE_DLR_HW
+	dlr_hw_set_dest_addr(sw, MAC_ADDR_BEACON);
+	dlr_hw_set_port_map(sw, dlr->member);
+#endif
+	sw->ops->release(sw);
+
+	/* SignOn is not forwarded automatically. */
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		false, false, 0);
+
+	/* Not 3-port switch and other ports are used. */
+	if (sw->overrides & HAVE_MORE_THAN_2_PORTS) {
+		u32 member = dlr->member | sw->HOST_MASK;
+
+		sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON,
+			member, false, false, 0);
+		sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE,
+			member, false, false, 0);
+		sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY,
+			MAC_ADDR_LEARNING_UPDATE, member, false, false, 0);
+		if (1 == sw->eth_cnt) {
+			sw->ops->acquire(sw);
+			sw_cfg_port_base_vlan(sw, dlr->ports[0], member);
+			sw_cfg_port_base_vlan(sw, dlr->ports[1], member);
+			sw_cfg_port_base_vlan(sw, sw->HOST_PORT, member);
+			sw->ops->release(sw);
+		}
+	}
+	dlr_setup_acl(dlr, dlr->ports[0]);
+	dlr_setup_acl(dlr, dlr->ports[1]);
+
+	/* Do not need to process beacons. */
+	if (DLR_ANNOUNCE_NODE == sw->info->dlr.node)
+		sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON,
+			dlr->member, false, false, 0);
+	sw->ops->acquire(sw);
+}  /* sw_setup_dlr */
+
+enum {
+	DEV_DLR_CHK_HW,
+	DEV_DLR_CLR_SUPER,
+	DEV_DLR_SETUP_DIR,
+	DEV_DLR_SETUP_DROP,
+	DEV_DLR_SETUP_TIMEOUT,
+	DEV_DLR_SETUP_VID,
+	DEV_DLR_FLUSH,
+	DEV_DLR_BLOCK,
+	DEV_DLR_STOP,
+	DEV_DLR_UPDATE,
+	DEV_DLR_RX,
+	DEV_DLR_START_ANNOUNCE,
+	DEV_DLR_TX_ANNOUNCE,
+	DEV_DLR_TX_LOCATE,
+	DEV_DLR_TX_SIGNON,
+	DEV_DLR_TX_REQ,
+	DEV_DLR_TX_RESP,
+	DEV_DLR_TX_STATUS,
+	DEV_DLR_TX_ADVERTISE,
+	DEV_DLR_TX_FLUSH_TABLES,
+	DEV_DLR_TX_LEARNING_UPDATE,
+};
+
+static void wait_for_timeout(u32 microsec)
+{
+	if (microsec >= 20000) {
+		microsec /= 1000;
+		delay_milli(microsec);
+	} else
+		delay_micro(microsec);
+}  /* wait_for_timeout */
+
+static void dlr_set_addr(struct ksz_dlr_active_node *node, u32 ip_addr,
+	u8 *addr)
+{
+	node->ip_addr = ip_addr;
+	memcpy(node->addr, addr, ETH_ALEN);
+}  /* dlr_set_addr */
+
+static int dlr_dev_xmit(struct ksz_dlr_info *info, u8 *data, int len,
+	int ports, u16 vid)
+{
+	int rc;
+	struct sk_buff *skb;
+	const struct net_device_ops *ops = info->dev->netdev_ops;
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + VLAN_HLEN + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	/* Regular transmit from DLR node. */
+	if (vid == info->vid)
+		memcpy(skb->data, data, len);
+
+	/* Need to replace source MAC address for self-address filtering. */
+	if (ports & info->member)
+		memcpy(&skb->data[6], info->src_addr, ETH_ALEN);
+
+	skb_put(skb, len);
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	skb->protocol = htons(DLR_TAG_TYPE);
+	skb->dev = info->dev;
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(info->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* dlr_dev_xmit */
+
+static int dlr_xmit(struct ksz_dlr_info *info, u16 ports)
+{
+	u8 *frame = info->tx_frame;
+	int len = info->len;
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(info->dev) || !netif_carrier_ok(info->dev))
+		return 0;
+
+	/* Do not send to port if its link is lost. */
+	if ((ports & (1 << info->ports[0])) && info->p1_down)
+		ports &= ~(1 << info->ports[0]);
+	if ((ports & (1 << info->ports[1])) && info->p2_down)
+		ports &= ~(1 << info->ports[1]);
+
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	return dlr_dev_xmit(info, info->tx_frame, len, ports, info->vid);
+}  /* dlr_xmit */
+
+static int prep_dlr_beacon(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_BEACON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_BEACON;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid_beacon++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.beacon.ring_state = dlr->ring_state;
+	frame->data.beacon.precedence = dlr->precedence;
+	frame->data.beacon.interval = htonl(dlr->beacon_interval);
+	frame->data.beacon.timeout = htonl(dlr->beacon_timeout);
+	return sizeof(struct ksz_dlr_beacon);
+}  /* prep_dlr_beacon */
+
+static int prep_dlr_announce(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_ANNOUNCE;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.announce.ring_state = dlr->ring_state;
+	return sizeof(struct ksz_dlr_announce);
+}  /* prep_dlr_announce */
+
+static int prep_dlr_link_status(struct ksz_dlr_info *dlr, int neigh)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, dlr->attrib.active_super_addr.addr,
+		ETH_ALEN);
+	frame->hdr.frame_type = DLR_LINK_STATUS;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.status.port1_active = 1;
+	frame->data.status.port2_active = 1;
+
+	if (neigh) {
+		frame->data.status.neighbor = 1;
+		if (dlr->p1_lost)
+			frame->data.status.port1_active = 0;
+		if (dlr->p2_lost)
+			frame->data.status.port2_active = 0;
+	} else {
+		frame->data.status.neighbor = 0;
+		if (dlr->p1_down)
+			frame->data.status.port1_active = 0;
+		if (dlr->p2_down)
+			frame->data.status.port2_active = 0;
+	}
+	return sizeof(struct ksz_dlr_status);
+}  /* prep_dlr_link_status */
+
+static int prep_dlr_locate_fault(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_LOCATE_FAULT;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	return 0;
+}  /* prep_dlr_locate_fault */
+
+static int prep_dlr_neigh_chk_req(struct ksz_dlr_info *dlr, int port)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_SIGNON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_NEIGH_CHK_REQ;
+	frame->hdr.src_port = port ? DLR_PORT_2 : DLR_PORT_1;
+	dlr->seqid_chk[port] = dlr->seqid++;
+	if (1 == dlr->port_chk[port])
+		dlr->seqid_first[port] = dlr->seqid_chk[port];
+	frame->hdr.seqid = htonl(dlr->seqid_chk[port]);
+
+	memset(frame->data.reserved, 0, 30);
+	return 0;
+}  /* prep_dlr_neigh_chk_req */
+
+static int prep_dlr_neigh_chk_resp(struct ksz_dlr_info *dlr, u32 seqid, u8 in,
+	int out)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_SIGNON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_NEIGH_CHK_RESP;
+	frame->hdr.src_port = out ? DLR_PORT_2 : DLR_PORT_1;
+	frame->hdr.seqid = htonl(seqid);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.neigh_chk_resp.src_port = in;
+	return sizeof(struct ksz_dlr_neigh_chk_resp);
+}  /* prep_dlr_neigh_chk_resp */
+
+static int prep_dlr_signon(struct ksz_dlr_info *dlr, int len)
+{
+	int i;
+	struct ksz_dlr_tx_frame *base = (struct ksz_dlr_tx_frame *)
+		dlr->signon_frame;
+	struct ksz_dlr_frame *frame = &base->body;
+	u16 num = ntohs(frame->data.signon.num);
+	struct ksz_dlr_node *node = frame->data.signon.node;
+	int space = 1000;
+
+	/* The very first signon frame. */
+	if (!len) {
+		memcpy(base->vlan.h_dest, dlr->signon_addr, ETH_ALEN);
+		frame->hdr.frame_type = DLR_SIGN_ON;
+		frame->hdr.src_port = dlr->tx_port;
+		dlr->seqid_signon = dlr->seqid;
+		frame->hdr.seqid = htonl(dlr->seqid++);
+		num = 0;
+		len = sizeof(struct vlan_ethhdr) +
+			sizeof(struct ksz_dlr_hdr) +
+			sizeof(struct ksz_dlr_signon) -
+			sizeof(struct ksz_dlr_node);
+		if ((dlr->overrides & DLR_TEST) && dlr->signon_space)
+			space = dlr->signon_space + 1;
+	}
+	memcpy(base->vlan.h_source, dlr->src_addr, ETH_ALEN);
+	if (len + sizeof(struct ksz_dlr_node) > 1500) {
+		memcpy(base->vlan.h_dest, dlr->attrib.active_super_addr.addr,
+			ETH_ALEN);
+		frame->hdr.src_port = dlr->rx_port;
+		dlr->signon_port = dlr->rx_port;
+		return len;
+	}
+	for (i = 0; i < num; i++)
+		node++;
+	do {
+		num++;
+		memcpy(node->addr, dlr->src_addr, ETH_ALEN);
+		node->ip_addr = dlr->ip_addr;
+		len += sizeof(struct ksz_dlr_node);
+		node++;
+	} while (len + sizeof(struct ksz_dlr_node) * space <= 1500);
+	frame->data.signon.num = htons(num);
+
+	dlr->signon_port = dlr->tx_port;
+	return len;
+}  /* prep_dlr_signon */
+
+static int prep_dlr_advertise(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ADVERTISE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_ADVERTISE;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.advertise.state = DLR_GW_ACTIVE_LISTEN_STATE;
+	frame->data.advertise.precedence = dlr->precedence;
+	frame->data.advertise.interval = htonl(dlr->beacon_interval);
+	frame->data.advertise.timeout = htonl(dlr->beacon_timeout);
+	frame->data.advertise.learning_update_enable = 1;
+	return sizeof(struct ksz_dlr_advertise);
+}  /* prep_dlr_advertise */
+
+static int prep_dlr_flush_tables(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_FLUSH_TABLES;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.flush.learning_update_enable = 1;
+	return sizeof(struct ksz_dlr_flush_tables);
+}  /* prep_dlr_flush_tables */
+
+static int prep_dlr_learning_update(struct ksz_dlr_info *dlr)
+{
+	dlr->update_frame.hdr.seqid = htonl(dlr->seqid++);
+
+	return sizeof(struct ksz_dlr_update_frame);
+}  /* prep_dlr_learning_update */
+
+static void dlr_tx_beacon(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_beacon(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_beacon */
+
+static void dlr_tx_announce(struct ksz_dlr_info *dlr)
+{
+	int rc;
+	u16 ports = dlr->member;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	if (dbg_ann > 0) {
+		dbg_msg(" tx ann: S=%d R=%d %x %lx\n",
+			dlr->state, dlr->ring_state,
+			dlr->seqid, jiffies);
+		dbg_ann--;
+	}
+#endif
+
+	if (RING_NORMAL_STATE == dlr->ring_state) {
+		if (dlr->ann_first)
+			dlr->ann_jiffies = jiffies;
+		else if (dlr->ann_jiffies == jiffies) {
+			dlr->ann_jiffies = 0;
+			return;
+		}
+	}
+
+	if (RING_NORMAL_STATE == dlr->ring_state)
+		ports = 1 << dlr->ports[dlr->port];
+
+	dlr->len = prep_dlr_announce(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, ports);
+
+	/* First delayed announce from initial fault state sent. */
+	if (dlr->ann_first)
+		dlr->ann_first = 0;
+}  /* dlr_tx_announce */
+
+static void dlr_tx_chk_req(struct ksz_dlr_info *dlr, int port)
+{
+	int rc;
+
+	dlr->len = prep_dlr_neigh_chk_req(dlr, port);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, (1 << dlr->ports[port]));
+}  /* dlr_tx_chk_req */
+
+static void dlr_tx_chk_resp(struct ksz_dlr_info *dlr, int port)
+{
+	int rc;
+
+	dlr->len = prep_dlr_neigh_chk_resp(dlr, dlr->seqid_rcv[port],
+		dlr->port_rcv[port], port);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, (1 << dlr->ports[port]));
+}  /* dlr_tx_chk_resp */
+
+static void dlr_tx_status(struct ksz_dlr_info *dlr, int neigh)
+{
+	int rc;
+
+	dlr->len = prep_dlr_link_status(dlr, neigh);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, 0);
+}  /* dlr_tx_status */
+
+static void dlr_tx_signon(struct ksz_dlr_info *dlr, int len)
+{
+	int rc;
+
+	dlr->len = prep_dlr_signon(dlr, len);
+	dlr->tx_frame = dlr->signon_frame;
+	rc = dlr_xmit(dlr, (1 << dlr->ports[dlr->signon_port]));
+	dlr->tx_frame = (u8 *) &dlr->frame;
+}  /* dlr_tx_signon */
+
+static void dlr_tx_locate_fault(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_locate_fault(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_locate_fault */
+
+static void dlr_tx_learning_update(struct ksz_dlr_info *dlr)
+{
+	int rc;
+	u16 ports = dlr->member;
+
+	dlr->len = prep_dlr_learning_update(dlr);
+	dlr->tx_frame = (u8 *) &dlr->update_frame;
+
+	/* Attempt to notify the other supervisor about incoming beacons. */
+	if (dlr->rogue_super) {
+		memcpy(dlr->update_frame.eth.h_dest,
+			&dlr->rogue_super->prec_addr[1], ETH_ALEN);
+		ports = (1 << dlr->ports[dlr->rogue_super->port]);
+		ports |= 0x80000000;
+	}
+	rc = dlr_xmit(dlr, ports);
+
+	/* Reset default Learning_Update address. */
+	if (dlr->rogue_super) {
+		memcpy(dlr->update_frame.eth.h_dest, MAC_ADDR_LEARNING_UPDATE,
+			ETH_ALEN);
+		dlr->rogue_super = NULL;
+	}
+	dlr->tx_frame = (u8 *) &dlr->frame;
+}  /* dlr_tx_learning_update */
+
+static void dlr_tx_advertise(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_advertise(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_advertise */
+
+static void dlr_tx_flush_tables(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_flush_tables(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_flush_tables */
+
+static void flushMacTable(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	sw->ops->acquire(sw);
+	sw->ops->flush_table(sw, sw->mib_port_cnt);
+	sw->ops->release(sw);
+}
+
+static void enableOnePort(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	int block_port = (info->port + 1) & 1;
+
+	sw->ops->acquire(sw);
+	port_set_stp_state(sw, info->ports[block_port], STP_STATE_BLOCKED);
+	sw->ops->release(sw);
+}
+
+static void enableBothPorts(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	sw->ops->acquire(sw);
+	port_set_stp_state(sw, info->ports[0], STP_STATE_FORWARDING);
+	port_set_stp_state(sw, info->ports[1], STP_STATE_FORWARDING);
+	sw->ops->release(sw);
+}
+
+static void disableLearn(struct ksz_dlr_info *info, int disable)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (disable != info->disable_learn) {
+		sw->ops->acquire(sw);
+		port_cfg_dis_learn(sw, info->ports[0], disable);
+		port_cfg_dis_learn(sw, info->ports[1], disable);
+		sw->ops->release(sw);
+		info->disable_learn = disable;
+	}
+}
+
+static void dlr_set_state(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u8 node;
+	u8 ring;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (DLR_IDLE_STATE == info->state)
+		node = DLR_NODE_STATE_IDLE;
+	else if (DLR_NORMAL_STATE == info->state ||
+		 DLR_ACTIVE_NORMAL_STATE == info->state)
+		node = DLR_NODE_STATE_NORMAL;
+	else
+		node = DLR_NODE_STATE_FAULT;
+	ring = RING_FAULT_STATE == info->ring_state ?
+		DLR_RING_STATE_FAULT : DLR_RING_STATE_NORMAL;
+
+	/* Backup supervisor needs to send ring fault with the first beacon. */
+	if (DLR_ACTIVE_SUPERVISOR != info->node ||
+	    DLR_RESTART_STATE == info->state)
+		ring = DLR_RING_STATE_FAULT;
+	sw->ops->acquire(sw);
+	dlr_hw_set_state(sw, node, ring);
+	sw->ops->release(sw);
+
+	/* Make sure beacon is sent with new ring state. */
+	if (DLR_ACTIVE_SUPERVISOR == info->node)
+		wait_for_timeout(info->beacon_interval * 2);
+#endif
+}  /* dlr_set_state */
+
+static void dlr_chk_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u32 data;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->ops->acquire(sw);
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	if (data & DLR_BEACON_TX_ENABLE) {
+#ifdef DBG_DLR_HW_OPER
+		dbg_msg("  %s %x\n", __func__, data);
+#endif
+		if (info->node != DLR_ACTIVE_SUPERVISOR) {
+			data &= ~DLR_BEACON_TX_ENABLE;
+			SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+			dbg_msg(" tx off\n");
+#endif
+		}
+	} else if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		data |= DLR_BEACON_TX_ENABLE;
+		SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+		dbg_msg(" tx on\n");
+#endif
+	}
+	sw->ops->release(sw);
+#endif
+	info->chk_hw = 0;
+}  /* dlr_chk_supervisor */
+
+static void dlr_set_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u8 node;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (info->node < DLR_SUPERVISOR)
+		node = DLR_BEACON_NODE;
+	else if (DLR_ACTIVE_SUPERVISOR == info->node)
+		node = DLR_ACTIVE_SUPERVISOR;
+	else
+		node = DLR_SUPERVISOR;
+
+	/* In case the backup supervisor starts sending beacons. */
+	if (DLR_SUPERVISOR == node && 7 == info->beacon_timeout_ports)
+		node = DLR_BEACON_NODE;
+	sw->ops->acquire(sw);
+	dlr_hw_set_supervisor(sw, node);
+	sw->ops->release(sw);
+#ifdef DBG_DLR_HW_OPER
+	dbg_msg("  %s %d %d\n", __func__, node, info->node);
+#endif
+#endif
+}  /* dlr_set_supervisor */
+
+static void setupBeacons(struct ksz_dlr_info *info)
+{
+	int use_hw = false;
+
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (sw->features & REDUNDANCY_SUPPORT) {
+		use_hw = true;
+	}
+#endif
+	if (use_hw) {
+		dlr_set_state(info);
+	} else {
+		info->interval = 0;
+		dlr_tx_beacon(info);
+	}
+}  /* setupBeacons */
+
+static void disableSupervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	u32 member = 0;
+
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	if (sw->overrides & HAVE_MORE_THAN_2_PORTS)
+		member = info->member | sw->HOST_MASK;
+	dlr_set_supervisor(info);
+	info->start = 0;
+
+	/* Allow beacons to be forwarded. */
+	sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON, member, false,
+		false, 0);
+	sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE, member,
+		false, false, 0);
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		false, false, 0);
+	sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY, MAC_ADDR_LEARNING_UPDATE,
+		member, false, false, 0);
+#ifdef CONFIG_HAVE_DLR_HW
+	sw->ops->acquire(sw);
+	dlr_hw_reset_seq_id(sw);
+	sw->ops->release(sw);
+#endif
+}  /* disableSupervisor */
+
+static void enableSupervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Do not need to process Announce. */
+	sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE, 0, true,
+		false, 0);
+
+	/* Force to receive messages as port will be closed. */
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		true, false, 0);
+#if 0
+/* For case where tag is removed and new tag is added. */
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		true, true, 2);
+#endif
+	sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY, MAC_ADDR_LEARNING_UPDATE, 0,
+		true, false, 0);
+	sw->ops->acquire(sw);
+#ifdef CONFIG_HAVE_DLR_HW
+	dlr_hw_set_precedence(sw, info->precedence);
+	dlr_hw_set_interval(sw, info->beacon_interval);
+	dlr_hw_set_timeout(sw, info->beacon_timeout);
+	dlr_hw_set_vlan_id(sw, info->vid);
+	dlr_hw_set_ip_addr(sw, ntohl(info->ip_addr));
+#endif
+	sw->ops->release(sw);
+
+	/* Beacons will only be forwarded to self. */
+	sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON, sw->HOST_MASK,
+		false, false, 0);
+
+	info->seqid_last[0] = info->seqid_last[1] = 0xdeadbeef;
+	info->start = 1;
+	dlr_set_supervisor(info);
+}  /* enableSupervisor */
+
+static void disableAnnounce(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	cancel_delayed_work_sync(&info->announce_tx);
+}
+
+static void startAnnounce(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	cancel_delayed_work_sync(&info->announce_tx);
+	schedule_delayed_work(&info->announce_tx, 100);
+	if (!info->ann_first)
+		dlr_tx_announce(info);
+	else
+		info->tx_announce = true;
+}  /* startAnnounce */
+
+static void enableAnnounce(struct ksz_dlr_info *info, int delay)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s %d %d %lx\n", __func__, delay, info->ann_delay,
+		jiffies);
+#endif
+	if (1 == delay) {
+
+		/* Wait for 2 * beacon timeout before sending announce. */
+		if (!info->ann_delay) {
+			info->ann_jiffies = jiffies;
+			info->ann_delay = 1;
+			info->ann_first = 1;
+			cancel_delayed_work_sync(&info->announce_tx);
+			schedule_delayed_work(&info->announce_tx, 0);
+		}
+
+	/* Wait until first announce is sent. */
+	} else if (!info->ann_delay)
+		startAnnounce(info);
+}
+
+static void disableAnnounceTimeout(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	ksz_stop_timer(&info->announce_timeout_timer_info);
+}
+
+static void disableNeighChkTimers(struct ksz_dlr_info *info)
+{
+	info->p1_lost = info->p2_lost = 0;
+	info->port_chk[0] = info->port_chk[1] = 0;
+	ksz_stop_timer(&info->neigh_chk_timer_info);
+	info->neigh_chk = 0;
+}
+
+static void disableSignOnTimer(struct ksz_dlr_info *info)
+{
+	ksz_stop_timer(&info->signon_timer_info);
+	info->signon_start = 0;
+}  /* disableSignOnTimer */
+
+static int updateValues(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_sw *sw = info->sw_dev;
+	int can_change = false;
+	int vid_change = false;
+	int prepare = false;
+	int cmp = memcmp(info->src_addr, attrib->active_super_addr.addr,
+		ETH_ALEN);
+
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	sw->ops->acquire(sw);
+	if (info->precedence != attrib->super_cfg.prec) {
+		info->precedence = attrib->super_cfg.prec;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_precedence(sw, info->precedence);
+#endif
+	}
+	sw->ops->release(sw);
+
+	/*
+	 * Changing precedence or MAC address can cause backup supervisor to
+	 * become active.
+	 */
+	if (info->precedence > attrib->active_super_prec ||
+	    (info->precedence == attrib->active_super_prec &&
+	    cmp > 0)) {
+		can_change = true;
+		prepare = true;
+	} else if (!cmp)
+		can_change = true;
+	if (can_change && info->vid != attrib->super_cfg.vid) {
+		vid_change = true;
+		setup_vlan_table(info, info->vid, false);
+	}
+	sw->ops->acquire(sw);
+	if (can_change &&
+	    info->beacon_interval != attrib->super_cfg.beacon_interval) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("interval %u %u\n",
+			info->beacon_interval,
+			attrib->super_cfg.beacon_interval);
+#endif
+		info->beacon_interval = attrib->super_cfg.beacon_interval;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_interval(sw, info->beacon_interval);
+#endif
+	}
+	if (can_change &&
+	    info->beacon_timeout != attrib->super_cfg.beacon_timeout) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("timeout %u %u\n",
+			info->beacon_timeout,
+			attrib->super_cfg.beacon_timeout);
+#endif
+		info->beacon_timeout = attrib->super_cfg.beacon_timeout;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_timeout(sw, info->beacon_timeout);
+#endif
+	}
+	if (can_change && info->vid != attrib->super_cfg.vid) {
+		info->vid = attrib->super_cfg.vid;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_vlan_id(sw, info->vid);
+#endif
+	}
+	sw->ops->release(sw);
+	if (vid_change)
+		setup_vlan_table(info, info->vid, true);
+	info->new_val = 0;
+	info->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+	memcpy(info->signon_frame, &info->frame, sizeof(struct vlan_ethhdr));
+	return prepare;
+}  /* updateValues */
+
+static void dlr_flush(struct ksz_dlr_info *info)
+{
+	flushMacTable(info);
+}
+
+static void setupDir(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_dlr_active_node *active;
+	int cmp;
+
+	if (port >= 0) {
+		active = &info->attrib.active_super_addr;
+		memcpy(info->last_sup.addr, active->addr, ETH_ALEN);
+		port = 1 << info->ports[port];
+	} else {
+		active = &info->last_sup;
+		port = 0;
+	}
+
+	/* Do not change entry of own address. */
+	cmp = memcmp(active->addr, info->src_addr, ETH_ALEN);
+	if (!cmp)
+		return;
+	info->active_port = port;
+	sw->ops->cfg_mac(sw, DLR_SUPERVISOR_ENTRY,
+		active->addr, port, false, false, 0);
+#if 0
+	if (sw->eth_cnt > 1)
+		sw->ops->cfg_mac(sw, DLR_SUPERVISOR_DEV_0_ENTRY, active->addr,
+			port, false, true, sw->eth_maps[0].vlan);
+#endif
+#ifdef DBG_DLR_HW_OPER
+	dbg_msg("%s x%x %02x:%02x:%02x\n", __func__, port,
+		active->addr[3],
+		active->addr[4],
+		active->addr[5]);
+#endif
+}
+
+
+#define announcedState		(info->ring_state)
+#define fromRingState		(RING_FAULT_STATE == info->ring_state ?	\
+	DLR_FAULT_STATE : DLR_NORMAL_STATE)
+#define announceRcvd		(info->ann_rcvd)
+#define announceTimeout		(info->ann_timeout)
+#define oneBeaconRcvd		(info->one_rcvd)
+#define twoBeaconsRcvd		(info->both_rcvd)
+#define oneBeaconTimeout	(info->one_timeout)
+#define twoBeaconsTimeout	(info->both_timeout)
+#define newSupervisor		(info->new_supervisor)
+#define newValue		(info->new_val)
+#define backupSupervisor	(DLR_SUPERVISOR == info->node)
+#define faultState		(RING_FAULT_STATE == info->ring_state)
+#define linkDown		(info->both_down)
+#define linkLoss		(info->one_down)
+#define linkStatus		(info->p1_lost || info->p2_lost)
+
+
+static void acceptBeacons(struct ksz_dlr_info *dlr)
+{
+#ifdef DBG_DLR_BEACON
+	dbg_msg("%s\n", __func__);
+#endif
+#ifdef DBG_DLR_BEACON
+	dbg_bcn += 4;
+#endif
+	dlr->skip_beacon = false;
+
+	/* Indicate in process of accepting beacons. */
+	dlr->drop_beacon++;
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_beacon_drop(dlr, 0);
+#endif
+	dlr->drop_beacon = false;
+	memset(&dlr->last_beacon[0], 0, sizeof(struct ksz_dlr_tx_frame));
+	memset(&dlr->last_beacon[1], 0, sizeof(struct ksz_dlr_tx_frame));
+}  /* acceptBeacons */
+
+static void dropBeacons(struct ksz_dlr_info *info)
+{
+	int drop = info->member;
+
+#ifdef DBG_DLR_BEACON
+	dbg_msg("%s\n", __func__);
+#endif
+	if (info->node == DLR_ACTIVE_SUPERVISOR)
+		drop = 0x8000;
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_beacon_drop(info, drop);
+#endif
+	info->skip_beacon = !!drop;
+}  /* dropBeacons */
+
+static int seq_ahead(u32 start, u32 num)
+{
+	u32 ahead;
+	u32 behind;
+
+	ahead = num - start;
+	behind = start - num;
+	return ahead < behind;
+}
+
+static int seq_in(u32 start, u32 end, u32 num)
+{
+	end -= start;
+	num -= start;
+	return 0 <= num && num <= end;
+}
+
+static struct ksz_dlr_node_info *find_dlr_node(struct ksz_dlr_info *info,
+	u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < info->attrib.participants_cnt; i++)
+		if (!memcmp(info->nodes[i].signon.addr, addr, ETH_ALEN))
+			return &info->nodes[i];
+	return NULL;
+}  /* find_dlr_node */
+
+static void dbg_dlr(struct ksz_dlr_info *info, char *msg)
+{
+	dbg_msg(" %s: S=%d R=%d L=%d ", msg, info->state, info->ring_state,
+		info->LastBcnRcvPort);
+	dbg_msg("d=%d:%d D=%d:%d l=%d:%d r=%d:%d R=%d:%d t=%d:%d T=%d:%d\n",
+		info->p1_down, info->p2_down,
+		info->one_down, info->both_down,
+		info->p1_lost, info->p2_lost,
+		info->p1_rcvd, info->p2_rcvd,
+		info->one_rcvd, info->both_rcvd,
+		info->p1_timeout, info->p2_timeout,
+		info->one_timeout, info->both_timeout);
+}
+
+#ifdef DBG_DLR_STATE
+static void dlr_print(struct ksz_dlr_info *info, char *msg)
+{
+	if (info->overrides & DLR_TEST)
+		printk(KERN_INFO "%s\n", msg);
+}
+#endif
+
+static int dlr_chk_beacon_timeout(struct ksz_dlr_info *info, int p,
+	struct ksz_dlr_frame *beacon,
+	struct ksz_dlr_super_info *active,
+	struct ksz_dlr_super_info *super)
+{
+	u32 crc;
+	int i;
+	struct ksz_dlr_super_info *next;
+	struct ksz_dlr_super_info *first = NULL;
+	struct ksz_dlr_super_info *found = NULL;
+	int accept = false;
+
+	if (DLR_ACTIVE_SUPERVISOR != info->node)
+		return accept;
+	crc = ether_crc(ETH_ALEN + 1, super->prec_addr);
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (!next->crc && !first)
+			first = next;
+		if (next->crc == crc) {
+			found = next;
+			break;
+		}
+	}
+	if (!found)
+		found = first;
+	if (found) {
+		u32 interval = ntohl(beacon->data.beacon.interval);
+
+		found->cnt++;
+		found->port = p;
+
+		/* First time. */
+		if (!found->crc) {
+#ifdef DBG_DLR_SUPERVISOR
+			dbg_msg("  A %x=%02x:%02x:%02x:%02x:%02x:%02x  "
+				"S %x=%02x:%02x:%02x:%02x:%02x:%02x  ",
+				active->prec_addr[0],
+				active->prec_addr[1],
+				active->prec_addr[2],
+				active->prec_addr[3],
+				active->prec_addr[4],
+				active->prec_addr[5],
+				active->prec_addr[6],
+				super->prec_addr[0],
+				super->prec_addr[1],
+				super->prec_addr[2],
+				super->prec_addr[3],
+				super->prec_addr[4],
+				super->prec_addr[5],
+				super->prec_addr[6]);
+			dbg_msg("cnt: %d %x\n", found->cnt, crc);
+#endif
+			found->crc = crc;
+			memcpy(found->prec_addr, super->prec_addr,
+				ETH_ALEN + 1);
+		}
+		found->timeout[p] += interval;
+		if (found->timeout[p] > (10000 + 4 * interval) &&
+		    !found->sent) {
+dbg_msg("rogue: %u\n", found->timeout[p]);
+			found->sent = 1;
+			accept = true;
+		}
+	}
+	return accept;
+}  /* dlr_chk_beacon_timeout */
+
+static void dbg_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_SUPERVISOR
+	struct ksz_dlr_super_info *next;
+	int i;
+
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (next->crc) {
+			dbg_msg("  super %d %x=%02x:%02x:%02x\n",
+				next->cnt,
+				next->prec_addr[0],
+				next->prec_addr[4],
+				next->prec_addr[5],
+				next->prec_addr[6]);
+		}
+	}
+#endif
+}  /* dbg_supervisor */
+
+static void dlr_clr_supervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_super_info *next;
+	int i;
+
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (next->crc) {
+			if (next->last_cnt == next->cnt) {
+#ifdef DBG_DLR_SUPERVISOR
+				dbg_msg("  clr %d %u:%u %x=%02x:%02x:%02x\n",
+					next->cnt,
+					next->timeout[0], next->timeout[1],
+					next->prec_addr[0],
+					next->prec_addr[4],
+					next->prec_addr[5],
+					next->prec_addr[6]);
+#endif
+				memset(next, 0,
+					sizeof(struct ksz_dlr_super_info));
+			} else
+				next->last_cnt = next->cnt;
+		}
+	}
+}  /* dlr_clr_supervisor */
+
+static void update_vlan(struct ksz_dlr_info *info, u16 vid)
+{
+	if (info->vid == vid)
+		return;
+	setup_vlan_table(info, info->vid, false);
+	info->vid = vid;
+	setup_vlan_table(info, info->vid, true);
+	info->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+	memcpy(info->signon_frame, &info->frame, sizeof(struct vlan_ethhdr));
+}  /* update_vlan */
+
+static void setup_dir(struct ksz_dlr_info *info, int port)
+{
+	info->rx_port = port;
+	info->tx_port = (port + 1) & 1;
+	setupDir(info, port);
+}  /* setup_dir */
+
+static void dlr_notify_link_lost(struct ksz_dlr_info *dlr)
+{
+	if (dlr->dev_info) {
+		u8 buf[sizeof(struct ksz_resp_msg) +
+			sizeof(struct ksz_dlr_active_node) * 2];
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) buf;
+
+		msg->module = DEV_MOD_DLR;
+		msg->cmd = DEV_INFO_DLR_LINK;
+		msg->resp.data[0] = 0;
+		if (dlr->p1_down)
+			msg->resp.data[0] |= 0x01;
+		if (dlr->p2_down)
+			msg->resp.data[0] |= 0x02;
+		if (dlr->p1_lost)
+			msg->resp.data[0] |= 0x04;
+		if (dlr->p2_lost)
+			msg->resp.data[0] |= 0x08;
+		memcpy(&msg->resp.data[1], dlr->attrib.last_active,
+			sizeof(struct ksz_dlr_active_node) * 2);
+		sw_setup_msg(dlr->dev_info, msg, sizeof(struct ksz_resp_msg) +
+			sizeof(struct ksz_dlr_active_node) * 2, NULL, NULL);
+	}
+}  /* dlr_notify_link_lost */
+
+static void dlr_notify_cfg_change(struct ksz_dlr_info *dlr, int notify)
+{
+	if (dlr->dev_info) {
+		struct ksz_resp_msg msg;
+
+		msg.module = DEV_MOD_DLR;
+		msg.cmd = DEV_INFO_DLR_CFG;
+		msg.resp.data[0] = notify;
+		sw_setup_msg(dlr->dev_info, &msg, sizeof(struct ksz_resp_msg),
+			NULL, NULL);
+	}
+}  /* dlr_notify_cfg_change */
+
+static int checkBeacon(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_super_info active;
+	struct ksz_dlr_super_info super;
+	u32 interval;
+	u32 timeout;
+	int cmp = 0;
+	int accept = false;
+	struct ksz_dlr_tx_frame beacon_frame;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *beacon = frame->body;
+	u32 seqid = ntohl(beacon->hdr.seqid);
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+
+	if (info->state && (info->overrides & DLR_TEST_SEQ) &&
+	    (info->seqid_last[port] + 1 != seqid) && !info->seqid_cnt) {
+		info->seqid_cnt = 1000;
+		dbg_msg("bcn seq %d: %d %d\n", port,
+			info->seqid_last[port], seqid);
+	}
+	if (info->seqid_cnt > 0)
+		info->seqid_cnt--;
+	info->seqid_last[port] = seqid;
+
+	/* Announce node does not accept beacons. */
+	if (DLR_ANNOUNCE_NODE == info->node)
+		return accept;
+
+#if 1
+/*
+ * THa  2015/11/03
+ * Hardware generates wrong ring state.
+ */
+	if (0 == beacon->data.beacon.ring_state) {
+		beacon->data.beacon.ring_state = 2;
+		if (!(info->overrides & DLR_BEACON_STATE_HACK)) {
+			info->overrides |= DLR_BEACON_STATE_HACK;
+			printk(KERN_INFO "beacon fault state value wrong\n");
+		}
+	}
+#endif
+
+	/* Ignore own beacon if stopped. */
+	/*
+	 * Only active supervisor can receive its own beacons because of
+	 * self-address filtering.
+	 */
+	if (!memcmp(info->src_addr, vlan->h_source, ETH_ALEN)) {
+		if (info->skip_beacon && info->drop_beacon && dbg_leak)
+			dbg_msg(" ^ ");
+		if (!info->start)
+			return accept;
+	}
+
+	/* Determine precedence level. */
+	super.prec_addr[0] = beacon->data.beacon.precedence;
+	memcpy(&super.prec_addr[1], vlan->h_source, ETH_ALEN);
+
+	/* Compare own address first if supervisor capable. */
+	if (info->node >= DLR_SUPERVISOR) {
+		active.prec_addr[0] = attrib->super_cfg.prec;
+		memcpy(&active.prec_addr[1], info->src_addr, ETH_ALEN);
+		cmp = memcmp(&super, &active, ETH_ALEN + 1);
+	}
+
+	if (cmp >= 0) {
+		active.prec_addr[0] = attrib->active_super_prec;
+		memcpy(&active.prec_addr[1], attrib->active_super_addr.addr,
+			ETH_ALEN);
+		cmp = memcmp(&super, &active, ETH_ALEN + 1);
+	}
+
+	/* Ignore lower precedence beacon. */
+	if (cmp < 0) {
+		/* Simulate beacon timeout as hardware cannot catch that. */
+		accept = dlr_chk_beacon_timeout(info, port, beacon, &active,
+			&super);
+
+		return accept;
+	} else if (cmp > 0) {
+#ifdef DBG_DLR_SUPERVISOR
+		dbg_msg("new %d p:%d %08x A=%02x:%02x:%02x:%02x:%02x:%02x  "
+			"S=%02x:%02x:%02x:%02x:%02x:%02x\n",
+			cmp,
+			beacon->data.beacon.precedence,
+			beacon->hdr.ip_addr,
+			attrib->active_super_addr.addr[0],
+			attrib->active_super_addr.addr[1],
+			attrib->active_super_addr.addr[2],
+			attrib->active_super_addr.addr[3],
+			attrib->active_super_addr.addr[4],
+			attrib->active_super_addr.addr[5],
+			vlan->h_source[0],
+			vlan->h_source[1],
+			vlan->h_source[2],
+			vlan->h_source[3],
+			vlan->h_source[4],
+			vlan->h_source[5]);
+		dbg_msg("  new prec: %d >= %d %d %d s:%d\n",
+			beacon->data.beacon.precedence,
+			attrib->active_super_prec, info->precedence,
+			attrib->super_cfg.prec,
+			beacon->data.beacon.ring_state);
+#endif
+		accept = true;
+		dlr_set_addr(&attrib->active_super_addr,
+			beacon->hdr.ip_addr, vlan->h_source);
+		attrib->active_super_prec = beacon->data.beacon.precedence;
+		info->new_supervisor = 1;
+		info->p1_set = info->p2_set = 1;
+		info->p1_rcvd = info->p2_rcvd =
+		info->one_rcvd = info->both_rcvd = 0;
+		info->p1_timeout = info->p2_timeout =
+		info->one_timeout = info->both_timeout = 0;
+
+		/* Set in following code. */
+		info->LastBcnRcvPort = 0;
+
+#ifdef DBG_DLR_STATE
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			dbg_dlr(info, "stop being active");
+#ifdef DBG_DLR_BEACON
+			dbg_bcn = 2;
+#endif
+		}
+#endif
+		memset(&info->last_beacon[0], 0,
+			sizeof(struct ksz_dlr_tx_frame));
+		memset(&info->last_beacon[1], 0,
+			sizeof(struct ksz_dlr_tx_frame));
+		memset(&info->beacon_info[0].last, 0,
+			sizeof(struct ksz_dlr_beacon));
+		memset(&info->beacon_info[1].last, 0,
+			sizeof(struct ksz_dlr_beacon));
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+
+	/* Process accepted beacon. */
+	info->LastBcnRcvPort |= port ? 2 : 1;
+
+	interval = ntohl(beacon->data.beacon.interval);
+	timeout = ntohl(beacon->data.beacon.timeout);
+
+#ifdef CONFIG_HAVE_DLR_HW
+	if (info->skip_beacon &&
+	    !memcmp(vlan->h_source, info->beacon_addr, ETH_ALEN)) {
+		if (dbg_leak > 0) {
+			dbg_msg(" ??: p=%d %d n=%d %x "
+				"%02x:%02x:%02x:%02x:%02x:%02x "
+				"%02x:%02x:%02x:%02x:%02x:%02x\n",
+				port, info->skip_beacon, info->node, seqid,
+				vlan->h_dest[0],
+				vlan->h_dest[1],
+				vlan->h_dest[2],
+				vlan->h_dest[3],
+				vlan->h_dest[4],
+				vlan->h_dest[5],
+				vlan->h_source[0],
+				vlan->h_source[1],
+				vlan->h_source[2],
+				vlan->h_source[3],
+				vlan->h_source[4],
+				vlan->h_source[5]);
+			--dbg_leak;
+		}
+	}
+#endif
+	if (!accept && info->skip_beacon)
+		return accept;
+	else if (beacon_info->timeout && !info->drop_beacon) {
+		beacon_info->timeout += interval;
+		if (beacon_info->timeout > timeout) {
+			info->drop_beacon = true;
+			accept = true;
+		}
+	}
+
+	/* Start the beacon drop process. */
+	if (!beacon_info->timeout &&
+	    !info->skip_beacon && !info->drop_beacon &&
+	    !newSupervisor &&
+	    RING_NORMAL_STATE == beacon->data.beacon.ring_state &&
+	    RING_NORMAL_STATE == info->ring_state &&
+	    (DLR_NORMAL_STATE == info->state ||
+	    DLR_ACTIVE_NORMAL_STATE == info->state)) {
+		beacon_info->timeout = 1;
+#ifdef DBG_DLR_BEACON
+		dbg_msg("  ready to drop: %d=r:%d\n",
+			port, beacon->data.beacon.ring_state);
+		dbg_bcn = 3;
+#endif
+	}
+
+	/* Zero out the sequence id for comparison. */
+	memcpy(&beacon_frame.vlan, frame->vlan, sizeof(struct vlan_ethhdr));
+	memcpy(&beacon_frame.body, frame->body, sizeof(struct ksz_dlr_frame));
+	beacon_frame.body.hdr.seqid = 0;
+	if (!accept)
+		accept = memcmp(&beacon_frame, &info->last_beacon[port],
+			sizeof(struct ksz_dlr_tx_frame));
+	if (accept)
+		memcpy(&info->last_beacon[port], &beacon_frame,
+			sizeof(struct ksz_dlr_tx_frame));
+
+	/* Try to process as few beacons as possible. */
+	if (memcmp(&beacon_info->last, &beacon->data.beacon,
+	    sizeof(struct ksz_dlr_beacon))) {
+		memcpy(&beacon_info->last, &beacon->data.beacon,
+			sizeof(struct ksz_dlr_beacon));
+		info->seqid_accept[port] = seqid;
+	}
+
+#ifdef DBG_DLR_BEACON
+	if (accept || dbg_bcn > 0) {
+		dbg_msg("b: %d=%d:%d %04x; %02x %02x:%02x:%02x %08lx; "
+			"%d %d %lx %d\n",
+			port,
+			beacon->data.beacon.ring_state, info->ring_state,
+			ntohs(vlan->h_vlan_TCI),
+			beacon->data.beacon.precedence,
+			vlan->h_source[3], vlan->h_source[4], vlan->h_source[5],
+			seqid, info->new_supervisor,
+			beacon_info->timeout,
+			jiffies, dbg_bcn);
+	}
+	if (dbg_bcn)
+		--dbg_bcn;
+#endif
+	return accept;
+}  /* checkBeacon */
+
+static int handleBeacon(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	u32 interval;
+	u32 timeout;
+	u16 vid;
+	int cmp = 0;
+	int update = false;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *beacon = frame->body;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = info->sw_dev;
+#endif
+
+	/* Only accepted beacon from active supervisor is processed. */
+	cmp = memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		ETH_ALEN);
+	if (cmp) {
+#if 0
+		struct ksz_dlr_super_info found;
+
+		memcpy(&found.prec_addr[1], vlan->h_source, ETH_ALEN);
+		found.port = port;
+		info->rogue_super = &found;
+		dlr_tx_learning_update(info);
+#endif
+		return false;
+	}
+
+	/* Process accepted beacon. */
+	if (info->new_supervisor) {
+		if (info->notifications & DLR_INFO_CFG_CHANGE)
+			dlr_notify_cfg_change(info, 1);
+		update = true;
+	}
+
+	interval = ntohl(beacon->data.beacon.interval);
+	timeout = ntohl(beacon->data.beacon.timeout);
+
+	/* Used to determine beacon timeout in software simulation. */
+	beacon_info->rcv_once = 1;
+	beacon_info->timeout_start = 0;
+
+	if (!info->skip_beacon && info->drop_beacon) {
+		dropBeacons(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+		return update;
+	}
+	if (update && info->skip_beacon)
+		acceptBeacons(info);
+
+	/* Not running as supervisor. */
+	if ((DLR_ACTIVE_SUPERVISOR != info->node &&
+	    info->ring_state != beacon->data.beacon.ring_state) ||
+	    update) {
+		if ((info->notifications & DLR_INFO_CFG_CHANGE) &&
+		    info->ring_state != beacon->data.beacon.ring_state)
+			dlr_notify_cfg_change(info, 2);
+		info->ring_state = beacon->data.beacon.ring_state;
+
+		/* Set in following code. */
+		info->p1_rcvd = info->p2_rcvd =
+		info->one_rcvd = info->both_rcvd = 0;
+		beacon_info->timeout = 0;
+
+#ifdef DBG_DLR_STATE
+		if (RING_FAULT_STATE == info->ring_state) {
+			dbg_dlr(info, "ring fault");
+		}
+#endif
+		/* Get back into backup mode. */
+		if (7 == info->beacon_timeout_ports)
+			info->beacon_timeout_ports = 0;
+	}
+	if (1 == port) {
+		info->p2_rcvd = 1;
+		info->p2_timeout = 0;
+		info->p2_down = 0;
+	} else {
+		info->p1_rcvd = 1;
+		info->p1_timeout = 0;
+		info->p1_down = 0;
+	}
+	if (!info->p1_timeout && !info->p2_timeout)
+		info->one_timeout = 0;
+	info->both_timeout = 0;
+
+	/* Change down state as beacon can be received before link status. */
+	if (!info->p1_down && !info->p2_down)
+		info->one_down = 0;
+	info->both_down = 0;
+
+	if (info->p1_rcvd && info->p2_rcvd) {
+
+		/* Running as supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR == info->node ||
+		    RING_NORMAL_STATE == info->ring_state) {
+			if (!info->both_rcvd)
+				update = true;
+			info->both_rcvd = 1;
+			info->one_rcvd = 0;
+		}
+	} else {
+		if (!info->one_rcvd)
+			update = true;
+		info->one_rcvd = 1;
+		beacon_info->timeout = 0;
+	}
+	if (attrib->active_super_prec != beacon->data.beacon.precedence) {
+		attrib->active_super_prec = beacon->data.beacon.precedence;
+	}
+	vid = info->vid;
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q))
+		vid = ntohs(vlan->h_vlan_TCI) & ((1 << VLAN_PRIO_SHIFT) - 1);
+	if (info->vid != vid) {
+
+		/* Use current VID. */
+		update_vlan(info, vid);
+		if (attrib->cap & DLR_CAP_SUPERVISOR_CAPABLE)
+			attrib->super_cfg.vid = vid;
+		if (attrib->super_cfg.enable) {
+			attrib->super_cfg.vid = vid;
+#ifdef CONFIG_HAVE_DLR_HW
+			sw->ops->acquire(sw);
+			dlr_hw_set_vlan_id(sw, info->vid);
+			sw->ops->release(sw);
+#endif
+		}
+	}
+	if (info->beacon_interval != interval) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("interval %u %u\n", info->beacon_interval, interval);
+#endif
+
+		/* Use current beacon interval. */
+		info->beacon_interval = interval;
+		if (attrib->cap & DLR_CAP_SUPERVISOR_CAPABLE)
+			attrib->super_cfg.beacon_interval = interval;
+		if (attrib->super_cfg.enable) {
+			attrib->super_cfg.beacon_interval = interval;
+#ifdef CONFIG_HAVE_DLR_HW
+			sw->ops->acquire(sw);
+			dlr_hw_set_interval(sw, info->beacon_interval);
+			sw->ops->release(sw);
+#endif
+		}
+	}
+	if (info->beacon_timeout != timeout) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("timeout %u %u\n", info->beacon_timeout, timeout);
+#endif
+
+		/* Use current beacon timeout. */
+		info->beacon_timeout = timeout;
+		if (attrib->cap & DLR_CAP_SUPERVISOR_CAPABLE)
+			attrib->super_cfg.beacon_timeout = timeout;
+		if (attrib->super_cfg.enable) {
+			attrib->super_cfg.beacon_timeout = timeout;
+#ifdef CONFIG_HAVE_DLR_HW
+			sw->ops->acquire(sw);
+			dlr_hw_set_timeout(sw, info->beacon_timeout);
+			sw->ops->release(sw);
+#endif
+		}
+		info->p1_set = info->p2_set = 1;
+	}
+#ifdef DBG_DLR_BEACON
+	if (update || dbg_bcn > 0) {
+		u32 seqid = ntohl(beacon->hdr.seqid);
+		dbg_msg("B: %d=%d:%d; r=%d:%d R=%d:%d; %08lx %d %lx\n",
+			port,
+			beacon->data.beacon.ring_state, info->ring_state,
+			info->p1_rcvd, info->p2_rcvd,
+			info->one_rcvd, info->both_rcvd,
+			seqid, info->new_supervisor,
+			jiffies);
+	}
+#endif
+
+	beacon_info->interval = 0;
+	if (info->p1_rcvd && info->p1_set) {
+		info->p1_set = 0;
+#ifdef CONFIG_HAVE_ACL_HW
+		setup_acl_beacon_timeout(info, info->ports[0]);
+#endif
+	}
+	if (info->p2_rcvd && info->p2_set) {
+		info->p2_set = 0;
+#ifdef CONFIG_HAVE_ACL_HW
+		setup_acl_beacon_timeout(info, info->ports[1]);
+#endif
+	}
+	return update;
+}  /* handleBeacon */
+
+static int handleSignOn(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	int i;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *signon = frame->body;
+	u16 num = ntohs(signon->data.signon.num);
+	struct ksz_dlr_node *node = signon->data.signon.node;
+
+	/* Ignore if not NORMAL_STATE. */
+	if (info->state != DLR_ACTIVE_NORMAL_STATE &&
+	    info->state != DLR_NORMAL_STATE &&
+	    !(info->overrides & DLR_TEST)) {
+#ifdef DBG_DLR_ANN_SIGNON
+		dbg_dlr(info, " ?signon");
+#endif
+#ifdef DBG_DLR_BEACON
+		dbg_bcn = 10;
+#endif
+		return false;
+	}
+#ifdef DBG_DLR_BEACON
+	dbg_bcn = 2;
+#endif
+	if (DLR_ACTIVE_SUPERVISOR == info->node &&
+	    !memcmp(node->addr, info->src_addr, ETH_ALEN)) {
+		struct ksz_dlr_node_info *cur;
+
+#ifdef DBG_DLR_SIGNON
+		dbg_msg("signon: %d; %d; %02x\n", num, port, vlan->h_dest[0]);
+		dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x\n",
+			node->addr[0],
+			node->addr[1],
+			node->addr[2],
+			node->addr[3],
+			node->addr[4],
+			node->addr[5]);
+#endif
+		for (i = 0; i < num; i++, node++) {
+			if (i && !memcmp(node->addr, info->src_addr, ETH_ALEN))
+				continue;
+#ifdef DBG_DLR_SIGNON
+			dbg_msg("%d %02x:%02x:%02x:%02x:%02x:%02x\n", i,
+				node->addr[0],
+				node->addr[1],
+				node->addr[2],
+				node->addr[3],
+				node->addr[4],
+				node->addr[5]);
+#endif
+			cur = &info->nodes[info->attrib.participants_cnt];
+			memcpy(&cur->signon, node,
+				sizeof(struct ksz_dlr_node));
+			cur->p1_down = cur->p2_down =
+			cur->p1_lost = cur->p2_lost = 0;
+			info->attrib.participants_cnt++;
+		}
+
+		/* Addressed to the supervisor instead of multicast address. */
+		if (!memcmp(info->src_addr, vlan->h_dest, ETH_ALEN)) {
+			memcpy(info->signon_addr, vlan->h_source,
+				ETH_ALEN);
+			dlr_tx_signon(info, 0);
+		} else
+			disableSignOnTimer(info);
+	} else {
+		int len;
+		struct ksz_dlr_tx_frame *tx = (struct ksz_dlr_tx_frame *)
+			info->signon_frame;
+
+		if ((info->overrides & DLR_TEST) && info->ignore_req) {
+			++info->req_cnt[0];
+			if (info->req_cnt[0] <= info->ignore_req)
+				printk(KERN_INFO
+					"ignore SignOn: %d\n",
+					info->req_cnt[0]);
+			if (info->req_cnt[0] <= info->ignore_req)
+				return false;
+			info->req_cnt[0] = 0;
+		}
+		len = sizeof(struct ksz_dlr_hdr) +
+			sizeof(struct ksz_dlr_signon) +
+			(num - 1) * sizeof(struct ksz_dlr_node);
+		memcpy(info->signon_frame, vlan, ETH_ALEN * 2);
+		memcpy(&tx->body, signon, len);
+		len += sizeof(struct vlan_ethhdr);
+		info->rx_port = port;
+		info->tx_port = (port + 1) & 1;
+		dlr_tx_signon(info, len);
+		if (info->active_port != (1 << info->ports[info->rx_port]))
+			setupDir(info, port);
+	}
+	return false;
+}  /* handleSignOn */
+
+static int handleLocateFault(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST)) {
+		dbg_msg(" ?%s %d\n", __func__, port);
+		return false;
+	}
+	if (info->node != DLR_ACTIVE_SUPERVISOR &&
+	    !memcmp(vlan->h_source, info->attrib.active_super_addr.addr,
+	    ETH_ALEN)) {
+		info->fault_jiffies = jiffies;
+		if (info->p1_down || info->p2_down)
+			dlr_tx_status(info, 0);
+		else if (!info->neigh_chk) {
+			info->neigh_chk = 1;
+			ksz_start_timer(&info->neigh_chk_timer_info,
+				info->neigh_chk_timer_info.period);
+			info->neigh_chk_timer_info.max = 3;
+			info->p1_lost = info->p2_lost = 0;
+			info->port_chk[0] = info->port_chk[1] = 1;
+			dlr_tx_chk_req(info, 0);
+			dlr_tx_chk_req(info, 1);
+		}
+	} else {
+		dbg_msg("%s ignored\n", __func__);
+	}
+	return false;
+}  /* handleLocateFault */
+
+static int handleAnnounce(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	u32 seqid;
+	u16 vid;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *announce = frame->body;
+	int new = 0;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	/* Supervisor direction is not set yet. */
+	if (RING_NORMAL_STATE == announce->data.announce.ring_state &&
+	    DLR_NORMAL_STATE == info->state &&
+	    (!info->active_port || info->rx_port != port)) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("%s p=%d r=%d x%x\n", __func__, port, info->rx_port,
+			info->active_port);
+#endif
+		setup_dir(info, port);
+	}
+
+#ifdef DBG_DLR_STATE
+	if (DLR_ANNOUNCE_NODE != info->node) {
+		if (announce->data.announce.ring_state != info->ring_state)
+			dbg_msg("  ann: p=%d r=%d != %d\n",
+				port, info->ring_state,
+				announce->data.announce.ring_state);
+		else if (RING_NORMAL_STATE == info->ring_state &&
+			 info->state != DLR_NORMAL_STATE)
+			dbg_msg("  ann ring normal: p=%d s=%d\n",
+				port, info->state);
+	}
+#endif
+
+	/* Rely on Announce frame to determine ring state. */
+	if (info->skip_beacon) {
+		if (memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		    ETH_ALEN)) {
+dbg_msg(" diff super\n");
+			dlr_set_addr(&attrib->active_super_addr,
+				announce->hdr.ip_addr, vlan->h_source);
+		}
+		if (!memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		    ETH_ALEN) && info->ring_state !=
+		    announce->data.announce.ring_state) {
+			if ((info->notifications & DLR_INFO_CFG_CHANGE) &&
+			    info->ring_state !=
+			    announce->data.announce.ring_state)
+				dlr_notify_cfg_change(info, 2);
+			info->ring_state = announce->data.announce.ring_state;
+			acceptBeacons(info);
+#ifdef DBG_DLR_BEACON
+			dbg_bcn += 4;
+#endif
+			return true;
+		}
+	}
+
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+#if 0
+		dbg_msg("%s ignored %d %d %d\n", __func__, port,
+			announce->data.announce.ring_state, info->skip_beacon);
+		if (vlan)
+			dbg_msg("src: %02x:%02x:%02x:%02x:%02x:%02x\n",
+				vlan->h_source[0], vlan->h_source[1],
+				vlan->h_source[2], vlan->h_source[3],
+				vlan->h_source[4], vlan->h_source[5]);
+#endif
+		return false;
+	}
+	if (DLR_SUPERVISOR == info->node)
+		goto done;
+	if (DLR_ANNOUNCE_NODE != info->node)
+		return false;
+
+	info->rx_port = port;
+	info->tx_port = (port + 1) & 1;
+	seqid = ntohl(announce->hdr.seqid);
+
+	if (memcmp(vlan->h_source, attrib->active_super_addr.addr, ETH_ALEN) ||
+	    info->ann_timeout) {
+		memcpy(info->last_sup.addr, attrib->active_super_addr.addr,
+			ETH_ALEN);
+		dlr_set_addr(&attrib->active_super_addr,
+			announce->hdr.ip_addr, vlan->h_source);
+		info->new_supervisor = 1;
+		new = 1;
+		if (info->notifications & DLR_INFO_CFG_CHANGE)
+			dlr_notify_cfg_change(info, 1);
+	} else {
+		/* Check valid sequence number. */
+		if (!seq_ahead(info->seqid_announce, seqid))
+			return false;
+	}
+	info->seqid_announce = seqid;
+	if (announce->data.announce.ring_state != info->ring_state || new) {
+		if ((info->notifications & DLR_INFO_CFG_CHANGE) &&
+		    info->ring_state != announce->data.announce.ring_state)
+			dlr_notify_cfg_change(info, 2);
+		info->ring_state = announce->data.announce.ring_state;
+		info->ann_rcvd = 1;
+		new = 1;
+	}
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		vid = ntohs(vlan->h_vlan_TCI) & ((1 << VLAN_PRIO_SHIFT) - 1);
+		if (vid != info->vid) {
+			update_vlan(info, vid);
+			new = 1;
+		}
+	}
+
+done:
+	info->ann_timeout = 0;
+
+	ksz_stop_timer(&info->announce_timeout_timer_info);
+	ksz_start_timer(&info->announce_timeout_timer_info,
+		info->announce_timeout_timer_info.period);
+	info->announce_timeout_timer_info.max = 1;
+	return new;
+}  /* handleAnnounce */
+
+static int handleNeighChkReq(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *req = frame->body;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    info->state != DLR_ACTIVE_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO "req: p=%d s=%08x %d; %lx\n",
+			port, ntohl(req->hdr.seqid), req->hdr.src_port,
+			jiffies);
+	}
+	if ((info->overrides & DLR_TEST) && info->ignore_req) {
+		++info->req_cnt[port];
+		if (info->req_cnt[port] <= info->ignore_req)
+			return false;
+		info->req_cnt[port] = 0;
+	}
+	info->port_rcv[port] = req->hdr.src_port;
+	info->seqid_rcv[port] = ntohl(req->hdr.seqid);
+	dlr_tx_chk_resp(info, port);
+	return false;
+}  /* handleNeighChkReq */
+
+static int handleNeighChkResp(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *resp = frame->body;
+	int src_port = port ? DLR_PORT_2 : DLR_PORT_1;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    info->state != DLR_ACTIVE_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO "resp: p=%d s=%08x %d; %08x - %08x; %lx\n",
+			port, ntohl(resp->hdr.seqid),
+			resp->data.neigh_chk_resp.src_port,
+			info->seqid_first[port],
+			info->seqid_chk[port], jiffies);
+	}
+	if (src_port == resp->data.neigh_chk_resp.src_port) {
+		u32 seqid = ntohl(resp->hdr.seqid);
+
+		if (seq_in(info->seqid_first[port], info->seqid_chk[port],
+		    seqid)) {
+			if (port)
+				info->p2_down = info->p2_lost = 0;
+			else
+				info->p1_down = info->p1_lost = 0;
+			info->port_chk[port] = 0;
+			if (!info->port_chk[(port + 1) & 1]) {
+				ksz_stop_timer(&info->neigh_chk_timer_info);
+				info->neigh_chk = 0;
+			}
+		}
+	}
+	return false;
+}  /* handleNeighChkResp */
+
+static int handleFlushTables(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *flush = frame->body;
+
+	/* Ignore if not in NORMAL_STATE or FAULT_STATE. */
+	if (info->state <= DLR_IDLE_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	dlr_flush(info);
+	if (flush->data.flush.learning_update_enable)
+		dlr_tx_learning_update(info);
+
+	return false;
+}  /* handleFlushTables */
+
+static int handleLinkStatus(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *status = frame->body;
+
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO
+			"link: %02x:%02x:%02x:%02x:%02x:%02x %d:%d %d\n",
+			vlan->h_source[0], vlan->h_source[1], vlan->h_source[2],
+			vlan->h_source[3], vlan->h_source[4], vlan->h_source[5],
+			status->data.status.port1_active,
+			status->data.status.port2_active,
+			status->data.status.neighbor);
+	}
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		int i;
+		struct ksz_dlr_node_info *node;
+
+#ifdef DBG_DLR_STATE
+		dbg_msg("link: %02x:%02x:%02x:%02x:%02x:%02x %x %d:%d %d\n",
+			vlan->h_source[0],
+			vlan->h_source[1],
+			vlan->h_source[2],
+			vlan->h_source[3],
+			vlan->h_source[4],
+			vlan->h_source[5],
+			ntohl(status->hdr.seqid),
+			status->data.status.port1_active,
+			status->data.status.port2_active,
+			status->data.status.neighbor);
+#endif
+		for (i = 1; i < info->attrib.participants_cnt; i++) {
+			node = &info->nodes[i];
+			if (!memcmp(node->signon.addr, vlan->h_source,
+			    ETH_ALEN)) {
+				if (status->data.status.neighbor) {
+					if (status->data.status.port1_active) {
+						node->p1_down = 0;
+						node->p1_lost = 0;
+					} else {
+						node->p1_lost = 1;
+					}
+					if (status->data.status.port2_active) {
+						node->p2_down = 0;
+						node->p2_lost = 0;
+					} else {
+						node->p2_lost = 1;
+					}
+				} else {
+					if (status->data.status.port1_active) {
+						node->p1_down = 0;
+					} else {
+						node->p1_down = 1;
+					}
+					if (status->data.status.port2_active) {
+						node->p2_down = 0;
+					} else {
+						node->p2_down = 1;
+					}
+				}
+				break;
+			}
+		}
+		if (!status->data.status.port1_active ||
+		    !status->data.status.port2_active) {
+			dlr_set_addr(&info->attrib.last_active[port],
+				status->hdr.ip_addr, vlan->h_source);
+			if (info->notifications & DLR_INFO_LINK_LOST)
+				dlr_notify_link_lost(info);
+			return true;
+		}
+	} else {
+		dbg_msg("%s ignored\n", __func__);
+	}
+	return false;
+}  /* handleLinkStatus */
+
+static int handleLearningUpdate(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+
+#ifdef DBG_DLR_SUPERVISOR
+	dbg_msg("%s %d %d\n", __func__, info->node, port);
+#endif
+	if (!memcmp(info->src_addr, vlan->h_dest, ETH_ALEN)) {
+		if (!newSupervisor) {
+dbg_msg(" %d %d %d  ", info->state, info->node, info->precedence);
+dbg_msg(" to self!\n");
+			dlr_chk_supervisor(info);
+		}
+	}
+	if (DLR_SUPERVISOR <= info->node) {
+		if (!memcmp(info->src_addr, vlan->h_source, ETH_ALEN)) {
+dbg_msg("%s %d\n", __func__, info->node);
+		}
+	}
+	return false;
+}  /* handleLearningUpdate */
+
+static void dlr_clr_last_beacon(struct ksz_dlr_info *info, int missed)
+{
+	int last = info->LastBcnRcvPort;
+
+	info->LastBcnRcvPort &= ~missed;
+	if (last != info->LastBcnRcvPort &&
+	    0 == info->LastBcnRcvPort) {
+#ifdef DBG_DLR_STATE
+		if (DLR_SUPERVISOR == info->node)
+			dbg_msg("become active\n");
+		else if (DLR_ACTIVE_SUPERVISOR != info->node)
+			dbg_msg("become idle\n");
+#endif
+	}
+}  /* dlr_clr_last_beacon */
+
+static int handleBeaconTimeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+
+	if (1 == port) {
+		info->p2_rcvd = 0;
+		info->p2_timeout = 1;
+	} else {
+		info->p1_rcvd = 0;
+		info->p1_timeout = 1;
+	}
+	memset(&beacon_info->last, 0, sizeof(struct ksz_dlr_beacon));
+	beacon_info->timeout = 0;
+	memset(&info->last_beacon[port], 0, sizeof(struct ksz_dlr_tx_frame));
+	if (info->p1_rcvd || info->p2_rcvd)
+		info->one_rcvd = 1;
+	else
+		info->one_rcvd = 0;
+	info->both_rcvd = 0;
+	if ((info->p1_timeout && info->p2_timeout)) {
+		info->both_timeout = 1;
+		info->one_timeout = 0;
+		info->chk_hw = 1;
+	} else
+		info->one_timeout = 1;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "timeout");
+#endif
+	if (info->both_timeout)
+		info->p1_timeout = info->p2_timeout = 0;
+	dlr_clr_last_beacon(info, port ? 2 : 1);
+
+	/* Automatic backup supervisor will start. */
+	if (info->both_timeout && DLR_SUPERVISOR == info->node)
+		info->start = 1;
+	return true;
+}  /* handleBeaconTimeout */
+
+static int handleLinkChange(struct ksz_dlr_info *info, int link1, int link2)
+{
+	int change[2];
+	int down[2];
+	int update = false;
+	int going_up = false;
+	int missed = 0;
+
+	/* State machine not ready yet. */
+	if (!info->state)
+		return update;
+	down[0] = down[1] = 0;
+	change[0] = !link1 ^ info->p1_down;
+	change[1] = !link2 ^ info->p2_down;
+	if (link1)
+		info->p1_down = 0;
+	else
+		info->p1_down = 1;
+	if (link2)
+		info->p2_down = 0;
+	else
+		info->p2_down = 1;
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		struct ksz_dlr_node_info *node;
+
+		node = find_dlr_node(info, info->src_addr);
+		if (node) {
+			node->p1_down = info->p1_down;
+			node->p2_down = info->p2_down;
+		}
+	}
+	if ((!change[0] || link1) && (!change[1] || link2)) {
+		going_up = true;
+		info->fault_jiffies = 0;
+		return update;
+	}
+	if (info->p1_down && info->p2_down) {
+		if (!info->both_down)
+			update = true;
+		info->both_down = 1;
+		info->one_down = 0;
+		info->one_rcvd = 0;
+	} else if (info->p1_down || info->p2_down) {
+		if (!info->one_down)
+			update = true;
+		info->both_down = 0;
+		info->one_down = 1;
+		if (info->both_rcvd)
+			info->one_rcvd = 1;
+	} else {
+		info->one_down = 0;
+		info->both_down = 0;
+	}
+
+	/* Reset beacon timeout if the link also goes down. */
+	if (info->one_timeout) {
+		if ((info->p1_timeout && info->p1_down) ||
+		    (info->p2_timeout && info->p2_down)) {
+			info->one_timeout = 0;
+			info->p1_timeout = info->p2_timeout = 0;
+		}
+	}
+	if (info->p1_down) {
+		info->p1_rcvd = 0;
+		info->both_rcvd = 0;
+		missed |= 1;
+
+		/* Stop neighbor check if link is down (unlikely). */
+		if (info->port_chk[0] && info->port_chk[0] < 3)
+			info->port_chk[0] = 3;
+	}
+	if (info->p2_down) {
+		info->p2_rcvd = 0;
+		info->both_rcvd = 0;
+		missed |= 2;
+
+		/* Stop neighbor check if link is down (unlikely). */
+		if (info->port_chk[1] && info->port_chk[1] < 3)
+			info->port_chk[1] = 3;
+	}
+	dlr_clr_last_beacon(info, missed);
+	down[0] = info->p1_down;
+	down[1] = info->p2_down;
+	if (info->node != DLR_ANNOUNCE_NODE) {
+		info->beacon_info[0].timer = !down[0];
+		info->beacon_info[1].timer = !down[1];
+	}
+	if (going_up)
+		update = false;
+	if ((info->p1_down || info->p2_down) && !going_up) {
+		int p;
+
+#ifdef DBG_DLR_STATE
+		dbg_msg("Lo: %d:%d %x\n", down[0], down[1],
+			info->LastBcnRcvPort);
+#endif
+		if (info->fault_jiffies)
+			dbg_msg("time from fault: %lu\n",
+				jiffies - info->fault_jiffies);
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+			for (p = 0; p < 2; p++) {
+				if (down[p]) {
+					dlr_set_addr(&attrib->last_active[p],
+						info->ip_addr, info->src_addr);
+				}
+			}
+		}
+
+		/* Reset last beacon in case timeout is not processed. */
+		for (p = 0; p < 2; p++) {
+			if (down[p]) {
+				memset(&info->beacon_info[p].last, 0,
+					sizeof(struct ksz_dlr_beacon));
+				info->beacon_info[p].timeout = 0;
+				memset(&info->last_beacon[p], 0,
+					sizeof(struct ksz_dlr_tx_frame));
+			}
+		}
+	}
+	return update;
+}  /* handleLinkChange */
+
+static void LocateFault(struct ksz_dlr_info *info)
+{
+	if (info->node != DLR_ACTIVE_SUPERVISOR)
+		return;
+	dlr_tx_locate_fault(info);
+}  /* LocateFault */
+
+static void NeighborCheck(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if (!info->p1_down || !info->p2_down) {
+		info->neigh_chk = 1;
+		ksz_start_timer(&info->neigh_chk_timer_info,
+			info->neigh_chk_timer_info.period);
+		info->neigh_chk_timer_info.max = 3;
+	}
+	if (info->p1_down) {
+		dlr_set_addr(&attrib->last_active[0],
+			info->ip_addr, info->src_addr);
+	} else {
+		info->p1_lost = 0;
+		info->port_chk[0] = 1;
+		dlr_tx_chk_req(info, 0);
+	}
+	if (info->p2_down) {
+		dlr_set_addr(&attrib->last_active[1],
+			info->ip_addr, info->src_addr);
+	} else {
+		info->p2_lost = 0;
+		info->port_chk[1] = 1;
+		dlr_tx_chk_req(info, 1);
+	}
+}  /* NeighborCheck */
+
+static void startSignOn(struct ksz_dlr_info *info, int now)
+{
+	info->attrib.participants_cnt = 0;
+	memcpy(info->signon_addr, MAC_ADDR_SIGNON, ETH_ALEN);
+	ksz_start_timer(&info->signon_timer_info,
+		info->signon_timer_info.period);
+	if (!info->signon_delay || !info->ann_delay) {
+		info->signon_delay = 0;
+		if (now && !info->ann_first)
+			dlr_tx_signon(info, 0);
+		else
+			info->tx_signon = true;
+	}
+#ifdef DBG_DLR_OPER
+	else
+		dbg_msg("%s %d %d %lx\n", __func__,
+			info->signon_delay, info->ann_delay,
+			jiffies);
+#endif
+}  /* startSignOn */
+
+static void sendLinkStatus(struct ksz_dlr_info *info)
+{
+	/* Supervisor is known. */
+	if (info->attrib.active_super_addr.addr[0] ||
+	    info->attrib.active_super_addr.addr[1])
+		dlr_tx_status(info, false);
+}  /* sendLinkStatus */
+
+#if 1
+static int getting_last_active;
+#endif
+
+static void dlr_clear(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_beacon_info *beacon_info;
+	int p;
+
+	info->p1_rcvd = info->p2_rcvd =
+	info->one_rcvd = info->both_rcvd =
+	info->p1_timeout = info->p2_timeout =
+	info->one_timeout = info->both_timeout = 0;
+	for (p = 0; p < 2; p++ ) {
+		beacon_info = &info->beacon_info[p];
+		beacon_info->timer =
+		beacon_info->rcv_once =
+		beacon_info->timeout_start =
+		beacon_info->timeout_stop = 0;
+		beacon_info->timeout = 0;
+		memset(&beacon_info->last, 0, sizeof(struct ksz_dlr_beacon));
+		memset(&info->last_beacon[p], 0,
+			sizeof(struct ksz_dlr_tx_frame));
+	}
+	memset(info->supers, 0, sizeof(struct ksz_dlr_super_info) *
+		DLR_SUPERVISOR_NUM);
+#if 1
+	getting_last_active = 0;
+#endif
+}  /* dlr_clear */
+
+struct dlr_state {
+	int change;
+	int delay_ann;
+	int new_state;
+};
+
+static void dlr_idle_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "idle");
+	dlr_print(info, "idle");
+#endif
+
+	oneBeaconRcvd = twoBeaconsRcvd = 0;
+	oneBeaconTimeout = twoBeaconsTimeout = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_topology = DLR_TOPOLOGY_LINEAR;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_NO_SUPERVISOR;
+	memset(&attrib->active_super_addr, 0,
+		sizeof(struct ksz_dlr_active_node));
+	attrib->active_super_prec = 0;
+
+	if (info->skip_beacon)
+		acceptBeacons(info);
+	disableLearn(info, 1);
+	flushMacTable(info);
+	dlr_set_state(info);
+	if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 2);
+}  /* dlr_idle_init */
+
+static void dlr_idle_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsRcvd && !faultState) {
+		state->new_state = DLR_NORMAL_STATE;
+	}
+	if (oneBeaconRcvd || (twoBeaconsRcvd && faultState)) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+}  /* dlr_idle_next */
+
+static void dlr_ann_idle_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "ann idle");
+	dlr_print(info, "idle");
+#endif
+
+	announceRcvd = 0;
+	announcedState = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_LINEAR;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_NO_SUPERVISOR;
+	memset(&attrib->active_super_addr, 0,
+		sizeof(struct ksz_dlr_active_node));
+	attrib->active_super_prec = 0;
+}  /* dlr_ann_idle_init */
+
+static void dlr_ann_idle_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+}  /* dlr_ann_idle_next */
+
+static void dlr_fault_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_BEACON
+	dbg_msg("  %08x - %08x; %08x - %08x;\n",
+		info->seqid_accept[0], info->seqid_last[0],
+		info->seqid_accept[1], info->seqid_last[1]);
+#endif
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "fault");
+	dlr_print(info, "fault");
+#endif
+
+	newSupervisor = 0;
+	if (DLR_ACTIVE_SUPERVISOR == info->node)
+		announcedState = RING_FAULT_STATE;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_RING_FAULT;
+	if (DLR_BEACON_NODE >= info->node)
+		attrib->super_status = DLR_STAT_RING_NODE;
+	else
+		attrib->super_status = DLR_STAT_BACKUP_SUPERVISOR;
+
+#if 1
+	disableLearn(info, 1);
+#endif
+	flushMacTable(info);
+#if 0
+	disableLearn(info, 0);
+#endif
+	dlr_set_state(info);
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+}  /* dlr_fault_init */
+
+static void dlr_fault_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsRcvd && !faultState) {
+		newSupervisor = 0;
+		disableNeighChkTimers(info);
+		state->new_state = DLR_NORMAL_STATE;
+	}
+	if ((twoBeaconsTimeout || (oneBeaconTimeout && !oneBeaconRcvd)) &&
+	    info->LastBcnRcvPort)
+		dbg_dlr(info, "  ! last beacon");
+	if (!info->LastBcnRcvPort) {
+		if (linkDown)
+dbg_msg(" f linkDown\n");
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+		else
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+	if (newSupervisor) {
+		newSupervisor = 0;
+		flushMacTable(info);
+	}
+
+	/* Apply only to supervisor. */
+	if (newValue) {
+		if (updateValues(info))
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (announceTimeout) {
+		announceTimeout = false;
+		dbg_msg("  ??? announce timeout\n");
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR == info->node)
+			state->new_state = DLR_PREPARE_STATE;
+	}
+}  /* dlr_fault_next */
+
+static void dlr_ann_fault_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "fault");
+	dlr_print(info, "fault");
+#endif
+
+	announceRcvd = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_RING_FAULT;
+	attrib->super_status = DLR_STAT_RING_NODE;
+
+	flushMacTable(info);
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+}  /* dlr_ann_fault_init */
+
+static void dlr_ann_fault_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceTimeout) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkDown) {
+		disableAnnounceTimeout(info);
+		disableNeighChkTimers(info);
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+	if (newSupervisor) {
+		newSupervisor = 0;
+		disableNeighChkTimers(info);
+	}
+}  /* dlr_ann_fault_next */
+
+static void dlr_normal_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_BEACON
+	dbg_msg("  %08x - %08x; %08x - %08x;\n",
+		info->seqid_accept[0], info->seqid_last[0],
+		info->seqid_accept[1], info->seqid_last[1]);
+#endif
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "normal");
+	dlr_print(info, "normal");
+#endif
+
+	newSupervisor = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_NORMAL;
+	if (DLR_BEACON_NODE >= info->node)
+		attrib->super_status = DLR_STAT_RING_NODE;
+	else
+		attrib->super_status = DLR_STAT_BACKUP_SUPERVISOR;
+	info->ok_ports = info->member;
+
+	flushMacTable(info);
+	disableLearn(info, 0);
+	dlr_set_state(info);
+}  /* dlr_normal_init */
+
+static void dlr_normal_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsTimeout || linkDown) {
+		if (linkDown)
+dbg_msg(" n linkDown\n");
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+		else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (newSupervisor || faultState || oneBeaconTimeout) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (linkLoss) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (announceTimeout) {
+		announceTimeout = false;
+		dbg_msg("  ??? announce timeout\n");
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR == info->node)
+			state->new_state = DLR_PREPARE_STATE;
+	}
+
+	/* Apply only to supervisor. */
+	if (newValue) {
+		if (updateValues(info))
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (state->new_state) {
+		setupDir(info, -1);
+		if (info->skip_beacon)
+			acceptBeacons(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+	if (DLR_FAULT_STATE == state->new_state) {
+		struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+		attrib->fault_cnt++;
+	}
+}  /* dlr_normal_next */
+
+static void dlr_ann_normal_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "normal");
+	dlr_print(info, "normal");
+#endif
+
+	announceRcvd = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_RING_NODE;
+	info->ok_ports = info->member;
+
+	flushMacTable(info);
+	disableNeighChkTimers(info);
+}  /* dlr_ann_normal_init */
+
+static void dlr_ann_normal_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceTimeout) {
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+	if (linkLoss) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (state->new_state)
+		setupDir(info, -1);
+	if (DLR_FAULT_STATE == state->new_state) {
+		struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+		attrib->fault_cnt++;
+	}
+}  /* dlr_ann_normal_next */
+
+static void dlr_active_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "active");
+#endif
+	dlr_clear(info);
+	info->beacon_info[0].timer =
+	info->beacon_info[1].timer = 1;
+#if 1
+	getting_last_active = 0;
+#endif
+
+	/* Reset incoming and outgoing ports. */
+	info->tx_port = info->port;
+	info->rx_port = (info->tx_port + 1) & 1;
+	info->LastBcnRcvPort = 0;
+	info->node = DLR_ACTIVE_SUPERVISOR;
+	info->interval = info->beacon_interval;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->super_status = DLR_STAT_ACTIVE_SUPERVISOR;
+	dlr_set_addr(&attrib->active_super_addr,
+		info->ip_addr, info->src_addr);
+	attrib->active_super_prec = attrib->super_cfg.prec;
+
+	/* Supervisor source address may change. */
+	info->p1_set = info->p2_set = 1;
+
+	if (!newSupervisor) {
+		enableSupervisor(info);
+		disableAnnounceTimeout(info);
+	}
+	if (state->change > 0)
+		state->change--;
+}  /* dlr_active_init */
+
+static void dlr_active_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	state->delay_ann = 1;
+	state->new_state = DLR_ACTIVE_FAULT_STATE;
+	if (newSupervisor)
+		state->new_state = DLR_BACKUP_STATE;
+	else if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 1);
+#ifdef DBG_DLR_BEACON_
+	if (!dbg_bcn)
+		dbg_bcn = 4;
+#endif
+}  /* dlr_active_next */
+
+static void dlr_active_fault_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_SUPERVISOR
+	dbg_msg("  %d %d;\n",
+		info->beacon_info[0].timeout, info->beacon_info[1].timeout);
+#endif
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "active fault");
+	dlr_print(info, "active fault");
+#endif
+	dbg_supervisor(info);
+#ifdef DBG_DLR_BEACON
+	dbg_bcn = 3;
+#endif
+
+	linkLoss = 0;
+	if (!memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+	    ETH_ALEN))
+		newSupervisor = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_status = DLR_NET_RING_FAULT;
+
+	/* Reset timeout notification. */
+	info->beacon_timeout_ports = 0;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	dbg_ann = 3;
+#endif
+	flushMacTable(info);
+	enableBothPorts(info);
+	setupBeacons(info);
+	enableAnnounce(info, state->delay_ann);
+
+	/* Coming here from active normal state. */
+	if (!state->delay_ann && !linkDown) {
+#if 1
+		getting_last_active = 1;
+#endif
+		LocateFault(info);
+		NeighborCheck(info);
+	}
+	state->delay_ann = 0;
+#if 0
+	if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 2);
+#endif
+}  /* dlr_active_fault_init */
+
+static void dlr_active_fault_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (oneBeaconTimeout || twoBeaconsTimeout) {
+		oneBeaconTimeout = twoBeaconsTimeout = 0;
+		if (info->start && info->chk_hw)
+			dlr_chk_supervisor(info);
+	}
+	if (twoBeaconsRcvd) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_ACTIVE_NORMAL_STATE;
+	}
+	if (newSupervisor) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+	if (!newSupervisor && newValue) {
+		state->new_state = DLR_RESTART_STATE;
+	}
+
+	if (!state->new_state && DLR_ACTIVE_SUPERVISOR == info->node &&
+	    memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+	    ETH_ALEN)) {
+		printk(KERN_INFO "still active fault\n");
+dbg_msg("prec: %02x %d\n", info->attrib.active_super_addr.addr[5],
+info->precedence);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+}  /* dlr_active_fault_next */
+
+static void dlr_active_normal_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "active normal");
+	dlr_print(info, "active normal");
+#endif
+#ifdef DBG_DLR_BEACON
+	dbg_bcn += 4;
+#endif
+
+	announcedState = RING_NORMAL_STATE;
+
+	attrib->net_status = DLR_NET_NORMAL;
+	memset(&attrib->last_active[0], 0,
+		sizeof(struct ksz_dlr_active_node));
+	memset(&attrib->last_active[1], 0,
+		sizeof(struct ksz_dlr_active_node));
+#if 1
+	getting_last_active = 0;
+#endif
+
+	/* Allow timeout notification. */
+	info->beacon_timeout_ports = 0;
+	info->ok_ports = info->member;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	dbg_ann = 3;
+#endif
+	enableOnePort(info);
+	flushMacTable(info);
+	setupBeacons(info);
+	enableAnnounce(info, 0);
+
+	/*
+	 * Need to wait until the normal beacons are
+	 * sent.
+	 */
+	info->signon_delay = 1;
+	if (!info->signon_start) {
+		info->signon_start = 1;
+		startSignOn(info, true);
+	}
+	if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 2);
+}  /* dlr_active_normal_init */
+
+static void dlr_active_normal_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if ((oneBeaconTimeout || twoBeaconsTimeout) && !linkLoss) {
+#ifdef DBG_DLR_STATE
+		dbg_dlr(info, "active timeout");
+#endif
+		disableSignOnTimer(info);
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			if (twoBeaconsTimeout && info->start)
+				dlr_chk_supervisor(info);
+			state->new_state = DLR_ACTIVE_FAULT_STATE;
+		} else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (linkDown) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_ACTIVE_FAULT_STATE;
+	}
+	if (linkLoss || linkStatus) {
+#ifdef DBG_DLR_STATE
+		dbg_dlr(info, "active loss");
+#endif
+		twoBeaconsRcvd = 0;
+		disableSignOnTimer(info);
+		if (DLR_ACTIVE_SUPERVISOR == info->node)
+			state->new_state = DLR_ACTIVE_FAULT_STATE;
+		else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (newSupervisor) {
+		twoBeaconsRcvd = 0;
+#ifdef DBG_DLR_BEACON
+		dbg_bcn += 4;
+#endif
+		disableSignOnTimer(info);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+	if (!newSupervisor && newValue) {
+		state->new_state = DLR_RESTART_STATE;
+
+		/* Ignore own beacons until they are changed. */
+		info->start = 0;
+	}
+	if (DLR_ACTIVE_FAULT_STATE == state->new_state)
+		attrib->fault_cnt++;
+	if (state->new_state) {
+		if (info->skip_beacon)
+			acceptBeacons(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+
+	if (!state->new_state && DLR_ACTIVE_SUPERVISOR == info->node &&
+	    memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+	    ETH_ALEN)) {
+		printk(KERN_INFO "still active normal\n");
+		state->new_state = DLR_BACKUP_STATE;
+	}
+}  /* dlr_active_normal_next */
+
+static void dlr_backup_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "backup");
+#endif
+
+	info->node = DLR_SUPERVISOR;
+	info->attrib.participants_cnt = 0;
+
+	disableSupervisor(info);
+	enableBothPorts(info);
+	state->new_state = DLR_FAULT_STATE;
+
+	/* Delay resetting so that node knows it is becoming backup. */
+	newSupervisor = 0;
+}  /* dlr_backup_init */
+
+static void dlr_backup_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+}
+
+static void dlr_prepare_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "prepare");
+#endif
+	wait_for_timeout(info->beacon_timeout);
+	info->wait_done = 1;
+}  /* dlr_prepare_init */
+
+static void dlr_prepare_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (info->wait_done) {
+		info->wait_done = 0;
+		state->new_state = DLR_ACTIVE_STATE;
+	}
+	if (newSupervisor)
+		state->new_state = DLR_BACKUP_STATE;
+}  /* dlr_prepare_init */
+
+static void dlr_restart_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info,"restart");
+#endif
+#ifdef DBG_DLR_BEACON
+	dbg_bcn += 5;
+#endif
+
+	/* Disable timeout notification. */
+	info->beacon_timeout_ports = 7;
+	info->wait_done = 0;
+	info->node = DLR_SUPERVISOR;
+
+	disableSupervisor(info);
+	disableAnnounce(info);
+	dlr_clear(info);
+
+	/* Reset to ring fault state. */
+	dlr_set_state(info);
+	wait_for_timeout(info->beacon_timeout * 2);
+	info->wait_done = 1;
+}  /* dlr_restart_init */
+
+static void dlr_restart_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (info->wait_done) {
+		info->wait_done = 0;
+		updateValues(info);
+		info->beacon_info[0].timer =
+		info->beacon_info[1].timer = 1;
+		state->new_state = DLR_ACTIVE_STATE;
+	}
+	if (oneBeaconRcvd || twoBeaconsRcvd) {
+		dbg_msg("may never get here\n");
+		enableBothPorts(info);
+
+		/* Reset timeout notification. */
+		info->beacon_timeout_ports = 0;
+		state->new_state = DLR_FAULT_STATE;
+	}
+}  /* dlr_restart_next */
+
+static int dlr_proc_state(struct ksz_dlr_info *info, struct dlr_state *state,
+	void (*state_init)(struct ksz_dlr_info *info, struct dlr_state *state),
+	void (*state_next)(struct ksz_dlr_info *info, struct dlr_state *state))
+{
+	if (state->new_state) {
+		state->new_state = 0;
+		state_init(info, state);
+		if (1 == state->change)
+			return 1;
+	}
+	state_next(info, state);
+	return 0;
+}  /* dlr_proc_state */
+
+static void RingSupervisor_state(struct ksz_dlr_info *info)
+{
+	struct dlr_state state_info;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	state_info.change = 1;
+	state_info.delay_ann = 0;
+	state_info.new_state = 0;
+	dbg_leak = 5;
+	do {
+		state_info.change--;
+		switch (info->state) {
+		case DLR_BEGIN:
+			dlr_clear(info);
+			info->ok_ports = info->member;
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 1;
+			info->LastBcnRcvPort = 0;
+			memset(&attrib->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attrib->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			attrib->participants_cnt = 0;
+			if (info->skip_beacon)
+				acceptBeacons(info);
+			if (DLR_BEACON_NODE >= info->node)
+				state_info.new_state = DLR_IDLE_STATE;
+			else
+				state_info.new_state = DLR_ACTIVE_STATE;
+			state_info.change = 1;
+			break;
+		case DLR_IDLE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_idle_init, dlr_idle_next))
+				goto done;
+			break;
+		case DLR_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_fault_init, dlr_fault_next))
+				goto done;
+			break;
+		case DLR_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_normal_init, dlr_normal_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_init, dlr_active_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_fault_init, dlr_active_fault_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_normal_init, dlr_active_normal_next))
+				goto done;
+			break;
+		case DLR_BACKUP_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_backup_init, dlr_backup_next))
+				goto done;
+			break;
+		case DLR_PREPARE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_prepare_init, dlr_prepare_next))
+				goto done;
+			break;
+		case DLR_RESTART_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_restart_init, dlr_restart_next))
+				goto done;
+			break;
+		}
+		if (info->reset) {
+			info->reset = 0;
+			if (DLR_NORMAL_STATE == info->state)
+				setupDir(info, -1);
+			state_info.new_state = 0;
+			info->state = DLR_BEGIN;
+			state_info.change++;
+		}
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			info->state = state_info.new_state;
+			state_info.change++;
+		}
+	} while (state_info.change);
+
+done:
+	return;
+}  /* RingSupervisor_state */
+
+static void AnnounceRingNode_state(struct ksz_dlr_info *info)
+{
+	struct dlr_state state_info;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	state_info.change = 1;
+	state_info.delay_ann = 0;
+	state_info.new_state = 0;
+	do {
+		state_info.change--;
+		switch (info->state) {
+		case DLR_BEGIN:
+			dlr_clear(info);
+			info->ok_ports = info->member;
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 0;
+			announceRcvd = 0;
+			announcedState = 0;
+			memset(&attrib->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attrib->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			attrib->participants_cnt = 0;
+			if (info->skip_beacon)
+				acceptBeacons(info);
+			state_info.new_state = DLR_IDLE_STATE;
+			state_info.change = 1;
+			break;
+		case DLR_IDLE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_idle_init, dlr_ann_idle_next))
+				goto done;
+			break;
+		case DLR_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_fault_init, dlr_ann_fault_next))
+				goto done;
+			break;
+		case DLR_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_normal_init, dlr_ann_normal_next))
+				goto done;
+			break;
+		}
+		if (info->reset) {
+			info->reset = 0;
+			if (DLR_NORMAL_STATE == info->state)
+				setupDir(info, -1);
+			state_info.new_state = 0;
+			info->state = DLR_BEGIN;
+			state_info.change++;
+		}
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			info->state = state_info.new_state;
+			state_info.change++;
+		}
+	} while (state_info.change);
+
+done:
+	return;
+}  /* AnnounceRingNode_state */
+
+static void *check_dlr_frame(u8 *data)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(DLR_TAG_TYPE))
+			return vlan + 1;
+	}
+
+	/* VLAN tag can be removed by the switch. */
+	if (vlan->h_vlan_proto == htons(DLR_TAG_TYPE)) {
+		struct ethhdr *eth = (struct ethhdr *) data;
+
+		return eth + 1;
+	}
+	return NULL;
+}  /* check_dlr_frame */
+
+static void dlr_proc_rx(struct ksz_dlr_info *info, struct sk_buff *skb,
+	int port)
+{
+	struct ksz_dlr_rx_frame frame;
+	struct ksz_dlr_frame *body;
+	int update = false;
+
+	body = check_dlr_frame(skb->data);
+	frame.vlan = (struct vlan_ethhdr *) skb->data;
+	frame.body = body;
+	switch (body->hdr.frame_type) {
+	case DLR_SIGN_ON:
+		update = handleSignOn(info, &frame, port);
+		break;
+	case DLR_NEIGH_CHK_REQ:
+		update = handleNeighChkReq(info, &frame, port);
+		break;
+	case DLR_NEIGH_CHK_RESP:
+		update = handleNeighChkResp(info, &frame, port);
+		break;
+	case DLR_LOCATE_FAULT:
+		update = handleLocateFault(info, &frame, port);
+		break;
+	case DLR_FLUSH_TABLES:
+		update = handleFlushTables(info, &frame, port);
+		break;
+	case DLR_BEACON:
+		update = handleBeacon(info, &frame, port);
+		break;
+	case DLR_ANNOUNCE:
+		update = handleAnnounce(info, &frame, port);
+		break;
+	case DLR_LINK_STATUS:
+		update = handleLinkStatus(info, &frame, port);
+		break;
+	case DLR_LEARNING_UPDATE:
+		update = handleLearningUpdate(info, &frame, port);
+		break;
+	}
+	if (update)
+		info->state_machine(info);
+}  /* dlr_proc_rx */
+
+static void dlr_stop(struct ksz_dlr_info *info)
+{
+	if (DLR_SUPERVISOR == info->node)
+		disableAnnounceTimeout(info);
+
+	/* Avoid trying to drop the beacons again. */
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 1;
+	info->node = DLR_BEACON_NODE;
+
+	disableSupervisor(info);
+	enableBothPorts(info);
+	disableAnnounce(info);
+	disableSignOnTimer(info);
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 0;
+}  /* dlr_stop */
+
+static void dlr_delay_proc(struct work_struct *work)
+{
+	struct ksz_dlr_info *dlr =
+		container_of(work, struct ksz_dlr_info, delay_proc);
+	bool empty;
+	bool last;
+	struct sk_buff *skb;
+	bool notify_link_lost = false;
+
+	if (dlr->stop) {
+		dlr->stop = false;
+		dlr_stop(dlr);
+	}
+	if (dlr->timeout_beacon) {
+		dlr->timeout_beacon = false;
+		if (dlr->beacon_timeout_ports) {
+			int p;
+
+			for (p = 0; p < 2; p++) {
+				if (dlr->beacon_timeout_ports & (1 << p))
+					handleBeaconTimeout(dlr, p);
+			}
+			dlr->beacon_timeout_ports = 0;
+		}
+	}
+	if (dlr->link_change) {
+		dlr->link_change = false;
+		if (dlr->notifications & DLR_INFO_LINK_LOST)
+			notify_link_lost = true;
+	}
+
+	last = skb_queue_empty(&dlr->rxq);
+	empty = last;
+	while (!last) {
+		skb = skb_dequeue(&dlr->rxq);
+		last = skb_queue_empty(&dlr->rxq);
+		if (skb) {
+			int port;
+
+			port = skb->cb[0];
+			dlr_proc_rx(dlr, skb, port);
+			dev_kfree_skb_irq(skb);
+		}
+	}
+	dlr->state_machine(dlr);
+	if (dlr->tx_announce) {
+		dlr->tx_announce = false;
+		dlr_tx_announce(dlr);
+	}
+	if (dlr->tx_signon) {
+		dlr->tx_signon = false;
+		dlr_tx_signon(dlr, 0);
+	}
+	if (dlr->tx_advertise) {
+		dlr->tx_advertise = false;
+		dlr_tx_advertise(dlr);
+	}
+	if (dlr->tx_flush_tables) {
+		dlr->tx_flush_tables = false;
+		dlr_tx_flush_tables(dlr);
+	}
+	if (dlr->clr_supervisor) {
+		dlr->clr_supervisor = false;
+		dlr_clr_supervisor(dlr);
+	}
+	if (notify_link_lost)
+		dlr_notify_link_lost(dlr);
+}  /* dlr_delay_proc */
+
+static int dlr_rcv(struct ksz_dlr_info *info, struct sk_buff *skb, int port)
+{
+	struct ksz_dlr_frame *body;
+	int dlr_port = port;
+
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		dlr_port = 0;
+	else if (port == info->ports[1])
+		dlr_port = 1;
+	body = check_dlr_frame(skb->data);
+	if (body) {
+		struct ksz_dlr_rx_frame frame;
+		int accept = true;
+
+		if (dlr_port < 2) {
+			frame.vlan = (struct vlan_ethhdr *) skb->data;
+			frame.body = body;
+			if (DLR_BEACON == body->hdr.frame_type)
+				accept = checkBeacon(info, &frame, dlr_port);
+		} else
+			accept = false;
+		if (accept) {
+
+			/* Use control buffer to save port information. */
+			skb->cb[0] = (char) dlr_port;
+			skb_queue_tail(&info->rxq, skb);
+			schedule_work(&info->delay_proc);
+		} else
+			dev_kfree_skb_irq(skb);
+		return 0;
+	}
+	return 1;
+}  /* dlr_rcv */
+
+static void dlr_link_change(struct ksz_dlr_info *info, int link1, int link2)
+{
+	if (handleLinkChange(info, link1, link2)) {
+		info->link_change = true;
+		schedule_work(&info->delay_proc);
+	}
+}  /* dlr_link_change */
+
+static void dlr_timeout(struct ksz_dlr_info *info, int port)
+{
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		port = 0;
+	else if (port == info->ports[1])
+		port = 1;
+	if (port > 1)
+		return;
+	if (!(info->beacon_timeout_ports & (1 << port))) {
+		info->beacon_timeout_ports |= (1 << port);
+		info->timeout_beacon = true;
+		schedule_work(&info->delay_proc);
+	}
+}  /* dlr_timeout */
+
+static void prep_dlr_addr(struct ksz_dlr_info *dlr, u8 *src)
+{
+	memcpy(dlr->src_addr, src, ETH_ALEN);
+	memcpy(dlr->frame.vlan.h_source, src, ETH_ALEN);
+	memcpy(dlr->update_frame.eth.h_source, src, ETH_ALEN);
+	if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+		memcpy(dlr->attrib.active_super_addr.addr, src, ETH_ALEN);
+}  /* prep_dlr_addr */
+
+static void dlr_change_addr(struct ksz_dlr_info *dlr, u8 *addr)
+{
+	struct ksz_dlr_gateway_capable *attrib = &dlr->attrib;
+	struct ksz_sw *sw = dlr->sw_dev;
+
+	/* Do not do anything if device is not ready. */
+	if (!dlr->dev || !netif_running(dlr->dev))
+		return;
+	if (!memcmp(dlr->src_addr, addr, ETH_ALEN))
+		return;
+
+	sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, dlr->src_addr, 0,
+		false, false, 0);
+	if (sw->eth_cnt > 1) {
+		sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, dlr->src_addr, 0,
+			false, true, sw->eth_maps[0].vlan);
+		sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, dlr->src_addr, 0,
+			false, true, sw->eth_maps[1].vlan);
+	}
+	prep_dlr_addr(dlr, addr);
+	memcpy(dlr->signon_frame, &dlr->frame, sizeof(struct vlan_ethhdr) +
+		sizeof(struct ksz_dlr_hdr));
+	sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, dlr->src_addr, sw->HOST_MASK,
+		false, false, 0);
+	if (sw->eth_cnt > 1) {
+		sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, dlr->src_addr,
+			sw->HOST_MASK, false, true, sw->eth_maps[0].vlan);
+		sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, dlr->src_addr,
+			sw->HOST_MASK, false, true, sw->eth_maps[1].vlan);
+	}
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_self(dlr, dlr->ports[0]);
+	setup_acl_self(dlr, dlr->ports[1]);
+#endif
+	if (DLR_SUPERVISOR == dlr->node &&
+	    dlr->attrib.super_cfg.prec == attrib->active_super_prec) {
+		int cmp = memcmp(dlr->src_addr, attrib->active_super_addr.addr,
+			ETH_ALEN);
+
+		if (cmp > 0) {
+			dlr->new_val = 1;
+			schedule_work(&dlr->delay_proc);
+		}
+	} else if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+		if (dlr->skip_beacon)
+			acceptBeacons(dlr);
+		dlr->new_val = 1;
+		schedule_work(&dlr->delay_proc);
+	}
+}  /* dlr_change_addr */
+
+static void prep_dlr_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, MAC_ADDR_BEACON, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_SIGNON, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_ADVERTISE, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_LEARNING_UPDATE, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_dlr_mcast */
+
+static void prep_dlr(struct ksz_dlr_info *dlr, struct net_device *dev, u8 *src)
+{
+	dlr->dev = dev;
+	prep_dlr_addr(dlr, src);
+	dlr->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | dlr->vid);
+	memcpy(dlr->signon_frame, &dlr->frame, sizeof(struct vlan_ethhdr) +
+		sizeof(struct ksz_dlr_hdr));
+
+	dlr->active_port = 0;
+	dlr->seqid = 0;
+#if 1
+	dlr->seqid = 0xffffffe0;
+#endif
+	dlr->seqid_beacon = 0;
+	dlr->state = DLR_BEGIN;
+	do {
+		struct ksz_sw *sw = dlr->sw_dev;
+
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, dlr->src_addr,
+			sw->HOST_MASK, false, false, 0);
+		if (sw->eth_cnt > 1) {
+			sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, dlr->src_addr,
+				sw->HOST_MASK, false, true,
+				sw->eth_maps[0].vlan);
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, dlr->src_addr,
+				sw->HOST_MASK, false, true,
+				sw->eth_maps[1].vlan);
+		}
+		dlr->p1_down = sw->port_info[dlr->ports[0]].state !=
+			media_connected;
+		dlr->p2_down = sw->port_info[dlr->ports[1]].state !=
+			media_connected;
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+			if (dlr->p1_down)
+				dlr_set_addr(&dlr->attrib.last_active[0],
+					dlr->ip_addr, dlr->src_addr);
+			if (dlr->p2_down)
+				dlr_set_addr(&dlr->attrib.last_active[1],
+					dlr->ip_addr, dlr->src_addr);
+		}
+		sw->ops->acquire(sw);
+		port_cfg(sw, dlr->ports[0], REG_PORT_LUE_CTRL,
+			PORT_SRC_ADDR_FILTER, false);
+		port_cfg(sw, dlr->ports[1], REG_PORT_LUE_CTRL,
+			PORT_SRC_ADDR_FILTER, false);
+		sw->ops->release(sw);
+	} while (0);
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_self(dlr, dlr->ports[0]);
+	setup_acl_self(dlr, dlr->ports[1]);
+#endif
+	setup_vlan_table(dlr, dlr->vid, true);
+	schedule_work(&dlr->delay_proc);
+}  /* prep_dlr */
+
+static void announce_monitor(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_dlr_info *dlr =
+		container_of(dwork, struct ksz_dlr_info, announce_tx);
+
+	dlr->clr_supervisor = true;
+
+	/* No longer being active supervisor after this scheduling. */
+	if (DLR_ACTIVE_SUPERVISOR != dlr->node) {
+#if 0
+if (dlr->ann_delay)
+dbg_msg(" ! %s %d %d\n", __func__, dlr->ann_delay, dlr->signon_delay);
+#endif
+		dlr->ann_delay = 0;
+		dlr->signon_delay = 0;
+		goto done;
+	}
+
+	if (dlr->ann_delay) {
+		u32 microsec;
+		unsigned long diff = jiffies - dlr->ann_jiffies;
+
+		microsec = dlr->beacon_timeout * 2;
+		if (diff >= 2)
+			diff = diff * 1000 * (1000 / HZ);
+		else
+			diff = 0;
+		if (diff < microsec) {
+			microsec -= diff;
+			wait_for_timeout(microsec);
+		}
+		dlr->ann_delay = 0;
+		dlr->ann_jiffies = 0;
+	}
+
+	/* No longer being active supervisor after the wait. */
+	if (DLR_ACTIVE_SUPERVISOR != dlr->node) {
+		dlr->signon_delay = 0;
+		goto done;
+	}
+
+	dlr->tx_announce = true;
+	if (dlr->signon_delay) {
+#ifdef DBG_DLR_ANN_SIGNON
+		dbg_msg("delay signon: %lx\n", jiffies);
+#endif
+		dlr->tx_signon = true;
+		dlr->signon_delay = 0;
+	}
+	schedule_delayed_work(&dlr->announce_tx, 100);
+
+done:
+	schedule_work(&dlr->delay_proc);
+}  /* announce_monitor */
+
+static void announce_timeout_monitor(unsigned long ptr)
+{
+	struct ksz_dlr_info *info = (struct ksz_dlr_info *) ptr;
+
+dbg_msg("ann timeout\n");
+	info->ann_timeout = 1;
+	if (DLR_ANNOUNCE_NODE == info->node ||
+	    DLR_SUPERVISOR == info->node)
+		schedule_work(&info->delay_proc);
+	ksz_update_timer(&info->announce_timeout_timer_info);
+}  /* announce_timeout_monitor */
+
+static void neigh_chk_proc(struct work_struct *work)
+{
+	int p;
+	int checking = 0;
+	int lost = false;
+	struct ksz_dlr_info *dlr =
+		container_of(work, struct ksz_dlr_info, neigh_chk_proc);
+
+	/* Neighbor_Check_Request timeout. */
+	for (p = 0; p < 2; p++) {
+
+		/* This port has sent a Neighbor_Check_Request frame. */
+		if (dlr->port_chk[p]) {
+			++checking;
+			++dlr->port_chk[p];
+			if (dlr->port_chk[p] > 3) {
+				if (p)
+					dlr->p2_lost = 1;
+				else
+					dlr->p1_lost = 1;
+				lost = true;
+				--checking;
+			} else
+				dlr_tx_chk_req(dlr, p);
+		}
+	}
+	if (lost) {
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+			struct ksz_dlr_node_info *node;
+
+			if (dlr->p1_lost)
+				dlr->ok_ports &= ~(1 << dlr->ports[0]);
+			if (dlr->p2_lost)
+				dlr->ok_ports &= ~(1 << dlr->ports[1]);
+			node = find_dlr_node(dlr, dlr->src_addr);
+			if (node) {
+				if (dlr->p1_down)
+					node->p1_down = 1;
+				if (dlr->p2_down)
+					node->p2_down = 1;
+				if (dlr->p1_lost)
+					node->p1_lost = 1;
+				if (dlr->p2_lost)
+					node->p2_lost = 1;
+			}
+			if (dlr->p1_lost)
+				dlr_set_addr(&dlr->attrib.last_active[0],
+					dlr->ip_addr, dlr->src_addr);
+			if (dlr->p2_lost)
+				dlr_set_addr(&dlr->attrib.last_active[1],
+					dlr->ip_addr, dlr->src_addr);
+#if 1
+			if (dlr->notifications & DLR_INFO_CFG_CHANGE)
+				dlr_notify_cfg_change(dlr, 2);
+#endif
+			dlr_notify_link_lost(dlr);
+		} else
+			dlr_tx_status(dlr, 1);
+	}
+}  /* neigh_chk_proc */
+
+static void neigh_chk_monitor(unsigned long ptr)
+{
+	struct ksz_dlr_info *dlr = (struct ksz_dlr_info *) ptr;
+
+	schedule_work(&dlr->neigh_chk_proc);
+	ksz_update_timer(&dlr->neigh_chk_timer_info);
+	dlr->neigh_chk = !!dlr->neigh_chk_timer_info.max;
+}  /* neigh_chk_monitor */
+
+static void signon_monitor(unsigned long ptr)
+{
+	struct ksz_dlr_info *info = (struct ksz_dlr_info *) ptr;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	dbg_msg("%s\n", __func__);
+#endif
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		info->tx_signon = true;
+		schedule_work(&info->delay_proc);
+	} else
+		info->signon_timer_info.max = 1;
+	ksz_update_timer(&info->signon_timer_info);
+}  /* signon_monitor */
+
+static void dlr_reset_attrib(struct ksz_dlr_info *dlr)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = dlr->sw_dev;
+#endif
+
+	memset(&dlr->attrib, 0, sizeof(struct ksz_dlr_gateway_capable));
+	switch (dlr->node) {
+	case DLR_ANNOUNCE_NODE:
+		dlr->attrib.cap = DLR_CAP_ANNOUNCE_BASED;
+		dlr->attrib.super_status = DLR_STAT_RING_NODE;
+		break;
+	case DLR_BEACON_NODE:
+		dlr->attrib.cap = DLR_CAP_BEACON_BASED;
+
+#ifdef CONFIG_HAVE_DLR_HW
+		if (sw->features & REDUNDANCY_SUPPORT)
+			dlr->attrib.cap |= DLR_CAP_SUPERVISOR_CAPABLE;
+#endif
+		dlr->attrib.super_status = DLR_STAT_RING_NODE;
+		dlr->attrib.super_cfg.beacon_interval = 400;
+		dlr->attrib.super_cfg.beacon_timeout = 1960;
+		dlr->attrib.super_cfg.beacon_interval *= 1;
+		dlr->attrib.super_cfg.beacon_timeout *= 1;
+		break;
+	default:
+
+#ifdef CONFIG_HAVE_DLR_HW
+		if (!(sw->features & REDUNDANCY_SUPPORT))
+			break;
+#endif
+		dlr->attrib.cap = DLR_CAP_BEACON_BASED;
+		dlr->attrib.cap |= DLR_CAP_SUPERVISOR_CAPABLE;
+		dlr->attrib.super_status = DLR_STAT_ACTIVE_SUPERVISOR;
+		dlr->attrib.super_cfg.enable = true;
+		dlr->attrib.super_cfg.beacon_interval = 400;
+		dlr->attrib.super_cfg.beacon_timeout = 1960;
+		dlr->attrib.super_cfg.beacon_interval *= 1;
+		dlr->attrib.super_cfg.beacon_timeout *= 1;
+	}
+}  /* dlr_reset_attrib */
+
+static void setup_dlr(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	dlr_reset_attrib(dlr);
+
+	dlr->ip_addr = 0;
+	dlr->vid = 0;
+	dlr->precedence = 0;
+	dlr->beacon_interval = 400;
+	dlr->beacon_timeout = 1960;
+	dlr->beacon_interval *= 1;
+	dlr->beacon_timeout *= 1;
+
+	dlr->frame.vlan.h_vlan_proto = htons(ETH_P_8021Q);
+	dlr->frame.vlan.h_vlan_encapsulated_proto = htons(DLR_TAG_TYPE);
+	frame->hdr.ring_subtype = DLR_RING_SUBTYPE;
+	frame->hdr.ring_protocol_version = 1;
+	frame->hdr.ip_addr = dlr->ip_addr;
+	dlr->tx_frame = (u8 *) &dlr->frame;
+
+	memcpy(dlr->update_frame.eth.h_dest, MAC_ADDR_LEARNING_UPDATE,
+		ETH_ALEN);
+	dlr->update_frame.eth.h_proto = htons(DLR_TAG_TYPE);
+	dlr->update_frame.hdr.ring_subtype = DLR_RING_SUBTYPE;
+	dlr->update_frame.hdr.ring_protocol_version = 1;
+	dlr->update_frame.hdr.frame_type = DLR_LEARNING_UPDATE;
+	dlr->update_frame.hdr.src_port = DLR_PORT_NONE;
+	memset(dlr->update_frame.reserved, 0, 34);
+
+	dlr->p1_set = dlr->p2_set = 1;
+
+	dlr->port = 0;
+	dlr->tx_port = dlr->port;
+	dlr->rx_port = (dlr->tx_port + 1) & 1;
+
+	ksz_init_timer(&dlr->announce_timeout_timer_info, 1200 * HZ / 1000,
+		announce_timeout_monitor, dlr);
+	ksz_init_timer(&dlr->neigh_chk_timer_info, 100 * HZ / 1000,
+		neigh_chk_monitor, dlr);
+	ksz_init_timer(&dlr->signon_timer_info, 60000 * HZ / 1000,
+		signon_monitor, dlr);
+
+}  /* setup_dlr */
+
+static int dlr_get_attrib(struct ksz_dlr_info *dlr, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_dlr_node_info *cur;
+	struct ksz_dlr_active_node *node;
+	int i;
+	struct ksz_dlr_gateway_capable *attr = &dlr->attrib;
+	u8 svc = (u8)(subcmd >> CIP_SVC_S);
+	u8 class = (u8)(subcmd >> CIP_CLASS_S);
+	u8 code = (u8)(subcmd >> CIP_ATTR_S);
+	u8 id = (u8) subcmd;
+
+	*len = 0;
+	*output = 0;
+	if (class != CLASS_DLR_OBJECT)
+		return DEV_IOC_INVALID_CMD;
+	if (svc != SVC_GET_ATTRIBUTES_ALL &&
+	    svc != SVC_GET_ATTRIBUTE_SINGLE &&
+	    svc != SVC_GET_MEMBER)
+		return DEV_IOC_INVALID_CMD;
+	if (CIP_INSTANCE_ATTRIBUTES == code) {
+		if (SVC_GET_ATTRIBUTES_ALL == svc) {
+			*len = sizeof(struct ksz_dlr_super_capable_2);
+			if (attr->cap & DLR_CAP_GATEWAY_CAPABLE)
+				*len = sizeof(struct ksz_dlr_gateway_capable);
+			memcpy(data, attr, *len);
+		} else if (SVC_GET_ATTRIBUTE_SINGLE == svc) {
+			union dlr_data *attrib = (union dlr_data *) data;
+
+			switch (id) {
+			case DLR_GET_NETWORK_TOPOLOGY:
+				*len = 1;
+				attrib->byte = attr->net_topology;
+				break;
+			case DLR_GET_NETWORK_STATUS:
+				*len = 1;
+				attrib->byte = attr->net_status;
+#if 1
+if (attrib->byte == DLR_NET_RING_FAULT && getting_last_active &&
+((!attr->last_active[0].addr[4] && !attr->last_active[0].addr[5]) ||
+(!attr->last_active[1].addr[4] && !attr->last_active[1].addr[5])))
+attrib->byte = DLR_NET_NORMAL;
+#endif
+				break;
+			case DLR_GET_RING_SUPERVISOR_STATUS:
+				*len = 1;
+				attrib->byte = attr->super_status;
+				break;
+			case DLR_SET_RING_SUPERVISOR_CONFIG:
+				*len = sizeof(struct ksz_dlr_super_cfg);
+				memcpy(&attrib->super_cfg, &attr->super_cfg,
+					*len);
+				break;
+			case DLR_SET_RING_FAULT_COUNT:
+				*len = 2;
+				attrib->word = attr->fault_cnt;
+				break;
+			case DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1:
+dbg_msg("+");
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,	&attr->last_active[0],
+					*len);
+				break;
+			case DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,	&attr->last_active[1],
+					*len);
+				break;
+			case DLR_GET_RING_PARTICIPANTS_COUNT:
+				*len = 2;
+				attrib->word = attr->participants_cnt;
+				break;
+			case DLR_GET_ACTIVE_SUPERVISOR_ADDRESS:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,
+					&attr->active_super_addr, *len);
+				break;
+			case DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE:
+				*len = 1;
+				attrib->byte = attr->active_super_prec;
+				break;
+			case DLR_GET_CAPABILITY_FLAGS:
+				*len = 4;
+				attrib->dword = attr->cap;
+				break;
+			case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = sizeof(struct ksz_dlr_gateway_cfg);
+				memcpy(&attrib->gateway_cfg, &attr->gateway_cfg,
+					*len);
+				break;
+			case DLR_GET_REDUNDANT_GATEWAY_STATUS:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = 1;
+				attrib->byte = attr->gateway_status;
+				break;
+			case DLR_GET_ACTIVE_GATEWAY_ADDRESS:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,
+					&attr->active_gateway_addr, *len);
+				break;
+			case DLR_GET_ACTIVE_GATEWAY_PRECEDENCE:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = 1;
+				attrib->byte = attr->active_gateway_prec;
+				break;
+			case DLR_GET_RING_PARTICIPANTS_LIST:
+				*len = sizeof(struct ksz_dlr_active_node);
+				if (attr->super_status !=
+				    DLR_STAT_ACTIVE_SUPERVISOR) {
+					*output = STATUS_OBJECT_STATE_CONFLICT;
+					break;
+				}
+				*len *= attr->participants_cnt;
+				if (size < *len) {
+					*output = STATUS_REPLY_DATA_TOO_LARGE;
+					break;
+				}
+				node = (struct ksz_dlr_active_node *) data;
+				for (i = 0; i < attr->participants_cnt; i++) {
+					cur = &dlr->nodes[i];
+					node->ip_addr = cur->signon.ip_addr;
+					memcpy(&node->addr, cur->signon.addr,
+						ETH_ALEN);
+					node++;
+				}
+				break;
+			}
+		} else if (SVC_GET_MEMBER == svc) {
+			switch (id) {
+			case DLR_GET_RING_PARTICIPANTS_LIST:
+				*len = sizeof(struct ksz_dlr_active_node);
+				if (attr->super_status !=
+				    DLR_STAT_ACTIVE_SUPERVISOR) {
+					*output = STATUS_OBJECT_STATE_CONFLICT;
+					break;
+				}
+				node = (struct ksz_dlr_active_node *) data;
+				i = 0;
+				cur = &dlr->nodes[i];
+				node->ip_addr = cur->signon.ip_addr;
+				memcpy(&node->addr, cur->signon.addr,
+					ETH_ALEN);
+				break;
+			}
+		}
+	} else if (CIP_CLASS_ATTRIBUTES == code) {
+		union dlr_data *attrib = (union dlr_data *) data;
+
+		if (DLR_GET_REVISION == id) {
+			*len = 2;
+			attrib->word = DLR_REVISION;
+		}
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	return DEV_IOC_OK;
+}  /* dlr_get_attrib */
+
+static int dlr_change_cfg(struct ksz_dlr_info *dlr,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	struct ksz_dlr_gateway_capable *attrib = &dlr->attrib;
+	struct ksz_dlr_super_cfg *super = &attrib->super_cfg;
+
+	if (cfg->beacon_interval < 100 || cfg->beacon_interval > 100000 ||
+	    cfg->beacon_timeout < 1000 || cfg->beacon_timeout > 2000000)
+		return STATUS_INVALID_ATTRIB_VALUE;
+	if (cfg->enable && !(dlr->attrib.cap & DLR_CAP_SUPERVISOR_CAPABLE))
+		return STATUS_INVALID_ATTRIB_VALUE;
+	if (cfg->enable != super->enable) {
+		if (super->enable) {
+			dlr->stop = true;
+		} else
+			dlr->node = DLR_SUPERVISOR;
+		super->enable = cfg->enable;
+		if (super->enable) {
+			super->prec = cfg->prec;
+			super->beacon_interval = cfg->beacon_interval;
+			super->beacon_timeout = cfg->beacon_timeout;
+			super->vid = cfg->vid;
+			dlr->precedence = super->prec;
+			dlr->beacon_interval = super->beacon_interval;
+			dlr->beacon_timeout = super->beacon_timeout;
+			dlr->vid = super->vid;
+		}
+		dlr->reset = true;
+		schedule_work(&dlr->delay_proc);
+	} else if (super->enable) {
+		if (cfg->prec != super->prec) {
+			super->prec = cfg->prec;
+			dlr->new_val = 1;
+		}
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node || dlr->new_val) {
+			if (cfg->beacon_interval != super->beacon_interval) {
+				super->beacon_interval = cfg->beacon_interval;
+				dlr->new_val = 1;
+			}
+			if (cfg->beacon_timeout != super->beacon_timeout) {
+				super->beacon_timeout = cfg->beacon_timeout;
+				dlr->new_val = 1;
+			}
+			if (cfg->vid != super->vid) {
+				super->vid = cfg->vid;
+				dlr->new_val = 1;
+			}
+		}
+		if (dlr->new_val)
+			schedule_work(&dlr->delay_proc);
+	}
+	return 0;
+}  /* dlr_change_cfg */
+
+static int dlr_set_attrib(struct ksz_dlr_info *dlr, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_dlr_gateway_capable *attr = &dlr->attrib;
+	int len = 0;
+	u8 svc = (u8)(subcmd >> CIP_SVC_S);
+	u8 class = (u8)(subcmd >> CIP_CLASS_S);
+	u8 code = (u8)(subcmd >> CIP_ATTR_S);
+	u8 id = (u8) subcmd;
+	union dlr_data *attrib = (union dlr_data *) data;
+
+	*output = 0;
+	if (class != CLASS_DLR_OBJECT)
+		return DEV_IOC_INVALID_CMD;
+	switch (svc) {
+	case SVC_SET_ATTRIBUTE_SINGLE:
+		if (CIP_INSTANCE_ATTRIBUTES != code)
+			return DEV_IOC_INVALID_CMD;
+		switch (id) {
+		case DLR_SET_RING_SUPERVISOR_CONFIG:
+			len = sizeof(struct ksz_dlr_super_cfg);
+			break;
+		case DLR_SET_RING_FAULT_COUNT:
+			len = 2;
+			break;
+		case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+			if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+				break;
+			len = sizeof(struct ksz_dlr_gateway_cfg);
+			break;
+		case DLR_SET_IP_ADDRESS:
+			len = sizeof(struct ksz_dlr_active_node);
+			break;
+		}
+		if (!len)
+			return DEV_IOC_INVALID_CMD;
+		if (size < len) {
+			*req_size = len + SIZEOF_ksz_request;
+			return DEV_IOC_INVALID_LEN;
+		}
+		switch (id) {
+		case DLR_SET_RING_SUPERVISOR_CONFIG:
+			*output = dlr_change_cfg(dlr, &attrib->super_cfg);
+			break;
+		case DLR_SET_RING_FAULT_COUNT:
+			if (attrib->word) {
+				*output = STATUS_INVALID_ATTRIB_VALUE;
+				break;
+			}
+			attr->fault_cnt = attrib->word;
+			break;
+		case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+			if (memcmp(&attr->gateway_cfg, &attrib->gateway_cfg,
+			    len)) {
+				memcpy(&attr->gateway_cfg, &attrib->gateway_cfg,
+					len);
+			}
+			break;
+		case DLR_SET_IP_ADDRESS:
+		{
+			struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+			dlr->ip_addr = attrib->active.ip_addr;
+			frame->hdr.ip_addr = dlr->ip_addr;
+			break;
+		}
+		}
+		break;
+	case SVC_DLR_VERIFY_FAULT_LOCATION:
+	{
+		struct ksz_dlr_node_info *node;
+		int i;
+
+		if (attr->super_status !=
+		    DLR_STAT_ACTIVE_SUPERVISOR ||
+		    dlr->state != DLR_ACTIVE_FAULT_STATE) {
+			*output = STATUS_OBJECT_STATE_CONFLICT;
+			memset(&attr->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attr->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			break;
+		}
+		for (i = 1; i < attr->participants_cnt; i++) {
+			node = &dlr->nodes[i];
+			node->p1_down = 0;
+			node->p1_lost = 0;
+			node->p2_down = 0;
+			node->p2_lost = 0;
+		}
+		dlr_tx_locate_fault(dlr);
+		break;
+	}
+	case SVC_DLR_CLEAR_RAPID_FAULTS:
+		break;
+	case SVC_DLR_RESTART_SIGN_ON:
+		if (attr->super_status !=
+		    DLR_STAT_ACTIVE_SUPERVISOR ||
+		    dlr->state != DLR_ACTIVE_NORMAL_STATE) {
+			*output = STATUS_OBJECT_STATE_CONFLICT;
+			break;
+		}
+
+		/* SignOn timer not started. */
+		if (!dlr->signon_start) {
+			dlr->signon_start = 1;
+			startSignOn(dlr, false);
+		}
+		break;
+	case SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT:
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+}  /* dlr_set_attrib */
+
+static int dlr_dev_req(struct ksz_dlr_info *dlr, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 0;
+				data[5] = dlr->member;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+				dlr->dev_info = info;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			msg->module = DEV_MOD_DLR;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			sw_setup_msg(info, msg, 8, NULL, NULL);
+			dlr->notifications = 0;
+			dlr->dev_info = NULL;
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dlr->notifications = *notify;
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = dlr_set_attrib(dlr, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		result = dlr_get_attrib(dlr, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* dlr_dev_req */
+
+#define _dlr_dev_req(a, b)		dlr_dev_req(a, b, NULL)
+
+static void set_dlr_req(void *ptr, int cmd,
+	u8 svc, u8 class, u8 code, u8 id, void *dlr, size_t dlr_size)
+{
+	struct ksz_request *req = ptr;
+
+	req->size = SIZEOF_ksz_request;
+	req->size += dlr_size;
+	req->cmd = cmd;
+	req->subcmd = (svc << CIP_SVC_S) | (class << CIP_CLASS_S) |
+		(code << CIP_ATTR_S) | id;
+	req->output = 0;
+	if (dlr)
+		memcpy(&req->param, dlr, dlr_size);
+}  /* set_dlr_req */
+
+static int get_dlr_revision(void *fd,
+	u16 *rev)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_CLASS_ATTRIBUTES, DLR_GET_REVISION,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*rev = data->word;
+	}
+	return rc;
+}  /* get_dlr_revision */
+
+static int get_dlr_all(void *fd,
+	struct ksz_dlr_gateway_capable *capable)
+{
+	struct ksz_request_actual *req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	req = kzalloc(sizeof(struct ksz_request_actual), GFP_KERNEL);
+	if (!req)
+		return -1;
+	set_dlr_req(req, DEV_CMD_GET, SVC_GET_ATTRIBUTES_ALL,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES, 0,
+		NULL, sizeof(struct ksz_dlr_gateway_capable));
+	rc = _dlr_dev_req(dlr, (char *) req);
+	if (!rc)
+		rc = req->result;
+	if (!rc) {
+		memcpy(capable, &req->param, req->size - SIZEOF_ksz_request);
+	}
+	kfree(req);
+	return rc;
+}  /* get_dlr_all */
+
+static int get_dlr_topology(void *fd,
+	u8 *topology)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_NETWORK_TOPOLOGY,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*topology = data->byte;
+	}
+	return rc;
+}  /* get_dlr_topology */
+
+static int get_dlr_network(void *fd,
+	u8 *network)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_NETWORK_STATUS,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*network = data->byte;
+	}
+	return rc;
+}  /* get_dlr_network */
+
+static int get_dlr_super_status(void *fd,
+	u8 *status)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_SUPERVISOR_STATUS,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*status = data->byte;
+	}
+	return rc;
+}  /* get_dlr_super_status */
+
+static int get_dlr_super_cfg(void *fd,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_SUPERVISOR_CONFIG,
+		NULL, sizeof(struct ksz_dlr_super_cfg));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(cfg, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_super_cfg */
+
+static int set_dlr_super_cfg(void *fd,
+	struct ksz_dlr_super_cfg *cfg, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_SET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_SUPERVISOR_CONFIG,
+		cfg, sizeof(struct ksz_dlr_super_cfg));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_super_cfg */
+
+static int get_dlr_ring_fault_cnt(void *fd,
+	u16 *cnt)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_FAULT_COUNT,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*cnt = data->word;
+	}
+	return rc;
+}  /* get_dlr_ring_fault_cnt */
+
+static int set_dlr_ring_fault_cnt(void *fd,
+	u16 cnt, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_SET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_FAULT_COUNT,
+		&cnt, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_ring_fault_cnt */
+
+static int get_dlr_active_node(void *fd,
+	u8 port, struct ksz_dlr_active_node *node)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+	u8 id;
+
+	if (1 == port)
+		id = DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2;
+	else
+		id = DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1;
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		id,
+		NULL, sizeof(struct ksz_dlr_active_node));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(node, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_active_node */
+
+static int get_dlr_ring_part_cnt(void *fd,
+	u16 *cnt)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_PARTICIPANTS_COUNT,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*cnt = data->word;
+	}
+	return rc;
+}  /* get_dlr_ring_part_cnt */
+
+static int get_dlr_ring_part_list(void *fd,
+	struct ksz_dlr_active_node *node, u16 *size, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_PARTICIPANTS_LIST,
+		NULL, *size);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+		*size = req.size - SIZEOF_ksz_request;
+		memcpy(node, &req.param, *size);
+	}
+	return rc;
+}  /* get_dlr_ring_part_list */
+
+static int get_dlr_active_super_addr(void *fd,
+	struct ksz_dlr_active_node *node)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_ACTIVE_SUPERVISOR_ADDRESS,
+		NULL, sizeof(struct ksz_dlr_active_node));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(node, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_active_super_addr */
+
+static int get_dlr_active_super_prec(void *fd,
+	u8 *prec)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*prec = data->byte;
+	}
+	return rc;
+}  /* get_dlr_active_super_prec */
+
+static int get_dlr_cap(void *fd,
+	u32 *flags)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_CAPABILITY_FLAGS,
+		NULL, sizeof(u32));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*flags = data->dword;
+	}
+	return rc;
+}  /* get_dlr_cap */
+
+static int set_dlr_verify_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_VERIFY_FAULT_LOCATION,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_verify_fault */
+
+static int set_dlr_clear_rapid_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_CLEAR_RAPID_FAULTS,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_clear_rapid_fault */
+
+static int set_dlr_restart_sign_on(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_RESTART_SIGN_ON,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_restart_sign_on */
+
+static int set_dlr_clear_gateway_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_clear_gateway_fault */
+
+static void test_monitor(unsigned long ptr)
+{
+	struct ksz_dlr_info *info = (struct ksz_dlr_info *) ptr;
+
+	if (DLR_SUPERVISOR <= info->node) {
+		u8 prec;
+
+		if (1 == info->precedence)
+			prec = 9;
+		else if (9 == info->precedence)
+			prec = 1;
+		else
+			prec = 0;
+		if (prec) {
+			int rc;
+			u8 err;
+			struct ksz_dlr_super_cfg super;
+
+			memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+			super.prec = prec;
+			rc = set_dlr_super_cfg(info, &super, &err);
+		}
+	}
+	ksz_update_timer(&info->test_timer_info);
+}  /* test_monitor */
+
+enum {
+	PROC_GET_DLR_INFO,
+	PROC_SET_DLR_NODE,
+	PROC_SET_DLR_PRECEDENCE,
+	PROC_SET_DLR_INTERVAL,
+	PROC_SET_DLR_TIMEOUT,
+	PROC_SET_DLR_VID,
+	PROC_SET_DLR_CFG,
+	PROC_SET_DLR_STATE,
+	PROC_SET_DLR_PORT,
+	PROC_SET_DLR_TEST,
+	PROC_SET_DLR_NEIGH_CHK_REQ,
+	PROC_SET_DLR_NEIGH_CHK_RESP,
+	PROC_SET_DLR_LINK_BREAK,
+	PROC_SET_DLR_LEARNING_UPDATE,
+	PROC_SET_DLR_LOCATE_FAULT,
+	PROC_SET_DLR_SIGN_ON,
+	PROC_SET_DLR_CLEAR_FAULT,
+	PROC_SET_DLR_CLEAR_GATEWAY_FAULT,
+
+	PROC_GET_DLR_ALL,
+	PROC_GET_DLR_REVISION,
+	PROC_GET_DLR_NETWORK_TOPOLOGY,
+	PROC_GET_DLR_NETWORK_STATUS,
+	PROC_GET_DLR_RING_SUPERVISOR_STATUS,
+	PROC_SET_DLR_RING_SUPERVISOR_CONFIG,
+	PROC_SET_DLR_RING_FAULT_COUNT,
+	PROC_GET_DLR_LAST_ACTIVE_NODE_1,
+	PROC_GET_DLR_LAST_ACTIVE_NODE_2,
+	PROC_GET_DLR_RING_PARTICIPANTS_COUNT,
+	PROC_GET_DLR_RING_PARTICIPANTS_LIST,
+	PROC_GET_DLR_ACTIVE_SUPERVISOR_ADDRESS,
+	PROC_GET_DLR_ACTIVE_SUPERVISOR_PRECEDENCE,
+	PROC_GET_DLR_CAPABILITIES,
+	PROC_SET_DLR_PORT_1,
+	PROC_SET_DLR_PORT_2,
+};
+
+static ssize_t display_faults(struct ksz_dlr_info *dlr, char *buf, ssize_t len)
+{
+	int i;
+	struct ksz_dlr_node_info *node;
+	struct ksz_dlr_node *signon;
+
+	for (i = 0; i < dlr->attrib.participants_cnt; i++) {
+		node = &dlr->nodes[i];
+		signon = &node->signon;
+		if (!node->p1_down && !node->p2_down &&
+		    !node->p1_lost && !node->p2_lost)
+			continue;
+		len += sprintf(buf + len, 
+			"%02x:%02x:%02x:%02x:%02x:%02x  %3u.%3u.%3u.%3u  ",
+			signon->addr[0], signon->addr[1], signon->addr[2],
+			signon->addr[3], signon->addr[4], signon->addr[5],
+			(u8) signon->ip_addr,
+			(u8)(signon->ip_addr >> 8),
+			(u8)(signon->ip_addr >> 24),
+			(u8)(signon->ip_addr >> 16));
+		len += sprintf(buf + len, "%u:%u %u:%u\n",
+			node->p1_down, node->p2_down,
+			node->p1_lost, node->p2_lost);
+		if (len >= 2048 - 80) {
+			len += sprintf(buf + len, "...\n");
+			break;
+		}
+	}
+	return len;
+}  /* display_faults */
+
+static ssize_t display_nodes(struct ksz_dlr_info *dlr, char *buf, ssize_t len)
+{
+	int i;
+	struct ksz_dlr_node_info *node;
+	struct ksz_dlr_node *signon;
+
+	for (i = 0; i < dlr->attrib.participants_cnt; i++) {
+		node = &dlr->nodes[i];
+		signon = &node->signon;
+		len += sprintf(buf + len,
+			"%02x:%02x:%02x:%02x:%02x:%02x  %3u.%3u.%3u.%3u  ",
+			signon->addr[0], signon->addr[1], signon->addr[2],
+			signon->addr[3], signon->addr[4], signon->addr[5],
+			(u8) signon->ip_addr,
+			(u8)(signon->ip_addr >> 8),
+			(u8)(signon->ip_addr >> 16),
+			(u8)(signon->ip_addr >> 24));
+		len += sprintf(buf + len, "%u:%u %u:%u\n",
+			node->p1_down, node->p2_down,
+			node->p1_lost, node->p2_lost);
+		if (len >= 2048 - 80) {
+			len += sprintf(buf + len, "...\n");
+			break;
+		}
+	}
+	return len;
+}  /* display_nodes */
+
+static ssize_t show_dlr_err(ssize_t len, char *buf, u8 err)
+{
+	if (buf)
+		len += sprintf(buf + len, "!0x%02x\n", err);
+	else
+		printk(KERN_INFO "!0x%02x\n", err);
+	return len;
+}  /* show_dlr_err */
+
+static ssize_t show_dlr_attrib(ssize_t len, char *buf, u8 err, char *format,
+	u32 data)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len, format, data);
+	return len;
+}  /* show_dlr_attrib */
+
+static ssize_t show_dlr_attrib_super(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len, "E:%u  P:%u  I:%u  T:%u  V:%u\n",
+			cfg->enable,
+			cfg->prec,
+			cfg->beacon_interval,
+			cfg->beacon_timeout,
+			cfg->vid);
+	return len;
+}  /* show_dlr_attrib_super */
+
+static ssize_t show_dlr_attrib_node(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_active_node *node)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len,
+			"%02x:%02x:%02x:%02x:%02x:%02x  %3u.%3u.%3u.%3u\n",
+			node->addr[0],
+			node->addr[1],
+			node->addr[2],
+			node->addr[3],
+			node->addr[4],
+			node->addr[5],
+			(u8) node->ip_addr,
+			(u8)(node->ip_addr >> 8),
+			(u8)(node->ip_addr >> 16),
+			(u8)(node->ip_addr >> 24));
+	return len;
+}  /* show_dlr_attrib_node */
+
+static ssize_t show_dlr_attrib_all(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_gateway_capable *capable)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else {
+		len = show_dlr_attrib(len, buf, 0,
+			"top:%u  ", capable->net_topology);
+		len = show_dlr_attrib(len, buf, 0,
+			"net:%u  ", capable->net_status);
+		len = show_dlr_attrib(len, buf, 0,
+			"sup:%u  ", capable->super_status);
+		len = show_dlr_attrib(len, buf, 0,
+			"fault:%u\n", capable->fault_cnt);
+		len = show_dlr_attrib_super(len, buf, 0,
+			&capable->super_cfg);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->last_active[0]);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->last_active[1]);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->active_super_addr);
+		len = show_dlr_attrib(len, buf, 0,
+			"cnt:%u  ", capable->participants_cnt);
+		len = show_dlr_attrib(len, buf, 0,
+			"prec:%u  ", capable->active_super_prec);
+		len = show_dlr_attrib(len, buf, 0,
+			"cap:%08x\n", capable->cap);
+	}
+	return len;
+}  /* show_dlr_attrib_all */
+
+static ssize_t sysfs_dlr_read(struct ksz_dlr_info *dlr, int proc_num,
+	ssize_t len, char *buf)
+{
+	struct ksz_dlr_active_node node;
+	struct ksz_dlr_super_cfg super;
+	int rc;
+	u32 dword;
+	u16 word;
+	u8 byte;
+	u8 err = 0;
+
+	switch (proc_num) {
+	case PROC_GET_DLR_INFO:
+		break;
+	case PROC_SET_DLR_NODE:
+		len += sprintf(buf + len, "%u\n", dlr->node);
+		break;
+	case PROC_SET_DLR_PRECEDENCE:
+		len += sprintf(buf + len, "%u\n", dlr->precedence);
+		break;
+	case PROC_SET_DLR_INTERVAL:
+		len += sprintf(buf + len, "%u\n", dlr->beacon_interval);
+		break;
+	case PROC_SET_DLR_TIMEOUT:
+		len += sprintf(buf + len, "%u\n", dlr->beacon_timeout);
+		break;
+	case PROC_SET_DLR_VID:
+		len += sprintf(buf + len, "0x%03x\n", dlr->vid);
+		break;
+	case PROC_SET_DLR_STATE:
+		len += sprintf(buf + len, "%u\n", dlr->ring_state);
+		break;
+	case PROC_SET_DLR_PORT:
+		len += sprintf(buf + len, "%u\n", dlr->port);
+		break;
+	case PROC_SET_DLR_TEST:
+		len += sprintf(buf + len, "%u\n",
+			(dlr->overrides & DLR_TEST) ? 1 : 0);
+		break;
+	case PROC_SET_DLR_LOCATE_FAULT:
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+			len = display_faults(dlr, buf, len);
+		break;
+	case PROC_SET_DLR_SIGN_ON:
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+			len = display_nodes(dlr, buf, len);
+		else
+			len += sprintf(buf + len, "%u\n", dlr->ignore_req);
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_REQ:
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_RESP:
+		len += sprintf(buf + len, "%u\n", dlr->ignore_req);
+		break;
+	case PROC_SET_DLR_LINK_BREAK:
+		len += sprintf(buf + len, "0x%x\n", dlr->link_break);
+		break;
+	case PROC_GET_DLR_REVISION:
+		rc = get_dlr_revision(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_ALL:
+	{
+		struct ksz_dlr_gateway_capable *capable;
+
+		capable = kzalloc(sizeof(struct ksz_dlr_gateway_capable),
+			GFP_KERNEL);
+		if (!capable)
+			break;
+		rc = get_dlr_all(dlr, capable);
+		if (!rc)
+			len = show_dlr_attrib_all(len, buf, err, capable);
+		kfree(capable);
+		break;
+	}
+	case PROC_GET_DLR_NETWORK_TOPOLOGY:
+		rc = get_dlr_topology(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_NETWORK_STATUS:
+		rc = get_dlr_network(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_RING_SUPERVISOR_STATUS:
+		rc = get_dlr_super_status(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_SET_DLR_RING_SUPERVISOR_CONFIG:
+		rc = get_dlr_super_cfg(dlr, &super);
+		if (!rc)
+			len = show_dlr_attrib_super(len, buf, err, &super);
+		break;
+	case PROC_SET_DLR_RING_FAULT_COUNT:
+		rc = get_dlr_ring_fault_cnt(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_LAST_ACTIVE_NODE_1:
+		rc = get_dlr_active_node(dlr, 0, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_LAST_ACTIVE_NODE_2:
+		rc = get_dlr_active_node(dlr, 1, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_RING_PARTICIPANTS_COUNT:
+		rc = get_dlr_ring_part_cnt(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_RING_PARTICIPANTS_LIST:
+	{
+		struct ksz_dlr_active_node *nodes;
+
+		rc = get_dlr_ring_part_cnt(dlr, &word);
+		if (rc || !word)
+			break;
+
+		word *= sizeof(struct ksz_dlr_active_node);
+		nodes = kzalloc(word, GFP_KERNEL);
+		if (!nodes)
+			break;
+
+		rc = get_dlr_ring_part_list(dlr, nodes, &word, &err);
+		if (!rc) {
+			int i;
+
+			if (err) {
+				len = show_dlr_err(len, buf, err);
+				break;
+			}
+			word /= sizeof(struct ksz_dlr_active_node);
+			for (i = 0; i < word; i++)
+				len = show_dlr_attrib_node(len, buf, 0,
+					&nodes[i]);
+		}
+		kfree(nodes);
+		break;
+	}
+	case PROC_GET_DLR_ACTIVE_SUPERVISOR_ADDRESS:
+		rc = get_dlr_active_super_addr(dlr, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_ACTIVE_SUPERVISOR_PRECEDENCE:
+		rc = get_dlr_active_super_prec(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_CAPABILITIES:
+		rc = get_dlr_cap(dlr, &dword);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%08x\n", dword);
+		break;
+	case PROC_SET_DLR_PORT_1:
+		len += sprintf(buf + len, "%u\n", dlr->ports[0]);
+		break;
+	case PROC_SET_DLR_PORT_2:
+		len += sprintf(buf + len, "%u\n", dlr->ports[1]);
+		break;
+	}
+	return len;
+}  /* sysfs_dlr_read */
+
+static void sysfs_dlr_write(struct ksz_dlr_info *info, int proc_num, int num,
+	const char *buf)
+{
+	int rc;
+	u8 err;
+	struct ksz_dlr_super_cfg super;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_sw *sw = info->sw_dev;
+	int node = DLR_ACTIVE_SUPERVISOR;
+	u32 member = 0;
+
+	if (sw->overrides & HAVE_MORE_THAN_2_PORTS)
+		member = info->member | sw->HOST_MASK;
+	switch (proc_num) {
+	case PROC_SET_DLR_NODE:
+#ifdef CONFIG_HAVE_DLR_HW
+		/* Cannot be a supervisor without beacon generation. */
+		if (!(sw->features & REDUNDANCY_SUPPORT))
+			node = DLR_BEACON_NODE;
+#endif
+		if (!(DLR_ANNOUNCE_NODE <= num && num <= node))
+			break;
+		if (num == info->node)
+			break;
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR <= info->node) {
+			int prev_node = info->node;
+
+			info->node = DLR_BEACON_NODE;
+			disableSupervisor(info);
+			if (DLR_ACTIVE_SUPERVISOR == prev_node) {
+				enableBothPorts(info);
+				disableAnnounce(info);
+				disableSignOnTimer(info);
+			}
+		} else if (DLR_NORMAL_STATE == info->state) {
+			setupDir(info, -1);
+		}
+		if (info->skip_beacon)
+			acceptBeacons(info);
+		if (DLR_ANNOUNCE_NODE == info->node)
+			sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY,
+				MAC_ADDR_BEACON, member, false, false, 0);
+		if (num == info->node)
+			break;
+		info->node = (u8) num;
+		dlr_reset_attrib(info);
+		info->attrib.super_cfg.beacon_interval = info->beacon_interval;
+		info->attrib.super_cfg.beacon_timeout = info->beacon_timeout;
+#ifdef DBG_DLR_BEACON
+		dbg_bcn = 4;
+#endif
+		do {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (DLR_ANNOUNCE_NODE == info->node) {
+#ifdef CONFIG_HAVE_ACL_HW
+				disable_acl_beacon_timeout(info,
+					info->ports[0]);
+				disable_acl_beacon_timeout(info,
+					info->ports[1]);
+#endif
+				sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY,
+					MAC_ADDR_BEACON, info->member,
+					false, false, 0);
+				info->state_machine = AnnounceRingNode_state;
+			} else
+				info->state_machine = RingSupervisor_state;
+			info->reset = true;
+			schedule_work(&info->delay_proc);
+		} while (0);
+		break;
+	case PROC_SET_DLR_PRECEDENCE:
+		/* Value can only be set through supervisor. */
+		if (info->node < DLR_SUPERVISOR)
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.prec = (u8) num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_INTERVAL:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num < 200) {
+			printk("too small\n");
+			break;
+		} else if (num * 4 > info->beacon_timeout) {
+			printk("too large\n");
+			break;
+		}
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.beacon_interval = num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_TIMEOUT:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num < info->beacon_interval * 4) {
+			printk("too small\n");
+			break;
+
+#ifdef CONFIG_HAVE_ACL_HW
+		} else if (num >= ACL_CNT_M * 1000) {
+			printk("too large\n");
+			break;
+#endif
+		}
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.beacon_timeout = num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_VID:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num >= 0x400)
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.vid = (u16) num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CFG:
+	{
+		int enable;
+		int prec;
+		int interval;
+		int timeout;
+		int vid;
+
+		sscanf(buf, "%u %u %u %u %u",
+			&enable, &prec, &interval, &timeout, &vid);
+		if (interval < 200) {
+			printk("too small\n");
+			break;
+		} else if (interval * 4 > timeout) {
+			printk("too large\n");
+			break;
+		}
+		if (timeout < interval * 4) {
+			printk("too small\n");
+			break;
+
+#ifdef CONFIG_HAVE_ACL_HW
+		} else if (timeout >= ACL_CNT_M * 1000) {
+			printk("too large\n");
+			break;
+#endif
+		}
+		super.enable = (u8) enable;
+		super.prec = (u8) prec;
+		super.beacon_interval = interval;
+		super.beacon_timeout = timeout;
+		super.vid = (u16) vid;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	}
+	case PROC_SET_DLR_STATE:
+		if (1 <= num && num <= 2) {
+			info->ring_state = (u8) num;
+			dlr_set_state(info);
+		}
+		break;
+	case PROC_SET_DLR_PORT:
+		if (0 <= num && num <= 1) {
+			info->port = (u8) num;
+			info->tx_port = (u8) num;
+			info->rx_port = (info->tx_port + 1) & 1;
+		}
+		break;
+	case PROC_SET_DLR_TEST:
+		if (num & 1)
+			info->overrides |= DLR_TEST;
+		else
+			info->overrides &= ~DLR_TEST;
+		if (num & 2)
+			info->overrides |= DLR_TEST_SEQ;
+		else
+			info->overrides &= ~DLR_TEST_SEQ;
+		if (num & 0x80)
+			ksz_start_timer(&info->test_timer_info,
+				info->test_timer_info.period);
+		else
+			ksz_stop_timer(&info->test_timer_info);
+		break;
+	case PROC_SET_DLR_LEARNING_UPDATE:
+		dlr_tx_learning_update(info);
+		break;
+	case PROC_SET_DLR_LOCATE_FAULT:
+		rc = set_dlr_verify_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_SIGN_ON:
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			info->signon_space = num;
+		} else {
+			info->ignore_req = (u8) num;
+			info->req_cnt[0] = info->req_cnt[1] = 0;
+			if (info->overrides & DLR_TEST)
+				break;
+		}
+		rc = set_dlr_restart_sign_on(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_REQ:
+		if (num & 3) {
+			if (!info->neigh_chk) {
+				info->neigh_chk = 1;
+				ksz_start_timer(&info->neigh_chk_timer_info,
+					info->neigh_chk_timer_info.period);
+			}
+			info->neigh_chk_timer_info.max = 3;
+		}
+		if (num & 1) {
+			info->port_chk[0] = 1;
+			dlr_tx_chk_req(info, 0);
+		}
+		if (num & 2) {
+			info->port_chk[1] = 1;
+			dlr_tx_chk_req(info, 1);
+		}
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_RESP:
+		info->ignore_req = (u8) num;
+		info->req_cnt[0] = info->req_cnt[1] = 0;
+		break;
+	case PROC_SET_DLR_LINK_BREAK:
+		info->link_break = num;
+		if (info->link_break & 1)
+			info->p1_down = 1;
+		else
+			info->p1_down = 0;
+		if (info->link_break & 2)
+			info->p2_down = 1;
+		else
+			info->p2_down = 0;
+		if (info->link_break & 4)
+			info->p1_lost = 1;
+		else
+			info->p1_lost = 0;
+		if (info->link_break & 8)
+			info->p2_lost = 1;
+		else
+			info->p2_lost = 0;
+		if (info->node != DLR_ACTIVE_SUPERVISOR)
+			dlr_tx_status(info, 0);
+		else if (attrib->participants_cnt > 0) {
+			struct ksz_dlr_node_info *node;
+
+			node = &info->nodes[0];
+			if (attrib->participants_cnt > 1) {
+				struct ksz_dlr_node_info *prev;
+				struct ksz_dlr_node_info *next;
+
+				prev = &info->nodes[attrib->participants_cnt -
+					1];
+				next = &info->nodes[1];
+				if (info->p1_lost)
+					prev->p2_lost = 1;
+				if (info->p2_lost)
+					next->p1_lost = 1;
+			}
+			node->p1_down = info->p1_down;
+			node->p2_down = info->p2_down;
+			node->p1_lost = info->p1_lost;
+			node->p2_lost = info->p2_lost;
+		}
+		break;
+	case PROC_SET_DLR_RING_SUPERVISOR_CONFIG:
+	{
+		struct ksz_dlr_super_cfg super;
+
+		/* Value can only be set through supervisor. */
+		if (!(info->attrib.cap & DLR_CAP_SUPERVISOR_CAPABLE))
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		if (num) {
+			super.enable = true;
+			if (!super.beacon_interval)
+				super.beacon_interval = 400;
+			if (!super.beacon_timeout)
+				super.beacon_timeout = 1960;
+		} else
+			super.enable = false;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	}
+	case PROC_SET_DLR_RING_FAULT_COUNT:
+		rc = set_dlr_ring_fault_cnt(info, num, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CLEAR_FAULT:
+		rc = set_dlr_clear_rapid_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CLEAR_GATEWAY_FAULT:
+		rc = set_dlr_clear_gateway_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_PORT_1:
+#ifdef CONFIG_HAVE_DLR_HW
+		if (netif_running(info->dev))
+			printk(KERN_ALERT "stop %s first", info->dev->name);
+		if (0 <= num && num < sw->port_cnt &&
+		    num != sw->HOST_PORT && num != info->ports[1])
+			info->ports[0] = (u8) num;
+		info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+#endif
+		break;
+	case PROC_SET_DLR_PORT_2:
+#ifdef CONFIG_HAVE_DLR_HW
+		if (netif_running(info->dev))
+			printk(KERN_ALERT "stop %s first", info->dev->name);
+		if (0 <= num && num < sw->port_cnt &&
+		    num != sw->HOST_PORT && num != info->ports[0])
+			info->ports[1] = (u8) num;
+		info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+#endif
+		break;
+	}
+}  /* sysfs_dlr_write */
+
+static struct dlr_ops dlr_ops = {
+	.change_addr		= dlr_change_addr,
+	.link_change		= dlr_link_change,
+	.timeout		= dlr_timeout,
+
+	.dev_req		= dlr_dev_req,
+
+	.sysfs_read		= sysfs_dlr_read,
+	.sysfs_write		= sysfs_dlr_write,
+};
+
+static void ksz_dlr_exit(struct ksz_dlr_info *dlr)
+{
+	ksz_stop_timer(&dlr->announce_timeout_timer_info);
+	ksz_stop_timer(&dlr->neigh_chk_timer_info);
+	ksz_stop_timer(&dlr->signon_timer_info);
+	ksz_stop_timer(&dlr->test_timer_info);
+	cancel_delayed_work_sync(&dlr->announce_tx);
+}  /* ksz_dlr_exit */
+
+static void ksz_dlr_init(struct ksz_dlr_info *dlr, struct ksz_sw *sw)
+{
+	dlr->ports[0] = 0;
+	dlr->ports[1] = 1;
+	dlr->member = (1 << dlr->ports[0]) | (1 << dlr->ports[1]);
+	dlr->ok_ports = dlr->member;
+	dlr->sw_dev = sw;
+	dlr->node = DLR_BEACON_NODE;
+	dlr->state_machine = RingSupervisor_state;
+	setup_dlr(dlr);
+	INIT_DELAYED_WORK(&dlr->announce_tx, announce_monitor);
+	INIT_WORK(&dlr->delay_proc, dlr_delay_proc);
+	INIT_WORK(&dlr->neigh_chk_proc, neigh_chk_proc);
+	skb_queue_head_init(&dlr->rxq);
+	dlr->ops = &dlr_ops;
+	ksz_init_timer(&dlr->test_timer_info, 1000 * HZ / 1000,
+		test_monitor, dlr);
+}  /* ksz_dlr_init */
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_dlr.h b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr.h
new file mode 100644
index 0000000..7f49fca
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr.h
@@ -0,0 +1,347 @@
+/**
+ * Microchip DLR driver header
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_DLR_H
+#define KSZ_DLR_H
+
+#include <linux/if_vlan.h>
+#include "ksz_dlr_api.h"
+
+
+#define DLR_TAG_TYPE			0x80E1
+
+#define DLR_RING_SUBTYPE		2
+
+#define DLR_BEACON			0x1
+#define DLR_NEIGH_CHK_REQ		0x2
+#define DLR_NEIGH_CHK_RESP		0x3
+#define DLR_LINK_STATUS			0x4
+#define DLR_LOCATE_FAULT		0x5
+#define DLR_ANNOUNCE			0x6
+#define DLR_SIGN_ON			0x7
+#define DLR_ADVERTISE			0x8
+#define DLR_FLUSH_TABLES		0x9
+#define DLR_LEARNING_UPDATE		0xA
+
+#define DLR_PORT_NONE			0
+#define DLR_PORT_1			1
+#define DLR_PORT_2			2
+
+#define RING_NORMAL_STATE		1
+#define RING_FAULT_STATE		2
+
+#define DLR_GW_ACTIVE_LISTEN_STATE	1
+#define DLR_GW_ACTIVE_NORMAL_STATE	2
+#define DLR_GW_FAULT_STATE		3
+
+
+struct ksz_dlr_hdr {
+	u8 ring_subtype;
+	u8 ring_protocol_version;
+	u8 frame_type;
+	u8 src_port;
+	u32 ip_addr;
+	u32 seqid;
+} __packed;
+
+struct ksz_dlr_advertise {
+	u8 state;
+	u8 precedence;
+	u32 interval;
+	u32 timeout;
+	u8 learning_update_enable;
+} __packed;
+
+struct ksz_dlr_announce {
+	u8 ring_state;
+} __packed;
+
+struct ksz_dlr_beacon {
+	u8 ring_state;
+	u8 precedence;
+	u32 interval;
+	u32 timeout;
+	u8 reserved[20];
+} __packed;
+
+struct ksz_dlr_flush_tables {
+	u8 learning_update_enable;
+} __packed;
+
+struct ksz_dlr_status {
+	u8 port1_active:1;
+	u8 port2_active:1;
+	u8 reserved:5;
+	u8 neighbor:1;
+} __packed;
+
+struct ksz_dlr_node {
+	u32 ip_addr;
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct ksz_dlr_signon {
+	u16 num;
+	struct ksz_dlr_node node[1];
+} __packed;
+
+struct ksz_dlr_neigh_chk_resp {
+	u8 src_port;
+} __packed;
+
+struct ksz_dlr_frame {
+	struct ksz_dlr_hdr hdr;
+	union {
+		struct ksz_dlr_advertise advertise;
+		struct ksz_dlr_announce announce;
+		struct ksz_dlr_beacon beacon;
+		struct ksz_dlr_flush_tables flush;
+		struct ksz_dlr_status status;
+		struct ksz_dlr_neigh_chk_resp neigh_chk_resp;
+		struct ksz_dlr_signon signon;
+		u8 reserved[30];
+	} data;
+} __packed;
+
+struct ksz_dlr_tx_frame {
+	struct vlan_ethhdr vlan;
+	struct ksz_dlr_frame body;
+} __packed;
+
+struct ksz_dlr_update_frame {
+	struct ethhdr eth;
+	struct ksz_dlr_hdr hdr;
+	u8 reserved[34];
+} __packed;
+
+struct ksz_dlr_rx_frame {
+	struct vlan_ethhdr *vlan;
+	struct ksz_dlr_frame *body;
+};
+
+#define DLR_SUPERVISOR_NUM	10
+
+struct ksz_dlr_super_info {
+	u8 prec_addr[ETH_ALEN + 1];
+	u32 crc;
+	int port;
+	u32 timeout[2];
+	u32 cnt;
+	u32 last_cnt;
+	u32 sent:1;
+};
+
+struct ksz_dlr_info;
+
+struct dlr_ops {
+	void (*change_addr)(struct ksz_dlr_info *dlr, u8 *addr);
+	void (*link_change)(struct ksz_dlr_info *dlr, int link1, int link2);
+	void (*timeout)(struct ksz_dlr_info *dir, int port);
+
+	int (*dev_req)(struct ksz_dlr_info *dlr, char *arg, void *info);
+
+	ssize_t (*sysfs_read)(struct ksz_dlr_info *dlr, int proc_num,
+		ssize_t len, char *buf);
+	void (*sysfs_write)(struct ksz_dlr_info *dlr, int proc_num, int num,
+		const char *buf);
+
+};
+
+struct ksz_dlr_node_info {
+	struct ksz_dlr_node signon;
+	u32 p1_down:1;
+	u32 p2_down:1;
+	u32 p1_lost:1;
+	u32 p2_lost:1;
+};
+
+struct ksz_dlr_beacon_info {
+	u32 timer:1;
+	u32 rcv_once:1;
+	u32 timeout_start:1;
+	u32 timeout_stop:1;
+	u32 interval;
+	u32 timeout;
+	struct ksz_dlr_beacon last;
+};
+
+#define DLR_BEACON_STATE_HACK		(1 << 1)
+#define DLR_TEST_SEQ			(1 << 30)
+#define DLR_TEST			(1 << 31)
+
+struct ksz_dlr_info {
+	u32 p1_down:1;
+	u32 p2_down:1;
+	u32 p1_lost:1;
+	u32 p2_lost:1;
+	u32 p1_rcvd:1;
+	u32 p2_rcvd:1;
+	u32 p1_set:1;
+	u32 p2_set:1;
+	u32 p1_timeout:1;
+	u32 p2_timeout:1;
+	u32 one_down:1;
+	u32 both_down:1;
+	u32 one_rcvd:1;
+	u32 both_rcvd:1;
+	u32 one_timeout:1;
+	u32 both_timeout:1;
+	u32 new_supervisor:1;
+	u32 ann_rcvd:1;
+	u32 ann_timeout:1;
+	u32 ann_delay:1;
+	u32 ann_first:1;
+	u32 signon_delay:1;
+	u32 signon_start:1;
+	u32 new_val:1;
+	u32 neigh_chk:1;
+	u32 wait_done:1;
+	u32 reset:1;
+	u32 start:1;
+	u32 chk_hw:1;
+
+	struct ksz_dlr_gateway_capable attrib;
+	struct ksz_dlr_active_node last_sup;
+
+	u32 beacon_interval;
+	u32 beacon_timeout;
+	u32 ip_addr;
+	u16 vid;
+	u8 src_addr[ETH_ALEN];
+	u8 next_node;
+	u8 node;
+	u8 port;
+	u8 precedence;
+	u8 ring_state;
+	u8 drop_beacon;
+	u32 skip_beacon:1;
+	u32 disable_learn:1;
+	u8 LastBcnRcvPort;
+	struct ksz_dlr_beacon_info beacon_info[2];
+	u32 interval;
+	unsigned long ann_jiffies;
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct ksz_timer_info announce_timer_info;
+	struct ksz_timer_info announce_timeout_timer_info;
+	struct ksz_timer_info neigh_chk_timer_info;
+	struct ksz_timer_info signon_timer_info;
+	struct ksz_timer_info test_timer_info;
+	struct delayed_work announce_tx;
+	u32 beacon_timeout_ports;
+	struct work_struct delay_proc;
+	struct work_struct neigh_chk_proc;
+	struct sk_buff_head rxq;
+	void (*state_machine)(struct ksz_dlr_info *info);
+	struct ksz_dlr_tx_frame last_beacon[2];
+	u8 beacon_addr[ETH_ALEN];
+	u32 stop:1;
+	u32 tx_signon:1;
+	u32 tx_announce:1;
+	u32 tx_advertise:1;
+	u32 tx_flush_tables:1;
+	u32 link_change:1;
+	u32 clr_supervisor:1;
+	u32 timeout_beacon:1;
+
+	struct ksz_dlr_tx_frame frame;
+	struct ksz_dlr_update_frame update_frame;
+	u8 signon_frame[2000];
+	u8 signon_addr[ETH_ALEN];
+	struct ksz_dlr_node_info nodes[500];
+	u8 *tx_frame;
+	int signon_port;
+	int len;
+	int cur_state;
+	int state;
+	int rx_port;
+	int tx_port;
+	int active_port;
+	u32 seqid;
+	u32 seqid_announce;
+	u32 seqid_beacon;
+	u32 seqid_signon;
+	u32 seqid_chk[2];
+	u32 seqid_first[2];
+	u32 seqid_last[2];
+	u32 seqid_rcv[2];
+	u8 port_chk[2];
+	u8 port_rcv[2];
+	u32 seqid_accept[2];
+	u8 ports[2];
+	u16 member;
+	u16 ok_ports;
+
+	struct ksz_dlr_super_info supers[DLR_SUPERVISOR_NUM];
+	struct ksz_dlr_super_info *rogue_super;
+
+	int seqid_cnt;
+	int signon_space;
+	u8 ignore_req;
+	u8 req_cnt[2];
+	u8 link_break;
+	unsigned long fault_jiffies;
+
+	struct sw_dev_info *dev_info;
+	uint notifications;
+
+	uint overrides;
+
+	const struct dlr_ops *ops;
+};
+
+struct dlr_attributes {
+	int info;
+	int node;
+	int prec;
+	int interval;
+	int timeout;
+	int vid;
+	int cfg;
+	int state;
+	int port;
+
+	int test;
+	int req;
+	int resp;
+	int link;
+	int learn;
+	int fault;
+	int signon;
+	int clear_rapid;
+	int clear_partial;
+
+	int all;
+	int rev;
+	int topology;
+	int network;
+	int super;
+	int super_cfg;
+	int fault_cnt;
+	int last_active_1;
+	int last_active_2;
+	int part_cnt;
+	int part_list;
+	int active_super_addr;
+	int active_super_prec;
+	int cap;
+	int port_1;
+	int port_2;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_dlr_api.h b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr_api.h
new file mode 100644
index 0000000..d0fd355
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr_api.h
@@ -0,0 +1,202 @@
+/**
+ * Microchip DLR driver API header
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_DLR_API_H
+#define KSZ_DLR_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN				6
+#endif
+
+
+enum {
+	DEV_INFO_DLR_LINK = DEV_INFO_LAST,
+	DEV_INFO_DLR_CFG,
+};
+
+#define DLR_INFO_LINK_LOST			(1 << 0)
+#define DLR_INFO_CFG_CHANGE			(1 << 1)
+
+
+#define STATUS_INVALID_ATTRIB_VALUE		0x09
+#define STATUS_OBJECT_STATE_CONFLICT		0x0C
+#define STATUS_REPLY_DATA_TOO_LARGE		0x11
+
+#define SVC_GET_ATTRIBUTES_ALL			0x01
+#define SVC_GET_ATTRIBUTE_SINGLE		0x0E
+#define SVC_SET_ATTRIBUTE_SINGLE		0x10
+#define SVC_GET_MEMBER				0x18
+
+
+#define CIP_CLASS_ATTRIBUTES			0
+#define CIP_INSTANCE_ATTRIBUTES			1
+
+
+#define CLASS_DLR_OBJECT			0x47
+
+
+#define CIP_SVC_S				24
+#define CIP_CLASS_S				16
+#define CIP_ATTR_S				8
+
+
+#define DLR_GET_REVISION			1
+
+#define DLR_REVISION				3
+
+
+#define DLR_GET_NETWORK_TOPOLOGY		1
+#define DLR_GET_NETWORK_STATUS			2
+#define DLR_GET_RING_SUPERVISOR_STATUS		3
+#define DLR_SET_RING_SUPERVISOR_CONFIG		4
+#define DLR_SET_RING_FAULT_COUNT		5
+#define DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1	6
+#define DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2	7
+#define DLR_GET_RING_PARTICIPANTS_COUNT		8
+#define DLR_GET_RING_PARTICIPANTS_LIST		9
+#define DLR_GET_ACTIVE_SUPERVISOR_ADDRESS	10
+#define DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE	11
+#define DLR_GET_CAPABILITY_FLAGS		12
+#define DLR_SET_REDUNDANT_GATEWAY_CONFIG	13
+#define DLR_GET_REDUNDANT_GATEWAY_STATUS	14
+#define DLR_GET_ACTIVE_GATEWAY_ADDRESS		15
+#define DLR_GET_ACTIVE_GATEWAY_PRECEDENCE	16
+#define DLR_SET_IP_ADDRESS			17
+
+#define SVC_DLR_VERIFY_FAULT_LOCATION		0x4B
+#define SVC_DLR_CLEAR_RAPID_FAULTS		0x4C
+#define SVC_DLR_RESTART_SIGN_ON			0x4D
+#define SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT	0x4E
+
+
+#define DLR_TOPOLOGY_LINEAR			0
+#define DLR_TOPOLOGY_RING			1
+
+#define DLR_NET_NORMAL				0
+#define DLR_NET_RING_FAULT			1
+#define DLR_NET_UNEXPECTED_LOOP_DETECTED	2
+#define DLR_NET_PARTIAL_FAULT			3
+#define DLR_NET_RAPID_FAULT			4
+
+#define DLR_STAT_BACKUP_SUPERVISOR		0
+#define DLR_STAT_ACTIVE_SUPERVISOR		1
+#define DLR_STAT_RING_NODE			2
+#define DLR_STAT_NO_SUPERVISOR			3
+#define DLR_STAT_NODE_NOT_SUPPORTED		4
+
+#define DLR_CAP_ANNOUNCE_BASED			(1 << 0)
+#define DLR_CAP_BEACON_BASED			(1 << 1)
+#define DLR_CAP_SUPERVISOR_CAPABLE		(1 << 5)
+#define DLR_CAP_GATEWAY_CAPABLE			(1 << 6)
+#define DLR_CAP_FLUSH_TABLE_CAPABLE		(1 << 7)
+
+#define DLR_STAT_NON_GATEWAY			0
+#define DLR_STAT_BACKUP_GATEWAY			1
+#define DLR_STAT_ACTIVE_GATEWAY			2
+#define DLR_STAT_GATEWAY_FAULT_TO_UPLINK	3
+#define DLR_STAT_GATEWAY_NOT_SUPPORTED		4
+#define DLR_STAT_GATEWAY_FAULT_TO_NETWORK	5
+
+
+struct ksz_dlr_active_node {
+	u32 ip_addr;
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct ksz_dlr_super_cfg {
+	u8 enable;
+	u8 prec;
+	u32 beacon_interval;
+	u32 beacon_timeout;
+	u16 vid;
+}  __packed;
+
+struct ksz_dlr_gateway_cfg {
+	u8 enable;
+	u8 prec;
+	u32 advertise_interval;
+	u32 advertise_timeout;
+	u8 learning_enable;
+}  __packed;
+
+struct ksz_dlr_non_super_capable_1 {
+	u8 net_topology;
+	u8 net_status;
+} __packed;
+
+struct ksz_dlr_super_capable_1 {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+} __packed;
+
+struct ksz_dlr_non_super_capable_2 {
+	u8 net_topology;
+	u8 net_status;
+	struct ksz_dlr_active_node active_super_addr;
+	u32 cap;
+} __packed;
+
+struct ksz_dlr_super_capable_2 {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+	u32 cap;
+} __packed;
+
+struct ksz_dlr_gateway_capable {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+	u32 cap;
+	struct ksz_dlr_gateway_cfg gateway_cfg;
+	u8 gateway_status;
+	struct ksz_dlr_active_node active_gateway_addr;
+	u8 active_gateway_prec;
+} __packed;
+
+union dlr_data {
+	struct ksz_dlr_gateway_capable gateway;
+	struct ksz_dlr_super_capable_2 super;
+	struct ksz_dlr_non_super_capable_2 non_super;
+	struct ksz_dlr_super_cfg super_cfg;
+	struct ksz_dlr_gateway_cfg gateway_cfg;
+	struct ksz_dlr_active_node active;
+	u32 dword;
+	u16 word;
+	u8 byte;
+} __packed;
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_dlr_sysfs.c b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr_sysfs.c
new file mode 100644
index 0000000..d58a57c
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_dlr_sysfs.c
@@ -0,0 +1,186 @@
+/**
+ * Microchip DLR common sysfs code
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static ssize_t dlr_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ksz_dlr_info *dlr;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	dlr = &sw->info->dlr;
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = dlr->ops->sysfs_read(dlr, proc_num, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t dlr_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ksz_dlr_info *dlr;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	dlr = &sw->info->dlr;
+	proc_num = offset / sizeof(int);
+	ret = count;
+	dlr->ops->sysfs_write(dlr, proc_num, num, buf);
+	up(proc_sem);
+	return ret;
+}
+
+#define DLR_ATTR(_name, _mode, _show, _store) \
+struct device_attribute dlr_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define DLR_RD_ENTRY(name)						\
+static ssize_t show_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return dlr_show(d, attr, buf,					\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static DLR_ATTR(name, S_IRUGO, show_dlr_##name, NULL)
+
+/* generate a write-able attribute */
+#define DLR_WR_ENTRY(name)						\
+static ssize_t show_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return dlr_show(d, attr, buf,					\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static ssize_t store_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return dlr_store(d, attr, buf, count,				\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static DLR_ATTR(name, S_IRUGO | S_IWUSR, show_dlr_##name, store_dlr_##name)
+
+DLR_RD_ENTRY(info);
+DLR_WR_ENTRY(node);
+DLR_WR_ENTRY(prec);
+DLR_WR_ENTRY(interval);
+DLR_WR_ENTRY(timeout);
+DLR_WR_ENTRY(vid);
+DLR_WR_ENTRY(cfg);
+DLR_WR_ENTRY(state);
+DLR_WR_ENTRY(port);
+DLR_WR_ENTRY(test);
+DLR_WR_ENTRY(req);
+DLR_WR_ENTRY(resp);
+DLR_WR_ENTRY(link);
+DLR_WR_ENTRY(learn);
+DLR_WR_ENTRY(fault);
+DLR_WR_ENTRY(signon);
+DLR_WR_ENTRY(clear_rapid);
+DLR_WR_ENTRY(clear_partial);
+
+DLR_RD_ENTRY(all);
+DLR_RD_ENTRY(rev);
+DLR_RD_ENTRY(topology);
+DLR_RD_ENTRY(network);
+DLR_RD_ENTRY(super);
+DLR_WR_ENTRY(super_cfg);
+DLR_WR_ENTRY(fault_cnt);
+DLR_RD_ENTRY(last_active_1);
+DLR_RD_ENTRY(last_active_2);
+DLR_RD_ENTRY(part_cnt);
+DLR_RD_ENTRY(part_list);
+DLR_RD_ENTRY(active_super_addr);
+DLR_RD_ENTRY(active_super_prec);
+DLR_RD_ENTRY(cap);
+DLR_WR_ENTRY(port_1);
+DLR_WR_ENTRY(port_2);
+
+static struct attribute *dlr_attrs[] = {
+	&dlr_attr_info.attr,
+	&dlr_attr_node.attr,
+	&dlr_attr_prec.attr,
+	&dlr_attr_interval.attr,
+	&dlr_attr_timeout.attr,
+	&dlr_attr_vid.attr,
+	&dlr_attr_cfg.attr,
+	&dlr_attr_state.attr,
+	&dlr_attr_port.attr,
+	&dlr_attr_test.attr,
+	&dlr_attr_req.attr,
+	&dlr_attr_resp.attr,
+	&dlr_attr_link.attr,
+	&dlr_attr_learn.attr,
+	&dlr_attr_fault.attr,
+	&dlr_attr_signon.attr,
+	&dlr_attr_clear_rapid.attr,
+	&dlr_attr_clear_partial.attr,
+
+	&dlr_attr_all.attr,
+	&dlr_attr_rev.attr,
+	&dlr_attr_topology.attr,
+	&dlr_attr_network.attr,
+	&dlr_attr_super.attr,
+	&dlr_attr_super_cfg.attr,
+	&dlr_attr_fault_cnt.attr,
+	&dlr_attr_last_active_1.attr,
+	&dlr_attr_last_active_2.attr,
+	&dlr_attr_part_cnt.attr,
+	&dlr_attr_part_list.attr,
+	&dlr_attr_active_super_addr.attr,
+	&dlr_attr_active_super_prec.attr,
+	&dlr_attr_cap.attr,
+	&dlr_attr_port_1.attr,
+	&dlr_attr_port_2.attr,
+	NULL
+};
+
+static struct attribute_group dlr_group = {
+	.name  = "dlrfs",
+	.attrs  = dlr_attrs,
+};
+
+static void exit_dlr_sysfs(struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, &dlr_group);
+}
+
+static int init_dlr_sysfs(struct device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->kobj, &dlr_group);
+	if (err)
+		return err;
+	return err;
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_dsa.c b/drivers/net/ethernet/micrel/ksz9897/ksz_dsa.c
new file mode 100644
index 0000000..a09b614
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_dsa.c
@@ -0,0 +1,280 @@
+/**
+ * Microchip tail tagging switch DSA driver
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ * Copyright (c) 2008-2009 Marvell Semiconductor
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/phy.h>
+#include <net/dsa.h>
+
+static int ksz_dsa_reg_read(struct dsa_switch *ds, int addr, int reg)
+{
+	struct mii_bus *bus = dsa_host_dev_to_mii_bus(ds->master_dev);
+
+	if (bus == NULL)
+		return -EINVAL;
+
+	++addr;
+	return mdiobus_read(bus, ds->pd->sw_addr + addr, reg);
+}
+
+static int ksz_dsa_reg_write(struct dsa_switch *ds, int addr, int reg, u16 val)
+{
+	struct mii_bus *bus = dsa_host_dev_to_mii_bus(ds->master_dev);
+
+	if (bus == NULL)
+		return -EINVAL;
+
+	++addr;
+	return mdiobus_write(bus, ds->pd->sw_addr + addr, reg, val);
+}
+
+static struct ksz_sw *get_sw_ptr(struct device *host_dev)
+{
+	struct mii_bus *bus = dsa_host_dev_to_mii_bus(host_dev);
+	struct ksz_sw *sw = NULL;
+
+	if (bus) {
+		struct phy_device *phydev;
+		struct phy_priv *phydata;
+
+		phydev = bus->phy_map[0];
+		if (phydev) {
+			phydata = phydev->priv;
+			if (phydata)
+				sw = phydata->port->sw;
+		}
+	}
+	return sw;
+}
+
+static char *ksz_dsa_probe(struct device *host_dev, int sw_addr)
+{
+	u8 id1;
+	u8 id2;
+	int id;
+	char dev_name[20];
+	static char switch_name[80];
+	struct ksz_sw *sw = get_sw_ptr(host_dev);
+
+	dev_name[0] = '\0';
+	switch_name[0] = '\0';
+	if (!sw)
+		return switch_name;
+
+	sw->ops->acquire(sw);
+	id = sw->ops->get_id(sw, &id1, &id2, dev_name);
+	sw->ops->release(sw);
+	strncpy(switch_name, "Microchip KSZ", sizeof(switch_name));
+	strcat(switch_name, dev_name);
+	if (!switch_name[13])
+		return NULL;
+	return switch_name;
+}
+
+static int ksz_dsa_switch_reset(struct dsa_switch *ds)
+{
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return -EINVAL;
+
+	sw->ops->acquire(sw);
+	sw_reset(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_setup_global(struct dsa_switch *ds)
+{
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return -EINVAL;
+
+	sw->ops->acquire(sw);
+	sw->features |= DSA_SUPPORT;
+	if (!(sw->overrides & TAIL_TAGGING)) {
+		sw->ops->cfg_tail_tag(sw, 1);
+		sw->overrides |= TAIL_TAGGING;
+	}
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_setup_port(struct dsa_switch *ds, int p)
+{
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return -EINVAL;
+
+	sw->ops->acquire(sw);
+	sw->ops->cfg_each_port(sw, p, dsa_is_cpu_port(ds, p));
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_setup(struct dsa_switch *ds)
+{
+	int i;
+	int ret;
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return -EINVAL;
+
+	ret = ksz_dsa_switch_reset(ds);
+	if (ret < 0)
+		return ret;
+
+	ret = ksz_dsa_setup_global(ds);
+	if (ret < 0)
+		return ret;
+
+	for (i = 0; i <= sw->dsa_port_cnt; i++) {
+		ret = ksz_dsa_setup_port(ds, i);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int ksz_dsa_set_addr(struct dsa_switch *ds, u8 *addr)
+{
+	int port;
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return -EINVAL;
+
+	sw->ops->acquire(sw);
+	sw_set_addr(sw, addr);
+	for (port = 0; port < sw->dsa_port_cnt; port++) {
+		sw->ops->set_port_addr(sw, port, addr);
+	}
+	sw->ops->release(sw);
+
+	return 0;
+}
+
+static int ksz_dsa_phy_read(struct dsa_switch *ds, int port, int regnum)
+{
+	int addr;
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return 0xffff;
+
+	addr = sw->ops->port_to_phy_addr(sw, port);
+	if (addr == -1)
+		return 0xffff;
+
+	return ksz_dsa_reg_read(ds, addr, regnum);
+}
+
+static int
+ksz_dsa_phy_write(struct dsa_switch *ds, int port, int regnum, u16 val)
+{
+	int addr;
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return 0xffff;
+
+	addr = sw->ops->port_to_phy_addr(sw, port);
+	if (addr == -1)
+		return 0xffff;
+
+	return ksz_dsa_reg_write(ds, addr, regnum, val);
+}
+
+static void ksz_dsa_poll_link(struct dsa_switch *ds)
+{
+	int i;
+	struct ksz_port_info *info;
+	struct ksz_sw *sw = get_sw_ptr(ds->master_dev);
+
+	if (!sw)
+		return;
+	for (i = 0; i < sw->dsa_port_cnt; i++) {
+		struct net_device *dev;
+		int link;
+		int speed;
+		int duplex;
+		int fc;
+		int p = 0;
+
+		dev = ds->ports[i];
+		if (dev == NULL)
+			continue;
+
+		if (sw->ops->get_first_port)
+			p = sw->ops->get_first_port(sw);
+		info = &sw->port_info[i + p];
+		link = 0;
+		if (dev->flags & IFF_UP)
+			link = (info->state == media_connected);
+
+		if (!link) {
+			if (netif_carrier_ok(dev)) {
+				netdev_info(dev, "link down\n");
+				netif_carrier_off(dev);
+			}
+			continue;
+		}
+
+		speed = info->tx_rate / TX_RATE_UNIT;
+		duplex = (info->duplex == 2);
+		fc = (info->flow_ctrl & 3) == 3;
+
+		if (!netif_carrier_ok(dev)) {
+			netdev_info(dev,
+				    "link up, %d Mb/s, %s duplex, "
+				    "flow control %sabled\n",
+				    speed, duplex ? "full" : "half",
+				    fc ? "en" : "dis");
+			netif_carrier_on(dev);
+		}
+	}
+}
+
+static struct dsa_switch_driver micrel_switch_driver = {
+	.tag_protocol	= DSA_TAG_PROTO_TRAILER,
+	.probe		= ksz_dsa_probe,
+	.setup		= ksz_dsa_setup,
+	.set_addr	= ksz_dsa_set_addr,
+	.phy_read	= ksz_dsa_phy_read,
+	.phy_write	= ksz_dsa_phy_write,
+	.poll_link	= ksz_dsa_poll_link,
+};
+
+static int ksz_dsa_init(void)
+{
+	register_switch_driver(&micrel_switch_driver);
+	return 0;
+}
+
+static void ksz_dsa_cleanup(void)
+{
+	unregister_switch_driver(&micrel_switch_driver);
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_hsr.c b/drivers/net/ethernet/micrel/ksz9897/ksz_hsr.c
new file mode 100644
index 0000000..d0e1aa0
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_hsr.c
@@ -0,0 +1,2162 @@
+/**
+ * Microchip HSR code
+ *
+ * Copyright (c) 2016-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright 2011-2014 Autronica Fire and Security AS
+
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Author(s):
+ *	2011-2014 Arvid Brodin, arvid.brodin@alten.se
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static int dbg_hsr;
+
+struct hsr_cfg_work {
+	struct work_struct work;
+	struct ksz_sw *sw;
+	u8 addr[ETH_ALEN];
+	u16 member;
+};
+
+static void proc_hsr_cfg_work(struct work_struct *work)
+{
+	struct hsr_cfg_work *cfg_work =
+		container_of(work, struct hsr_cfg_work, work);
+	struct ksz_sw *sw = cfg_work->sw;
+
+	sw->ops->cfg_mac(sw, 0, cfg_work->addr, cfg_work->member,
+		false, false, 0);
+
+	kfree(cfg_work);
+}  /* proc_hsr_work */
+
+static void proc_hsr_cfg(struct ksz_hsr_info *info, u8 *addr, u16 member)
+{
+	struct hsr_cfg_work *cfg_work;
+
+	cfg_work = kzalloc(sizeof(struct hsr_cfg_work), GFP_KERNEL);
+	if (!cfg_work)
+		return;
+	INIT_WORK(&cfg_work->work, proc_hsr_cfg_work);
+	cfg_work->sw = info->sw_dev;
+	memcpy(cfg_work->addr, addr, ETH_ALEN);
+	cfg_work->member = member;
+	schedule_work(&cfg_work->work);
+}  /* proc_hsr_cfg */
+
+#if 0
+static bool is_admin_up(struct net_device *dev)
+{
+	return dev && (dev->flags & IFF_UP);
+}
+#endif
+
+static
+struct hsr_port *hsr_port_get_hsr(struct hsr_priv *hsr, enum hsr_port_type pt)
+{
+	struct ksz_hsr_info *info = container_of(hsr,
+		struct ksz_hsr_info, hsr);
+
+	if (pt < HSR_PT_PORTS)
+		return &info->hsr_ports[pt];
+	return NULL;
+}
+
+struct hsr_node {
+	struct list_head	mac_list;
+	unsigned char		MacAddressA[ETH_ALEN];
+	unsigned char		MacAddressB[ETH_ALEN];
+	/* Local slave through which AddrB frames are received from this node */
+	enum hsr_port_type	AddrB_port;
+	unsigned long		time_in[HSR_PT_PORTS];
+	bool			time_in_stale[HSR_PT_PORTS];
+	u16			seq_out[HSR_PT_PORTS];
+	unsigned long		time_out[HSR_PT_PORTS];
+	struct rcu_head		rcu_head;
+	int			slave;
+};
+
+
+/*	TODO: use hash lists for mac addresses (linux/jhash.h)?    */
+
+
+/* seq_nr_after(a, b) - return true if a is after (higher in sequence than) b,
+ * false otherwise.
+ */
+static bool seq_nr_after(u16 a, u16 b)
+{
+	/* Remove inconsistency where
+	 * seq_nr_after(a, b) == seq_nr_before(a, b)
+	 */
+	if ((int) b - a == 32768)
+		return false;
+
+	return (((s16) (b - a)) < 0);
+}
+#define seq_nr_before(a, b)		seq_nr_after((b), (a))
+#define seq_nr_after_or_eq(a, b)	(!seq_nr_before((a), (b)))
+#define seq_nr_before_or_eq(a, b)	(!seq_nr_after((a), (b)))
+
+
+static
+bool hsr_addr_is_self(struct hsr_priv *hsr, unsigned char *addr)
+{
+	struct hsr_node *node;
+
+	node = list_first_or_null_rcu(&hsr->self_node_db, struct hsr_node,
+				      mac_list);
+	if (!node) {
+		WARN_ONCE(1, "HSR: No self node\n");
+		return false;
+	}
+
+	if (ether_addr_equal(addr, node->MacAddressA))
+		return true;
+	if (ether_addr_equal(addr, node->MacAddressB))
+		return true;
+
+	return false;
+}
+
+/* Search for mac entry. Caller must hold rcu read lock.
+ */
+static struct hsr_node *find_node_by_AddrA(struct list_head *node_db,
+					   const unsigned char addr[ETH_ALEN])
+{
+	struct hsr_node *node;
+
+	list_for_each_entry_rcu(node, node_db, mac_list) {
+		if (ether_addr_equal(node->MacAddressA, addr))
+			return node;
+	}
+
+	return NULL;
+}
+
+
+/* Helper for device init; the self_node_db is used in hsr_rcv() to recognize
+ * frames from self that's been looped over the HSR ring.
+ */
+static
+int hsr_create_self_node(struct list_head *self_node_db,
+			 unsigned char addr_a[ETH_ALEN],
+			 unsigned char addr_b[ETH_ALEN])
+{
+	struct hsr_node *node, *oldnode;
+
+	node = kmalloc(sizeof(*node), GFP_KERNEL);
+	if (!node)
+		return -ENOMEM;
+
+	ether_addr_copy(node->MacAddressA, addr_a);
+	ether_addr_copy(node->MacAddressB, addr_b);
+
+	rcu_read_lock();
+	oldnode = list_first_or_null_rcu(self_node_db,
+						struct hsr_node, mac_list);
+	if (oldnode) {
+		list_replace_rcu(&oldnode->mac_list, &node->mac_list);
+		rcu_read_unlock();
+		synchronize_rcu();
+		kfree(oldnode);
+	} else {
+		rcu_read_unlock();
+		list_add_tail_rcu(&node->mac_list, self_node_db);
+	}
+
+	return 0;
+}
+
+
+/* Allocate an hsr_node and add it to node_db. 'addr' is the node's AddressA;
+ * seq_out is used to initialize filtering of outgoing duplicate frames
+ * originating from the newly added node.
+ */
+static
+struct hsr_node *hsr_add_node_(struct list_head *node_db, unsigned char addr[],
+			       u16 seq_out)
+{
+	struct hsr_node *node;
+	unsigned long now;
+	int i;
+
+#if 1
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x %04x\n", __func__,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5], seq_out);
+#endif
+	node = kzalloc(sizeof(*node), GFP_ATOMIC);
+	if (!node)
+		return NULL;
+
+	ether_addr_copy(node->MacAddressA, addr);
+
+	/* We are only interested in time diffs here, so use current jiffies
+	 * as initialization. (0 could trigger an spurious ring error warning).
+	 */
+	now = jiffies;
+	for (i = 0; i < HSR_PT_PORTS; i++)
+		node->time_in[i] = now;
+	for (i = 0; i < HSR_PT_PORTS; i++)
+		node->seq_out[i] = seq_out;
+	for (i = 0; i < HSR_PT_PORTS; i++)
+		node->time_out[i] = now;
+
+	list_add_tail_rcu(&node->mac_list, node_db);
+
+	return node;
+}
+
+static
+struct hsr_node *hsr_add_node(struct list_head *node_db, unsigned char addr[],
+			      u16 seq_out)
+{
+	struct hsr_node *node;
+
+	node = hsr_add_node_(node_db, addr, seq_out);
+	if (!node)
+		return node;
+
+	do {
+		struct hsr_priv *hsr = container_of(node_db,
+			struct hsr_priv, node_db);
+		struct ksz_hsr_info *info = container_of(hsr,
+			struct ksz_hsr_info, hsr);
+		int not_self;
+
+		not_self = memcmp(addr, info->src_addr, ETH_ALEN);
+		if (not_self)
+			proc_hsr_cfg(info, addr, info->member);
+		info->part_cnt++;
+	} while (0);
+
+	return node;
+}
+
+static
+struct hsr_node *hsr_add_slave(struct list_head *node_db,
+	unsigned char addr[], u16 seq_out)
+{
+	struct hsr_node *node;
+
+	node = hsr_add_node_(node_db, addr, seq_out);
+	if (!node)
+		return node;
+
+	node->slave = 1;
+
+	return node;
+}
+
+/* Get the hsr_node from which 'skb' was sent.
+ */
+static
+struct hsr_node *hsr_get_node(struct list_head *node_db, struct sk_buff *skb,
+			      bool is_sup)
+{
+	struct hsr_node *node;
+	struct ethhdr *ethhdr;
+	struct ethhdr *exthdr;
+	u16 seq_out;
+
+	if (!skb_mac_header_was_set(skb))
+		return NULL;
+
+	ethhdr = (struct ethhdr *) skb_mac_header(skb);
+
+	list_for_each_entry_rcu(node, node_db, mac_list) {
+		if (ether_addr_equal(node->MacAddressA, ethhdr->h_source))
+			return node;
+		if (ether_addr_equal(node->MacAddressB, ethhdr->h_source))
+			return node;
+	}
+
+	if (!is_sup)
+		return NULL; /* Only supervision frame may create node entry */
+
+	exthdr = ethhdr;
+	if (exthdr->h_proto == htons(ETH_P_8021Q))
+		exthdr = (struct ethhdr *)((u8 *) exthdr + VLAN_HLEN);
+	if (exthdr->h_proto == htons(ETH_P_HSR))
+		exthdr = (struct ethhdr *)((u8 *) exthdr + HSR_HLEN);
+	else {
+		struct hsr_priv *hsr = container_of(node_db,
+			struct hsr_priv, node_db);
+
+		/* Supervision frame has its own sequence number. */
+		seq_out = hsr->sequence_nr - 1;
+		goto get_node_done;
+	}
+	if (exthdr->h_proto == htons(ETH_P_PRP)) {
+		/* Use the existing sequence_nr from the tag as starting point
+		 * for filtering duplicate frames.
+		 */
+		seq_out = hsr_get_skb_sequence_nr(skb) - 1;
+	} else {
+int i;
+for (i = 0; i < 20; i++)
+dbg_msg("%02x ", skb->data[i]);
+dbg_msg("  %04x\n", ethhdr->h_proto);
+		WARN_ONCE(1, "%s: Non-HSR frame\n", __func__);
+		seq_out = 0;
+	}
+
+get_node_done:
+	return hsr_add_node(node_db, ethhdr->h_source, seq_out);
+}
+
+/* Use the Supervision frame's info about an eventual MacAddressB for merging
+ * nodes that has previously had their MacAddressB registered as a separate
+ * node.
+ */
+static
+void hsr_handle_sup_frame(struct sk_buff *skb, struct hsr_node *node_curr,
+			  struct hsr_port *port_rcv)
+{
+	struct hsr_node *node_real;
+	struct hsr_sup_payload *hsr_sp;
+	struct hsr_sup_type *hsr_stype;
+	struct list_head *node_db;
+	int i;
+
+	skb_pull(skb, sizeof(struct hsr_ethhdr_sp));
+	skb_pull(skb, sizeof(struct hsr_tag));
+	hsr_sp = (struct hsr_sup_payload *) skb->data;
+
+	if (ether_addr_equal(eth_hdr(skb)->h_source, hsr_sp->MacAddressA))
+		/* Not sent from MacAddressB of a PICS_SUBS capable node */
+		goto done;
+
+	/* Check frame sent by RedBox. */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	if (HSR_TLV_REDBOX == hsr_stype->HSR_TLV_Type) {
+		hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+		if (ether_addr_equal(eth_hdr(skb)->h_source,
+		    hsr_sp->MacAddressA))
+			goto done;
+	}
+
+	/* Merge node_curr (registered on MacAddressB) into node_real */
+	node_db = &port_rcv->hsr->node_db;
+	node_real = find_node_by_AddrA(node_db, hsr_sp->MacAddressA);
+	if (!node_real)
+{
+	u8 *data = (u8 *) eth_hdr(skb)->h_source;
+dbg_msg("add new: %02x:%02x:%02x\n", data[3], data[4], data[5]);
+}
+	if (!node_real)
+		/* No frame received from AddrA of this node yet */
+		node_real = hsr_add_node(node_db, hsr_sp->MacAddressA,
+					 HSR_SEQNR_START - 1);
+	if (!node_real)
+		goto done; /* No mem */
+	if (node_real == node_curr)
+		/* Node has already been merged */
+		goto done;
+
+	ether_addr_copy(node_real->MacAddressB, eth_hdr(skb)->h_source);
+#if 1
+do {
+	u8 *data = (u8 *) eth_hdr(skb)->h_source;
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x\n", __func__,
+data[0], data[1], data[2], data[3], data[4], data[5]);
+} while (0);
+#endif
+	for (i = 0; i < HSR_PT_PORTS; i++) {
+		if (!node_curr->time_in_stale[i] &&
+		    time_after(node_curr->time_in[i], node_real->time_in[i])) {
+			node_real->time_in[i] = node_curr->time_in[i];
+			node_real->time_in_stale[i] = node_curr->time_in_stale[i];
+		}
+		if (seq_nr_after(node_curr->seq_out[i], node_real->seq_out[i]))
+			node_real->seq_out[i] = node_curr->seq_out[i];
+	}
+	node_real->AddrB_port = port_rcv->type;
+
+	list_del_rcu(&node_curr->mac_list);
+	kfree_rcu(node_curr, rcu_head);
+
+done:
+	skb_push(skb, sizeof(struct hsr_tag));
+	skb_push(skb, sizeof(struct hsr_ethhdr_sp));
+}
+
+
+/* 'skb' is a frame meant for this host, that is to be passed to upper layers.
+ *
+ * If the frame was sent by a node's B interface, replace the source
+ * address with that node's "official" address (MacAddressA) so that upper
+ * layers recognize where it came from.
+ */
+static
+void hsr_addr_subst_source(struct hsr_node *node, struct sk_buff *skb)
+{
+#ifdef DEBUG
+	if (!skb_mac_header_was_set(skb)) {
+		WARN_ONCE(1, "%s: Mac header not set\n", __func__);
+		return;
+	}
+#endif
+
+	memcpy(&eth_hdr(skb)->h_source, node->MacAddressA, ETH_ALEN);
+}
+
+#if 0
+/* 'skb' is a frame meant for another host.
+ * 'port' is the outgoing interface
+ *
+ * Substitute the target (dest) MAC address if necessary, so the it matches the
+ * recipient interface MAC address, regardless of whether that is the
+ * recipient's A or B interface.
+ * This is needed to keep the packets flowing through switches that learn on
+ * which "side" the different interfaces are.
+ */
+static
+void hsr_addr_subst_dest(struct hsr_node *node_src, struct sk_buff *skb,
+			 struct hsr_port *port)
+{
+	struct hsr_node *node_dst;
+
+#ifdef DEBUG
+	if (!skb_mac_header_was_set(skb)) {
+		WARN_ONCE(1, "%s: Mac header not set\n", __func__);
+		return;
+	}
+#endif
+
+	if (!is_unicast_ether_addr(eth_hdr(skb)->h_dest))
+		return;
+
+	node_dst = find_node_by_AddrA(&port->hsr->node_db, eth_hdr(skb)->h_dest);
+	if (!node_dst) {
+#if 1
+u8 *addr = eth_hdr(skb)->h_dest;
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x\n", __func__,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5]);
+#endif
+		WARN_ONCE(1, "%s: Unknown node\n", __func__);
+		return;
+	}
+	if (port->type != node_dst->AddrB_port)
+		return;
+
+	ether_addr_copy(eth_hdr(skb)->h_dest, node_dst->MacAddressB);
+}
+#endif
+
+
+static
+void hsr_register_frame_in(struct hsr_node *node, struct hsr_port *port,
+			   u16 sequence_nr)
+{
+	/* Don't register incoming frames without a valid sequence number. This
+	 * ensures entries of restarted nodes gets pruned so that they can
+	 * re-register and resume communications.
+	 */
+	if (seq_nr_before(sequence_nr, node->seq_out[port->type]))
+	{
+		unsigned long diff = jiffies - node->time_in[port->type];
+#if 0
+dbg_msg("%s %d %04x %04x %lu\n", __func__, port->type,
+sequence_nr, node->seq_out[port->type], diff);
+#endif
+		if (diff <= msecs_to_jiffies(HSR_ENTRY_FORGET_TIME))
+			return;
+	}
+
+	node->time_in[port->type] = jiffies;
+	node->time_in_stale[port->type] = false;
+}
+
+static
+void hsr_update_frame_out(struct hsr_port *port, struct hsr_node *node,
+	u16 sequence_nr)
+{
+	node->seq_out[port->type] = sequence_nr;
+}
+
+/* 'skb' is a HSR Ethernet frame (with a HSR tag inserted), with a valid
+ * ethhdr->h_source address and skb->mac_header set.
+ *
+ * Return:
+ *	 1 if frame can be shown to have been sent recently on this interface,
+ *	 0 otherwise, or
+ *	 negative error code on error
+ */
+static
+int hsr_register_frame_out(struct hsr_port *port, struct hsr_node *node,
+			   u16 sequence_nr)
+{
+	if (seq_nr_before_or_eq(sequence_nr, node->seq_out[port->type]))
+	{
+		unsigned long diff = jiffies - node->time_out[port->type];
+#if 1
+#if 0
+if (port->type != HSR_PT_MASTER)
+#endif
+if (dbg_hsr < 10)
+dbg_msg("%s %d %04x %04x %lu\n", __func__, port->type, sequence_nr,
+node->seq_out[port->type], diff);
+#endif
+		if (diff <= msecs_to_jiffies(HSR_ENTRY_FORGET_TIME))
+			return 1;
+	}
+	node->time_out[port->type] = jiffies;
+
+	node->seq_out[port->type] = sequence_nr;
+	return 0;
+}
+
+
+#if 0
+static struct hsr_port *get_late_port(struct hsr_priv *hsr,
+				      struct hsr_node *node)
+{
+	if (node->time_in_stale[HSR_PT_SLAVE_A])
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_A);
+	if (node->time_in_stale[HSR_PT_SLAVE_B])
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_B);
+
+	if (time_after(node->time_in[HSR_PT_SLAVE_B],
+		       node->time_in[HSR_PT_SLAVE_A] +
+					msecs_to_jiffies(MAX_SLAVE_DIFF)))
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_A);
+	if (time_after(node->time_in[HSR_PT_SLAVE_A],
+		       node->time_in[HSR_PT_SLAVE_B] +
+					msecs_to_jiffies(MAX_SLAVE_DIFF)))
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_B);
+
+	return NULL;
+}
+#endif
+
+
+static void hsr_notify_link_lost(struct ksz_hsr_info *info)
+{
+dbg_msg(" hsr: %u %u:%u %u:%u\n", info->ring,
+	info->p1_down, info->p2_down, info->p1_lost, info->p2_lost);
+	if (info->dev_info) {
+		u8 buf[sizeof(struct ksz_resp_msg) +
+			sizeof(struct ksz_hsr_node)];
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) buf;
+		struct ksz_hsr_node active;
+
+		msg->module = DEV_MOD_HSR;
+		msg->cmd = DEV_INFO_HSR_LINK;
+		msg->resp.data[0] = 0;
+		if (info->p1_down)
+			msg->resp.data[0] |= 0x01;
+		if (info->p2_down)
+			msg->resp.data[0] |= 0x02;
+		if (info->p1_lost)
+			msg->resp.data[0] |= 0x04;
+		if (info->p2_lost)
+			msg->resp.data[0] |= 0x08;
+		memcpy(active.addr, info->src_addr, ETH_ALEN);
+		memcpy(&msg->resp.data[1], &active,
+			sizeof(struct ksz_hsr_node));
+		sw_setup_msg(info->dev_info, msg, sizeof(struct ksz_resp_msg) +
+			sizeof(struct ksz_hsr_node), NULL, NULL);
+	}
+}  /* hsr_notify_link_lost */
+
+#ifdef CONFIG_HAVE_HSR_HW
+static void hsr_chk_ring(struct work_struct *work)
+{
+	struct hsr_node *node;
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_hsr_info *info =
+		container_of(dwork, struct ksz_hsr_info, chk_ring);
+	struct hsr_priv *hsr = &info->hsr;
+	int change = false;
+	int no_drop_win = false;
+	u16 start_seq[2];
+	u16 exp_seq[2];
+
+	memset(start_seq, 0, sizeof(start_seq));
+	memset(exp_seq, 0, sizeof(exp_seq));
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		if (!memcmp(node->MacAddressA, info->src_addr, ETH_ALEN))
+			continue;
+		if (!node->slave) {
+			struct ksz_hsr_table entry;
+			struct ksz_sw *sw = info->sw_dev;
+
+			memcpy(entry.src_mac, node->MacAddressA, ETH_ALEN);
+			entry.path_id = 0;
+			sw_r_hsr_table(sw, 0, &entry);
+			if (entry.start_seq[0]) {
+				u16 win[2];
+
+				win[0] = entry.exp_seq[0] - entry.start_seq[0];
+				win[1] = entry.exp_seq[1] - entry.start_seq[1];
+				if (!win[0] && !win[1] &&
+				    info->seq_num != entry.exp_seq[0]) {
+					no_drop_win = true;
+					info->center = node;
+				}
+				if (info->center == node) {
+					exp_seq[0] = entry.exp_seq[0];
+					exp_seq[1] = entry.exp_seq[1];
+					start_seq[0] = entry.start_seq[0];
+					start_seq[1] = entry.start_seq[1];
+				}
+			}
+			if (no_drop_win)
+				break;
+		}
+	}
+	rcu_read_unlock();
+
+	if (info->ring && !no_drop_win) {
+		if (info->center) {
+			int p;
+
+dbg_msg("%04x:%04x %04x:%04x\n", start_seq[0], start_seq[1],
+exp_seq[0], exp_seq[1]);
+			if (start_seq[0] == exp_seq[0]) {
+				info->p1_lost = 1;
+				p = 1;
+			} else {
+				info->p2_lost = 1;
+				p = 0;
+			}
+			info->seq_num = exp_seq[p];
+		}
+		info->ring = 0;
+		change = 1;
+	} else if (!info->ring && no_drop_win) {
+		info->ring = 1;
+		info->p1_down = info->p2_down =
+		info->p1_lost = info->p2_lost = 0;
+		change = 1;
+	}
+	if (change)
+		hsr_notify_link_lost(info);
+	info->check = 0;
+}  /* hsr_chk_ring */
+#endif
+
+
+/* Remove stale sequence_nr records. Called by timer every
+ * HSR_LIFE_CHECK_INTERVAL (two seconds or so).
+ */
+static
+void hsr_prune_nodes(unsigned long data)
+{
+	struct hsr_priv *hsr;
+	struct hsr_node *node;
+#if 0
+	struct hsr_port *port;
+#endif
+	unsigned long timestamp;
+	unsigned long time_a, time_b;
+
+	hsr = (struct hsr_priv *) data;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		/* Shorthand */
+		time_a = node->time_in[HSR_PT_SLAVE_A];
+		time_b = node->time_in[HSR_PT_SLAVE_B];
+
+		/* Check for timestamps old enough to risk wrap-around */
+		if (time_after(jiffies, time_a + MAX_JIFFY_OFFSET/2))
+			node->time_in_stale[HSR_PT_SLAVE_A] = true;
+		if (time_after(jiffies, time_b + MAX_JIFFY_OFFSET/2))
+			node->time_in_stale[HSR_PT_SLAVE_B] = true;
+
+		/* Get age of newest frame from node.
+		 * At least one time_in is OK here; nodes get pruned long
+		 * before both time_ins can get stale
+		 */
+		timestamp = time_a;
+		if (node->time_in_stale[HSR_PT_SLAVE_A] ||
+		    (!node->time_in_stale[HSR_PT_SLAVE_B] &&
+		    time_after(time_b, time_a)))
+			timestamp = time_b;
+
+#if 0
+		/* Warn of ring error only as long as we get frames at all */
+		if (time_is_after_jiffies(timestamp +
+					msecs_to_jiffies(1.5*MAX_SLAVE_DIFF))) {
+			rcu_read_lock();
+			port = get_late_port(hsr, node);
+			if (port != NULL)
+				hsr_nl_ringerror(hsr, node->MacAddressA, port);
+			rcu_read_unlock();
+		}
+#endif
+
+		if (1 == node->slave && time_is_before_jiffies(timestamp +
+			msecs_to_jiffies(HSR_NODE_FORGET_TIME / 4))) {
+#if 1
+dbg_msg("drop: %02x:%02x:%02x:%02x:%02x:%02x %ld\n",
+node->MacAddressA[0],
+node->MacAddressA[1],
+node->MacAddressA[2],
+node->MacAddressA[3],
+node->MacAddressA[4],
+node->MacAddressA[5],
+jiffies - timestamp);
+#endif
+			node->slave = 2;
+		}
+
+		/* Prune old entries */
+		if (time_is_before_jiffies(timestamp +
+					msecs_to_jiffies(HSR_NODE_FORGET_TIME))) {
+			struct ksz_hsr_info *info = container_of(hsr,
+				struct ksz_hsr_info, hsr);
+			if (!node->slave) {
+				proc_hsr_cfg(info, node->MacAddressA, 0);
+				info->part_cnt--;
+			}
+
+#if 1
+dbg_msg("forget: %02x:%02x:%02x:%02x:%02x:%02x %lx %lx %lx; %ld\n",
+node->MacAddressA[0],
+node->MacAddressA[1],
+node->MacAddressA[2],
+node->MacAddressA[3],
+node->MacAddressA[4],
+node->MacAddressA[5],
+time_a, time_b, timestamp, jiffies - timestamp);
+#endif
+			if (node == info->center) {
+				info->center = NULL;
+				info->seq_num = 0;
+				info->ring = 0;
+			}
+#if 0
+			hsr_nl_nodedown(hsr, node->MacAddressA);
+#endif
+			list_del_rcu(&node->mac_list);
+			/* Note that we need to free this entry later: */
+			kfree_rcu(node, rcu_head);
+		}
+	}
+	rcu_read_unlock();
+
+	hsr->prune_timer.expires = jiffies + msecs_to_jiffies(PRUNE_PERIOD);
+	add_timer(&hsr->prune_timer);
+}
+
+
+#if 0
+static
+void *hsr_get_next_node(struct hsr_priv *hsr, void *_pos,
+			unsigned char addr[ETH_ALEN])
+{
+	struct hsr_node *node;
+
+	if (!_pos) {
+		node = list_first_or_null_rcu(&hsr->node_db,
+					      struct hsr_node, mac_list);
+		if (node)
+			ether_addr_copy(addr, node->MacAddressA);
+		return node;
+	}
+
+	node = _pos;
+	list_for_each_entry_continue_rcu(node, &hsr->node_db, mac_list) {
+		ether_addr_copy(addr, node->MacAddressA);
+		return node;
+	}
+
+	return NULL;
+}
+
+
+static
+int hsr_get_node_data(struct hsr_priv *hsr,
+		      const unsigned char *addr,
+		      unsigned char addr_b[ETH_ALEN],
+		      unsigned int *addr_b_ifindex,
+		      int *if1_age,
+		      u16 *if1_seq,
+		      int *if2_age,
+		      u16 *if2_seq)
+{
+	struct hsr_node *node;
+	struct hsr_port *port;
+	unsigned long tdiff;
+
+
+	rcu_read_lock();
+	node = find_node_by_AddrA(&hsr->node_db, addr);
+	if (!node) {
+		rcu_read_unlock();
+		return -ENOENT;	/* No such entry */
+	}
+
+	ether_addr_copy(addr_b, node->MacAddressB);
+
+	tdiff = jiffies - node->time_in[HSR_PT_SLAVE_A];
+	if (node->time_in_stale[HSR_PT_SLAVE_A])
+		*if1_age = INT_MAX;
+#if HZ <= MSEC_PER_SEC
+	else if (tdiff > msecs_to_jiffies(INT_MAX))
+		*if1_age = INT_MAX;
+#endif
+	else
+		*if1_age = jiffies_to_msecs(tdiff);
+
+	tdiff = jiffies - node->time_in[HSR_PT_SLAVE_B];
+	if (node->time_in_stale[HSR_PT_SLAVE_B])
+		*if2_age = INT_MAX;
+#if HZ <= MSEC_PER_SEC
+	else if (tdiff > msecs_to_jiffies(INT_MAX))
+		*if2_age = INT_MAX;
+#endif
+	else
+		*if2_age = jiffies_to_msecs(tdiff);
+
+	/* Present sequence numbers as if they were incoming on interface */
+	*if1_seq = node->seq_out[HSR_PT_SLAVE_B];
+	*if2_seq = node->seq_out[HSR_PT_SLAVE_A];
+
+	if (node->AddrB_port != HSR_PT_NONE) {
+		port = hsr_port_get_hsr(hsr, node->AddrB_port);
+		*addr_b_ifindex = port->dev->ifindex;
+	} else {
+		*addr_b_ifindex = -1;
+	}
+
+	rcu_read_unlock();
+
+	return 0;
+}
+#endif
+
+static int hsr_dev_xmit(struct ksz_hsr_info *info, struct net_device *dev,
+	struct sk_buff *skb)
+{
+	int rc;
+	const struct net_device_ops *ops = dev->netdev_ops;
+
+	skb->dev = dev;
+	rc = ops->ndo_start_xmit(skb, skb->dev);
+	if (NETDEV_TX_BUSY == rc) {
+		dev_kfree_skb_irq(skb);
+	}
+	return rc;
+}  /* hsr_dev_xmit */
+
+static int hsr_xmit(struct ksz_hsr_info *info)
+{
+	struct sk_buff *skb;
+	u8 *frame = info->tx_frame;
+	int len = info->len;
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(info->dev) || !netif_carrier_ok(info->dev))
+		return 0;
+
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, info->tx_frame, len);
+
+	skb_put(skb, len);
+	skb->protocol = htons(ETH_P_PRP);
+	return hsr_dev_xmit(info, info->dev, skb);
+}  /* hsr_xmit */
+
+static void prep_hsr_supervision_frame(struct ksz_hsr_info *info)
+{
+	int len;
+	int tlen;
+	struct hsr_port *master;
+	struct hsr_sup_tag *hsr_stag;
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+	struct ethhdr *eth = (struct ethhdr *) info->master_sup_frame;
+
+	master = hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+
+	memcpy(eth->h_dest, master->hsr->sup_multicast_addr, ETH_ALEN);
+	memcpy(eth->h_source, info->src_addr, ETH_ALEN);
+	eth->h_proto = htons(ETH_P_PRP);
+	len = sizeof(struct ethhdr);
+	tlen = 60;
+
+	/* Use VLAN. */
+	if (info->vid) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->master_sup_frame;
+
+		vlan->h_vlan_encapsulated_proto = vlan->h_vlan_proto;
+		vlan->h_vlan_TCI = htons(info->vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+		len += VLAN_HLEN;
+		tlen += VLAN_HLEN;
+	}
+
+	hsr_stag = (struct hsr_sup_tag *) &info->master_sup_frame[len];
+
+	/* Remember the supervision tag location. */
+	info->master_hsr_stag = hsr_stag;
+
+	/* HSR tag will be inserted by the standard hsr_fill_tag routine. */
+	set_hsr_stag_path(hsr_stag, 0);
+	set_hsr_stag_HSR_Ver(hsr_stag, 1);
+	len += sizeof(struct hsr_sup_tag);
+
+	hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+	hsr_stype->HSR_TLV_Type = HSR_TLV_LIFE_CHECK;
+	hsr_stype->HSR_TLV_Length = 6;
+	len += sizeof(struct hsr_sup_type);
+
+	/* Payload: MacAddressA */
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+	len += sizeof(struct hsr_sup_payload);
+
+	/* Last TLV */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_stype->HSR_TLV_Type = 0;
+	hsr_stype->HSR_TLV_Length = 0;
+	len += sizeof(struct hsr_sup_type);
+	if (len < tlen) {
+		memset(&info->master_sup_frame[len], 0, tlen - len);
+		len = tlen;
+	}
+	info->master_len = len;
+}  /* prep_hsr_supervision_frame */
+
+static void prep_hsr_supervision_slave_frame(struct ksz_hsr_info *info)
+{
+	int len;
+	int tlen;
+	struct hsr_port *master;
+	struct hsr_sup_tag *hsr_stag;
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+	struct ethhdr *eth = (struct ethhdr *) info->slave_sup_frame;
+
+	master = hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+
+	memcpy(eth->h_dest, master->hsr->sup_multicast_addr, ETH_ALEN);
+	memcpy(eth->h_source, info->src_addr, ETH_ALEN);
+	eth->h_proto = htons(ETH_P_PRP);
+	len = sizeof(struct ethhdr);
+	tlen = 60;
+
+	/* Use VLAN. */
+	if (info->vid) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->slave_sup_frame;
+
+		vlan->h_vlan_encapsulated_proto = vlan->h_vlan_proto;
+		vlan->h_vlan_TCI = htons(info->vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+		len += VLAN_HLEN;
+		tlen += VLAN_HLEN;
+	}
+
+	hsr_stag = (struct hsr_sup_tag *) &info->slave_sup_frame[len];
+
+	/* Remember the supervision tag location. */
+	info->slave_hsr_stag = hsr_stag;
+
+	/* HSR tag will be inserted by the standard hsr_fill_tag routine. */
+	set_hsr_stag_path(hsr_stag, 0);
+	set_hsr_stag_HSR_Ver(hsr_stag, 1);
+	len += sizeof(struct hsr_sup_tag);
+
+	hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+	hsr_stype->HSR_TLV_Type = HSR_TLV_LIFE_CHECK;
+	hsr_stype->HSR_TLV_Length = 6;
+	len += sizeof(struct hsr_sup_type);
+
+	/* Payload: MacAddressA */
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	len += sizeof(struct hsr_sup_payload);
+
+	/* Insert RedBox MAC address if available. */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_stype->HSR_TLV_Type = HSR_TLV_REDBOX;
+	hsr_stype->HSR_TLV_Length = 6;
+	len += sizeof(struct hsr_sup_type);
+
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->redbox->dev_addr);
+
+	/* Cannot have different source MAC address. */
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+	len += sizeof(struct hsr_sup_payload);
+
+	/* Last TLV */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_stype->HSR_TLV_Type = 0;
+	hsr_stype->HSR_TLV_Length = 0;
+	len += sizeof(struct hsr_sup_type);
+	if (len < tlen) {
+		memset(&info->slave_sup_frame[len], 0, tlen - len);
+		len = tlen;
+	}
+	info->slave_len = len;
+}  /* prep_hsr_supervision_slave_frame */
+
+static void send_hsr_supervision_frame(struct hsr_port *master)
+{
+	struct ksz_hsr_info *info = container_of(master->hsr,
+		struct ksz_hsr_info, hsr);
+	struct hsr_sup_tag *hsr_stag = info->hsr_stag;
+
+	hsr_stag->sequence_nr = htons(master->hsr->sup_sequence_nr);
+	master->hsr->sup_sequence_nr++;
+
+	hsr_xmit(info);
+}
+
+/* Announce (supervision frame) timer function
+ */
+static void hsr_announce(unsigned long data)
+{
+	struct hsr_priv *hsr;
+	struct hsr_port *master;
+	struct ksz_hsr_info *info;
+	struct hsr_node *node;
+
+	hsr = (struct hsr_priv *) data;
+	info = container_of(hsr, struct ksz_hsr_info, hsr);
+
+	master = hsr_port_get_hsr(hsr, HSR_PT_MASTER);
+
+	info->tx_frame = info->master_sup_frame;
+	info->len = info->master_len;
+	info->hsr_stag = info->master_hsr_stag;
+	send_hsr_supervision_frame(master);
+
+	info->tx_frame = info->slave_sup_frame;
+	info->len = info->slave_len;
+	info->hsr_stag = info->slave_hsr_stag;
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		struct hsr_sup_type *hsr_stype;
+		struct hsr_sup_payload *hsr_sp;
+		struct hsr_sup_tag *hsr_stag = info->slave_hsr_stag;
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->slave_sup_frame;
+
+		if (1 != node->slave)
+			continue;
+
+		/*
+		 * Cannot use different source MAC address because
+		 * self-address filtering is used to drop frames sent by self.
+		 */
+		memcpy(vlan->h_source, node->MacAddressA, ETH_ALEN);
+		memcpy(vlan->h_source, info->src_addr, ETH_ALEN);
+
+		hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+		hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+		ether_addr_copy(hsr_sp->MacAddressA, node->MacAddressA);
+		send_hsr_supervision_frame(master);
+	}
+
+	hsr->announce_timer.expires = jiffies +
+		msecs_to_jiffies(HSR_LIFE_CHECK_INTERVAL);
+
+	if (netif_running(master->dev) && netif_carrier_ok(master->dev))
+		add_timer(&hsr->announce_timer);
+}
+
+
+/* The uses I can see for these HSR supervision frames are:
+ * 1) Use the frames that are sent after node initialization ("HSR_TLV.Type =
+ *    22") to reset any sequence_nr counters belonging to that node. Useful if
+ *    the other node's counter has been reset for some reason.
+ *    --
+ *    Or not - resetting the counter and bridging the frame would create a
+ *    loop, unfortunately.
+ *
+ * 2) Use the LifeCheck frames to detect ring breaks. I.e. if no LifeCheck
+ *    frame is received from a particular node, we know something is wrong.
+ *    We just register these (as with normal frames) and throw them away.
+ *
+ * 3) Allow different MAC addresses for the two slave interfaces, using the
+ *    MacAddressA field.
+ */
+static bool is_supervision_frame(struct hsr_priv *hsr, struct sk_buff *skb)
+{
+	struct hsr_ethhdr_sp *hdr;
+
+	hdr = (struct hsr_ethhdr_sp *) skb_mac_header(skb);
+
+	if (!ether_addr_equal(hdr->ethhdr.h_dest,
+			      hsr->sup_multicast_addr))
+		return false;
+
+	/* Sent internally with VLAN tag. */
+	if (hdr->ethhdr.h_proto == htons(ETH_P_8021Q))
+		hdr = (struct hsr_ethhdr_sp *)((u8 *) hdr + VLAN_HLEN);
+
+	/* Received from outside with HSR tag. */
+	if (hdr->ethhdr.h_proto == htons(ETH_P_HSR))
+		hdr = (struct hsr_ethhdr_sp *)((u8 *) hdr + HSR_HLEN);
+	if (get_hsr_stag_HSR_ver(&hdr->hsr_sup) < 1)
+		return false;
+	if (hdr->sup_type.HSR_TLV_Type != HSR_TLV_LIFE_CHECK)
+		return false;
+	if (hdr->sup_type.HSR_TLV_Length != 6)
+		return false;
+
+	return true;
+}
+
+static struct sk_buff *create_stripped_skb(struct sk_buff *skb,
+					   struct hsr_frame_info *frame)
+{
+	int copylen;
+	unsigned char *dst, *src;
+
+	copylen = 2 * ETH_ALEN;
+	if (frame->is_vlan)
+		copylen += VLAN_HLEN;
+
+	src = skb->data;
+	dst = skb_pull(skb, HSR_HLEN);
+	memmove(dst, src, copylen);
+	skb_reset_mac_header(skb);
+
+	/* Move HSR frame to non-HSR frame. */
+	frame->skb_std = skb;
+	frame->skb_hsr = NULL;
+
+	return skb;
+}
+
+static struct sk_buff *frame_get_stripped_skb(struct hsr_frame_info *frame,
+					      struct hsr_port *port)
+{
+	if (!frame->skb_std)
+		frame->skb_std = create_stripped_skb(frame->skb_hsr, frame);
+	return frame->skb_std;
+}
+
+static void hsr_fill_tag(struct sk_buff *skb, struct hsr_frame_info *frame,
+			 struct hsr_port *port)
+{
+	int lane_id;
+	int lsdu_size;
+
+	if (hsr_addr_is_self(port->hsr, &skb->data[6]))
+		lane_id = port->hsr->net_id;
+	else
+		lane_id = port->hsr->redbox_id;
+	lane_id <<= 1;
+
+	lsdu_size = skb->len - 14;
+
+	if (lsdu_size < 60 - 14)
+		lsdu_size = 60 - 14;
+	if (frame->is_vlan)
+		lsdu_size -= 4;
+
+	set_hsr_tag_path(frame->hsr_tag, lane_id);
+	set_hsr_tag_LSDU_size(frame->hsr_tag, lsdu_size);
+	frame->hsr_tag->sequence_nr = htons(frame->sequence_nr);
+	*frame->proto = htons(ETH_P_HSR);
+}
+
+static struct sk_buff *create_tagged_skb(struct sk_buff *skb,
+					 struct hsr_frame_info *frame,
+					 struct hsr_port *port)
+{
+	int movelen;
+	unsigned char *dst, *src;
+
+	movelen = 2 * ETH_ALEN;
+	if (frame->is_vlan)
+		movelen += VLAN_HLEN;
+
+	src = skb_mac_header(skb);
+	dst = skb_push(skb, HSR_HLEN);
+	memmove(dst, src, movelen);
+	skb_reset_mac_header(skb);
+
+	hsr_fill_tag(skb, frame, port);
+
+	/* Move non-HSR frame to HSR frame. */
+	frame->skb_hsr = skb;
+	frame->skb_std = NULL;
+
+	return skb;
+}
+
+/* If the original frame was an HSR tagged frame, just clone it to be sent
+ * unchanged. Otherwise, create a private frame especially tagged for 'port'.
+ */
+static struct sk_buff *frame_get_tagged_skb(struct hsr_frame_info *frame,
+					    struct hsr_port *port)
+{
+	if (frame->skb_hsr)
+		return frame->skb_hsr;
+
+	return create_tagged_skb(frame->skb_std, frame, port);
+}
+
+/* Forward the frame through all devices except:
+ * - Back through the receiving device
+ * - If it's a HSR frame: through a device where it has passed before
+ * - To the local HSR master only if the frame is directly addressed to it, or
+ *   a non-supervision multicast or broadcast frame.
+ *
+ * HSR slave devices should insert a HSR tag into the frame, or forward the
+ * frame unchanged if it's already tagged. Interlink devices should strip HSR
+ * tags if they're of the non-HSR type (but only after duplicate discard). The
+ * master device always strips HSR tags.
+ */
+static void hsr_forward_do(struct hsr_frame_info *frame)
+{
+	struct hsr_port *from;
+	struct hsr_port *other;
+	struct hsr_port *port;
+	struct sk_buff *skb;
+	struct ksz_hsr_info *info;
+
+	from = frame->port_rcv;
+
+	/* Coming from host. */
+	if (from->type == HSR_PT_MASTER) {
+		port = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_A);
+		other = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_B);
+
+#if 0
+		/* Deliver frames directly addressed to us to master only */
+		if (frame->is_local_exclusive)
+{
+#if 1
+dbg_msg("local only: %d\n", port->type);
+#endif
+			goto tx_drop;
+}
+#endif
+
+		/* Don't send frame over port where it has been sent before */
+		if (hsr_register_frame_out(port, frame->node_src,
+					   frame->sequence_nr))
+		{
+#if 1
+dbg_msg("saw before: %d %04x %d\n", port->type, frame->sequence_nr,
+frame->is_supervision);
+#endif
+
+			goto tx_drop;
+		}
+		hsr_update_frame_out(other, frame->node_src,
+			frame->sequence_nr);
+
+		/* Simulate incoming frame so that node is not removed. */
+		if (frame->is_supervision && !frame->node_src->slave) {
+			hsr_register_frame_in(frame->node_src, port,
+				frame->sequence_nr);
+		}
+
+		skb = frame_get_tagged_skb(frame, port);
+		skb->protocol = htons(ETH_P_HSR);
+		return;
+	} else {
+		int forward;
+		u16 path_id;
+
+		port = hsr_port_get_hsr(from->hsr, HSR_PT_MASTER);
+		if (from->type == HSR_PT_SLAVE_A)
+			other = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_B);
+		else
+			other = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_A);
+
+		if (hsr_addr_is_self(port->hsr,
+		    eth_hdr(frame->skb_hsr)->h_source)) {
+if (dbg_hsr < 10)
+dbg_msg(" S ");
+++dbg_hsr;
+			goto rx_drop;
+		}
+
+		/* Don't deliver locally unless we should */
+		if (!frame->is_local_dest)
+{
+dbg_msg("not local: %d\n", port->type);
+			goto rx_drop;
+}
+
+		/* Don't send frame over port where it has been sent before */
+		if (hsr_register_frame_out(port, frame->node_src,
+					   frame->sequence_nr))
+		{
+#if 1
+if (dbg_hsr < 10) {
+	if (hsr_addr_is_self(port->hsr, eth_hdr(frame->skb_hsr)->h_source))
+dbg_msg(" self ");
+dbg_msg("saw before: %d %d %04x %d\n", from->type, port->type, frame->sequence_nr,
+frame->is_supervision);
+}
+++dbg_hsr;
+#endif
+
+			goto rx_drop;
+		}
+if (hsr_addr_is_self(port->hsr, eth_hdr(frame->skb_hsr)->h_source)) {
+if (dbg_hsr < 10)
+dbg_msg(" S ");
+}
+		info = container_of(from->hsr, struct ksz_hsr_info, hsr);
+
+		hsr_update_frame_out(other, frame->node_src,
+			frame->sequence_nr);
+		if (frame->is_supervision) {
+			hsr_handle_sup_frame(frame->skb_hsr,
+					     frame->node_src,
+					     frame->port_rcv);
+
+#ifdef CONFIG_HAVE_HSR_HW
+			if (!info->check) {
+				info->check = 1;
+				schedule_delayed_work(&info->chk_ring, 5);
+			}
+#endif
+			goto rx_drop;
+		}
+
+		/* Get the path id before the HSR tag is removed. */
+		path_id = get_hsr_tag_path(frame->hsr_tag);
+
+		skb = frame_get_stripped_skb(frame, port);
+		hsr_addr_subst_source(frame->node_src, skb);
+
+		if (!info->redbox)
+			return;
+		do {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (sw->info->forward & FWD_UCAST)
+				forward = 1;
+			else if (sw->info->forward & FWD_MCAST)
+				forward = 2;
+			else
+				forward = 0;
+		} while (0);
+#if 0
+#if 1
+		if (forward && (path_id >> 1) == info->hsr.redbox_id)
+			forward = 0;
+#endif
+#ifdef CONFIG_1588_PTP_
+		/* Do not forward PTP messages. */
+		if (forward) {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (sw->tag.ports & 0x80)
+				forward = 0;
+		}
+#endif
+		if (forward) {
+			struct sk_buff *new_skb;
+
+			if (1 == forward) {
+				new_skb = skb;
+				frame->skb_std = NULL;
+				frame->skb_hsr = NULL;
+			} else {
+				new_skb = skb_copy(skb, GFP_ATOMIC);
+			}
+			if (new_skb)
+				hsr_dev_xmit(info, info->redbox, new_skb);
+		}
+#endif
+		return;
+
+	}
+tx_drop:
+	if (frame->skb_std) {
+		kfree_skb(frame->skb_std);
+		frame->skb_std = NULL;
+	}
+
+rx_drop:
+	if (frame->skb_hsr) {
+		kfree_skb(frame->skb_hsr);
+		frame->skb_hsr = NULL;
+	}
+}
+
+static void check_local_dest(struct hsr_priv *hsr, struct sk_buff *skb,
+			     struct hsr_frame_info *frame)
+{
+	if (hsr_addr_is_self(hsr, eth_hdr(skb)->h_dest)) {
+		frame->is_local_exclusive = true;
+		skb->pkt_type = PACKET_HOST;
+	} else {
+		frame->is_local_exclusive = false;
+	}
+
+	if ((skb->pkt_type == PACKET_HOST) ||
+	    (skb->data[0] & 0x01)) {
+		frame->is_local_dest = true;
+	} else {
+dbg_msg("not local_dest\n");
+		frame->is_local_dest = false;
+	}
+}
+
+static int hsr_fill_frame_info(struct hsr_frame_info *frame,
+			       struct sk_buff *skb, struct hsr_port *port)
+{
+	unsigned long irqflags;
+
+	frame->is_supervision = is_supervision_frame(port->hsr, skb);
+	frame->node_src = hsr_get_node(&port->hsr->node_db, skb,
+				       frame->is_supervision);
+	if (frame->node_src == NULL)
+		return -1; /* Unknown node and !is_supervision, or no mem */
+
+	/* Most likely this is a received frame. */
+	if (*frame->proto == htons(ETH_P_HSR)) {
+		frame->skb_std = NULL;
+		frame->skb_hsr = skb;
+		frame->sequence_nr = hsr_get_skb_sequence_nr(skb);
+
+	/* Received frame without HSR tag will not come here. */
+	} else {
+		frame->skb_std = skb;
+		frame->skb_hsr = NULL;
+		/* Sequence nr for the master node */
+		spin_lock_irqsave(&port->hsr->seqnr_lock, irqflags);
+		frame->sequence_nr = port->hsr->sequence_nr;
+		port->hsr->sequence_nr++;
+		spin_unlock_irqrestore(&port->hsr->seqnr_lock, irqflags);
+	}
+
+	frame->port_rcv = port;
+#if 1
+	if (port->type != HSR_PT_MASTER)
+		check_local_dest(port->hsr, skb, frame);
+#endif
+
+	return 0;
+}
+
+static
+int hsr_forward_skb(struct sk_buff *skb, struct hsr_port *port)
+{
+	struct hsr_frame_info _frame;
+	struct hsr_frame_info *frame = &_frame;
+
+	skb_reset_mac_header(skb);
+
+	/* Coming from host. */
+	if (port->type == HSR_PT_MASTER) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+		u8 *proto;
+		u8 *hsr_tag;
+
+		memset(frame, 0, sizeof(struct hsr_frame_info));
+		proto = (u8 *) &vlan->h_vlan_proto;
+		hsr_tag = (u8 *) &vlan->h_vlan_TCI;
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			frame->is_vlan = true;
+			proto = (u8 *) &vlan->h_vlan_encapsulated_proto;
+			hsr_tag = (u8 *)(vlan + 1);
+		}
+		proto -= HSR_HLEN;
+		hsr_tag -= HSR_HLEN;
+		frame->proto = (u16 *) proto;
+		frame->hsr_tag = (struct hsr_tag *) hsr_tag;
+	} else {
+		struct ksz_hsr_info *info = container_of(port->hsr,
+			struct ksz_hsr_info, hsr);
+
+		frame = &info->frame;
+	}
+	if (hsr_fill_frame_info(frame, skb, port) < 0)
+		goto out_drop;
+	hsr_register_frame_in(frame->node_src, port, frame->sequence_nr);
+	hsr_forward_do(frame);
+
+	/* Frame is supervision or was dropped. */
+	if (!frame->skb_hsr && !frame->skb_std)
+{
+#if 0
+if (!frame->is_supervision)
+dbg_msg(" d ");
+#endif
+		return 0;
+}
+
+	/* Pass to upper layer or down. */
+	return 1;
+
+out_drop:
+dbg_msg("fwd drop: %02x:%02x:%02x:%02x:%02x:%02x  %02x:%02x:%02x:%02x:%02x:%02x\n",
+skb->data[0],
+skb->data[1],
+skb->data[2],
+skb->data[3],
+skb->data[4],
+skb->data[5],
+skb->data[6],
+skb->data[7],
+skb->data[8],
+skb->data[9],
+skb->data[10],
+skb->data[11]);
+#if 0
+	port->dev->stats.tx_dropped++;
+#endif
+	kfree_skb(skb);
+	return 0;
+}
+
+static void *check_hsr_frame(u8 *data, struct hsr_frame_info *frame)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+
+	frame->is_vlan = false;
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		frame->is_vlan = true;
+		frame->proto = &vlan->h_vlan_encapsulated_proto;
+		frame->hsr_tag = (struct hsr_tag *)(vlan + 1);
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_HSR))
+			return vlan + 1;
+	}
+
+	/* VLAN tag can be removed by the switch. */
+	if (vlan->h_vlan_proto == htons(ETH_P_HSR)) {
+		struct ethhdr *eth = (struct ethhdr *) data;
+
+		frame->proto = &vlan->h_vlan_proto;
+		frame->hsr_tag = (struct hsr_tag *)(eth + 1);
+		return eth + 1;
+	}
+	return NULL;
+}  /* check_hsr_frame */
+
+static int hsr_chk(struct ksz_hsr_info *info, struct sk_buff *skb, int port)
+{
+#if 0 
+	struct sk_buff *new_skb;
+#endif
+	struct hsr_node *node;
+	struct list_head *node_db;
+	int forward;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+	int ret = 2;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (!info->redbox)
+		return ret;
+
+	/* Stop processing if coming from HSR ports. */
+	if (port == info->ports[0] || port == info->ports[1])
+		return ret;
+
+	if (sw->info->forward & FWD_UCAST)
+		forward = 1;
+	else if (sw->info->forward & FWD_MCAST)
+		forward = 2;
+	else
+		forward = 0;
+	if (!forward)
+		return ret;
+
+	node_db = &info->hsr.node_db;
+	node = find_node_by_AddrA(node_db, vlan->h_source);
+	if (!node) {
+		struct hsr_sup_type *hsr_stype;
+		struct hsr_sup_payload *hsr_sp;
+		struct hsr_sup_tag *hsr_stag = info->slave_hsr_stag;
+		struct hsr_port *master = hsr_port_get_hsr(&info->hsr,
+			HSR_PT_MASTER);
+
+		node = hsr_add_slave(node_db, vlan->h_source,
+			info->hsr.sequence_nr - 1);
+		if (!node)
+			return ret;
+
+		info->tx_frame = info->slave_sup_frame;
+		info->len = info->slave_len;
+		info->hsr_stag = info->slave_hsr_stag;
+
+		vlan = (struct vlan_ethhdr *) info->slave_sup_frame;
+
+		/*
+		 * Cannot use different source MAC address because
+		 * self-address filtering is used to drop frames sent by self.
+		 */
+		memcpy(vlan->h_source, node->MacAddressA, ETH_ALEN);
+		memcpy(vlan->h_source, info->src_addr, ETH_ALEN);
+
+		hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+		hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+		ether_addr_copy(hsr_sp->MacAddressA, node->MacAddressA);
+		send_hsr_supervision_frame(master);
+	} else {
+		node->slave = 1;
+		node->time_in[HSR_PT_SLAVE_A] = jiffies;
+		node->time_in_stale[HSR_PT_SLAVE_A] = false;
+	}
+#if 0
+	if (1 == forward) {
+		new_skb = skb;
+		ret = 0;
+	} else {
+		new_skb = skb_copy(skb, GFP_ATOMIC);
+		if (!new_skb)
+			return ret;
+	}
+
+	/* Replace source MAC address. */
+	memcpy(&new_skb->data[6], info->src_addr, ETH_ALEN);
+
+	/* Send frame to HSR device. */
+	hsr_dev_xmit(info, info->dev, new_skb);
+#endif
+	return ret;
+}  /* hsr_chk */
+
+static int hsr_rcv(struct ksz_hsr_info *info, struct sk_buff *skb, int port)
+{
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		port = 0;
+	else if (port == info->ports[1])
+		port = 1;
+	if (port > 1)
+		return 2;
+	if(check_hsr_frame(skb->data, &info->frame)) {
+		enum hsr_port_type pt;
+		struct hsr_port *from;
+
+		if (1 == port)
+			pt = HSR_PT_SLAVE_B;
+		else
+			pt = HSR_PT_SLAVE_A;
+		from = hsr_port_get_hsr(&info->hsr, pt);
+		return hsr_forward_skb(skb, from);
+	}
+	return 2;
+}  /* hsr_rcv */
+
+static
+void hsr_add_port(struct hsr_priv *hsr, struct net_device *dev,
+	enum hsr_port_type type)
+{
+	struct hsr_port *port;
+
+	port = hsr_port_get_hsr(hsr, type);
+
+	port->hsr = hsr;
+	port->dev = dev;
+	port->type = type;
+}
+
+/* Default multicast address for HSR Supervision frames */
+static const unsigned char def_multicast_addr[ETH_ALEN] __aligned(2) = {
+	0x01, 0x15, 0x4e, 0x00, 0x01, 0x00
+};
+
+static void prep_hsr_addr(struct ksz_hsr_info *info, u8 *src)
+{
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+		info->master_sup_frame;
+
+	memcpy(info->src_addr, src, ETH_ALEN);
+
+	memcpy(vlan->h_source, src, ETH_ALEN);
+	hsr_stype = (struct hsr_sup_type *)(info->master_hsr_stag + 1);
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+}  /* prep_hsr_addr */
+
+static void prep_hsr_redbox_addr(struct ksz_hsr_info *info)
+{
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+
+	if (!info->redbox)
+		return;
+	hsr_stype = (struct hsr_sup_type *)(info->slave_hsr_stag + 1);
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->redbox->dev_addr);
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+}  /* prep_hsr_redbox_addr */
+
+static void hsr_change_addr(struct ksz_hsr_info *info, struct net_device *dev)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Do not do anything if device is not ready. */
+	if (!info->dev || !netif_running(dev))
+		return;
+	if (info->dev == dev) {
+		if (!memcmp(info->src_addr, dev->dev_addr, ETH_ALEN))
+			return;
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, info->src_addr, 0,
+			false, false, 0);
+		if (sw->eth_cnt > 1) {
+			sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, info->src_addr,
+				0, false, false, sw->eth_maps[0].vlan);
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, info->src_addr,
+				0, false, false, sw->eth_maps[1].vlan);
+		}
+		prep_hsr_addr(info, dev->dev_addr);
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, info->src_addr,
+			sw->HOST_MASK, false, false, 0);
+		if (sw->eth_cnt > 1) {
+			sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, info->src_addr,
+				sw->HOST_MASK, false, false,
+				sw->eth_maps[0].vlan);
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, info->src_addr,
+				sw->HOST_MASK, false, false,
+				sw->eth_maps[1].vlan);
+		}
+	} else {
+		prep_hsr_redbox_addr(info);
+	}
+}  /* hsr_change_addr */
+
+static void hsr_link_change(struct ksz_hsr_info *info, int link1, int link2)
+{
+	info->p1_down = !link1;
+	info->p2_down = !link2;
+	if (info->ring && (info->p1_down || info->p2_down)) {
+		info->ring = 0;
+		hsr_notify_link_lost(info);
+	}
+}  /* hsr_link_change */
+
+static u8 hsr_get_redbox_id(struct ksz_hsr_info *info)
+{
+	return info->hsr.redbox_id;
+}  /* hsr_get_redbox_id */
+
+static void hsr_set_redbox_id(struct ksz_hsr_info *info, u8 id)
+{
+	info->hsr.redbox_id = id;
+}  /* hsr_set_redbox_id */
+
+static u8 hsr_get_net_id(struct ksz_hsr_info *info)
+{
+	return info->hsr.net_id;
+}  /* hsr_get_net_id */
+
+static void hsr_set_net_id(struct ksz_hsr_info *info, u8 id)
+{
+	info->hsr.net_id = id;
+}  /* hsr_set_net_id */
+
+static void prep_hsr_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, def_multicast_addr, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_hsr_mcast */
+
+static void hsr_check_announce(struct ksz_hsr_info *info)
+{
+	int state;
+	struct hsr_priv *hsr = &info->hsr;
+
+	if (info->state < 0)
+		return;
+	state = netif_running(info->dev) && netif_carrier_ok(info->dev);
+	if (state != info->state) {
+dbg_msg("%s %d %d\n", __func__, info->state, state);
+		if (state) {
+			hsr->announce_timer.expires = jiffies +
+				msecs_to_jiffies(1);
+			add_timer(&hsr->announce_timer);
+		} else
+			del_timer(&hsr->announce_timer);
+		info->state = state;
+	}
+}
+
+static void hsr_dev_init(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+
+	INIT_LIST_HEAD(&hsr->node_db);
+	INIT_LIST_HEAD(&hsr->self_node_db);
+
+	spin_lock_init(&hsr->seqnr_lock);
+
+	init_timer(&hsr->announce_timer);
+	hsr->announce_timer.function = hsr_announce;
+	hsr->announce_timer.data = (unsigned long) hsr;
+
+	init_timer(&hsr->prune_timer);
+	hsr->prune_timer.function = hsr_prune_nodes;
+	hsr->prune_timer.data = (unsigned long) hsr;
+
+	ether_addr_copy(hsr->sup_multicast_addr, def_multicast_addr);
+	hsr->sup_multicast_addr[ETH_ALEN - 1] = 0;
+
+	hsr_add_port(hsr, info->dev, HSR_PT_MASTER);
+
+	hsr_add_port(hsr, info->dev, HSR_PT_SLAVE_A);
+	hsr_add_port(hsr, info->dev, HSR_PT_SLAVE_B);
+
+	info->state = -1;
+}
+
+static int hsr_dev_finalize(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+	int res;
+
+	res = hsr_create_self_node(&hsr->self_node_db, info->dev->dev_addr,
+				   info->dev->dev_addr);
+	if (res < 0)
+		return res;
+
+	/* Overflow soon to find bugs easier: */
+	hsr->sequence_nr = HSR_SEQNR_START;
+	hsr->sup_sequence_nr = HSR_SEQNR_START;
+	hsr->redbox_id = 7;
+	hsr->net_id = 0;
+
+#if 1
+	msleep(HSR_NODE_REBOOT_INTERVAL);
+#endif
+	/* Ready for announcement. */
+	info->state = 0;
+	hsr_check_announce(info);
+
+	hsr->prune_timer.expires = jiffies + msecs_to_jiffies(PRUNE_PERIOD);
+	add_timer(&hsr->prune_timer);
+
+	return 0;
+}
+
+static void hsr_dev_destroy(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+	struct hsr_node *node;
+
+	node = list_first_or_null_rcu(&hsr->self_node_db, struct hsr_node,
+				      mac_list);
+	if (node) {
+		list_del_rcu(&node->mac_list);
+		kfree_rcu(node, rcu_head);
+	}
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		list_del_rcu(&node->mac_list);
+		kfree_rcu(node, rcu_head);
+	}
+	del_timer_sync(&hsr->prune_timer);
+	del_timer_sync(&hsr->announce_timer);
+	info->state = -1;
+}
+
+static void setup_hsr(struct ksz_hsr_info *info, struct net_device *dev)
+{
+	info->dev = dev;
+	info->vid = 0;
+
+	hsr_dev_init(info);
+	prep_hsr_supervision_frame(info);
+	info->tx_frame = info->master_sup_frame;
+	info->len = info->master_len;
+	info->hsr_stag = info->master_hsr_stag;
+	prep_hsr_mcast(dev);
+}  /* setup_hsr */
+
+static void setup_hsr_redbox(struct ksz_hsr_info *info, struct net_device *dev)
+{
+	info->redbox = dev;
+	prep_hsr_supervision_slave_frame(info);
+}  /* setup_hsr_redbox */
+
+static int hsr_get_attrib(struct ksz_hsr_info *info, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct hsr_node *node;
+	struct ksz_hsr_node *active;
+	struct hsr_priv *hsr = &info->hsr;
+	union hsr_data *attrib = (union hsr_data *) data;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case HSR_GET_NETWORK_STATUS:
+		*len = 1;
+		attrib->byte = info->ring;
+		break;
+	case HSR_GET_CAPABILITY_FLAGS:
+		*len = 4;
+		attrib->dword = info->cap;
+		break;
+	case HSR_GET_RING_PARTICIPANTS_COUNT:
+		*len = 2;
+		attrib->word = info->part_cnt;
+		break;
+	case HSR_GET_RING_PARTICIPANTS_LIST:
+		*len = sizeof(struct ksz_hsr_node);
+		*len *= info->part_cnt;
+		if (size < *len) {
+			break;
+		}
+		active = (struct ksz_hsr_node *) data;
+		rcu_read_lock();
+		list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+			if (!node->slave) {
+				memcpy(&active->addr, node->MacAddressA,
+					ETH_ALEN);
+				active++;
+			}
+		}
+		rcu_read_unlock();
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	return DEV_IOC_OK;
+}  /* hsr_get_attrib */
+
+static int hsr_dev_req(struct ksz_hsr_info *hsr, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 0;
+				data[5] = hsr->member;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+				hsr->dev_info = info;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			msg->module = DEV_MOD_HSR;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			sw_setup_msg(info, msg, 8, NULL, NULL);
+			hsr->notifications = 0;
+			hsr->dev_info = NULL;
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				hsr->notifications = *notify;
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+#if 0
+		result = hsr_set_attrib(hsr, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+#endif
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		result = hsr_get_attrib(hsr, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* hsr_dev_req */
+
+static struct hsr_ops hsr_ops = {
+	.change_addr		= hsr_change_addr,
+	.link_change		= hsr_link_change,
+	.check_announce		= hsr_check_announce,
+	.get_redbox_id		= hsr_get_redbox_id,
+	.set_redbox_id		= hsr_set_redbox_id,
+	.get_net_id		= hsr_get_net_id,
+	.set_net_id		= hsr_set_net_id,
+
+	.dev_req		= hsr_dev_req,
+};
+
+static void prep_hsr(struct ksz_hsr_info *info, struct net_device *dev,
+	u8 *src)
+{
+	info->dev = dev;
+	info->center = NULL;
+	info->seq_num = 0;
+	info->ring = 0;
+	prep_hsr_addr(info, src);
+	prep_hsr_redbox_addr(info);
+	if (info->vid) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->master_sup_frame;
+
+		vlan->h_vlan_TCI = htons(info->vid);
+		vlan = (struct vlan_ethhdr *) info->slave_sup_frame;
+		vlan->h_vlan_TCI = htons(info->vid);
+	}
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, info->src_addr,
+			sw->HOST_MASK, false, false, 0);
+		if (sw->eth_cnt > 1) {
+			sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, info->src_addr,
+				sw->HOST_MASK, false, false,
+				sw->eth_maps[0].vlan);
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, info->src_addr,
+				sw->HOST_MASK, false, false,
+				sw->eth_maps[1].vlan);
+		}
+	} while (0);
+	hsr_dev_finalize(info);
+}  /* prep_hsr */
+
+static void sw_setup_hsr(struct ksz_sw *sw)
+{
+/*
+ * THa  2015/12/06
+ * The HSR register 0x640 needs to be set, even though the value read is 3.
+ * The NODE_UNICAST bit in register 0x644 needs to be turned off for multicast
+ * address to work.
+ * If HSR_LEARN_UCAST_DISABLE bit in register 0x645 is turned on, multicast
+ * address also does not work, even though HSR_LEARN_MCAST_DISABLE bit in
+ * register 0x644 can be used to control that.
+ */
+	SW_D data;
+	int n;
+	u16 mask = 0;
+
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->eth_maps[n].proto & HSR_HW) {
+			mask = sw->eth_maps[n].mask;
+			break;
+		}
+	}
+	sw->reg->w32(sw, REG_HSR_PORT_MAP__4, mask);
+	data = SW_R(sw, REG_HSR_ALU_CTRL_0__1);
+	data &= ~HSR_NODE_UNICAST;
+	SW_W(sw, REG_HSR_ALU_CTRL_0__1, data);
+	if ((sw->overrides & HAVE_MORE_THAN_2_PORTS) && 1 == sw->eth_cnt)
+		sw_cfg_port_base_vlan(sw, sw->HOST_PORT, mask | sw->HOST_MASK);
+}  /* sw_setup_hsr */
+
+static void stop_hsr(struct ksz_hsr_info *info)
+{
+	hsr_dev_destroy(info);
+}
+
+static void ksz_hsr_exit(struct ksz_hsr_info *info)
+{
+#ifdef CONFIG_HAVE_HSR_HW
+	cancel_delayed_work_sync(&info->chk_ring);
+#endif
+}  /* ksz_hsr_exit */
+
+static void ksz_hsr_init(struct ksz_hsr_info *info, struct ksz_sw *sw)
+{
+	info->state = -1;
+	info->ports[0] = 0;
+	info->ports[1] = 1;
+	info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+#ifdef CONFIG_HAVE_HSR_HW
+	info->cap = HSR_CAP_DUPLICATE_DISCARD;
+	info->cap |= HSR_CAP_REDBOX_CAPABLE;
+#endif
+	info->sw_dev = sw;
+	info->ops = &hsr_ops;
+
+#ifdef CONFIG_HAVE_HSR_HW
+	INIT_DELAYED_WORK(&info->chk_ring, hsr_chk_ring);
+#endif
+}  /* ksz_hsr_init */
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_hsr.h b/drivers/net/ethernet/micrel/ksz9897/ksz_hsr.h
new file mode 100644
index 0000000..b4d1011
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_hsr.h
@@ -0,0 +1,99 @@
+/**
+ * Microchip HSR driver header
+ *
+ * Copyright (c) 2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_HSR_H
+#define KSZ_HSR_H
+
+#include "hsr_main.h"
+#include "ksz_hsr_api.h"
+
+
+struct hsr_frame_info {
+	struct sk_buff *skb_std;
+	struct sk_buff *skb_hsr;
+	struct hsr_port *port_rcv;
+	struct hsr_node *node_src;
+	u16 sequence_nr;
+	u16 *proto;
+	struct hsr_tag *hsr_tag;
+	bool is_supervision;
+	bool is_vlan;
+	bool is_local_dest;
+	bool is_local_exclusive;
+};
+
+
+struct ksz_hsr_info;
+
+struct hsr_ops {
+	void (*change_addr)(struct ksz_hsr_info *info, struct net_device *dev);
+	void (*link_change)(struct ksz_hsr_info *info, int link1, int link2);
+	void (*check_announce)(struct ksz_hsr_info *info);
+	u8 (*get_redbox_id)(struct ksz_hsr_info *info);
+	void (*set_redbox_id)(struct ksz_hsr_info *info, u8 id);
+	u8 (*get_net_id)(struct ksz_hsr_info *info);
+	void (*set_net_id)(struct ksz_hsr_info *info, u8 id);
+
+	int (*dev_req)(struct ksz_hsr_info *hsr, char *arg, void *info);
+
+};
+
+
+struct ksz_hsr_info {
+	u16 vid;
+	u8 src_addr[ETH_ALEN];
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct net_device *redbox;
+	struct hsr_priv hsr;
+	struct hsr_port hsr_ports[HSR_PT_PORTS];
+	struct hsr_node *center;
+
+	u8 master_sup_frame[80];
+	u8 slave_sup_frame[80];
+	u8 *tx_frame;
+	struct hsr_sup_tag *hsr_stag;
+	struct hsr_sup_tag *master_hsr_stag;
+	struct hsr_sup_tag *slave_hsr_stag;
+	struct hsr_frame_info frame;
+	int master_len;
+	int slave_len;
+	int len;
+	int state;
+	u8 ports[2];
+	u32 cap;
+	u16 member;
+	u16 part_cnt;
+	struct delayed_work chk_ring;
+	u16 seq_num;
+	u16 check:1;
+	u16 ring:1;
+	u16 p1_down:1;
+	u16 p2_down:1;
+	u16 p1_lost:1;
+	u16 p2_lost:1;
+
+	struct sw_dev_info *dev_info;
+	uint notifications;
+
+	uint overrides;
+
+	const struct hsr_ops *ops;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_hsr_api.h b/drivers/net/ethernet/micrel/ksz9897/ksz_hsr_api.h
new file mode 100644
index 0000000..97e4327
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_hsr_api.h
@@ -0,0 +1,53 @@
+/**
+ * Microchip HSR driver API header
+ *
+ * Copyright (c) 2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_HSR_API_H
+#define KSZ_HSR_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN				6
+#endif
+
+
+enum {
+	DEV_INFO_HSR_LINK = DEV_INFO_LAST,
+};
+
+#define HSR_INFO_LINK_LOST			(1 << 0)
+
+
+#define HSR_GET_NETWORK_STATUS			2
+#define HSR_GET_RING_PARTICIPANTS_COUNT		8
+#define HSR_GET_RING_PARTICIPANTS_LIST		9
+#define HSR_GET_CAPABILITY_FLAGS		12
+
+#define HSR_CAP_DUPLICATE_DISCARD		(1 << 0)
+#define HSR_CAP_REDBOX_CAPABLE			(1 << 1)
+
+
+struct ksz_hsr_node {
+	u8 addr[ETH_ALEN];
+} __packed;
+
+union hsr_data {
+	struct ksz_hsr_node active;
+	u32 dword;
+	u16 word;
+	u8 byte;
+} __packed;
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_iba.c b/drivers/net/ethernet/micrel/ksz9897/ksz_iba.c
new file mode 100644
index 0000000..d4bd471
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_iba.c
@@ -0,0 +1,3148 @@
+/**
+ * Microchip IBA code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#define ETH_P_IBA			IBA_TAG_TYPE
+
+static void prepare_iba(struct ksz_iba_info *iba, u8 *dst, u8 *src)
+{
+#ifndef CAPTURE_IBA
+	if (iba->dst != dst)
+		memcpy(iba->dst, dst, ETH_ALEN);
+	if (iba->src != src)
+		memcpy(iba->src, src, ETH_ALEN);
+#endif
+	memcpy(iba->packet, iba->dst, ETH_ALEN);
+	memcpy(&iba->packet[ETH_ALEN], iba->src, ETH_ALEN);
+
+	iba->frame->tag.type = htons(iba->tag_type);
+	iba->frame->tag.prio = 0;
+	iba->frame->tag.cfi = 0;
+	iba->frame->tag.mode = 1;
+	iba->frame->format.format = htons(IBA_FORMAT_KSZ98XX);
+	iba->frame->format.reserved = 0;
+
+	iba->cmds[0].cmd = 0;
+}  /* prepare_iba */
+
+static void *iba_command(void *frame, int *size, u32 cmd, int cnt, u32 *data)
+{
+	struct iba_cmd *iba = frame;
+	int i;
+	int len = 4;
+	int final_len = *size + sizeof(u32) * cnt + 8;
+
+	if (final_len > IBA_LEN_MAX && cmd) {
+		cnt = (IBA_LEN_MAX - *size - 8) / (int) sizeof(u32);
+		if (cnt > 0)
+			data[0] = cnt - 1;
+		else
+			cmd = 0;
+dbg_msg(" command: %d\n", cnt);
+	}
+
+	iba->cmd = htonl(cmd);
+	for (i = 0; i < cnt; i++) {
+		iba->data[i] = htonl(data[i]);
+		len += 4;
+	}
+	frame = &iba->data[i];
+	*size += len;
+	return frame;
+}  /* iba_command */
+
+static unsigned long last_iba_jiffies;
+static int dbg_iba;
+static int last_ok_iba;
+static int last_ok_reg;
+
+static void *iba_pre_cmd(struct ksz_iba_info *info, u16 code)
+{
+	struct iba_frame *iba = info->frame;
+
+if (info->respid != info->seqid) {
+dbg_msg(" pre %x %x; %x; %x %x\n", info->respid, info->seqid, code,
+last_iba_jiffies, jiffies);
+dbg_iba = 1;
+}
+last_iba_jiffies = jiffies;
+	info->index = 0;
+	info->len = sizeof(struct iba_frame) - sizeof(struct iba_cmd) +
+		ETH_ALEN * 2;
+	iba->code = htons(code);
+	return &iba->cmd;
+}  /* iba_pre_cmd */
+
+static u32 iba_get_val(u32 size, u32 val)
+{
+	int shift;
+
+	switch (size) {
+	case IBA_CMD_32:
+		break;
+	case IBA_CMD_16:
+		val >>= 16;
+		break;
+	case IBA_CMD_16_M:
+		val >>= 8;
+		val &= 0xffff;
+		break;
+	case IBA_CMD_16_H:
+		val &= 0xffff;
+		break;
+	case IBA_CMD_24:
+		val >>= 8;
+		break;
+	case IBA_CMD_24_H:
+		val &= 0xffffff;
+		break;
+	default:
+		switch (size) {
+		case IBA_CMD_BYTE_0:
+			shift = 3;
+			break;
+		case IBA_CMD_BYTE_1:
+			shift = 2;
+			break;
+		case IBA_CMD_BYTE_2:
+			shift = 1;
+			break;
+		default:
+			shift = 0;
+			break;
+		}
+		val >>= shift * 8;
+		val &= 0xff;
+	}
+	return val;
+}  /* iba_get_val */
+
+static u32 iba_set_size(u32 addr, u32 size)
+{
+	switch (size) {
+	case IBA_CMD_8:
+		size = IBA_CMD_8 >> (addr & 3);
+		break;
+	case IBA_CMD_16:
+		if (addr & 2)
+			size = IBA_CMD_16 >> 2;
+		else if (addr & 1)
+			size = IBA_CMD_16 >> 1;
+		break;
+	case IBA_CMD_24:
+		size = IBA_CMD_24 >> (addr & 1);
+		break;
+	}
+	return size;
+}  /* iba_set_size */
+
+static u32 iba_set_val(u32 size, u32 addr, u32 val)
+{
+	switch (size) {
+	case IBA_CMD_8:
+		val &= 0xff;
+		val <<= (3 - (addr & 3)) * 8;
+		break;
+	case IBA_CMD_16:
+		val &= 0xffff;
+		if (!(addr & 2)) {
+			if (addr & 1)
+				val <<= 1 * 8;
+			else
+				val <<= 2 * 8;
+		}
+		break;
+	case IBA_CMD_24:
+		val &= 0xffffff;
+		val <<= (1 - (addr & 1)) * 8;
+		break;
+	}
+	return val;
+}  /* iba_set_val */
+
+/**
+ * iba_chk_regs - IBA register check
+ * @sw:		The switch instance.
+ * @cmds:	The IBA command.
+ * @data:	The IBA data.
+ *
+ * This routine checks the value written to specific registers to determine
+ * the correct tail tag length to use.
+ */
+static void iba_chk_regs(struct ksz_sw *sw, u32 cmds, u32 data)
+{
+	u32 port_reg;
+	u32 reg = cmds & IBA_CMD_ADDR_M;
+	u32 size = cmds & IBA_CMD_32;
+	u32 val = iba_get_val(size, data);
+	int need_ptp_tag = 0;
+	int need_tail_tag = 0;
+
+	if (IBA_CMD_BYTE_1 == size && (REG_PTP_MSG_CONF1 + 1) == reg) {
+		need_ptp_tag++;
+		if (val & PTP_ENABLE)
+			need_ptp_tag++;
+	} else if (IBA_CMD_16 == size && REG_PTP_MSG_CONF1 == reg) {
+		need_ptp_tag++;
+		if (val & PTP_ENABLE)
+			need_ptp_tag++;
+	} else if (IBA_CMD_32 == size && REG_PTP_MSG_CONF1 == reg) {
+		need_ptp_tag++;
+		if (val & (PTP_ENABLE << 16))
+			need_ptp_tag++;
+	}
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+	if (!(sw->features & NEW_CAP) &&
+	    sw->TAIL_TAG_LOOKUP >= 0x100 && 1 == need_ptp_tag)
+		need_ptp_tag = 2;
+#endif
+	if (2 == need_ptp_tag)
+		sw->overrides |= PTP_TAG;
+	else if (1 == need_ptp_tag)
+		sw->overrides &= ~PTP_TAG;
+
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if (IBA_CMD_BYTE_0 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & PORT_TAIL_TAG_ENABLE)
+			need_tail_tag++;
+	} else if (IBA_CMD_16 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & (PORT_TAIL_TAG_ENABLE << 8))
+			need_tail_tag++;
+	} else if (IBA_CMD_32 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & (PORT_TAIL_TAG_ENABLE << 24))
+			need_tail_tag++;
+	}
+	if (2 == need_tail_tag)
+		sw->overrides |= TAIL_TAGGING;
+	else if (1 == need_tail_tag)
+		sw->overrides &= ~TAIL_TAGGING;
+}  /* iba_chk_regs */
+
+static u32 last_iba_addr;
+
+static void *iba_cmd_data(struct ksz_iba_info *info, u32 cmd, u32 size,
+	u32 addr)
+{
+	int cnt = 1;
+	int shift = IBA_CMD_S;
+	struct iba_frame *iba = info->frame;
+
+	if (iba->code == htons(IBA_CODE_BURST)) {
+		cnt = info->data[0] + 1;
+		shift = IBA_BURST_S;
+	} else {
+		if (IBA_CMD_16 == size && 3 == (addr & 3))
+			pr_info("16-bit used with register ended with 3\n");
+
+		/* write can be 8-bit, 16-bit, or 32-bit. */
+		if (IBA_CMD_WRITE == cmd) {
+			if (REG_SW_IBA__4 <= addr && addr < REG_SW_IBA__4 + 4) {
+				info->data[0] = info->cfg;
+				addr = REG_SW_IBA__4;
+				size = IBA_CMD_32;
+			}
+			info->data[0] = iba_set_val(size, addr, info->data[0]);
+		}
+		size = iba_set_size(addr, size);
+	}
+	cmd <<= shift;
+	cmd |= size;
+	cmd |= addr & IBA_CMD_ADDR_M;
+	last_iba_addr = addr;
+	info->cmds[info->index].data[0] = info->data[0];
+	info->cmds[info->index++].cmd = cmd;
+	info->fptr = iba_command(info->fptr, &info->len, cmd, cnt, info->data);
+	if (info->len + 4 >= IBA_LEN_MAX && iba->code == htons(IBA_CODE_BURST))
+		info->cmds[info->index - 1].data[0] = info->data[0];
+	return info->fptr;
+}  /* iba_cmd_data */
+
+static void *iba_post_cmd(struct ksz_iba_info *info)
+{
+	struct iba_frame *iba = info->frame;
+
+if (info->respid != info->seqid) {
+dbg_msg(" post %x %x\n", info->respid, info->seqid);
+dbg_iba = 1;
+}
+	info->cmds[info->index++].cmd = 0;
+	info->fptr = iba_command(info->fptr, &info->len, 0, 0, NULL);
+	iba->tag.seqid = ++info->seqid;
+	iba->length = htons(info->len);
+	return info->fptr;
+}  /* iba_post_cmd */
+
+/**
+ * iba_xmit - Transmit IBA request.
+ * @info:	The IBA instance.
+ *
+ * This function prepares IBA request for transmit.
+ */
+static int iba_xmit(struct ksz_iba_info *info)
+{
+	int rc;
+	struct sk_buff *skb;
+	int len = ntohs(info->frame->length);
+	const struct net_device_ops *ops = info->dev->netdev_ops;
+
+	if (len < 60) {
+		memset(&info->packet[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, info->packet, len);
+	skb_put(skb, len);
+	skb->protocol = htons(ETH_P_IBA);
+	skb->dev = info->dev;
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(info->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* iba_xmit */
+
+#ifdef VERIFY_IBA
+static void prepare_cmd(struct ksz_iba_info *info, int message)
+{
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	switch (message) {
+	case 0:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, 0);
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, 4);
+		break;
+	case 1:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_UNIT_INDEX__4);
+		info->data[0] = TS_RESET;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_RESET;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_ENABLE;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_ENABLE;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 2:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0xB;
+		info->data[0] <<= MIB_COUNTER_INDEX_S;
+		info->data[0] |= MIB_COUNTER_READ;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_CTRL_STAT__4));
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_DATA));
+		break;
+	case 3:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_BURST);
+		info->data[0] = 2;
+		info->data[1] = 0x12340000;
+		info->data[2] = 0x56780000;
+		info->fptr = iba_cmd_data(info, IBA_BURST_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 4;
+		info->data[1] = 0x12340000;
+		info->data[2] = 0x56780000;
+		info->data[3] = 0x00001234;
+		info->data[4] = 0x00005678;
+		info->fptr = iba_cmd_data(info, IBA_BURST_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 60;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		if (info->len + 4 >= IBA_LEN_MAX)
+			break;
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 4:
+#if 1
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#else
+		info->fptr = iba_pre_cmd(info, 4);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#if 0
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#if 0
+		info->fptr = iba_cmd_data(info, 3,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#endif
+		break;
+	case 5:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_BURST);
+		info->data[0] = 63;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 6:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 1
+		info->data[0] = 0x1;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0x00800000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			0x00);
+		info->data[0] = 0x10000000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0x10000000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x8;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			0x304);
+		break;
+	case 7:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 0
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		break;
+	case 8:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 0
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		break;
+	default:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+	}
+	info->fptr = iba_post_cmd(info);
+}
+
+static int dbg_iba_test;
+static int iba_test(struct ksz_iba_info *info, int n)
+{
+	int k;
+	int rc;
+	unsigned long wait;
+
+	prepare_cmd(info, n);
+	info->regs[0].cmd = (u32) -1;
+	init_completion(&info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		dbg_msg("send err: %d\n", rc);
+		return rc;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	dbg_msg("wait: %lx\n", wait);
+
+	k = 0;
+	while (info->regs[k].cmd != (u32) -1) {
+		dbg_msg("%08x=%08x\n", info->regs[k].cmd,
+			info->regs[k].data[0]);
+		k++;
+	}
+#ifndef TEST_IBA
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+		u32 status;
+
+if (mutex_is_locked(sw->reglock))
+printk(" reg locked\n");
+		mutex_lock(sw->reglock);
+		info->use_iba = 0;
+		status = sw_r32(sw, REG_SW_IBA_STATUS__4);
+		dbg_msg("status %08x q:%d p:%d d:%d f:%d o:%d m:%d\n", status,
+			!!(status & SW_IBA_REQ),
+			!!(status & SW_IBA_RESP),
+			!!(status & SW_IBA_DA_MISMATCH),
+			!!(status & SW_IBA_FMT_MISMATCH),
+			!!(status & SW_IBA_CODE_ERROR),
+			!!(status & SW_IBA_CMD_ERROR));
+		status = sw_r32(sw, REG_SW_IBA_STATES__4);
+		dbg_msg("states %08x %u\n", status,
+			((status >> SW_IBA_PACKET_SIZE_S) &
+			SW_IBA_PACKET_SIZE_M) * 4);
+		status = sw_r32(sw, REG_SW_IBA_RESULT__4);
+		dbg_msg("result %08x %u\n", status, status >> SW_IBA_SIZE_S);
+		info->use_iba = 1;
+		mutex_unlock(sw->reglock);
+	} while (0);
+#endif
+
+	return 0;
+}
+#endif
+
+static struct ksz_iba_info *iba_info;
+
+static void iba_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->hwlock);
+	++sw->info->iba.cnt;
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed\n", __func__);
+dbg_msg("  %s changed\n", __func__);
+		mutex_unlock(sw->hwlock);
+		sw->reg->lock(sw);
+	}
+}  /* iba_lock */
+
+static void iba_unlock(struct ksz_sw *sw)
+{
+	if (sw->info->iba.cnt)
+		--sw->info->iba.cnt;
+	else
+printk("wrong release\n");
+	mutex_unlock(sw->hwlock);
+}  /* iba_unlock */
+
+#ifdef CONFIG_1588_PTP
+/**
+ * This helper function is used to prepare the read registers for use with
+ * the iba_r_post function.
+ */
+static u32 *iba_prepare_data(u32 reg, u32 *data)
+{
+	data[0] = reg;
+	if (-1 == reg)
+		return data + 1;
+	data[1] = 0xdeadfeed;
+	return data + 2;
+}  /* iba_prepare_data */
+
+/**
+ * This helper routine is used to check the allocated buffers for use with
+ * the iba_reqs function.
+ */
+static void assert_buf(const char *name, int i, size_t func_size, u32 *buf,
+	u32 *data, size_t buf_size)
+{
+	int assert = false;
+
+	if ((i + 1) > func_size / sizeof(void *)) {
+		printk(KERN_INFO "  [%s func] %u %u\n",
+			name, i, func_size / sizeof(void *));
+		assert = true;
+	}
+	if (data > buf + buf_size / sizeof(u32)) {
+		printk(KERN_INFO "  [%s data] %u\n",
+			name, (data - buf));
+		assert = true;
+	}
+	if (assert)
+		BUG();
+}  /* assert_buf */
+#endif
+
+/**
+ * iba_r_pre - IBA register read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for register read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *iba_r_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, data[0], data[1]);
+	return info->fptr;
+}  /* iba_r_pre */
+
+/**
+ * iba_r_post - IBA register read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA register read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_r_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+if (info->seqid != info->respid)
+dbg_msg(" id %x %x\n", info->seqid, info->respid);
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+			int j = 0;
+
+			while (data[j] != -1) {
+				if (reg == data[j] &&
+				    (data[j + 1] & 0xffff0000) == 0xdead0000) {
+					data[j + 1] = iba_get_val(size,
+						info->regs[i].data[0]);
+					break;
+				}
+				j += 2;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* iba_r_post */
+
+/**
+ * iba_w_pre - IBA register write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for register write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *iba_w_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = data[2];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, data[0], data[1]);
+	return info->fptr;
+}  /* iba_w_pre */
+
+/**
+ * sw_setup_iba - Switch IBA setup
+ * @sw:		The switch instance.
+ *
+ * This routines setups IBA function of the switch.
+ */
+static void sw_setup_iba(struct ksz_sw *sw)
+{
+	u32 data;
+
+	data = sw_r32(sw, REG_SW_IBA__4);
+	sw->info->iba.tag_type = (data & SW_IBA_FRAME_TPID_M);
+	data &= ~(SW_IBA_PORT_M << SW_IBA_PORT_S);
+	data |= sw->HOST_PORT << SW_IBA_PORT_S;
+	data |= SW_IBA_ENABLE;
+	data |= SW_IBA_INIT;
+#if 0
+	data |= SW_IBA_DA_MATCH;
+#endif
+	sw_w32(sw, REG_SW_IBA__4, data);
+	data &= ~SW_IBA_INIT;
+	sw->info->iba.cfg = data;
+	dbg_msg("status %08x\n", sw_r32(sw, REG_SW_IBA_STATUS__4));
+	dbg_msg("states %08x\n", sw_r32(sw, REG_SW_IBA_STATES__4));
+	dbg_msg("result %08x\n", sw_r32(sw, REG_SW_IBA_RESULT__4));
+}  /* sw_setup_iba */
+
+/**
+ * iba_to_spi - Switch IBA to SPI
+ * @sw:		The switch instance.
+ * @info:	The IBA instance.
+ *
+ * This routine switches from using IBA to using SPI.
+ */
+static void iba_to_spi(struct ksz_sw *sw, struct ksz_iba_info *info)
+{
+	if (2 <= info->use_iba) {
+if (info->use_iba != 5)
+dbg_msg(" iba stops: %x\n", info->use_iba);
+		info->use_iba = 5;
+		return;
+	}
+
+	/* Not calling from interrupt handling. */
+	if (sw->intr_using != 2)
+		mutex_lock(sw->reglock);
+	sw_set_spi(sw, info);
+	if (sw->intr_using != 2)
+		mutex_unlock(sw->hwlock);
+	printk(KERN_ALERT "revert to SPI\n");
+
+#if 1
+	sw_setup_iba(sw);
+	schedule_delayed_work(&sw->set_ops, 100);
+#endif
+}  /* iba_to_spi */
+
+static void iba_dbg_states(struct ksz_iba_info *info)
+{
+	int i;
+	u32 status;
+	struct ksz_sw *sw = info->sw_dev;
+	int iba_test_override = (sw->overrides & IBA_TEST);
+	int use_iba = info->use_iba;
+
+	if (2 <= info->use_iba)
+		return;
+#if 0
+iba_test_override = 1;
+#endif
+	if (!iba_test_override)
+		return;
+
+dbg_msg(" w seq: %x\n", info->seqid);
+if (sw->intr_using < 2 && mutex_is_locked(sw->reglock))
+printk(" reg locked: %d\n", sw->intr_using);
+	if (sw->intr_using < 2)
+		mutex_lock(sw->reglock);
+	info->use_iba = 0;
+	status = sw_r32(sw, REG_SW_IBA_STATUS__4);
+	dbg_msg("status %08x q:%d p:%d d:%d f:%d o:%d m:%d\n", status,
+		!!(status & SW_IBA_REQ),
+		!!(status & SW_IBA_RESP),
+		!!(status & SW_IBA_DA_MISMATCH),
+		!!(status & SW_IBA_FMT_MISMATCH),
+		!!(status & SW_IBA_CODE_ERROR),
+		!!(status & SW_IBA_CMD_ERROR));
+	status = sw_r32(sw, REG_SW_IBA_STATES__4);
+	dbg_msg("states %08x %u\n", status, ((status >> SW_IBA_PACKET_SIZE_S) &
+		SW_IBA_PACKET_SIZE_M) * 4);
+	status = sw_r32(sw, REG_SW_IBA_RESULT__4);
+	dbg_msg("result %08x %u\n", status, status >> SW_IBA_SIZE_S);
+	info->use_iba = use_iba;
+	if (sw->intr_using < 2)
+		mutex_unlock(sw->reglock);
+
+	for (i = 0; i < ntohs(info->frame->length); i++) {
+		dbg_msg("%02x ", info->packet[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (i % 16)
+		dbg_msg("\n");
+}  /* iba_dbg_states */
+
+/**
+ * iba_reqs - IBA register request
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @func:	The pre-processing routines.
+ * @post:	The post-processing function.
+ *
+ * This function sends a request with many pre-processing routines to IBA and
+ * waits for a response.
+ *
+ */
+static int iba_reqs(struct ksz_iba_info *info, void **in, void *out, void *obj,
+	void **func,
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int rc;
+	unsigned long wait;
+	u16 code = IBA_CODE_NORMAL;
+	void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj);
+
+	if (5 == info->use_iba) {
+dbg_msg(" %s\n", __func__);
+		return 0;
+	}
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		if (!mutex_is_locked(sw->hwlock))
+			pr_alert("IBA not locked\n");
+	} while (0);
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	info->fptr = iba_pre_cmd(info, code);
+
+	do {
+		prepare = *func;
+		info->fptr = prepare(info, *in, obj);
+		++func;
+		++in;
+	} while (*func);
+
+	info->fptr = iba_post_cmd(info);
+	info->regs[0].cmd = (u32) -1;
+	init_completion(&info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+printk("  !! %x\n", rc);
+		iba_dbg_states(info);
+
+#ifndef CONFIG_LAN78XX_KSZ9897_IBA
+		/* Not testing if IBA is okay. */
+		if (!(info->use_iba & 0x80))
+			iba_to_spi(info->sw_dev, info);
+#endif
+		return 0;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	if (!wait) {
+if (dbg_iba)
+dbg_msg("  w timeout\n");
+		iba_dbg_states(info);
+
+#ifndef CONFIG_LAN78XX_KSZ9897_IBA
+		/* Not testing if IBA is okay. */
+		if (!(info->use_iba & 0x80))
+			iba_to_spi(info->sw_dev, info);
+#endif
+		return 0;
+	}
+
+	rc = 1;
+	if (post)
+		rc = post(info, out, obj);
+	return rc * 4;
+}  /* iba_reqs */
+
+/**
+ * iba_req - IBA basic register request
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @prepare:	The pre-processing routine.
+ * @post:	The post-processing function.
+ *
+ * This function sends a request to IBA and waits for a response.
+ *
+ * Return number of bytes read.
+ */
+static int iba_req(struct ksz_iba_info *info, void *in, void *out, void *obj,
+	void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj),
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int rc;
+	void *func[2];
+	void *data_in[1];
+	int i = 0;
+
+#ifdef CONFIG_LAN78XX_KSZ9897_IBA
+	if (usb_disconnected)
+		return 0;
+#endif
+
+	data_in[i] = in;
+	func[i++] = prepare;
+
+	func[i] = NULL;
+	rc = iba_reqs(info, data_in, out, obj, func, post);
+	return rc;
+}  /* iba_req */
+
+/**
+ * iba_r - IBA basic register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ * @size:	The data size.
+ *
+ * This function reads a register through IBA.
+ */
+static u32 iba_r(struct ksz_iba_info *info, unsigned reg, u32 size)
+{
+	u32 data[4];
+	int rc;
+	static int iba_r_enter;
+
+	if (5 == info->use_iba) {
+		return 0;
+	}
+	if (4 == info->use_iba)
+		printk(KERN_WARNING " %s %x\n", __func__, reg);
+#if 1
+if (info->respid != info->seqid || iba_r_enter) {
+dbg_msg(" iba_r %x %x %d; %x %x; %d\n", info->respid, info->seqid, info->cnt, reg,
+last_ok_reg, iba_r_enter);
+}
+#endif
+	++iba_r_enter;
+	data[0] = size;
+	data[1] = reg;
+	data[2] = 0xdeadbeaf;
+	data[3] = -1;
+	rc = iba_req(info, data, data + 1, NULL, iba_r_pre, iba_r_post);
+	if (!rc)
+dbg_msg("r %x %x %08x\n", reg, size, data[2]);
+else if (dbg_iba)
+dbg_msg(" ?? %d %08x\n", rc, data[2]);
+	last_ok_reg = reg;
+	--iba_r_enter;
+	return data[2];
+}  /* iba_r */
+
+/**
+ * iba_r8 - IBA 8-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 8-bit register through IBA.
+ */
+static u8 iba_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return (u8) iba_r(&sw->info->iba, reg, IBA_CMD_8);
+}  /* iba_r8 */
+
+/**
+ * iba_r16 - IBA 16-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 16-bit register through IBA.
+ */
+static u16 iba_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return (u16) iba_r(&sw->info->iba, reg, IBA_CMD_16);
+}  /* iba_r16 */
+
+/**
+ * iba_r24 - IBA 24-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 24-bit register through IBA.
+ */
+static u32 iba_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return iba_r(&sw->info->iba, reg, IBA_CMD_24);
+}  /* iba_r24 */
+
+/**
+ * iba_r32 - IBA 32-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 32-bit register through IBA.
+ */
+static u32 iba_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return iba_r(&sw->info->iba, reg, IBA_CMD_32);
+}  /* iba_r32 */
+
+/**
+ * iba_w - IBA basic register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ * @size:	The data size.
+ *
+ * This function writes a register through IBA.
+ */
+static void iba_w(struct ksz_iba_info *info, unsigned reg, unsigned val,
+	u32 size)
+{
+	u32 data[3];
+	int rc;
+
+	if (5 == info->use_iba) {
+		return;
+	}
+	if (4 == info->use_iba)
+		printk(KERN_WARNING " %s %x\n", __func__, reg);
+	data[0] = size;
+	data[1] = reg;
+	data[2] = val;
+	rc = iba_req(info, data, NULL, NULL, iba_w_pre, NULL);
+	if (!rc)
+dbg_msg("w %x %x\n", reg, size);
+}  /* iba_w */
+
+/**
+ * iba_w8 - IBA 8-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 8-bit register through IBA.
+ */
+static void iba_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_8);
+}  /* iba_w8 */
+
+/**
+ * iba_w16 - IBA 16-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 16-bit register through IBA.
+ */
+static void iba_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_16);
+}  /* iba_w16 */
+
+/**
+ * iba_w24 - IBA 24-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 24-bit register through IBA.
+ */
+static void iba_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_24);
+}  /* iba_w24 */
+
+/**
+ * iba_w32 - IBA 32-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 32-bit register through IBA.
+ */
+static void iba_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_32);
+}  /* iba_w32 */
+
+/**
+ * iba_get_pre - IBA burst read pre-processing
+ * @info:	The IBA instance.
+ * @cnt:	The buffer count.
+ * @buf:	The buffer.
+ *
+ * This routine prepares IBA for burst read operation.
+ */
+static void iba_get_pre(u32 *data, int cnt, char *buf)
+{}
+
+/**
+ * iba_get_post - IBA burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @b:		Endian indication.
+ *
+ * This helper function retrieves the result of IBA burst read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post(struct ksz_iba_info *info, void *out, void *obj, int b)
+{
+	u32 *ptr = (u32 *) out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		*ptr = iba_get_val((info->regs[i].cmd & IBA_CMD_32),
+			info->regs[i].data[0]);
+		if (b)
+			*ptr = cpu_to_be32(*ptr);
+		ptr++;
+		i++;
+	}
+	return i;
+}  /* iba_get_post */
+
+/**
+ * iba_get_post_be - IBA big-endian burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA big-endian burst read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post_be(struct ksz_iba_info *info, void *out, void *obj)
+{
+	return iba_get_post(info, out, obj, 1);
+}  /* iba_get_post_be */
+
+/**
+ * iba_get_post_le - IBA little-endian burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA little-endian burst read
+ * operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post_le(struct ksz_iba_info *info, void *out, void *obj)
+{
+	return iba_get_post(info, out, obj, 0);
+}  /* iba_get_post_le */
+
+/**
+ * iba_set_pre - IBA burst write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This routine prepares IBA for burst write operation.
+ */
+static void iba_set_pre(u32 *data, int cnt, char *buf)
+{
+	u32 *ptr = (u32 *) buf;
+	int i;
+
+	i = 0;
+	if (cnt > 1)
+		i = 1;
+	while (cnt > 0) {
+		data[i++] = *ptr++;
+		cnt--;
+	}
+}  /* iba_set_pre */
+
+/**
+ * iba_set_post - IBA burst write post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA burst write operation.
+ *
+ * Return number of registers written.
+ */
+static int iba_set_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		dbg_msg("%08x=%08x\n", info->regs[i].cmd,
+			info->regs[i].data[0]);
+		i++;
+	}
+	return i;
+}  /* iba_set_post */
+
+/**
+ * iba_burst - IBA burst request
+ * @info:	The IBA instance.
+ * @addr:	The starting address.
+ * @cnt:	The number of addresses.
+ * @buf:	Buffer holding the data.
+ * @write:	Write indication.
+ * @prepare:	The pre-processing routine.
+ * @post:	The post-processing function.
+ *
+ * This function sends a burst request to IBA and waits for a response.
+ *
+ * Return number of bytes read.
+ */
+static int iba_burst(struct ksz_iba_info *info, u32 addr, size_t cnt,
+	char *buf, int write, void (*prepare)(u32 *data, int cnt, char *buf),
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int mult;
+	int rc;
+	unsigned long wait;
+	u32 val;
+	u16 code = IBA_CODE_NORMAL;
+	u32 cmd = IBA_CMD_READ;
+	u32 size = IBA_CMD_32;
+	void *data = buf;
+
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	if (cnt > 4) {
+		mult = cnt / 4;
+		info->data[0] = mult;
+		code = IBA_CODE_BURST;
+		cmd = IBA_BURST_READ;
+		if contain_reg(addr, cnt, REG_SW_IBA__4) {
+			u32 *ptr = (u32 *) buf;
+			u32 loc = (REG_SW_IBA__4 - addr) / 4;
+
+			if (write)
+				ptr[loc] = info->cfg;
+		}
+	} else {
+		mult = 1;
+		if (1 == cnt) {
+#ifdef VERIFY_IBA
+if (!dbg_iba_test) {
+iba_test(info, 7);
+dbg_iba_test = 1;
+}
+#endif
+			if (write) {
+				u8 *ptr = data;
+
+				val = *ptr;
+				data = &val;
+			}
+			size = IBA_CMD_8;
+		} else if (2 == cnt) {
+			if (write) {
+				u16 *ptr = data;
+
+				val = *ptr;
+				data = &val;
+			}
+			size = IBA_CMD_16;
+		} else if (addr & 1)
+			size = IBA_CMD_8;
+		else if (addr & 2)
+			size = IBA_CMD_16;
+	}
+	cmd += write;
+	info->fptr = iba_pre_cmd(info, code);
+
+	prepare(info->data, mult, data);
+	info->fptr = iba_cmd_data(info, cmd, size, addr);
+
+	info->fptr = iba_post_cmd(info);
+	info->regs[0].cmd = (u32) -1;
+	init_completion(&info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		iba_dbg_states(info);
+		iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	if (!wait) {
+dbg_msg("burst to\n");
+		iba_dbg_states(info);
+		iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+
+	rc = post(info, data, NULL);
+	rc *= 4;
+	return rc;
+}  /* iba_burst */
+
+static void iba_r_buf(struct ksz_sw *sw, unsigned reg, void *buf, size_t count)
+{
+	u8 *orig_buf = buf;
+	size_t orig_cnt = count;
+	int start = 0;
+
+	/* Not in multiple of 4. */
+	if ((count & 3) || (reg & 3)) {
+		orig_buf = buf;
+		orig_cnt = count;
+		start = reg & 3;
+		reg &= ~3;
+		buf = sw->info->iba.buf;
+		count += start;
+		count += 3;
+		count &= ~3;
+	}
+	iba_burst(&sw->info->iba, reg, count, buf, 0,
+		iba_get_pre, iba_get_post_be);
+	if (orig_buf != buf)
+		memcpy(orig_buf, &sw->info->iba.buf[start], orig_cnt);
+}  /* iba_r_buf */
+
+static u32 buf_to_val(u8 *buf, int i, int cnt)
+{
+	int j;
+	u32 val = buf[i];
+
+	for (j = 1; j < cnt; j++) {
+		val <<= 8;
+		val |= buf[i + j];
+	}
+	return val;
+}  /* buf_to_val */
+
+/**
+ * w_buf_pre - IBA buffer write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for buffer write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_buf_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 *buf = obj;
+	u16 reg = data[0];
+	size_t cnt = data[1];
+	int i;
+	u32 size;
+	u32 val;
+
+	/* Register may not be in 4-byte boundary. */
+	switch (reg & 3) {
+	case 1:
+		size = IBA_CMD_24;
+		i = 3;
+		break;
+	case 2:
+		size = IBA_CMD_16;
+		i = 2;
+		break;
+	case 3:
+		size = IBA_CMD_8;
+		i = 1;
+		break;
+	default:
+		size = IBA_CMD_32;
+		i = 4;
+		break;
+	}
+
+	/* Count may be too small. */
+	if (i > cnt) {
+		i = cnt;
+		switch (i) {
+		case 1:
+			size = IBA_CMD_8;
+			break;
+		case 2:
+			size = IBA_CMD_16;
+			break;
+		default:
+			size = IBA_CMD_24;
+			break;
+		}
+	}
+
+	/* Prepare the initial value. */
+	val = buf_to_val(buf, 0, i);
+
+	cnt -= i;
+	info->data[0] = val;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, size, reg);
+	reg &= ~3;
+	size = IBA_CMD_32;
+	while (cnt >= 4) {
+		val = buf_to_val(buf, i, 4);
+		reg += 4;
+		info->data[0] = val;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, size, reg);
+		i += 4;
+		cnt -= 4;
+	}
+	if (cnt) {
+		switch (cnt) {
+		case 1:
+			size = IBA_CMD_8;
+			break;
+		case 2:
+			size = IBA_CMD_16;
+			break;
+		default:
+			size = IBA_CMD_24;
+			break;
+		}
+		val = buf_to_val(buf, i, cnt);
+		reg += 4;
+		info->data[0] = val;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, size, reg);
+	}
+	return info->fptr;
+}  /* w_buf_pre */
+
+static void iba_w_buf(struct ksz_sw *sw, unsigned reg, void *buf, size_t count)
+{
+	/* Not in multiple of 4. */
+	if ((count & 3) || (reg & 3)) {
+		u32 data[3];
+
+		data[0] = reg;
+		data[1] = count;
+		iba_req(&sw->info->iba, data, NULL, buf, w_buf_pre,
+			iba_set_post);
+	} else {
+		int i;
+		u32 *src = buf;
+		u32 *dst = (u32 *) sw->info->iba.buf;
+
+		for (i = 0; i < count; i += 4)
+			*dst++ = be32_to_cpu(*src++);
+		buf = sw->info->iba.buf;
+		iba_burst(&sw->info->iba, reg, count, buf, 1,
+			iba_set_pre, iba_set_post);
+	}
+}  /* iba_w_buf */
+
+static int iba_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	int rc;
+
+	rc = iba_burst(&sw->info->iba, reg, count, buf, 0,
+		iba_get_pre, iba_get_post_le);
+
+	/*
+	 * Return zero to let the calling program know the boundary must be
+	 * 32-bit.
+	 */
+	if (4 == count && (reg & 3))
+		rc = 0;
+	return rc;
+}  /* iba_get */
+
+static int iba_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	return iba_burst(&sw->info->iba, reg, count, buf, 1,
+		iba_set_pre, iba_set_post);
+}  /* iba_set */
+
+/**
+ * r_mac_table_pre - IBA MAC table read pre-processing
+ * @info:	The IBA instance.
+ *
+ * This routine prepares IBA for MAC table read operation.
+ */
+static void r_mac_table_pre(struct ksz_iba_info *info)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_A);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_B);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_C);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_VAL_D);
+}  /* r_mac_table_pre */
+
+/**
+ * r_mac_table_post - IBA MAC table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA MAC table read operation.
+ *
+ * Return number of registers read.
+ */
+static int r_mac_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u16 *entry = out;
+	struct ksz_mac_table *mac = obj;
+	int i = 0;
+	u32 data[5];
+
+	memset(data, 0, sizeof(data));
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_SW_ALU_VAL_A:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_B:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_C:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_D:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				get_mac_table_info(mac, data);
+				++mac;
+				memset(data, 0, sizeof(data));
+				break;
+			case REG_SW_LUE_INDEX_0__2:
+				data[4] = iba_get_val(size,
+					info->regs[i].data[0]);
+				if (entry) {
+					*entry = (data[4] & ENTRY_INDEX_M) +
+						1;
+					++entry;
+				}
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_mac_table_post */
+
+/**
+ * w_mac_table_pre - IBA MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @mac:	The MAC table entries.
+ *
+ * This routine prepares IBA for MAC table write operation.
+ */
+static void w_mac_table_pre(struct ksz_iba_info *info,
+	struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	set_mac_table_info(mac, data);
+	info->data[0] = data[0];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_A);
+	info->data[0] = data[1];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_B);
+	info->data[0] = data[2];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_C);
+	info->data[0] = data[3];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_D);
+}  /* w_mac_table_pre */
+
+/**
+ * s_dyn_mac_pre - IBA dynamic MAC table set pre-processing
+ * @info:	The IBA instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ *
+ * This helper routine prepares IBA for dynamic MAC table set operation.
+ */
+static u32 s_dyn_mac_pre(struct ksz_iba_info *info, u16 addr, u8 *src_addr,
+	u16 src_fid)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & ALU_DIRECT_INDEX_M;
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_1);
+		ctrl |= ALU_DIRECT;
+	} else {
+		data = (u32) src_fid << ALU_FID_INDEX_S;
+		data |= ((u32) src_addr[0] << 8) | src_addr[1];
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_0);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_1);
+	}
+	ctrl |= ALU_START;
+	return ctrl;
+}  /* s_dyn_mac_pre */
+
+/**
+ * r_dyn_mac_pre - IBA dynamic MAC table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[0];
+	u8 *src_addr = (u8 *) data[1];
+	u16 src_fid = data[2];
+	u32 ctrl;
+
+	ctrl = s_dyn_mac_pre(info, addr, src_addr, src_fid);
+	ctrl |= ALU_READ;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_B);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	info->data[0] = ALU_START;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	r_mac_table_pre(info);
+
+	/* Hash read. */
+	if (!addr) {
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+			REG_SW_LUE_INDEX_0__2);
+	}
+	return info->fptr;
+}  /* r_dyn_mac_pre */
+
+/**
+ * iba_r_dyn_mac_hw - read from dynamic MAC table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This function reads an entry of the dynamic MAC table using IBA.
+ */
+static int iba_r_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	u32 data[3];
+
+	data[0] = addr;
+	data[1] = (u32) src_addr;
+	data[2] = src_fid;
+	return iba_req(&sw->info->iba, data, entry, mac, r_dyn_mac_pre,
+		r_mac_table_post);
+}  /* iba_r_dyn_mac_hw */
+
+/**
+ * w_dyn_mac_pre - IBA dynamic MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ksz_mac_table *mac = obj;
+	u16 addr = data[0];
+	u8 *src_addr = (u8 *) data[1];
+	u16 src_fid = data[2];
+	u32 ctrl;
+
+	ctrl = s_dyn_mac_pre(info, addr, src_addr, src_fid);
+	ctrl |= ALU_WRITE;
+	w_mac_table_pre(info, mac);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	return info->fptr;
+}  /* w_dyn_mac_pre */
+
+/**
+ * iba_w_dyn_mac_hw - write to dynamic MAC table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This function writes an entry of the dynamic MAC table using IBA.
+ */
+static int iba_w_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	u32 data[3];
+
+	data[0] = addr;
+	data[1] = (u32) src_addr;
+	data[2] = src_fid;
+	return iba_req(&sw->info->iba, data, NULL, mac, w_dyn_mac_pre, NULL);
+}  /* iba_w_dyn_mac_hw */
+
+/**
+ * start_dyn_mac_pre - IBA dynamic MAC table start pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table start operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *start_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = ALU_SEARCH;
+	ctrl |= ALU_START;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_VAL_B);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	return info->fptr;
+}  /* start_dyn_mac_pre */
+
+/**
+ * iba_start_dyn_mac_hw - start dynamic MAC table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function starts dynamic MAC table search using IBA.
+ */
+static int iba_start_dyn_mac_hw(struct ksz_sw *sw)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, NULL, start_dyn_mac_pre,
+		NULL);
+}  /* iba_start_dyn_mac_hw */
+
+/**
+ * g_dyn_mac_pre - IBA dynamic MAC table retrieve pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table retrieve operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *g_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	r_mac_table_pre(info);
+	return info->fptr;
+}  /* g_dyn_mac_pre */
+
+/**
+ * iba_g_dyn_mac_hw - retrieve dynamic MAC table result using IBA
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves dynamic MAC table result using IBA.
+ */
+static int iba_g_dyn_mac_hw(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, mac, g_dyn_mac_pre,
+		r_mac_table_post);
+}  /* iba_g_dyn_mac_hw */
+
+/**
+ * stop_dyn_mac_pre - IBA dynamic MAC table stop pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table stop operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *stop_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_SW_ALU_CTRL__4);
+	return info->fptr;
+}  /* stop_dyn_mac_pre */
+
+/**
+ * stop_dyn_mac_post - IBA dynamic MAC table stop post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA dynamic MAC table stop operation.
+ *
+ * Return number of registers read.
+ */
+static int stop_dyn_mac_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			if (reg == REG_SW_ALU_CTRL__4)
+				*data = iba_get_val(size,
+					info->regs[i].data[0]);
+		}
+		i++;
+	}
+	return i;
+}  /* stop_dyn_mac_post */
+
+/**
+ * iba_stop_dyn_mac_hw - stop dynamic MAC table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function stops dynamic MAC table search using IBA.
+ *
+ * Return the last MAC table control.
+ */
+static u32 iba_stop_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	iba_req(&sw->info->iba, NULL, &ctrl, NULL, stop_dyn_mac_pre,
+		stop_dyn_mac_post);
+	return ctrl;
+}  /* iba_stop_dyn_mac_hw */
+
+/**
+ * r_sta_mac_pre - IBA static MAC table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for static MAC table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_sta_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int cnt = 0;
+	int num = data[0];
+	u32 *ctrl = &data[1];
+
+	do {
+		info->data[0] = *ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		info->data[0] = ALU_STAT_START;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		r_mac_table_pre(info);
+		++cnt;
+		++ctrl;
+	} while (cnt < num);
+	return info->fptr;
+}  /* r_sta_mac_pre */
+
+/**
+ * iba_r_sta_mac_hw - read from static MAC table using IBA
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for read operation.
+ * @num:	Number of entries to read.
+ * @mac:	Buffer to hold the MAC table entries.
+ *
+ * This function reads from static MAC table using IBA.
+ */
+static int iba_r_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 data[MAX_IBA_MAC_ENTRIES + 1];
+
+	if (num > MAX_IBA_MAC_ENTRIES)
+		num = MAX_IBA_MAC_ENTRIES;
+	data[0] = num;
+	memcpy(&data[1], ctrl, sizeof(u32) * num);
+	return iba_req(&sw->info->iba, data, NULL, mac, r_sta_mac_pre,
+		r_mac_table_post);
+}  /* iba_r_sta_mac_hw */
+
+/**
+ * w_sta_mac_pre - IBA static MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for static MAC table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_sta_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int cnt;
+	int num = data[0];
+	struct ksz_mac_table *mac = obj;
+
+	for (cnt = 0; cnt < num; cnt++, data++) {
+		w_mac_table_pre(info, mac);
+		info->data[0] = data[1];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		info->data[0] = ALU_STAT_START;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4);
+		++mac;
+	}
+	return info->fptr;
+}  /* w_sta_mac_pre */
+
+/**
+ * iba_w_sta_mac_hw - write to static MAC table using IBA
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for write operation.
+ * @num:	Number of entries to write.
+ * @mac:	MAC table entries.
+ *
+ * This function writes to static MAC table using IBA.
+ */
+static int iba_w_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 data[MAX_IBA_MAC_ENTRIES + 1];
+
+	if (num > MAX_IBA_MAC_ENTRIES)
+		num = MAX_IBA_MAC_ENTRIES;
+	data[0] = num;
+	memcpy(&data[1], ctrl, sizeof(u32) * num);
+	return iba_req(&sw->info->iba, data, NULL, mac, w_sta_mac_pre, NULL);
+}  /* iba_w_sta_mac_hw */
+
+/**
+ * r_vlan_table_pre - IBA VLAN table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for VLAN table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_vlan_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[3];
+	u32 ctrl;
+	int cnt = 0;
+	int num = *((int *) obj);
+
+	if (num > MAX_IBA_VLAN_ENTRIES)
+		num = MAX_IBA_VLAN_ENTRIES;
+	do {
+		info->data[0] = addr;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_SW_VLAN_ENTRY_INDEX__2);
+		ctrl = VLAN_READ;
+		ctrl |= VLAN_START;
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+			REG_SW_VLAN_CTRL);
+		info->data[0] = VLAN_START << 24;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_VLAN_CTRL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_UNTAG__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_PORTS__4);
+		++cnt;
+		++addr;
+	} while (cnt < num);
+	return info->fptr;
+}  /* r_vlan_table_pre */
+
+/**
+ * r_vlan_table_post - IBA VLAN table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA VLAN table read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_vlan_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+	u32 *data = out;
+	int num = *((int *) obj);
+
+	memset(data, 0, sizeof(u32) * num * READ_VLAN_ENTRY_SIZE);
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_SW_VLAN_ENTRY__4:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_VLAN_ENTRY_UNTAG__4:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_VLAN_ENTRY_PORTS__4:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data += READ_VLAN_ENTRY_SIZE;
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_vlan_table_post */
+
+/**
+ * iba_r_vlan_hw - read from VLAN table using IBA
+ * @sw:		The switch instance.
+ * @data:	Buffer to hold the VLAN data.
+ * @num:	Number of entries to read.
+ *
+ * This function reads from VLAN table using IBA.
+ */
+static int iba_r_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	return iba_req(&sw->info->iba, data, data, &num, r_vlan_table_pre,
+		r_vlan_table_post);
+}  /* iba_r_vlan_hw */
+
+/**
+ * w_vlan_table_pre - IBA VLAN table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for VLAN table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_vlan_table_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[3];
+	u32 ctrl;
+	int cnt;
+	int num = *((int *) obj);
+
+	if (num > MAX_IBA_VLAN_ENTRIES)
+		num = MAX_IBA_VLAN_ENTRIES;
+	for (cnt = 0; cnt < num; cnt++) {
+		info->data[0] = data[0];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY__4);
+		info->data[0] = data[1];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_UNTAG__4);
+		info->data[0] = data[2];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_PORTS__4);
+		addr = data[3];
+		info->data[0] = addr;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_SW_VLAN_ENTRY_INDEX__2);
+		ctrl = VLAN_WRITE;
+		ctrl |= VLAN_START;
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+			REG_SW_VLAN_CTRL);
+		info->data[0] = VLAN_START << 24;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_VLAN_CTRL);
+		data += WRITE_VLAN_ENTRY_SIZE;
+	}
+	return info->fptr;
+}  /* w_vlan_table_pre */
+
+/**
+ * iba_w_vlan_hw - write to VLAN table using IBA
+ * @sw:		The switch instance.
+ * @data:	VLAN data to write.
+ * @num:	Number of entries to write.
+ *
+ * This function writes to VLAN table using IBA.
+ */
+static int iba_w_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	return iba_req(&sw->info->iba, data, NULL, &num, w_vlan_table_pre,
+		NULL);
+}  /* iba_w_vlan_hw */
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * r_hsr_table_pre - IBA HSR table read pre-processing
+ * @info:	The IBA instance.
+ *
+ * This routine prepares IBA for HSR table read operation.
+ */
+static void r_hsr_table_pre(struct ksz_iba_info *info)
+{
+	int i;
+
+	info->data[0] = 0;
+	for (i = 0; i < 7; i++) {
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_HSR_ALU_VAL_A + i * 4);
+	}
+}  /* r_hsr_table_pre */
+
+/**
+ * r_hsr_table_post - IBA HSR table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA HSR table read operation.
+ *
+ * Return number of registers read.
+ */
+static int r_hsr_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	struct ksz_hsr_table *hsr = obj;
+	int i = 0;
+	u32 data[7];
+
+	memset(data, 0, sizeof(data));
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_HSR_ALU_VAL_A:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_B:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_C:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_D:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_E:
+				data[4] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_F:
+				data[5] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_G:
+				data[6] = iba_get_val(size,
+					info->regs[i].data[0]);
+				get_hsr_table_info(hsr, data);
+				++hsr;
+				memset(data, 0, sizeof(data));
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_hsr_table_post */
+
+/**
+ * w_hsr_table_pre - IBA HSR table write pre-processing
+ * @info:	The IBA instance.
+ * @mac:	The HSR table entries.
+ *
+ * This routine prepares IBA for HSR table write operation.
+ */
+static void w_hsr_table_pre(struct ksz_iba_info *info,
+	struct ksz_hsr_table *hsr)
+{
+	u32 data[7];
+	int i;
+
+	set_hsr_table_info(hsr, data);
+	for (i = 0; i < 7; i++) {
+		info->data[0] = data[i];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_VAL_A + i * 4);
+	}
+}  /* w_hsr_table_pre */
+
+/**
+ * s_hsr_pre - IBA HSR table set pre-processing
+ * @info:	The IBA instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @path_id:	The path ID.
+ *
+ * This helper routine prepares IBA for HSR table set operation.
+ */
+static u32 s_hsr_pre(struct ksz_iba_info *info, u16 addr, u8 *src_addr,
+	u8 path_id)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & HSR_DIRECT_INDEX_M;
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_2);
+		ctrl |= HSR_DIRECT;
+	} else {
+		data = 0;
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_0);
+		data = ((u32) src_addr[0] << 8) | src_addr[1];
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_1);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_2);
+		data = path_id & HSR_PATH_INDEX_M;
+		info->data[0] = data;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_3);
+	}
+	ctrl |= HSR_START;
+	return ctrl;
+}  /* s_hsr_pre */
+
+/**
+ * r_hsr_pre - IBA HSR table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[0];
+	struct ksz_hsr_table *hsr = obj;
+	u32 ctrl;
+
+	ctrl = s_hsr_pre(info, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_READ;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_HSR_ALU_CTRL__4);
+	info->data[0] = HSR_START;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+		REG_HSR_ALU_CTRL__4);
+	r_hsr_table_pre(info);
+	return info->fptr;
+}  /* r_hsr_pre */
+
+/**
+ * iba_r_hsr_hw - read from HSR table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function reads an entry of the HSR table using IBA.
+ */
+static int iba_r_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 data[1];
+
+	data[0] = addr;
+	return iba_req(&sw->info->iba, data, NULL, hsr, r_hsr_pre,
+		r_hsr_table_post);
+}  /* iba_r_hsr_hw */
+
+/**
+ * w_hsr_pre - IBA HSR table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ksz_hsr_table *hsr = obj;
+	u16 addr = data[0];
+	u32 ctrl;
+
+	ctrl = s_hsr_pre(info, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_WRITE;
+	w_hsr_table_pre(info, hsr);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_HSR_ALU_CTRL__4);
+	return info->fptr;
+}  /* w_hsr_pre */
+
+/**
+ * iba_w_hsr_hw - write to HSR table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	The HSR table entry.
+ *
+ * This function writes an entry of the HSR table using IBA.
+ */
+static int iba_w_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 data[1];
+
+	data[0] = addr;
+	return iba_req(&sw->info->iba, data, NULL, hsr, w_hsr_pre, NULL);
+}  /* iba_w_hsr_hw */
+
+/**
+ * start_hsr_pre - IBA HSR table start pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table start operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *start_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = HSR_SEARCH;
+	ctrl |= HSR_START;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_HSR_ALU_CTRL__4);
+	return info->fptr;
+}  /* start_hsr_pre */
+
+/**
+ * iba_start_hsr_hw - start HSR table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function starts HSR table search using IBA.
+ */
+static int iba_start_hsr_hw(struct ksz_sw *sw)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, NULL, start_hsr_pre, NULL);
+}  /* iba_start_hsr_hw */
+
+/**
+ * g_hsr_pre - IBA HSR table retrieve pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table retrieve operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *g_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	r_hsr_table_pre(info);
+	return info->fptr;
+}  /* g_hsr_pre */
+
+/**
+ * iba_g_hsr_hw - retrieve HSR table result using IBA
+ * @sw:		The switch instance.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function retrieves HSR table result using IBA.
+ */
+static int iba_g_hsr_hw(struct ksz_sw *sw, struct ksz_hsr_table *hsr)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, hsr, g_hsr_pre,
+		r_hsr_table_post);
+}  /* iba_g_hsr_hw */
+
+/**
+ * stop_hsr_pre - IBA HSR table stop pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table stop operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *stop_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_HSR_ALU_CTRL__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_HSR_ALU_CTRL__4);
+	return info->fptr;
+}  /* stop_hsr_pre */
+
+/**
+ * stop_hsr_post - IBA HSR table stop post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA HSR table stop operation.
+ *
+ * Return number of registers read.
+ */
+static int stop_hsr_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			if (reg == REG_HSR_ALU_CTRL__4)
+				*data = iba_get_val(size,
+					info->regs[i].data[0]);
+		}
+		i++;
+	}
+	return i;
+}  /* stop_hsr_post */
+
+/**
+ * iba_stop_hsr_hw - stop HSR table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function stops HSR table search using IBA.
+ *
+ * Return the last HSR table control.
+ */
+static u32 iba_stop_hsr_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	iba_req(&sw->info->iba, NULL, &ctrl, NULL, stop_hsr_pre,
+		stop_hsr_post);
+	return ctrl;
+}  /* iba_stop_hsr_hw */
+#endif
+
+/**
+ * r_mib_cnt_pre - IBA MIB counter read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for MIB counter read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_mib_cnt_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+	u32 *data = in;
+	uint *port = obj;
+	int cnt;
+	int num = data[0];
+	struct ksz_sw *sw = info->sw_dev;
+	u32 freeze = sw->info->port_cfg[*port].freeze ?
+		MIB_COUNTER_FLUSH_FREEZE : 0;
+
+	for (cnt = 0; cnt < num; cnt++, data++) {
+		ctrl = data[1] & MIB_COUNTER_INDEX_M;
+		ctrl <<= MIB_COUNTER_INDEX_S;
+		ctrl |= MIB_COUNTER_READ;
+		ctrl |= freeze;
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = MIB_COUNTER_VALID;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_MIB_DATA));
+	}
+	return info->fptr;
+}  /* r_mib_cnt_pre */
+
+/**
+ * r_mib_cnt_post - IBA MIB counter read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA MIB counter read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_mib_cnt_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 cmd;
+	int i = 0;
+	u32 *data = out;
+	uint *port = obj;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		cmd = (info->regs[i].cmd >> IBA_CMD_S);
+		if (IBA_CMD_READ == cmd || IBA_CMD_WAIT_ON_1 == cmd) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+#if 1
+if (((reg >> 12) & 0xf) != *port + 1)
+dbg_msg(" ?? %s %x %x\n", __func__, reg, *port);
+#endif
+			reg &= ((1 << 12) - 1);
+			switch (reg) {
+			case REG_PORT_MIB_CTRL_STAT__4:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PORT_MIB_DATA:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data += READ_MIB_ENTRY_SIZE;
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_mib_cnt_post */
+
+/**
+ * iba_r_mib_cnt_hw - read MIB counters using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The addresses of the counters.
+ * @num:	Number of entries to read.
+ * @data:	Buffer to store the counters.
+ *
+ * This function reads MIB counters of the port using IBA.
+ */
+static int iba_r_mib_cnt_hw(struct ksz_sw *sw, uint port, u32 addr[], int num,
+	u32 data[])
+{
+	u32 data_in[MAX_IBA_MIB_ENTRIES + 1];
+
+	if (num > MAX_IBA_MIB_ENTRIES)
+		num = MAX_IBA_MIB_ENTRIES;
+	data_in[0] = num;
+	memcpy(&data_in[1], addr, sizeof(u32) * num);
+	memset(data, 0, sizeof(u32) * num * READ_MIB_ENTRY_SIZE);
+	return iba_req(&sw->info->iba, data_in, data, &port, r_mib_cnt_pre,
+		r_mib_cnt_post);
+}  /* iba_r_mib_cnt_hw */
+
+#if defined(CONFIG_HAVE_KSZ9897)
+/**
+ * r_acl_table_pre - IBA ACL table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for ACL table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_acl_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	uint *port = obj;
+	u16 addr = data[4];
+	u32 ctrl = (addr & PORT_ACL_INDEX_M);
+	int i;
+
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0));
+	info->data[0] = PORT_ACL_READ_DONE << 8;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0 & ~3));
+	for (i = 0; i < 4; i++) {
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_ACL_0 + 4 * i));
+	}
+	return info->fptr;
+}  /* r_acl_table_pre */
+
+/**
+ * r_acl_table_post - IBA ACL table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA ACL table read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_acl_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+	u32 *data = out;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			reg &= ((1 << 12) - 1);
+			switch (reg) {
+			case REG_PORT_ACL_0:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[0] = cpu_to_be32(data[0]);
+				break;
+			case REG_PORT_ACL_4:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[1] = cpu_to_be32(data[1]);
+				break;
+			case REG_PORT_ACL_8:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[2] = cpu_to_be32(data[2]);
+				break;
+			case REG_PORT_ACL_C:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[3] = cpu_to_be32(data[3]);
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_acl_table_post */
+
+/**
+ * iba_r_acl_hw - read from ACL table using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This function reads from ACL table of the port using IBA.
+ */
+static int iba_r_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u32 *ptr_32 = (u32 *) data;
+
+	memset(data, 0, sizeof(u32) * 4);
+	ptr_32[4] = addr;
+	return iba_req(&sw->info->iba, data, data, &port, r_acl_table_pre,
+		r_acl_table_post);
+}  /* iba_r_acl_hw */
+
+/**
+ * w_acl_table_pre - IBA ACL table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for ACL table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_acl_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	uint *port = obj;
+	u16 addr = data[4];
+	u32 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		info->data[0] = be32_to_cpu(data[i]);
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_ACL_0 + 4 * i));
+	}
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_8,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0));
+	info->data[0] = PORT_ACL_WRITE_DONE << 8;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0 & ~3));
+	return info->fptr;
+}  /* w_acl_table_pre */
+
+/**
+ * iba_w_acl_hw - write to ACL table using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This function writes to ACL table of the port using IBA.
+ */
+static int iba_w_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u32 *ptr_32 = (u32 *) data;
+
+	ptr_32[4] = addr;
+	return iba_req(&sw->info->iba, data, NULL, &port, w_acl_table_pre,
+		NULL);
+}  /* iba_w_acl_hw */
+#endif
+
+static struct ksz_sw_reg_ops sw_iba_ops = {
+	.lock			= iba_lock,
+	.unlock			= iba_unlock,
+
+	.r8			= iba_r8,
+	.r16			= iba_r16,
+	.r24			= iba_r24,
+	.r32			= iba_r32,
+	.w8			= iba_w8,
+	.w16			= iba_w16,
+	.w24			= iba_w24,
+	.w32			= iba_w32,
+
+	.r			= iba_r_buf,
+	.w			= iba_w_buf,
+
+	.get			= iba_get,
+	.set			= iba_set,
+
+	.r_dyn_mac_hw		= iba_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= iba_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= iba_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= iba_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= iba_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= iba_r_sta_mac_hw,
+	.w_sta_mac_hw		= iba_w_sta_mac_hw,
+	.r_vlan_hw		= iba_r_vlan_hw,
+	.w_vlan_hw		= iba_w_vlan_hw,
+
+#ifdef CONFIG_KSZ_HSR
+	.r_hsr_hw		= iba_r_hsr_hw,
+	.w_hsr_hw		= iba_w_hsr_hw,
+	.start_hsr_hw		= iba_start_hsr_hw,
+	.g_hsr_hw		= iba_g_hsr_hw,
+	.stop_hsr_hw		= iba_stop_hsr_hw,
+#endif
+
+	.r_mib_cnt_hw		= iba_r_mib_cnt_hw,
+
+#if defined(CONFIG_HAVE_KSZ9897)
+	.r_acl_hw		= iba_r_acl_hw,
+	.w_acl_hw		= iba_w_acl_hw,
+#endif
+};
+
+/**
+ * iba_rcv - Receive IBA response.
+ * @info:	The IBA instance.
+ * @skb:	The received socket buffer.
+ *
+ * This function processes IBA response.
+ */
+static int iba_rcv(struct ksz_iba_info *info, struct sk_buff *skb)
+{
+	int i;
+	int j;
+	int k;
+	int cnt;
+	int len;
+	int cmd_shift;
+	u32 cmd;
+	u32 cmds;
+	u32 addr;
+	u32 data;
+	struct iba_cmd *frame;
+	struct iba_frame *iba;
+	u8 *ptr;
+	int ret = 1;
+
+	ptr = skb->data;
+	ptr += ETH_ALEN * 2;
+	iba = (struct iba_frame *) ptr;
+
+	if (iba->tag.type != htons(info->tag_type) ||
+	    iba->format.format != htons(IBA_FORMAT_KSZ98XX))
+		goto out_drop;
+
+if (dbg_iba)
+dbg_msg(" iba rx: %x %x\n", info->seqid, iba->tag.seqid);
+
+	if (!info->cmds[0].cmd)
+		goto out_drop;
+
+#if 0
+	dbg_msg("seq: %x\n", iba->tag.seqid);
+#endif
+	if (iba->tag.seqid != info->seqid)
+		goto out_debug;
+
+	len = ntohs(iba->length);
+	cnt = skb->len;
+	if (len != cnt) {
+		if (skb->len > 61 && len + 4 != cnt)
+			dbg_msg("len: %d != %d\n", len, cnt);
+		if (len > cnt)
+			len = cnt;
+	}
+	len -= ETH_ALEN * 2 + sizeof(struct iba_frame) -
+		sizeof(struct iba_cmd);
+	if (ntohs(iba->code) == IBA_CODE_NORMAL) {
+#if 0
+		dbg_msg("normal\n");
+#endif
+		cmd_shift = IBA_CMD_S;
+	} else {
+#if 0
+		dbg_msg("burst\n");
+#endif
+		cmd_shift = IBA_BURST_S;
+	}
+	frame = &iba->cmd;
+	j = 0;
+	k = 0;
+	while (len >= 4 && frame->cmd) {
+		cmd = ntohl(frame->cmd);
+		if (0xdeadbeef == cmd) {
+dbg_msg("apb: %08x\n", last_iba_addr);
+			break;
+		}
+		cmds = cmd;
+		addr = cmd & IBA_CMD_ADDR_M;
+		i = 0;
+		data = ntohl(frame->data[i++]);
+		if (cmd != info->cmds[j].cmd || (IBA_BURST_S == cmd_shift
+				&& data != info->cmds[j].data[0]))
+			dbg_msg("?cmd %x=%x %x=%x\n", info->cmds[j].cmd, cmd,
+				info->cmds[j].data[0], data);
+		cmd >>= cmd_shift;
+		if (IBA_BURST_S == cmd_shift) {
+			cnt = data;
+			if (IBA_BURST_WRITE == cmd) {
+#if 0
+				dbg_msg("w: %08x=%d\n", addr, cnt);
+#endif
+#if 0
+				i += cnt;
+#else
+				for (; i <= cnt; i++) {
+					data = ntohl(frame->data[i]);
+					iba_chk_regs(info->sw_dev, cmds, data);
+					cmds += 4;
+#if 0
+					dbg_msg("%08x ", data);
+#endif
+				}
+#if 0
+				if (cnt)
+					dbg_msg("\n");
+#endif
+#endif
+			} else if (IBA_BURST_READ == cmd) {
+#if 0
+				dbg_msg("r: %08x=%d\n", addr, cnt);
+#endif
+				info->regs[k].cmd = cmds;
+				for (; i <= cnt; i++) {
+					data = ntohl(frame->data[i]);
+					info->regs[k++].data[0] = data;
+					info->regs[k].cmd =
+						info->regs[k - 1].cmd + 4;
+#if 0
+					dbg_msg("%08x ", data);
+#endif
+				}
+#if 0
+				if (cnt)
+					dbg_msg("\n");
+#endif
+			} else
+				break;
+			len -= sizeof(u32) * cnt;
+		} else {
+			switch (cmd) {
+			case IBA_CMD_READ:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("r: ");
+#endif
+				break;
+			case IBA_CMD_WRITE:
+				iba_chk_regs(info->sw_dev, cmds, data);
+#if 0
+				dbg_msg("w: ");
+#endif
+				break;
+			case IBA_CMD_WAIT_ON_0:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("z: ");
+#endif
+				break;
+			case IBA_CMD_WAIT_ON_1:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("s: ");
+#endif
+				break;
+			case IBA_CMD_WRITE_0:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("0: ");
+#endif
+				break;
+			case IBA_CMD_WRITE_1:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#if 0
+				dbg_msg("1: ");
+#endif
+				break;
+			}
+#if 0
+			dbg_msg("%08x=%08x\n", addr, data);
+#endif
+		}
+		j++;
+		len -= sizeof(struct iba_cmd);
+		frame = (struct iba_cmd *) &frame->data[i];
+	}
+#if 0
+	dbg_msg("\n");
+#endif
+	ptr = skb->data;
+	if (len != 4)
+		dbg_msg("?len: %d\n", len);
+	if (info->cmds[j].cmd != 0)
+		dbg_msg("? %x\n", info->cmds[j].cmd);
+	if (len != 4 && info->cmds[j].cmd != 0) {
+		for (i = 0; i < skb->len + ETH_ALEN * 2 + 2; i++) {
+			dbg_msg("%02x ", ptr[i]);
+			if (15 == (i % 16))
+				dbg_msg("\n");
+		}
+		if (15 != (i % 16))
+			dbg_msg("\n");
+	}
+	info->cmds[0].cmd = 0;
+	info->regs[k].cmd = (u32) -1;
+	info->respid = iba->tag.seqid;
+if (dbg_iba) {
+dbg_iba = 0;
+dbg_msg("ok %x %x\n", info->seqid, last_ok_iba);
+}
+last_ok_iba = info->respid;
+
+	dev_kfree_skb_irq(skb);
+	complete(&info->done);
+	return 0;
+
+out_debug:
+dbg_iba = 1;
+dbg_msg("last ok: %x\n", last_ok_iba);
+	dbg_msg("seq: %x\n", info->seqid);
+#if 0
+	for (i = 0; i < sizeof(struct iba_frame) + 14; i++) {
+		dbg_msg("%02x ", ptr[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (15 != (i % 16))
+		dbg_msg("\n");
+#endif
+	for (i = 0; i < ntohs(info->frame->length); i++) {
+		dbg_msg("%02x ", info->packet[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (15 != (i % 16))
+		dbg_msg("\n");
+	for (i = 0; i < skb->len; i++) {
+		dbg_msg("%02x ", skb->data[i]);
+		if (15 == (i % 16))
+			dbg_msg("\n");
+	}
+	if (15 != (i % 16))
+		dbg_msg("\n");
+
+out_drop:
+	return ret;
+}  /* iba_rcv */
+
+static void ksz_iba_init(struct ksz_iba_info *iba, struct ksz_sw *sw)
+{
+	u32 data;
+	u16 tag_type;
+
+	/* Running nuttcp UDP TX can affect IBA communication if too short. */
+	data = 200;
+	iba->delay_ticks = data * HZ / 1000;
+
+	if (!iba->use_iba) {
+		sw->ops->acquire(sw);
+		data = sw_r32(sw, REG_SW_IBA__4);
+		tag_type = (data & SW_IBA_FRAME_TPID_M);
+		sw->ops->release(sw);
+	} else
+		tag_type = ETH_P_IBA;
+
+	iba->sw_dev = sw;
+	iba->packet = kzalloc(IBA_LEN_MAX, GFP_KERNEL);
+	iba->buf = kzalloc(IBA_LEN_MAX, GFP_KERNEL);
+	iba->data = kzalloc(IBA_BURST_CNT_MAX * sizeof(u32), GFP_KERNEL);
+	iba->regs = kmalloc(IBA_BURST_CNT_MAX * sizeof(struct iba_cmd),
+		GFP_KERNEL);
+	iba->cmds = kmalloc(IBA_BURST_CNT_MAX * sizeof(struct iba_cmd) / 4,
+		GFP_KERNEL);
+	iba->frame = (struct iba_frame *) &iba->packet[ETH_ALEN * 2];
+	iba->tag_type = tag_type;
+	iba->dst[0] = 0x01;
+	iba->dst[1] = 0x00;
+	iba->dst[2] = 0x5E;
+	iba->dst[3] = 0x00;
+	iba->dst[4] = 0x01;
+	iba->dst[5] = 0x81;
+#ifndef CAPTURE_IBA
+	memcpy(iba->dst, sw->info->mac_addr, ETH_ALEN);
+#endif
+	iba->src[0] = 0x00;
+#ifdef CAPTURE_IBA
+	iba->src[0] = 0x01;
+#endif
+	iba->src[1] = 0x10;
+	iba->src[2] = 0xA1;
+	iba->src[3] = 0x98;
+	iba->src[4] = 0x97;
+	iba->src[5] = 0x81;
+	init_completion(&iba->done);
+	init_waitqueue_head(&iba->queue);
+	mutex_init(&iba->lock);
+
+	iba_info = iba;
+}  /* ksz_iba_init */
+
+static void ksz_iba_exit(struct ksz_iba_info *iba)
+{
+	kfree(iba->cmds);
+	kfree(iba->regs);
+	kfree(iba->data);
+	kfree(iba->buf);
+	kfree(iba->packet);
+}  /* ksz_iba_exit */
+
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_iba.c"
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_iba.h b/drivers/net/ethernet/micrel/ksz9897/ksz_iba.h
new file mode 100644
index 0000000..3a326a9
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_iba.h
@@ -0,0 +1,125 @@
+/**
+ * Microchip IBA header
+ *
+ * Copyright (c) 2015 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_IBA_H
+#define KSZ_IBA_H
+
+
+#define IBA_TAG_TYPE		0x40FE
+
+struct iba_tag {
+	u16 type;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 prio:3;
+	u8 cfi:1;
+	u8 mode:4;
+#else
+	u8 mode:4;
+	u8 cfi:1;
+	u8 prio:3;
+#endif
+	u8 seqid;
+} __packed;
+
+#define IBA_FORMAT_KSZ98XX	0x9800
+
+struct iba_format {
+	u16 format;
+	u16 reserved;
+} __packed;
+
+#define IBA_CODE_NORMAL		0x0001
+#define IBA_CODE_BURST		0x0002
+
+#define IBA_CMD_READ		1
+#define IBA_CMD_WRITE		2
+#define IBA_CMD_WAIT_ON_0	4
+#define IBA_CMD_WAIT_ON_1	5
+#define IBA_CMD_WRITE_0		6
+#define IBA_CMD_WRITE_1		7
+#define IBA_CMD_S		29
+
+#define IBA_CMD_BYTE_0		(1 << 27)
+#define IBA_CMD_BYTE_1		(1 << 26)
+#define IBA_CMD_BYTE_2		(1 << 25)
+#define IBA_CMD_BYTE_3		(1 << 24)
+
+#define IBA_CMD_32		\
+	(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_24		\
+	(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2)
+#define IBA_CMD_24_H		\
+	(IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_16		(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1)
+#define IBA_CMD_16_M		(IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2)
+#define IBA_CMD_16_H		(IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_8		(IBA_CMD_BYTE_0)
+
+#define IBA_CMD_ADDR_M		((1 << 24) - 1)
+
+#define IBA_BURST_READ		1
+#define IBA_BURST_WRITE		2
+#define IBA_BURST_S		30
+#define IBA_BURST_CNT_MAX	(1 << 7)
+#define IBA_BURST_CNT_M		((1 << 7) - 1)
+
+struct iba_cmd {
+	u32 cmd;
+	u32 data[1];
+} __packed;
+
+struct iba_frame {
+	struct iba_tag tag;
+	u16 length;
+	struct iba_format format;
+	u16 code;
+	struct iba_cmd cmd;
+};
+
+#define IBA_LEN_MAX		288
+
+struct ksz_iba_info {
+	int use_iba;
+	void *sw_dev;
+	u16 tag_type;
+	u8 dst[ETH_ALEN];
+	u8 src[ETH_ALEN];
+	u8 *buf;
+	u8 *packet;
+	struct iba_frame *frame;
+	struct iba_cmd *cmds;
+	struct iba_cmd *regs;
+	u8 seqid;
+	u8 respid;
+	struct completion done;
+	wait_queue_head_t queue;
+	unsigned long delay_ticks;
+	struct net_device *dev;
+	struct mutex lock;
+	int cnt;
+	u32 cfg;
+
+	/* Used for putting in commands. */
+	u32 *data;
+	void *fptr;
+	int index;
+	int len;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_mrp.c b/drivers/net/ethernet/micrel/ksz9897/ksz_mrp.c
new file mode 100644
index 0000000..a2cb003
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_mrp.c
@@ -0,0 +1,8438 @@
+/**
+ * Microchip MRP driver code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 0
+#define DEBUG_MRP_MEM
+#endif
+#if 1
+#define DEBUG_MRP_OPER
+#endif
+#if 1
+#define DEBUG_MVRP
+static int dbg_mrp_vlan = 1;
+#endif
+#if 1
+#define DEBUG_MSRP
+#endif
+
+#if 1
+static int mrp_10_1_2f_hack;
+static int mrp_10_1_8a_hack;
+static int mrp_10_5_1_hack;
+static int mrp_10_5_1c_hack;
+static int mrp_10_5_1d_hack;
+static int msrp_35_1_14g_hack;
+static int fqtss_hack;
+static int fqtss_34_2_3_hack;
+static int fqtss_34_2_1b_hack;
+static int fqtss_34_2_5b_hack;
+static int fqtss_34_2_9b_hack;
+static int regeneration_hack;
+#endif
+
+
+static const u8 bcast_addr[] = {
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+};
+
+static const u8 maap_addr[] = {
+	0x91, 0xE0, 0xF0, 0x00, 0xFF, 0x01,
+};
+
+#define BCAST_DA_ACL_ENTRY		10
+#define AVB_BOUNDARY_ACL_ENTRY		11
+
+struct avtpdu {
+	u8 subtype;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 sv:1;
+	u8 version:3;
+	u8 mr:1;
+	u8 fsd:2;
+	u8 tv:2;
+#else
+	u8 tv:2;
+	u8 fsd:2;
+	u8 mr:1;
+	u8 version:3;
+	u8 sv:1;
+#endif
+	u8 seq_num;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 format_spec_data_1:7;
+	u8 tu:1;
+#else
+	u8 tu:1;
+	u8 format_spec_data_1:7;
+#endif
+	u8 stream_id[8];
+	u32 avtp_timestamp;
+	u32 format_spec_data_2;
+	u16 stream_data_len;
+	u16 format_spec_data_3;
+} __packed;
+
+struct avtpdu_ctrl {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u32 subtype:8;
+	u32 sv:1;
+	u32 version:3;
+	u32 format_spec_data:9;
+	u32 ctrl_data_len:11;
+#else
+	u32 ctrl_data_len:11;
+	u32 format_spec_data:9;
+	u8 version:3;
+	u8 sv:1;
+	u32 subtype:8;
+#endif
+	u8 stream_id[8];
+} __packed;
+
+struct maap_pdu {
+	u8 subtype;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 sv:1;
+	u8 version:3;
+	u8 message_type:4;
+	u16 maap_version:5;
+	u16 ctrl_data_len:11;
+#else
+	u8 message_type:4;
+	u8 version:3;
+	u8 sv:1;
+	u16 ctrl_data_len:11;
+	u16 maap_version:5;
+#endif
+	u8 stream_id[8];
+	u8 req_start_addr[6];
+	u16 req_cnt;
+	u8 conflict_start_addr[6];
+	u16 conflict_cnt;
+} __packed;
+
+#define AVTP_SUBTYPE_AAF		0x02
+#define AVTP_SUBTYPE_CVF		0x03
+#define AVTP_SUBTYPE_CRF		0x04
+#define AVTP_SUBTYPE_TSCF		0x05
+#define AVTP_SUBTYPE_SVF		0x06
+#define AVTP_SUBTYPE_RVF		0x07
+#define AVTP_SUBTYPE_VSF_STREAM		0x6F
+#define AVTP_SUBTYPE_EF_STREAM		0x7F
+#define AVTP_SUBTYPE_NTSCF		0x82
+#define AVTP_SUBTYPE_ADP		0xFA
+#define AVTP_SUBTYPE_AECP		0xFB
+#define AVTP_SUBTYPE_ACMP		0xFC
+#define AVTP_SUBTYPE_MAAP		0xFE
+#define AVTP_SUBTYPE_EF_CONTROL		0xFF
+
+static void setup_acl_drop(struct mrp_info *mrp, uint port)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = BCAST_DA_ACL_ENTRY;
+
+	mutex_lock(&sw->acllock);
+
+	acl = &cfg->acl_info[i];
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, bcast_addr, ETH_ALEN);
+	acl->eth_type = 0x22F0;
+
+	acl->first_rule = i;
+	acl->ruleset = (1 << i);
+	acl->ruleset = 0;
+
+	acl->map_mode = ACL_MAP_MODE_REPLACE;
+	acl->ports = 0;
+	sw_w_acl_table(sw, port, i, acl);
+
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_drop */
+
+static void setup_acl_remap(struct mrp_info *mrp, uint port)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct mrp_port_info *info = &mrp->port_info[port];
+	struct ksz_acl_table *acl;
+	int i = AVB_BOUNDARY_ACL_ENTRY;
+
+	mutex_lock(&sw->acllock);
+
+	acl = &cfg->acl_info[i];
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_TYPE;
+	acl->equal = 1;
+	acl->eth_type = 0x22F0;
+	if (regeneration_hack) {
+		acl->eth_type = 0;
+		acl->equal = 0;
+	}
+
+	acl->first_rule = i;
+	acl->ruleset = (1 << i);
+
+	acl->map_mode = ACL_MAP_MODE_DISABLE;
+	acl->ports = 0;
+	acl->prio_mode = ACL_PRIO_MODE_REPLACE;
+	acl->prio = info->priority[SR_CLASS_B].regenerated_priority;
+	acl->vlan_prio_replace = 1;
+	acl->vlan_prio = info->priority[SR_CLASS_B].regenerated_priority;
+	sw_w_acl_table(sw, port, i, acl);
+
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_remap */
+
+static void enable_acl_remap(struct mrp_info *mrp, uint port, bool remap)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = AVB_BOUNDARY_ACL_ENTRY;
+
+	mutex_lock(&sw->acllock);
+
+	acl = &cfg->acl_info[i];
+dbg_msg(" remap: %d=%d %d:%d\n", port, remap, cfg->avb_a, cfg->avb_b);
+	if (remap)
+		acl->ruleset = (1 << i);
+	else
+		acl->ruleset = 0;
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	i = BCAST_DA_ACL_ENTRY;
+	acl = &cfg->acl_info[i];
+	if (remap)
+		acl->ruleset = 0;
+	else
+		acl->ruleset = (1 << i);
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	mutex_unlock(&sw->acllock);
+	sw->ops->acquire(sw);
+	if (!remap) {
+		if (!(mrp->mcast_ports & (1 << port))) {
+			mrp->mcast_ports |= (1 << port);
+			if (!mrp->mcast_port_cnt)
+				sw->ops->fwd_unk_mcast(sw, true);
+			mrp->mcast_port_cnt++;
+		}
+	} else {
+		if (mrp->mcast_ports & (1 << port)) {
+			mrp->mcast_ports &= ~(1 << port);
+			mrp->mcast_port_cnt--;
+			if (!mrp->mcast_port_cnt)
+				sw->ops->fwd_unk_mcast(sw, false);
+		}
+	}
+	sw->ops->release(sw);
+}  /* enable_acl_remap */
+
+static int get_actual_port(struct mrp_info *mrp, u8 port)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	return sw_get_dev_port(sw, 0, mrp->ports, port + 1);
+}  /* get_actual_port */
+
+static int is_host_port(struct mrp_info *mrp, u8 port)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	u8 p = sw_get_dev_port(sw, 0, mrp->ports, port + 1);
+
+	return (p == sw->HOST_PORT);
+}  /* is_host_port */
+
+static int skip_mrp_port(struct mrp_info *mrp, u8 p)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	u8 lp = sw_get_net_port(sw, 0, mrp->ports, p);
+
+	if (lp > mrp->ports && p != sw->HOST_PORT)
+		return true;
+	return false;
+}  /* skip_mrp_port */
+
+#ifdef CONFIG_KSZ_MSRP
+#define SW_CREDIT_SHAPING_SCALE	0x10000
+#define SW_CREDIT_SHAPING_S	16
+
+#define CREDIT_PERCENTAGE_S	(16 + 9)
+
+#define NETWORK_SPEED_IN_MBIT	1000000
+
+static char *format_num(char *str, u32 num)
+{
+	u32 num0;
+	u32 num1;
+	u32 num2;
+	u32 num3;
+
+	num0 = num % 1000;
+	num1 = (num / 1000) % 1000;
+	num2 = (num / 1000000) % 1000;
+	num3 = (num / 1000000000);
+	if (num3)
+		sprintf(str, "%u,%03u,%03u,%03u", num3, num2, num1, num0);
+	else if (num2)
+		sprintf(str, "%u,%03u,%03u", num2, num1, num0);
+	else if (num1)
+		sprintf(str, "%u,%3u", num1, num0);
+	else
+		sprintf(str, "%u", num0);
+	return str;
+}  /* format_num */
+
+static char *format_per(char *str, u32 per)
+{
+	u64 val;
+	u32 num;
+	u32 num0;
+	u32 num1;
+
+	val = per;
+	val *= 100;
+	val += 1 << (CREDIT_PERCENTAGE_S - 1);
+	val >>= CREDIT_PERCENTAGE_S;
+	num = (u32) val;
+	num0 = num % 100;
+	num1 = num / 100;
+	if (num1)
+		sprintf(str, "%u.%02u%%", num1, num0);
+	else
+		sprintf(str, "0.%02u%%", num0);
+	return str;
+}  /* format_per */
+
+static u16 get_credit_increment(u32 speed, u32 bandwidth)
+{
+	u64 val;
+	u32 rem;
+
+	speed *= NETWORK_SPEED_IN_MBIT;
+
+	/* Cannot get higher than the running speed. */
+	if (bandwidth > speed)
+		return 0;
+	val = bandwidth;
+	val <<= SW_CREDIT_SHAPING_S;
+	val += speed / 2;
+	val = div_u64_rem(val, speed, &rem);
+
+	/* Cannot become zero. */
+	if (!val)
+		val = 1;
+	return (u16) val;
+}  /* get_credit_increment */
+
+static u16 get_credit_watermark(u16 size)
+{
+	/* Preamable + vlan_ethhdr + CRC + IFG = 42 */
+	size += 8 + 14 + 4 + 4 + 12;
+	return size;
+}  /* get_credit_watermark */
+
+static u32 get_idle_slope(u32 speed, u32 bandwidth)
+{
+	u64 val;
+	u32 rem;
+
+	speed *= NETWORK_SPEED_IN_MBIT;
+
+	/* Cannot get higher than the running speed. */
+	if (bandwidth > speed)
+		return 0;
+	val = bandwidth;
+	val *= 100;
+	val <<= CREDIT_PERCENTAGE_S;
+	val = div_u64_rem(val, speed, &rem);
+	return (u32) val;
+}  /* get_idle_slope */
+
+static u32 get_send_slope(u32 idle_slope)
+{
+	u32 slope;
+
+	slope = 100;
+	slope <<= CREDIT_PERCENTAGE_S;
+	slope -= idle_slope;
+	return slope;
+}  /* get_send_slope */
+
+static u8 get_queue_priority(struct mrp_info *mrp, int tc)
+{
+	return mrp->prio[tc];
+}  /* get_queue_priority */
+
+static void srp_cfg_credit_shaper(struct mrp_info *mrp, u8 lp,
+	struct mrp_port_info *info, struct mrp_traffic_info *traffic)
+{
+	u16 credit;
+	u32 idle;
+	u32 send;
+	uint port;
+	u8 queue;
+#ifdef DEBUG_MRP_OPER
+	char bw_str[20];
+	char idle_str[20];
+	char send_str[20];
+#endif
+
+	port = get_actual_port(mrp, lp);
+	credit = get_credit_increment(info->speed,
+		traffic->bandwidth_used);
+	idle = get_idle_slope(info->speed, traffic->bandwidth_used);
+	send = get_send_slope(idle);
+	queue = traffic->queue;
+	info->bandwidth[traffic->queue].operIdleSlope = idle;
+
+#ifdef DEBUG_MRP_OPER
+	format_num(bw_str, traffic->bandwidth_used);
+	format_per(idle_str, idle);
+	format_per(send_str, send);
+dbg_msg("  %s %d:%d=%u; %s %s %s\n", __func__, port,
+	queue, credit,
+	bw_str, idle_str, send_str);
+#endif
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	do {
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		if (credit < 2)
+			credit = info->credit[queue];
+		sw->reg->lock(sw);
+		if (credit > 1) {
+			port_set_increment(sw, port, queue, credit);
+#if 0
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_STRICT_PRIO);
+			port_set_shaping(sw, port, queue, MTI_SHAPING_SRP);
+#endif
+		} else {
+#if 0
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_WRR);
+			port_set_shaping(sw, port, queue, MTI_SHAPING_OFF);
+#endif
+		}
+		sw->reg->unlock(sw);
+	} while (0);
+#endif
+}  /* srp_cfg_credit_shaper */
+
+static void srp_cfg_idle_slope(struct mrp_info *mrp, uint port, uint queue,
+	struct mrp_port_info *info, u32 idle)
+{
+	u16 credit;
+	u16 credit_lo;
+	u16 credit_hi;
+	u32 send;
+	u32 rem;
+	u64 idle_slope;
+#if 0
+	char bw_str[20];
+	char idle_str[20];
+	char send_str[20];
+#endif
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	idle_slope = info->speed;
+	idle_slope *= NETWORK_SPEED_IN_MBIT;
+	idle_slope *= idle;
+	send = 100;
+	send <<= CREDIT_PERCENTAGE_S;
+	idle_slope = div_u64_rem(idle_slope, send, &rem);
+
+	credit = get_credit_increment(info->speed, idle_slope);
+	info->credit[queue] = credit;
+	send = get_send_slope(idle);
+	credit_hi = get_credit_watermark(mrp->max_interference_size);
+	credit_lo = get_credit_watermark(1500);
+#if 0
+	format_num(bw_str, idle_slope);
+	format_per(idle_str, idle);
+	format_per(send_str, send);
+dbg_msg("  %s %d:%d=%u %u %u; %s %s %s\n", __func__, port,
+	queue, credit, credit_hi, credit_lo,
+	bw_str, idle_str, send_str);
+#endif
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	sw->reg->lock(sw);
+	if (credit > 1) {
+		port_set_hi_water_mark(sw, port, queue, credit_hi);
+		port_set_lo_water_mark(sw, port, queue, credit_lo);
+		port_set_increment(sw, port, queue, credit);
+		port_set_schedule_mode(sw, port, queue,
+			MTI_SCHEDULE_STRICT_PRIO);
+		port_set_shaping(sw, port, queue, MTI_SHAPING_SRP);
+	} else {
+		port_set_schedule_mode(sw, port, queue,
+			MTI_SCHEDULE_WRR);
+		port_set_shaping(sw, port, queue, MTI_SHAPING_OFF);
+	}
+	sw->reg->unlock(sw);
+#endif
+}  /* srp_cfg_idle_slope */
+#endif
+
+static void mrp_cfg_dest_addr(struct mrp_info *mrp, u8 index, u8 *dest,
+	u32 ports, u16 fid)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+#ifdef DEBUG_MRP_OPER
+dbg_msg("  %s %d=%02x:%02x:%02x:%02x:%02x:%02x %04x %x\n", __func__, index,
+dest[0], dest[1], dest[2], dest[3], dest[4],dest[5],
+ports, fid);
+#endif
+	ports &= SRP_PORT_AVAIL | SRP_PORT_READY;
+	ports = sw_get_dest_port(sw, 0, mrp->ports, ports);
+	sw->ops->cfg_mac(sw, index, dest, ports, false, fid != 0, fid);
+}  /* mrp_cfg_dest_addr */
+
+static void mrp_cfg_vlan(struct mrp_info *mrp, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+#ifdef DEBUG_MRP_OPER
+if (dbg_mrp_vlan || vid == 2 || vid > 4090)
+dbg_msg("  %s %d=%x %x %04x\n", __func__, index, vid, fid, ports);
+#endif
+	ports &= SRP_PORT_AVAIL | SRP_PORT_READY;
+	ports = sw_get_dest_port(sw, 0, mrp->ports, ports);
+
+#if 0
+	/* Needed to catch unwanted multicast traffic. */
+	if (ports)
+		ports |= sw->HOST_MASK;
+#endif
+	sw->ops->cfg_vlan(sw, index, vid, fid, ports);
+}  /* mrp_cfg_vlan */
+
+#if 0
+static int mrp_req_cfg_vlan(struct mrp_info *mrp, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct sk_buff *skb;
+	u16 *data;
+	u32 *dword;
+
+	skb = alloc_skb(64, GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	data = (u16 *)skb->data;
+	*data++ = index;
+	*data++ = vid;
+	*data++ = fid;
+	dword = (u32 *)data;
+	*dword = ports;
+	skb_queue_tail(&mrp->vlanq, skb);
+	if (!mrp->vlanq_sched) {
+		mrp->vlanq_sched = 1;
+		schedule_work(&mrp->cfg_vlan);
+	}
+	return 0;
+}  /* mrp_req_cfg_vlan */
+#endif
+
+static void mrp_cfg_vlan_work(struct work_struct *work)
+{
+	int save_dbg_vlan;
+	u16 *data;
+	u32 *dword;
+	u8 index;
+	u16 vid;
+	u16 fid;
+	u32 ports;
+	bool last;
+	struct sk_buff *skb;
+	struct mrp_info *mrp = container_of(work, struct mrp_info, cfg_vlan);
+
+	last = skb_queue_empty(&mrp->vlanq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->vlanq);
+		last = skb_queue_empty(&mrp->vlanq);
+		if (!skb)
+			continue;
+		data = (u16 *)skb->data;
+		index = (u8) *data++;
+		vid = *data++;
+		fid = *data++;
+		dword = (u32 *)data;
+		ports = *dword;
+		save_dbg_vlan = dbg_mrp_vlan;
+		dbg_mrp_vlan = 0;
+		mrp_cfg_vlan(mrp, index, vid, fid, ports);
+		dbg_mrp_vlan = save_dbg_vlan;
+		kfree_skb(skb);
+	}
+	mrp->vlanq_sched = 0;
+}  /* mrp_cfg_vlan_work */
+
+#ifdef CONFIG_KSZ_MSRP
+static int get_traffic_index(int tc)
+{
+	if (SR_CLASS_A == tc)
+		return 1;
+	return 0;
+}
+
+static int get_traffic_class(struct mrp_info *mrp, u8 prio)
+{
+	return mrp->tc[prio];
+}  /* get_traffic_class */
+
+static struct mrp_traffic_info *get_traffic_info(struct mrp_port_info *port,
+	int tc)
+{
+	if (SR_CLASS_A == tc)
+		return &port->traffic[1];
+	else
+		return &port->traffic[0];
+}  /* get_traffic_info */
+
+static struct mrp_port_info *get_port_info(struct mrp_info *mrp, u8 port)
+{
+	port = get_actual_port(mrp, port);
+	return &mrp->port_info[port];
+}  /* get_port_info */
+
+static int frames_per_sec(int traffic_class)
+{
+	int frames;
+
+	switch (traffic_class) {
+	case SR_CLASS_A:
+		frames = 8000;
+		break;
+	case SR_CLASS_B:
+	default:
+		frames = 4000;
+	}
+	return frames;
+}  /* frames_per_sec */
+
+static u64 calculate_bandwidth(u32 size, u32 interval, u32 frames)
+{
+	u64 bandwidth;
+
+	/* Mininum frame size is 68. */
+#if 1
+	if (size < 46 && fqtss_34_2_5b_hack)
+		size = 46 - 4;
+	else
+#endif
+	if (size < 46)
+		size = 46;
+#if 1
+	if (size < 100 && fqtss_34_2_9b_hack)
+		size = 100;
+#endif
+	bandwidth = size;
+
+	/* Preamable + vlan_ethhdr + CRC + IFG */
+	bandwidth += 8 + 14 + 4 + 4 + 12;
+
+	/* AVnu MSRP test says to add one more byte for bridge. */
+	bandwidth += 1;
+	bandwidth *= interval;
+	bandwidth *= frames;
+	bandwidth *= 8;
+	return bandwidth;
+}  /* calculate_bandwidth */
+#endif
+
+static int cmp_mac(void *first, void *second)
+{
+	int cmp;
+	struct mrp_mac_info *a = first;
+	struct mrp_mac_info *b = second;
+
+#if 0
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x=%x %02x:%02x:%02x:%02x:%02x:%02x=%x\n", __func__,
+a->addr[0], a->addr[1], a->addr[2], a->addr[3], a->addr[4], a->addr[5], a->fid,
+b->addr[0], b->addr[1], b->addr[2], b->addr[3], b->addr[4], b->addr[5], b->fid);
+#endif
+	cmp = a->fid - b->fid;
+	if (!cmp)
+		cmp = memcmp(a->addr, b->addr, ETH_ALEN);
+	return cmp;
+}  /* cmp_mac */
+
+static void show_mac_info(void *this)
+{
+	struct mrp_mac_info *info = this;
+
+	dbg_msg(
+		"%02x:%02x:%02x:%02x:%02x:%02x %d=%03x r:%04x s:%04x t:%04x\n",
+		info->addr[0], info->addr[1], info->addr[2],
+		info->addr[3], info->addr[4], info->addr[5],
+		info->index, info->fid,
+		info->mrp_ports, info->srp_ports, info->tx_ports);
+}  /* show_mac_info */
+
+static int cmp_vlan(void *first, void *second)
+{
+	int cmp;
+	struct mrp_vlan_info *a = first;
+	struct mrp_vlan_info *b = second;
+
+	cmp = a->vid - b->vid;
+	if (!cmp)
+		cmp = memcmp(a->addr, b->addr, ETH_ALEN);
+	return cmp;
+}  /* cmp_vlan */
+
+static void show_vlan_info(void *this)
+{
+	struct mrp_vlan_info *info = this;
+
+	if (info->addr[0] != 0xff)
+		dbg_msg(
+			"[%02x:%02x:%02x:%02x:%02x:%02x] ",
+			info->addr[0], info->addr[1], info->addr[2],
+			info->addr[3], info->addr[4], info->addr[5]);
+	dbg_msg(
+		"%d=%03x.%03x r:%04x t:%04x\n", info->index,
+		info->vid, info->fid, info->ports, info->tx_ports);
+}  /* show_vlan_info */
+
+#ifdef CONFIG_KSZ_MSRP
+static u64 get_stream_age(struct mrp_port_info *info)
+{
+	struct timespec ts;
+	u64 age;
+
+	ts = ktime_to_timespec(ktime_get_real());
+	age = ts.tv_sec;
+	age *= 1000000000;
+	age += ts.tv_nsec;
+	if (age == info->age)
+		age++;
+	info->age = age;
+	return age;
+}
+
+static void prepare_stream_info(struct SRP_reserv *reserv,
+				struct srp_stream_info *x)
+{
+	x->reserv = reserv;
+	x->id = reserv->stream->id;
+	x->age = reserv->streamAge;
+	x->rank = reserv->stream->rank;
+}  /* prepare_stream_info */
+
+static int cmp_stream(struct srp_stream_info *a, struct srp_stream_info *b)
+{
+	int cmp;
+
+	/* Only rank and stream age are compared. */
+	cmp = a->rank - b->rank;
+	if (cmp)
+		return cmp;
+
+	if (a->age > b->age)
+		return 1;
+	else if (a->age < b->age)
+		return -1;
+	cmp = memcmp(a->id, b->id, 8);
+	return cmp;
+}  /* cmp_stream */
+
+static int cmp_lower_stream(void *first, void *second)
+{
+	return cmp_stream(second, first);
+}  /* cmp_lower_stream */
+
+static int cmp_higher_stream(void *first, void *second)
+{
+	return cmp_stream(first, second);
+}  /* cmp_higher_stream */
+
+static void show_stream_info(void *this)
+{
+	struct srp_stream_info *info = this;
+
+	dbg_msg(
+		"r:%u t:%08llx %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x\n",
+		info->rank, info->age,
+		info->id[0], info->id[1], info->id[2], info->id[3],
+		info->id[4], info->id[5], info->id[6], info->id[7]);
+}  /* show_stream_info */
+#endif
+
+#ifdef DEBUG_MRP_BASIC
+static void *get_show(int (*cmp)(void *a, void *b))
+{
+	if (cmp == cmp_mac)
+		return show_mac_info;
+	else if (cmp == cmp_vlan)
+		return show_vlan_info;
+
+#ifdef CONFIG_KSZ_MSRP
+	else
+		return show_stream_info;
+#else
+	else
+		return NULL;
+#endif
+}  /* get_show */
+#endif
+
+static void mrp_init_list(struct mrp_node_anchor *list)
+{
+	list->last = &list->anchor;
+	list->anchor.next = NULL;
+}  /* mrp_init_list */
+
+static void *mrp_find_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), void *data)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_find_node_done;
+
+		/* Will not be found as list is sorted. */
+		if (c > 0) {
+#ifdef DEBUG_MRP_BASIC
+			void (*show)(void *this) = get_show(cmp);
+
+dbg_msg(" %s ", __func__);
+			show(next->data);
+#endif
+			next = NULL;
+			break;
+		}
+		prev = next;
+		next = prev->next;
+	}
+
+mrp_find_node_done:
+	list->last = prev;
+	return next;
+}  /* mrp_find_node */
+
+static void mrp_insert_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	if (list->last != &list->anchor) {
+		prev = list->last;
+		next = prev->next;
+#ifdef DEBUG_MRP_BASIC
+dbg_msg(" %s ", __func__);
+		if (next) {
+			void (*show)(void *this) = get_show(cmp);
+
+			show(next->data);
+		} else
+dbg_msg("last one\n");
+#endif
+		c = 1;
+		if (next) {
+			c = cmp(next->data, this->data);
+		}
+		if (c > 0) {
+			this->next = prev->next;
+			prev->next = this;
+			list->last = &list->anchor;
+			list->cnt++;
+			return;
+		}
+	}
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, this->data);
+
+		/* Stop if next one is higher in the list. */
+		if (c > 0)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	this->next = prev->next;
+	prev->next = this;
+	list->last = &list->anchor;
+	list->cnt++;
+}  /* mrp_insert_node */
+
+static struct mrp_node *mrp_delete_this_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this, int delete)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	if (list->last != &list->anchor) {
+		prev = list->last;
+		next = prev->next;
+#ifdef DEBUG_MRP_BASIC
+dbg_msg(" %s ", __func__);
+		if (next) {
+			void (*show)(void *this) = get_show(cmp);
+
+			show(next->data);
+		} else
+dbg_msg("last one\n");
+#endif
+		c = 1;
+		if (next == this)
+			c = 0;
+		else if (next)
+			c = cmp(next->data, this->data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_delete_this_node_done;
+	}
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, this->data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_delete_this_node_done;
+
+		/* Stop if next one is higher in the list. */
+		if (c > 0)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+
+	/* Nothing is removed. */
+	return NULL;
+
+mrp_delete_this_node_done:
+	prev->next = this->next;
+	list->last = &list->anchor;
+	list->cnt--;
+
+	/* Just remove the node. */
+	if (!delete)
+		return next;
+
+	/* Free the node data. */
+#ifdef DEBUG_MRP_MEM
+if (delete > 1)
+dbg_msg(" %s %p\n", __func__, next->data);
+#endif
+	if (delete > 1)
+		kfree(next->data);
+
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p\n", __func__, next);
+#endif
+	/* Free the node. */
+	kfree(next);
+	return NULL;
+}  /* mrp_delete_this_node */
+
+static void mrp_delete_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	mrp_delete_this_node(list, cmp, this, 2);
+}  /* mrp_delete_node */
+
+static struct mrp_node *mrp_remove_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	return mrp_delete_this_node(list, cmp, this, 0);
+}  /* mrp_remove_node */
+
+static void mrp_show_node(struct mrp_node_anchor *list,
+	void (*show)(void *a))
+{
+	struct mrp_node *next;
+
+	next = list->anchor.next;
+	while (next) {
+		show(next->data);
+		next = next->next;
+	}
+}  /* mrp_show_node */
+
+#ifdef CONFIG_KSZ_MSRP
+static struct SRP_stream *srp_create_stream(u8 *id, u8 *dest, u16 vlan_id,
+	u16 size, u16 interval, u8 prio, u8 rank, u8 reserved, u32 latency)
+{
+	struct SRP_stream *stream;
+
+	stream = kzalloc(sizeof(struct SRP_stream), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p\n", __func__, stream);
+#endif
+	if (stream) {
+		memcpy(stream->id, id, 8);
+		memcpy(stream->dest, dest, ETH_ALEN);
+		stream->vlan_id = vlan_id;
+		stream->MaxFrameSize = size;
+		stream->MaxIntervalFrames = interval;
+		stream->priority = prio;
+		stream->rank = rank;
+		stream->reserved = reserved;
+		stream->latency = latency;
+	}
+	return stream;
+}  /* srp_create_stream */
+
+static struct SRP_stream *srp_find_stream_id(struct mrp_info *mrp, u8 *id)
+{
+	int cmp;
+	struct SRP_stream *stream;
+
+	stream = mrp->stream_by_id.id_next;
+	while (stream) {
+		cmp = memcmp(stream->id, id, 8);
+		if (!cmp)
+			break;
+		stream = stream->id_next;
+	}
+	return stream;
+}  /* srp_find_stream_id */
+
+static struct SRP_stream *srp_find_dest_addr(struct mrp_info *mrp, u8 *dest)
+{
+	int cmp;
+	struct SRP_stream *stream;
+
+	stream = mrp->stream_by_dest.dest_next;
+	while (stream) {
+		cmp = memcmp(stream->dest, dest, ETH_ALEN);
+		if (!cmp)
+			break;
+		stream = stream->dest_next;
+	}
+	return stream;
+}  /* srp_find_dest_addr */
+
+static void srp_insert_stream_by_id(struct mrp_info *mrp,
+	struct SRP_stream *stream)
+{
+	struct SRP_stream *prev;
+	struct SRP_stream *next;
+
+	prev = &mrp->stream_by_id;
+	next = prev->id_next;
+	while (next) {
+		if (memcmp(next->id, stream->id, 8) > 0)
+			break;
+		prev = next;
+		next = prev->id_next;
+	}
+	if (next) {
+		stream->id_next = next;
+		next->id_prev = stream;
+	}
+	stream->id_prev = prev;
+	prev->id_next = stream;
+}  /* srp_insert_stream_by_id */
+
+static void srp_insert_stream_by_dest(struct mrp_info *mrp,
+	struct SRP_stream *stream)
+{
+	struct SRP_stream *prev;
+	struct SRP_stream *next;
+
+	prev = &mrp->stream_by_dest;
+	next = prev->dest_next;
+	while (next) {
+		if (memcmp(next->dest, stream->dest, ETH_ALEN) > 0)
+			break;
+		prev = next;
+		next = prev->dest_next;
+	}
+	if (next) {
+		stream->dest_next = next;
+		next->dest_prev = stream;
+	}
+	stream->dest_prev = prev;
+	prev->dest_next = stream;
+}  /* srp_insert_stream_by_dest */
+
+static void srp_remove_stream(struct SRP_stream *stream, int free)
+{
+	if (stream->id_next)
+		stream->id_next->id_prev = stream->id_prev;
+	stream->id_prev->id_next = stream->id_next;
+	if (stream->dest_next)
+		stream->dest_next->dest_prev = stream->dest_prev;
+	stream->dest_prev->dest_next = stream->dest_next;
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %d\n", __func__, stream, free);
+#endif
+	if (free)
+		kfree(stream);
+}  /* srp_remove_stream */
+
+static struct SRP_reserv *srp_create_reserv(u8 *id, u8 dir, u8 dec,
+					    u32 latency, const u8 *bridge_id,
+					    u8 code)
+{
+	struct SRP_reserv *reserv;
+
+	reserv = kzalloc(sizeof(struct SRP_reserv), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p\n", __func__, reserv);
+#endif
+	if (reserv) {
+		memcpy(reserv->id, id, 8);
+		reserv->direction = dir;
+		reserv->declaration = dec;
+		reserv->latency = latency;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+		reserv->code = code;
+		reserv->rx_code = code;
+	}
+	return reserv;
+}  /* srp_create_reserv */
+
+static struct SRP_reserv *srp_find_reserv(struct SRP_reserv *head, u8 *id,
+					  u8 dir)
+{
+	int cmp;
+	struct SRP_reserv *reserv;
+
+	reserv = head->next;
+	while (reserv) {
+		cmp = memcmp(reserv->id, id, 8);
+		if (!cmp && dir == reserv->direction)
+			break;
+		if (cmp > 0)
+			return NULL;
+		reserv = reserv->next;
+	}
+	return reserv;
+}  /* srp_find_reserv */
+
+static void srp_insert_reserv(struct SRP_reserv *head,
+			      struct SRP_reserv *reserv)
+{
+	int cmp;
+	struct SRP_reserv *prev;
+	struct SRP_reserv *next;
+
+	prev = head;
+	next = prev->next;
+	while (next) {
+		cmp = memcmp(next->id, reserv->id, 8);
+		if (cmp > 0)
+			break;
+		if (0 == cmp && next->declaration > reserv->declaration)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	if (next) {
+		reserv->next = next;
+		next->prev = reserv;
+	}
+	reserv->prev = prev;
+	prev->next = reserv;
+}  /* srp_insert_reserv */
+
+static void srp_remove_reserv(struct SRP_reserv *reserv, int free)
+{
+	if (reserv->next)
+		reserv->next->prev = reserv->prev;
+	reserv->prev->next = reserv->next;
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %d\n", __func__, reserv, free);
+#endif
+	if (free)
+		kfree(reserv);
+}  /* srp_remove_reserv */
+
+static void chk_reserv(struct mrp_port_info *info, uint port)
+{
+	struct SRP_reserv *reserv;
+	int tc;
+	struct mrp_traffic_info *traffic;
+
+#if 0
+	if (!info->link)
+#else
+	if (!info->link && port > 2)
+#endif
+		return;
+dbg_msg("%d %d:\n", info->index, port);
+dbg_msg("  registered: %p\n", &info->registered);
+	reserv = info->registered.next;
+	while (reserv) {
+dbg_msg("%p %02x:%02x:%02x %d %d %02d=%04x\n", reserv,
+reserv->id[5], reserv->id[6], reserv->id[7], reserv->direction,
+	reserv->declaration, reserv->code, reserv->code_bits);
+		reserv = reserv->next;
+	}
+dbg_msg("  declared: %p\n", &info->declared);
+	reserv = info->declared.next;
+	while (reserv) {
+dbg_msg("%p %02x:%02x:%02x %d %d %02d=%04x\n", reserv,
+reserv->id[5], reserv->id[6], reserv->id[7], reserv->direction,
+	reserv->declaration, reserv->code, reserv->code_bits);
+		reserv = reserv->next;
+	}
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+dbg_msg("  active:\n");
+		mrp_show_node(&traffic->active, show_stream_info);
+#if 1
+dbg_msg("  passive:\n");
+		mrp_show_node(&traffic->passive, show_stream_info);
+dbg_msg("m=%u u=%u l=%u\n",
+		traffic->bandwidth_max,
+		traffic->bandwidth_used,
+		traffic->bandwidth_left);
+#endif
+	}
+dbg_msg("T:%u\n", info->bandwidth_used);
+dbg_msg("\n");
+}  /* chk_reserv */
+#endif
+
+static struct mrp_node *mrp_alloc_node(size_t data_size)
+{
+	struct mrp_node *node;
+
+	node = kzalloc(sizeof(struct mrp_node), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %u\n", __func__, node, data_size);
+#endif
+	if (!node)
+		return NULL;
+	node->data = kzalloc(data_size, GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p\n", __func__, node->data);
+#endif
+	if (!node->data) {
+		kfree(node);
+		return NULL;
+	}
+	return node;
+}  /* mrp_alloc_node */
+
+static void mrp_free_node(struct mrp_node *node)
+{
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %p\n", __func__, node->data, node);
+#endif
+	kfree(node->data);
+	kfree(node);
+}  /* mrp_free_node */
+
+static struct mrp_node *mrp_get_mac_info(struct mrp_node_anchor *list,
+	u8 *addr, u16 fid)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info data;
+
+	data.fid = fid;
+	memcpy(data.addr, addr, ETH_ALEN);
+	node = mrp_find_node(list, cmp_mac, &data);
+	if (!node) {
+		struct mrp_mac_info *info;
+
+		node = mrp_alloc_node(sizeof(struct mrp_mac_info));
+		if (!node)
+			return NULL;
+		info = node->data;
+		info->fid = fid;
+		memcpy(info->addr, data.addr, ETH_ALEN);
+		mrp_insert_node(list, cmp_mac, node);
+	}
+	return node;
+}  /* mrp_get_mac_info */
+
+static struct mrp_node *mrp_get_vlan_info(struct mrp_node_anchor *list,
+	u16 vid, u8 *addr)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info data;
+
+	data.vid = vid;
+	if (addr)
+		memcpy(data.addr, addr, ETH_ALEN);
+	else
+		memset(data.addr, 0xff, ETH_ALEN);
+	node = mrp_find_node(list, cmp_vlan, &data);
+	if (!node) {
+		struct mrp_vlan_info *info;
+
+		node = mrp_alloc_node(sizeof(struct mrp_vlan_info));
+		if (!node)
+			return NULL;
+		info = node->data;
+		info->vid = vid;
+		memcpy(info->addr, data.addr, ETH_ALEN);
+		mrp_insert_node(list, cmp_vlan, node);
+	}
+	return node;
+}  /* mrp_get_vlan_info */
+
+#if 0
+static u16 mrp_find_vlan_ports(struct mrp_node_anchor *list, u16 vid,
+	u8 *index, u16 *fid)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *info;
+	u16 ports = 0;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		info = next->data;
+		if (vid == info->vid && !(info->ports & SRP_PORT_IGNORE)) {
+			ports |= info->ports;
+			if (info->ports) {
+				if (index && *index != info->index)
+					*index = info->index;
+				if (fid && *fid != info->fid)
+					*fid = info->fid;
+			}
+		} else if (vid < info->vid)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	return ports;
+}  /* mrp_find_vlan_ports */
+#endif
+
+static struct mrp_report *mrp_create_report(struct SRP_reserv *reserv,
+	u8 port)
+{
+	struct mrp_report *attrib;
+
+	attrib = kzalloc(sizeof(struct mrp_report), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p\n", __func__, attrib);
+#endif
+	if (attrib) {
+		attrib->attrib = reserv;
+		attrib->port = port;
+	}
+	return attrib;
+}  /* mrp_create_report */
+
+static void add_attrib_report(struct mrp_info *mrp, void *ptr, u8 action,
+	u8 type, u8 port)
+{
+	struct mrp_report *attrib;
+
+	attrib = mrp_create_report(ptr, port);
+	if (!attrib)
+		return;
+
+	attrib->action = action;
+	attrib->type = type;
+
+	if (mrp->report_tail)
+		mrp->report_tail->next = attrib;
+	mrp->report_tail = attrib;
+	if (!mrp->report_head)
+		mrp->report_head = attrib;
+}  /* add_attrib_report */
+
+static u8 mrp_alloc_mac(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	return sw->ops->alloc_mac(sw);
+}  /* mrp_alloc_mac */
+
+static void mrp_free_mac(struct mrp_info *mrp, u8 index)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	sw->ops->free_mac(sw, index);
+}  /* mrp_free_mac */
+
+static u8 mrp_alloc_vlan(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	return sw->ops->alloc_vlan(sw);
+}  /* mrp_alloc_vlan */
+
+static void mrp_free_vlan(struct mrp_info *mrp, u8 index)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	sw->ops->free_vlan(sw, index);
+}  /* mrp_free_vlan */
+
+static u16 mrp_alloc_fid(struct mrp_info *mrp, u16 vid)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	return sw->ops->alloc_fid(sw, vid);
+}  /* mrp_alloc_fid */
+
+static void mrp_free_fid(struct mrp_info *mrp, u16 fid)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	sw->ops->free_fid(sw, fid);
+}  /* mrp_free_fid */
+
+#ifdef CONFIG_KSZ_MSRP
+static u16 mrp_get_fid(struct mrp_info *mrp, u16 vid, u8 *addr)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info data;
+	u16 fid = 0;
+
+	data.vid = vid;
+#if 0
+	memcpy(data.addr, addr, ETH_ALEN);
+#else
+	memset(data.addr, 0xFF, ETH_ALEN);
+#endif
+	node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (node) {
+		struct mrp_vlan_info *vlan = node->data;
+
+		fid = vlan->fid;
+	}
+	return fid;
+}  /* mrp_get_fid */
+#endif
+
+static int proc_mrp_lv(struct mrp_info *mrp, struct mrp_node *node, u16 ports,
+	u16 *tx_ports, u8 type)
+{
+	int n;
+	int result = DEV_IOC_OK;
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan)
+#endif
+dbg_msg(" %s %x %x\n", __func__, ports, *tx_ports);
+	if (!ports) {
+
+		/* Ask all ports to withdraw the declaration. */
+		for (n = 0; n <= mrp->ports; n++) {
+			if (*tx_ports & (1 << n)) {
+				add_attrib_report(mrp, node,
+					MRP_ACTION_LV, type, n);
+			}
+		}
+		*tx_ports = 0;
+	} else {
+		int uninitialized_var(q);
+		int cnt = 0;
+
+		for (n = 0; n <= mrp->ports; n++)
+			if (ports & (1 << n)) {
+				q = n;
+				cnt++;
+			}
+		if (1 == cnt) {
+			*tx_ports &= ~(1 << q);
+			add_attrib_report(mrp, node, MRP_ACTION_LV, type, q);
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv */
+
+static int proc_mrp_lv_mac(struct mrp_info *mrp, u8 port,
+	struct MRP_mac *mac)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info data;
+	struct mrp_mac_info *info;
+	u16 mrp_ports;
+	u16 ports;
+	int result = DEV_IOC_OK;
+
+dbg_msg(" %s p:%d %02x:%02x:%02x:%02x:%02x:%02x\n", __func__, port,
+	mac->addr[0],
+	mac->addr[1],
+	mac->addr[2],
+	mac->addr[3],
+	mac->addr[4],
+	mac->addr[5]);
+
+	data.fid = 0;
+	memcpy(data.addr, mac->addr, ETH_ALEN);
+	node = mrp_find_node(&mrp->mac_list, cmp_mac, &data);
+	if (!node)
+		return DEV_IOC_INVALID_CMD;
+
+	info = node->data;
+	mrp_ports = info->mrp_ports;
+	ports = info->ports;
+	info->rx_ports &= ~(1 << get_actual_port(mrp, port));
+	info->mrp_ports &= ~(1 << port);
+	info->ports = info->mrp_ports | info->srp_ports;
+
+	/* There is no change in port membership. */
+	if (mrp_ports == info->mrp_ports)
+		return result;
+	if (ports != info->ports) {
+		mrp_cfg_dest_addr(mrp, info->index, info->addr, info->ports,
+			info->fid);
+		if (!info->ports) {
+			mrp_free_mac(mrp, info->index);
+			if (mrp->no_report)
+				mrp_delete_node(&mrp->mac_list, cmp_mac, node);
+			else
+				mrp_remove_node(&mrp->mac_list, cmp_mac, node);
+		}
+	}
+	if (!mrp->no_report)
+		result = proc_mrp_lv(mrp, node, info->mrp_ports,
+			&info->tx_ports, MRP_TYPE_MAC);
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	return result;
+}  /* proc_mrp_lv_mac */
+
+static int proc_mrp_rx_mac(struct mrp_info *mrp, u8 port,
+	struct MRP_mac *mac, u8 new_decl)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info *info;
+	u16 mrp_ports;
+	u16 ports;
+	int n;
+	int q;
+	int result = DEV_IOC_OK;
+
+dbg_msg(" %s p:%d %d %02x:%02x:%02x:%02x:%02x:%02x\n", __func__, port,
+	new_decl,
+	mac->addr[0],
+	mac->addr[1],
+	mac->addr[2],
+	mac->addr[3],
+	mac->addr[4],
+	mac->addr[5]);
+
+	node = mrp_get_mac_info(&mrp->mac_list, mac->addr, 0);
+	if (!node)
+		return -ENOMEM;
+	info = node->data;
+	info->rx_ports |= (1 << get_actual_port(mrp, port));
+	info->srp_ports &= ~SRP_PORT_BLACKLIST;
+	info->ports &= ~SRP_PORT_BLACKLIST;
+	mrp_ports = info->mrp_ports;
+	ports = info->ports;
+	if (new_decl & 0x80)
+		new_decl &= ~0x80;
+	else
+		info->mrp_ports |= (1 << port);
+	info->ports = info->mrp_ports | info->srp_ports;
+
+	/* There is change in port membership. */
+	if (mrp_ports != info->mrp_ports) {
+
+		/* First time setting up MAC table. */
+		if (!ports) {
+			info->index = mrp_alloc_mac(mrp);
+			if (!info->index) {
+				mrp_delete_node(&mrp->mac_list, cmp_mac, node);
+				return -ENOMEM;
+			}
+		}
+		mrp_cfg_dest_addr(mrp, info->index, info->addr, info->ports,
+			info->fid);
+	}
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	if (mrp->no_report)
+		return result;
+
+	if (!new_decl) {
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		q = get_actual_port(mrp, port);
+		if (sw->ops->get_tcDetected(sw, q))
+			new_decl = true;
+	}
+
+	/* Ask all other ports to declare the attribute. */
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_actual_port(mrp, n);
+		if (n != port && (mrp->tx_ports & (1 << q)) &&
+		    (mrp->mmrp_tx_ports & (1 << q))) {
+			u8 action = MRP_ACTION_TX;
+			int existed = info->tx_ports & (1 << n);
+
+			/* Need to update declaration. */
+			if (new_decl) {
+				action = MRP_ACTION_TX_NEW;
+				existed = false;
+			}
+
+			/* Attribute was declared. */
+			if (existed)
+				continue;
+			info->tx_ports |= (1 << n);
+
+			add_attrib_report(mrp, node, action,
+				MRP_TYPE_MAC, n);
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_rx_mac */
+
+static int proc_mrp_lv_vlan(struct mrp_info *mrp, u8 port,
+	struct MRP_vlan *vlan)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct mrp_vlan_info data;
+	u16 ports;
+	int result = DEV_IOC_OK;
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan || vlan->id == 2 || vlan->id > 4090)
+dbg_msg(" %s p:%d %d\n", __func__, port, vlan->id);
+#endif
+	data.vid = vlan->id;
+	memset(data.addr, 0xff, ETH_ALEN);
+	node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (!node)
+		return DEV_IOC_INVALID_CMD;
+
+	info = node->data;
+	ports = info->ports;
+	info->rx_ports &= ~BIT(port);
+	info->ports &= ~(1 << port);
+
+	/* There is no change in port membership. */
+	if (ports == info->ports)
+		return result;
+#if 0
+	ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL, NULL);
+#endif
+	ports = info->ports;
+	mrp_cfg_vlan(mrp, info->index, info->vid, info->fid, ports);
+	if (!ports) {
+		mrp_free_fid(mrp, info->fid);
+		mrp_free_vlan(mrp, info->index);
+	}
+	if (!info->ports) {
+		if (mrp->no_report)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan, node);
+		else
+			mrp_remove_node(&mrp->vlan_list, cmp_vlan, node);
+#ifdef DEBUG_MVRP
+		if (mrp->vlan_list.cnt < 3)
+			dbg_mrp_vlan = 1;
+#endif
+	}
+
+	if (!mrp->no_report)
+		result = proc_mrp_lv(mrp, node, info->ports,
+			&info->tx_ports, MRP_TYPE_VLAN);
+#ifdef DEBUG_MVRP
+#ifdef DEBUG
+if (dbg_mrp_vlan)
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+#endif
+	return result;
+}  /* proc_mrp_lv_vlan */
+
+static int proc_mrp_rx_vlan(struct mrp_info *mrp, u8 port,
+	struct MRP_vlan *vlan, u8 new_decl)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct ksz_port_cfg *cfg;
+	u16 ports;
+	int bit;
+	int index;
+	int n;
+	int q;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan || vlan->id == 2 || vlan->id > 4090)
+dbg_msg(" %s p:%d %x %d\n", __func__, port, new_decl, vlan->id);
+#endif
+	node = mrp_get_vlan_info(&mrp->vlan_list, vlan->id, NULL);
+	if (!node)
+		return -ENOMEM;
+	info = node->data;
+	ports = info->ports;
+	info->rx_ports |= BIT(port);
+
+	/* MRP.c.10.1.5b
+	 * Non-participant does not send out declarations, but received
+	 * declaration is propagated, and traffic is not forwarded.
+	 */
+	if (new_decl & 0x80)
+		new_decl &= ~0x80;
+	else
+		info->ports |= (1 << port);
+
+	/* MVRP.c
+	 *
+	 */
+	q = get_actual_port(mrp, port);
+	cfg = &sw->info->port_cfg[q];
+	index = vlan->id / VID_IN_DATA;
+	bit = vlan->id % VID_IN_DATA;
+	if (cfg->restricted) {
+		if (!(sw->info->vid[index] & (1 << bit)))
+			return result;
+	}
+
+	/* There is change in port membership. */
+	if (ports != info->ports) {
+#if 0
+		info->ports |= SRP_PORT_IGNORE;
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vlan->id,
+			&info->index, &info->fid);
+		info->ports &= ~SRP_PORT_IGNORE;
+#endif
+
+		/* First time setting up VLAN table. */
+		if (!ports) {
+			info->index = mrp_alloc_vlan(mrp);
+			if (!info->index) {
+				mrp_delete_node(&mrp->vlan_list, cmp_vlan,
+					node);
+				return -ENOMEM;
+			}
+			info->fid = mrp_alloc_fid(mrp, info->vid);
+		}
+		ports |= info->ports;
+		mrp_cfg_vlan(mrp, info->index, info->vid, info->fid, ports);
+	}
+#ifdef DEBUG_MVRP
+#ifdef DEBUG
+if (dbg_mrp_vlan)
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+	if (mrp->vlan_list.cnt > 4) {
+if (dbg_mrp_vlan)
+dbg_msg(" stop dbg vlan\n");
+		dbg_mrp_vlan = 0;
+	}
+#endif
+#endif
+	if (mrp->no_report)
+		return result;
+
+#if 1
+	if (mrp->rx_ports) {
+
+if (mrp->rx_ports != sw->rx_ports[0])
+dbg_msg(" rx: %x %x\n", mrp->rx_ports, sw->rx_ports[0]);
+	}
+#endif
+	if (!(mrp->rx_ports & (1 << q)))
+		return result;
+
+	if (!new_decl) {
+		if (sw->ops->get_tcDetected(sw, q))
+			new_decl = true;
+	} else if (new_decl != 1)
+		return result;
+
+	/* Ask all other ports to declare the attribute. */
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_actual_port(mrp, n);
+		if (n != port && (mrp->tx_ports & (1 << q)) &&
+		    (mrp->mvrp_tx_ports & (1 << q))) {
+			u8 action = MRP_ACTION_TX;
+			int existed = info->tx_ports & (1 << n);
+
+			/* Need to update declaration. */
+			if (new_decl) {
+				action = MRP_ACTION_TX_NEW;
+				existed = false;
+			}
+
+			/* Attribute was declared. */
+			if (existed)
+				continue;
+			info->tx_ports |= (1 << n);
+
+			add_attrib_report(mrp, node, action,
+				MRP_TYPE_VLAN, n);
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_rx_vlan */
+
+#ifdef CONFIG_KSZ_MSRP
+#define RFC_NO_RESOURCES_BIT		BIT(0)
+#define RFC_LATENCY_CHANGED_BIT		BIT(1)
+#define RFC_FIRSTVALUE_CHANGED_BIT	BIT(2)
+#define RFC_MAXFRAMESIZE_BIT		BIT(3)
+#define RFC_ASCAPABLE_BIT		BIT(4)
+#define RFC_SRP_BOUNDARY_BIT		BIT(5)
+#define RFC_PRIORITY_BIT		BIT(6)
+#define RFC_NO_BANDWIDTH_BIT		BIT(7)
+#define RFC_NO_BANDWIDTH_TC_BIT		BIT(8)
+#define RFC_STREAM_ID_BIT		BIT(9)
+#define RFC_DEST_ADDR_BIT		BIT(10)
+#define RFC_PREEMPTED_BIT		BIT(11)
+#define RFC_SR_MISMATCHED_BIT		BIT(12)
+#define RFC_VLAN_TAGGING_BIT		BIT(13)
+#define RFC_CHECK_BIT			BIT(31)
+
+static u8 msrp_failure_code(u32 code_bits)
+{
+	u8 code = RFC_NO_ERROR;
+
+	if (code_bits & RFC_NO_RESOURCES_BIT)
+		code = RFC_NO_RESOURCES;
+	else if (code_bits & RFC_LATENCY_CHANGED_BIT)
+		code = RFC_LATENCY_CHANGED;
+	else if (code_bits & RFC_FIRSTVALUE_CHANGED_BIT)
+		code = RFC_FIRSTVALUE_CHANGED;
+	else if (code_bits & RFC_MAXFRAMESIZE_BIT)
+		code = RFC_MAXFRAMESIZE_TOO_LARGE;
+	else if (code_bits & (RFC_ASCAPABLE_BIT | RFC_SRP_BOUNDARY_BIT))
+		code = RFC_PORT_IS_NOT_AVB;
+	else if (code_bits & RFC_PRIORITY_BIT)
+		code = RFC_PRIORITY_IS_NOT_SR_CLASS;
+	else if (code_bits & RFC_NO_BANDWIDTH_BIT)
+		code = RFC_NO_BANDWIDTH;
+	else if (code_bits & RFC_NO_BANDWIDTH_TC_BIT)
+		code = RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS;
+	else if (code_bits & RFC_STREAM_ID_BIT)
+		code = RFC_STREAM_ID_USED;
+	else if (code_bits & RFC_DEST_ADDR_BIT)
+		code = RFC_DEST_ADDR_USED;
+	else if (code_bits & RFC_PREEMPTED_BIT)
+		code = RFC_PREEMPTED_BY_RANK;
+	else if (code_bits & RFC_SR_MISMATCHED_BIT)
+		code = RFC_SR_CLASS_PRIORITY_MISMATCHED;
+	else if (code_bits & RFC_VLAN_TAGGING_BIT)
+		code = RFC_VLAN_TAGGING_DISABLED;
+	return code;
+}  /* msrp_failure_code */
+
+static void proc_mrp_attribute(struct mrp_info *mrp, u8 *data);
+
+static void mrp_set_delta(struct mrp_info *mrp, u8 port, u32 A, u32 B)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+
+	cmd->action = MRP_ACTION_DELTA;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = port;
+	cmd->new_decl = 0;
+	cmd->data.data[0] = A;
+	cmd->data.data[1] = B;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_set_delta */
+
+static void mrp_set_speed(struct mrp_info *mrp, u8 port, u32 speed, bool duplex)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+
+	cmd->action = MRP_ACTION_SPEED;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = port;
+	cmd->new_decl = 0;
+	cmd->data.data[0] = speed;
+	cmd->data.data[1] = duplex;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_set_speed */
+
+static void mrp_chk_talker(struct mrp_info *mrp, u8 port)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+
+	cmd->action = MRP_ACTION_CHK_TALKER;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = port;
+	cmd->new_decl = 0;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_chk_talker */
+
+static void mrp_chk_registered(struct mrp_info *mrp, u8 port)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+
+	cmd->action = MRP_ACTION_CHK_REG;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = port;
+	cmd->new_decl = 0;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_chk_registered */
+
+static u32 calculate_max_bandwidth(u32 speed, u32 percent)
+{
+	u64 bandwidth;
+	u32 rem;
+
+	bandwidth = speed;
+	bandwidth *= percent;
+	bandwidth *= NETWORK_SPEED_IN_MBIT;
+	bandwidth = div_u64_rem(bandwidth, 100, &rem);
+	return (u32) bandwidth;
+}  /* calculate_max_bandwidth */
+
+static int srp_update_mac(struct mrp_info *mrp, u8 *addr, u16 fid, u16 ports,
+	int up)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_node *node;
+	struct mrp_mac_info *mac;
+	struct mrp_mac_info *update;
+
+	node = mrp_get_mac_info(&mrp->mac_list, addr, fid);
+	if (!node)
+		return -ENOMEM;
+	mac = node->data;
+	mac->srp_ports &= ~SRP_PORT_BLACKLIST;
+	if (up) {
+		/* Forward in port. */
+		mac->srp_ports |= ports;
+		ports = mac->srp_ports | mac->mrp_ports;
+		list = &mrp->mac_up;
+	} else {
+		/* Filter in port. */
+		mac->srp_ports &= ~ports;
+		ports = mac->ports & ~ports;
+		list = &mrp->mac_down;
+	}
+	mac->ports = mac->srp_ports | mac->mrp_ports;
+#ifdef DEBUG_MRP_BASIC
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	node = mrp_get_mac_info(list, addr, fid);
+	if (!node)
+		return -ENOMEM;
+	update = node->data;
+	update->ports = ports;
+	update->index = mac->index;
+	return 0;
+}  /* srp_update_mac */
+
+#if 0
+static int srp_update_vlan(struct mrp_info *mrp, u16 vid, u8 *addr, u16 ports,
+	int up)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_node *node;
+	struct mrp_vlan_info *vlan;
+	struct mrp_vlan_info *update;
+
+	node = mrp_get_vlan_info(&mrp->vlan_list, vid, addr);
+	if (!node)
+		return -ENOMEM;
+	vlan = node->data;
+	if (up) {
+		/* Forward in port. */
+		vlan->ports |= ports;
+		list = &mrp->vlan_up;
+	} else {
+		/* Filter in port. */
+		vlan->ports &= ~ports;
+		list = &mrp->vlan_down;
+	}
+#ifdef DEBUG_MRP_BASIC
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	node = mrp_get_vlan_info(list, vid, addr);
+	if (!node)
+		return -ENOMEM;
+	update = node->data;
+	update->fid = vlan->fid;
+	update->ports = vlan->ports;
+	return 0;
+}  /* srp_update_vlan */
+#endif
+
+static int stream_iter(struct mrp_node_anchor *list, void *param[],
+		       int (*oper)(struct mrp_node *prev,
+				   struct srp_stream_info *data,
+				   void *param[]))
+{
+	int rc;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		rc = oper(prev, next->data, param);
+
+		/* return ok or some other codes */
+		if (rc != -EAGAIN)
+			return rc;
+
+		prev = next;
+		next = prev->next;
+	}
+	return -EAGAIN;
+}
+
+static int stream_oper(struct mrp_node_anchor *list, void *param[],
+		       int (*oper)(struct mrp_node *prev,
+				   struct srp_stream_info *data,
+				   void *param[]))
+{
+	int rc;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		rc = oper(prev, next->data, param);
+
+		/* no more operation */
+		if (!rc)
+			return 0;
+
+		/* stream not removed */
+		if (rc == -EAGAIN)
+			prev = next;
+		next = prev->next;
+	}
+	return -EAGAIN;
+}
+
+static void mrp_set_traffic(struct mrp_info *mrp, struct SRP_stream *stream,
+			    uint port, bool open)
+{
+	u16 fid;
+	u16 ports;
+	int result;
+
+	ports = 1 << port;
+	fid = mrp_get_fid(mrp, stream->vlan_id, stream->dest);
+	result = srp_update_mac(mrp, stream->dest, fid, ports, open);
+#if 0
+	if (!result)
+		result = srp_update_vlan(mrp, stream->vlan_id, stream->dest,
+					 ports, open);
+#endif
+}
+
+static int stream_drop(struct mrp_node *prev,
+		       struct srp_stream_info *data, void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_port_info *info = param[1];
+	struct mrp_traffic_info *traffic = param[2];
+	int *port = param[3];
+	struct SRP_reserv *t_reserv = param[4];
+	int *active = param[5];
+
+	reserv = data->reserv;
+	if (reserv == t_reserv) {
+		if (*active) {
+			u32 bandwidth;
+
+			stream = reserv->stream;
+			bandwidth = stream->bandwidth;
+
+			traffic->bandwidth_used -= bandwidth;
+			traffic->bandwidth_left = traffic->bandwidth_max -
+				traffic->bandwidth_used;
+			if (traffic->bandwidth_other)
+				*traffic->bandwidth_other =
+					*traffic->bandwidth_avail +
+					traffic->bandwidth_left;
+			info->bandwidth_used -= bandwidth;
+			info->bandwidth_left = info->bandwidth_max -
+				info->bandwidth_used;
+
+			mrp_set_traffic(mrp, stream, *port, false);
+		}
+
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		mrp_free_node(next);
+
+		/* stop doing next in the list */
+		return 0;
+	}
+	return -EAGAIN;
+}
+
+static bool drop_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			struct SRP_reserv *reserv, int active)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_traffic_info *traffic;
+	int rc;
+	int tc;
+	void *param[6];
+	bool result = false;
+
+	tc = get_traffic_class(mrp, reserv->stream->priority);
+	traffic = get_traffic_info(info, tc);
+	if (active)
+		list = &traffic->active;
+	else
+		list = &traffic->passive;
+	param[0] = mrp;
+	param[1] = info;
+	param[2] = traffic;
+	param[3] = &port;
+	param[4] = reserv;
+	param[5] = &active;
+	rc = stream_oper(list, param, stream_drop);
+	if (!rc)
+		result = true;
+
+	return result;
+}  /* drop_reserv */
+
+static void srp_cfg_mac(struct mrp_info *mrp, struct mrp_node_anchor *list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		prev->next = next->next;
+
+		mac = next->data;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+
+		mrp_free_node(next);
+
+		next = prev->next;
+	}
+	list->last = &list->anchor;
+}  /* srp_cfg_mac */
+
+#if 0
+static void srp_cfg_vlan(struct mrp_info *mrp, struct mrp_node_anchor *list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *vlan;
+	u16 ports;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		prev->next = next->next;
+
+		vlan = next->data;
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vlan->vid,
+			&vlan->index, &vlan->fid);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+
+		mrp_free_node(next);
+
+		next = prev->next;
+	}
+	list->last = &list->anchor;
+}  /* srp_cfg_vlan */
+#endif
+
+static int stream_chk_size(struct mrp_node *prev,
+			   struct srp_stream_info *data, void *param[])
+{
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_traffic_info *traffic = param[0];
+
+	reserv = data->reserv;
+	stream = reserv->stream;
+	if (stream->MaxFrameSize > traffic->max_frame_size)
+		traffic->max_frame_size = stream->MaxFrameSize;
+	return -EAGAIN;
+}
+
+static void srp_cfg_reserv(struct mrp_info *mrp, struct mrp_port_info *port)
+{
+	u8 n;
+	u32 max_size;
+	int tc;
+	struct mrp_port_info *info;
+	struct mrp_traffic_info *traffic;
+	void *param[6];
+
+	srp_cfg_mac(mrp, &mrp->mac_down);
+#if 0
+	srp_cfg_vlan(mrp, &mrp->vlan_down);
+#endif
+	for (n = 0; n <= mrp->ports; n++) {
+		info = get_port_info(mrp, n);
+		if (port && info != port)
+			continue;
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		if (!info->link)
+			continue;
+		for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+			traffic = get_traffic_info(info, tc);
+
+			/* Find out the maximum frame size. */
+			max_size = traffic->max_frame_size;
+			traffic->max_frame_size = 0;
+			param[0] = traffic;
+			stream_iter(&traffic->active, param, stream_chk_size);
+			if (max_size && !traffic->max_frame_size)
+				traffic->max_frame_size = max_size;
+			if (max_size != traffic->max_frame_size) {
+dbg_msg("  max size: %u %u\n", max_size, traffic->max_frame_size);
+				/* Make sure credit settings will be updated. */
+				if (traffic->bandwidth_set)
+					traffic->bandwidth_set = 1;
+			}
+			if (traffic->bandwidth_used !=
+			    traffic->bandwidth_set) {
+				srp_cfg_credit_shaper(mrp, n, info, traffic);
+#if 0
+				srp_cfg_credit_shaper(mrp, 4, info, traffic);
+#endif
+				traffic->bandwidth_set =
+					traffic->bandwidth_used;
+			}
+		}
+	}
+	srp_cfg_mac(mrp, &mrp->mac_up);
+#if 0
+	srp_cfg_vlan(mrp, &mrp->vlan_up);
+#endif
+}  /* srp_cfg_reserv */
+
+static struct mrp_node *create_stream_info(struct mrp_info *mrp,
+					   struct SRP_reserv *t_reserv)
+{
+	struct mrp_node *node;
+	struct srp_stream_info *info;
+
+	node = mrp_alloc_node(sizeof(struct srp_stream_info));
+	if (!node)
+		return NULL;
+	info = node->data;
+	prepare_stream_info(t_reserv, info);
+	return node;
+}  /* create_stream_info */
+
+static void add_reserv(struct mrp_traffic_info *traffic, int active,
+		       struct mrp_node *node)
+{
+	struct mrp_node_anchor *list;
+	int (*cmp)(void *a, void *b);
+
+	if (active) {
+		list = &traffic->active;
+		cmp = cmp_lower_stream;
+	} else {
+		list = &traffic->passive;
+		cmp = cmp_higher_stream;
+	}
+	mrp_insert_node(list, cmp, node);
+}  /* add_reserv */
+
+static struct SRP_reserv *mrp_link_listener(struct mrp_info *mrp,
+					    struct mrp_port_info *info,
+					    struct SRP_stream *stream)
+{
+	struct SRP_reserv *reserv;
+
+	/* Create linked listener declaration. */
+	reserv = srp_find_reserv(&info->declared, stream->id,
+				 SRP_LISTENER);
+	if (!reserv) {
+
+		/* Listener declaration is not known yet. */
+		reserv = srp_create_reserv(stream->id, SRP_LISTENER,
+					   0, 0, mrp->id, 0);
+		if (!reserv)
+			return NULL;
+
+		reserv->stream = stream;
+		srp_insert_reserv(&info->declared, reserv);
+	}
+	reserv->stream = stream;
+	return reserv;
+}  /* mrp_link_listener */
+
+static int mrp_update_listener(struct mrp_info *mrp,
+			       struct SRP_reserv *l_reserv,
+			       struct SRP_stream *stream, u8 new_decl)
+{
+	int declaration;
+	int n;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *t_reserv;
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+	int active = mrp->status.msrpEnabledStatus &&
+		     info->status.msrpPortEnabledStatus;
+
+	info = get_port_info(mrp, stream->in_port);
+	if (!l_reserv)
+		l_reserv = srp_find_reserv(&info->declared, stream->id,
+					   SRP_LISTENER);
+	if (!l_reserv) {
+dbg_msg("  ! L decl not found: %d\n", stream->in_port);
+	}
+	if (!l_reserv)
+		return result;
+
+	declaration = 0;
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == stream->in_port)
+			continue;
+		info = get_port_info(mrp, n);
+		active = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!active)
+			continue;
+		reserv = srp_find_reserv(&info->registered, stream->id,
+					 SRP_LISTENER);
+		t_reserv = srp_find_reserv(&info->declared, stream->id,
+					 SRP_TALKER);
+		if (reserv && t_reserv) {
+			if (SRP_FAILED == t_reserv->declaration)
+				declaration |= SRP_ASKING_FAILED_SCALE;
+			else
+				declaration |= (1 << reserv->declaration);
+		} else if (reserv) {
+dbg_msg("  no t_reserv?\n");
+				declaration |= SRP_ASKING_FAILED_SCALE;
+		}
+	}
+
+	/* info will need to be initialized. */
+
+	/* No more listeners. */
+	if (!declaration) {
+		int free = true;
+
+#ifdef DEBUG_MSRP
+dbg_msg("listener leaving\n");
+#endif
+		info = get_port_info(mrp, stream->in_port);
+		active = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!mrp->no_report && active) {
+			free = false;
+			add_attrib_report(mrp, l_reserv, MRP_ACTION_LV,
+				MRP_TYPE_LISTENER, stream->in_port);
+			result = DEV_IOC_MRP_REPORT;
+		}
+		srp_remove_reserv(l_reserv, free);
+		return result;
+	}
+
+	if (declaration > SRP_READY_SCALE)
+		declaration = SRP_READY_FAILED;
+	else if (declaration == SRP_READY_SCALE)
+		declaration = SRP_READY;
+	else
+		declaration = SRP_ASKING_FAILED;
+
+#if 1
+	if (fqtss_34_2_1b_hack &&
+	    declaration == l_reserv->declaration && declaration == SRP_READY) {
+		l_reserv->declaration = SRP_READY_FAILED;
+		new_decl = true;
+	}
+#endif
+
+	/* Declaration is different. */
+	if (declaration != l_reserv->declaration) {
+		l_reserv->declaration = declaration;
+#ifdef DEBUG_MSRP
+dbg_msg("l dec: %d=%d\n", stream->in_port, declaration);
+#endif
+		info = get_port_info(mrp, stream->in_port);
+		active = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!mrp->no_report && active) {
+			u8 action = MRP_ACTION_TX;
+
+			if (new_decl)
+				action = MRP_ACTION_TX_NEW;
+			add_attrib_report(mrp, l_reserv, action,
+				MRP_TYPE_LISTENER, stream->in_port);
+
+			/* Ask application to retrieve attributes. */
+			result = DEV_IOC_MRP_REPORT;
+		}
+	}
+	return result;
+}  /* mrp_update_listener */
+
+static int stream_clr_mark(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	int *chk = param[0];
+
+	if (*chk && data->mark)
+dbg_msg("  mark !!\n");
+	data->mark = false;
+	return -EAGAIN;
+}
+
+static int stream_set_mark(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	data->mark = true;
+	return -EAGAIN;
+}
+
+static int stream_chk_bandwidth(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	int cmp;
+	struct srp_stream_info *b = param[0];
+	u32 *avail = param[1];
+	u32 *required = param[2];
+	int *one_stream = param[3];
+	char bw_str1[20];
+	char bw_str2[20];
+	struct SRP_reserv *reserv;
+
+	cmp = cmp_lower_stream(b, data);
+#if 0
+dbg_msg(" cmp %d  %d:%d %llx:%llx %02x:%02x\n", cmp,
+b->rank, data->rank, b->age, data->age,
+b->id[7], data->id[7]);
+#endif
+
+	/* stream has higher priority. */
+	if (cmp < 0)
+		return -ENXIO;
+	if (cmp == 0)
+		return 0;
+	if (!data->mark) {
+		reserv = data->reserv;
+		if (*one_stream) {
+			*avail = reserv->stream->bandwidth;
+		} else {
+			*avail += reserv->stream->bandwidth;
+			data->mark = true;
+		}
+		format_num(bw_str1, *avail);
+		format_num(bw_str2, *required);
+dbg_msg("  avail: %s %s\n", bw_str1, bw_str2);
+		if (*avail >= *required) {
+
+			/* in case it is not marked above */
+			data->mark = true;
+			return 0;
+		}
+	}
+	return -EAGAIN;
+}
+
+static int stream_decr_bandwidth(struct mrp_node *prev,
+				 struct srp_stream_info *data, void *param[])
+{
+	u32 bandwidth;
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info = param[0];
+	struct mrp_traffic_info *traffic = param[1];
+	int *mark = param[2];
+
+	if (traffic->bandwidth_used > traffic->bandwidth_max) {
+		reserv = data->reserv;
+		bandwidth = reserv->stream->bandwidth;
+		reserv->code_bits = RFC_NO_BANDWIDTH_TC_BIT;
+		if (info->bandwidth_used > info->bandwidth_max)
+dbg_msg(" no: %u %u\n", info->bandwidth_used, info->bandwidth_max);
+		if (info->bandwidth_used > info->bandwidth_max)
+			reserv->code_bits |= RFC_NO_BANDWIDTH_BIT;
+		traffic->bandwidth_used -= bandwidth;
+		info->bandwidth_used -= bandwidth;
+		data->mark = true;
+		*mark = true;
+	}
+	return -EAGAIN;
+}
+
+static int stream_cmp(struct mrp_node *prev, struct srp_stream_info *data,
+		      void *param[])
+{
+	struct SRP_reserv *reserv = param[0];
+
+	if (data->reserv == reserv) {
+		data->mark = true;
+		return 0;
+	}
+	return -EAGAIN;
+}
+
+static int stream_stop(struct mrp_node *prev, struct srp_stream_info *data,
+		       void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_traffic_info *traffic = param[1];
+	int *port = param[2];
+	int result;
+
+	if (data->mark) {
+		u8 decl;
+
+		data->mark = false;
+
+		reserv = data->reserv;
+		decl = reserv->declaration;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		if (reserv->code)
+			reserv->declaration = SRP_FAILED;
+
+		stream = reserv->stream;
+		mrp_set_traffic(mrp, stream, *port, false);
+
+		if (!mrp->no_report && decl != reserv->declaration) {
+dbg_msg("  no band stop: %x\n", reserv->code_bits);
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+		}
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		next->next = NULL;
+
+		add_reserv(traffic, false, next);
+
+		return -ENODEV;
+	}
+	return -EAGAIN;
+}
+
+static int stream_reset(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	struct SRP_reserv *reserv;
+
+	reserv = data->reserv;
+	if (reserv->code_bits &
+	    ~(RFC_NO_BANDWIDTH_BIT | RFC_NO_BANDWIDTH_TC_BIT |
+	     RFC_PREEMPTED_BIT))
+		return -EAGAIN;
+	reserv->pair->code = RFC_NO_ERROR;
+	return -EAGAIN;
+}  /* stream_reset */
+
+static int stream_start(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_port_info *info = param[1];
+	struct mrp_traffic_info *traffic = param[2];
+	int *port = param[3];
+	int result;
+	u32 bandwidth;
+	bool adv = false;
+
+	reserv = data->reserv;
+	if (reserv->code_bits &
+	    ~(RFC_NO_BANDWIDTH_BIT | RFC_NO_BANDWIDTH_TC_BIT |
+	     RFC_PREEMPTED_BIT) || reserv->pair->code != RFC_NO_ERROR)
+{
+#if 0
+dbg_msg(" other error: %x %d\n", reserv->code_bits, reserv->pair->code);
+#endif
+		return -EAGAIN;
+}
+	bandwidth = reserv->stream->bandwidth;
+	traffic->bandwidth_left = traffic->bandwidth_max -
+		traffic->bandwidth_used;
+	if (bandwidth <= traffic->bandwidth_left) {
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		next->next = NULL;
+
+		add_reserv(traffic, true, next);
+
+		traffic->bandwidth_used += bandwidth;
+		info->bandwidth_used += bandwidth;
+
+		if (data->mark) {
+			data->mark = false;
+			return -ENODEV;
+		}
+
+		if (reserv->declaration != SRP_ADVERTISE) {
+			reserv->declaration = SRP_ADVERTISE;
+			reserv->code_bits = 0;
+			reserv->code = RFC_NO_ERROR;
+			adv = true;
+		}
+
+		stream = reserv->stream;
+		mrp_set_traffic(mrp, stream, *port, true);
+
+		if (!mrp->no_report && adv) {
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+		}
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+		return -ENODEV;
+	} else if (data->mark) {
+dbg_msg(" no bandwidth for this\n");
+		data->mark = false;
+
+		reserv->declaration = SRP_FAILED;
+		reserv->code_bits = RFC_NO_BANDWIDTH_TC_BIT;
+		info->bandwidth_left = info->bandwidth_max -
+			info->bandwidth_used;
+		if (bandwidth > info->bandwidth_left)
+			reserv->code_bits |= RFC_NO_BANDWIDTH_BIT;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+
+		stream = reserv->stream;
+		mrp_set_traffic(mrp, stream, *port, false);
+
+		if (!mrp->no_report) {
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+		}
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+	} else if (bandwidth <= info->bandwidth_left &&
+		   (reserv->code_bits & RFC_NO_BANDWIDTH_BIT)) {
+		reserv->code_bits &= ~RFC_NO_BANDWIDTH_BIT;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		if (!mrp->no_report) {
+dbg_msg("  no tc\n");
+			add_attrib_report(mrp, reserv,
+					  MRP_ACTION_TX,
+					  MRP_TYPE_TALKER, *port);
+		}
+	} else if (bandwidth > info->bandwidth_left &&
+		   !(reserv->code_bits & RFC_NO_BANDWIDTH_BIT)) {
+		reserv->code_bits |= RFC_NO_BANDWIDTH_BIT;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		if (!mrp->no_report) {
+dbg_msg("  no bw\n");
+			add_attrib_report(mrp, reserv,
+					  MRP_ACTION_TX,
+					  MRP_TYPE_TALKER, *port);
+		}
+	} else if (reserv->pair && reserv->pair->code == RFC_NO_ERROR) {
+		reserv->pair->code = RFC_NO_BANDWIDTH;
+	}
+	return -EAGAIN;
+}
+
+static int stream_drop_other(struct mrp_node *prev,
+			     struct srp_stream_info *data, void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_traffic_info *traffic = param[1];
+	int *port = param[2];
+	int *avail = param[3];
+	int result;
+
+dbg_msg("%s %p %p\n", __func__, param, mrp);
+	if (data->mark) {
+		u32 bandwidth;
+
+		data->mark = false;
+
+		reserv = data->reserv;
+		stream = reserv->stream;
+
+		bandwidth = stream->bandwidth;
+		*avail += bandwidth;
+
+		traffic->bandwidth_left += bandwidth;
+		traffic->bandwidth_used -= bandwidth;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other =
+				*traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+
+		reserv->declaration = SRP_FAILED;
+		reserv->code_bits = RFC_PREEMPTED_BIT;
+		reserv->code = RFC_PREEMPTED_BY_RANK;
+
+		mrp_set_traffic(mrp, stream, *port, false);
+
+		if (!mrp->no_report)
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		next->next = NULL;
+
+		add_reserv(traffic, false, next);
+
+		return -ENODEV;
+	}
+	return -EAGAIN;
+}
+
+static bool have_bandwidth(struct mrp_port_info *info,
+			   struct mrp_traffic_info *traffic,
+			   struct SRP_reserv *t_reserv,
+			   u32 required_bandwidth)
+{
+	u32 avail_bandwidth = 0;
+	int cmp;
+	int one;
+	void *param[6];
+	struct srp_stream_info b;
+	int rc;
+
+	prepare_stream_info(t_reserv, &b);
+
+	/* Make sure stream age is latest for correct comparison. */
+	if (!b.age)
+		b.age = get_stream_age(info);
+
+	cmp = 1;
+	param[0] = &cmp;
+	rc = stream_iter(&traffic->active, param, stream_clr_mark);
+
+	param[0] = &b;
+	param[1] = &avail_bandwidth;
+	param[2] = &required_bandwidth;
+	one = 1;
+	param[3] = &one;
+	rc = stream_iter(&traffic->active, param, stream_chk_bandwidth);
+	if (!rc)
+		return true;
+
+	param[0] = &b;
+	param[1] = &avail_bandwidth;
+	param[2] = &required_bandwidth;
+	one = 0;
+	param[3] = &one;
+	rc = stream_iter(&traffic->active, param, stream_chk_bandwidth);
+	if (!rc)
+		return true;
+
+dbg_msg(" no bandwidth %s %u %u\n", __func__, required_bandwidth, info->bandwidth_left);
+
+	/* No bandwidth at all for the new stream. */
+	return false;
+}  /* have_bandwidth */
+
+static bool drop_other_reserv(struct mrp_info *mrp, u8 port,
+			      struct mrp_port_info *info,
+			      struct mrp_traffic_info *traffic,
+			      struct SRP_reserv *t_reserv,
+			      u32 required_bandwidth, bool drop)
+{
+	u32 avail_bandwidth = 0;
+	void *param[6];
+	int rc;
+	bool ok;
+
+	ok = have_bandwidth(info, traffic, t_reserv, required_bandwidth);
+	if (!ok) {
+		return false;
+	} else if (!drop) {
+		int cmp = 0;
+
+		/* reset marked streams */
+		param[0] = &cmp;
+		rc = stream_iter(&traffic->active, param, stream_clr_mark);
+		return true;
+	}
+
+	param[0] = mrp;
+	param[1] = traffic;
+	param[2] = &port;
+	param[3] = &avail_bandwidth;
+dbg_msg(" stream_drop_other %p %p\n", param, mrp);
+	rc = stream_oper(&traffic->active, param, stream_drop_other);
+
+	/* Increase bandwidth */
+	info->bandwidth_left += avail_bandwidth;
+	info->bandwidth_used -= avail_bandwidth;
+	return true;
+}  /* drop_other_reserv */
+
+static bool drop_active_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			struct mrp_traffic_info *traffic,
+			struct SRP_reserv *reserv)
+{
+	int rc;
+	void *param[6];
+	bool drop = false;
+
+dbg_msg("%s\n", __func__);
+	param[0] = reserv;
+	rc = stream_iter(&traffic->active, param, stream_cmp);
+
+	/* A stream was marked for stopping.*/
+	if (!rc) {
+		u32 bandwidth = reserv->stream->bandwidth;
+
+		traffic->bandwidth_used -= bandwidth;
+		traffic->bandwidth_left += bandwidth;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other = *traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+		info->bandwidth_used -= bandwidth;
+		info->bandwidth_left += bandwidth;
+		param[0] = mrp;
+		param[1] = traffic;
+		param[2] = &port;
+		rc = stream_oper(&traffic->active, param, stream_stop);
+		drop = true;
+	}
+
+	return drop;
+}
+
+static void stop_active_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			struct mrp_traffic_info *traffic)
+{
+	int rc;
+	void *param[6];
+	bool mark = false;
+
+	param[0] = info;
+	if (traffic->bandwidth_used > traffic->bandwidth_max) {
+		param[1] = traffic;
+		param[2] = &mark;
+		rc = stream_iter(&traffic->active, param,
+			stream_decr_bandwidth);
+		traffic->bandwidth_left = traffic->bandwidth_max -
+			traffic->bandwidth_used;
+
+		/* Increase Class B available bandwidth. */
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other =
+				*traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+		info->bandwidth_left = info->bandwidth_max -
+			info->bandwidth_used;
+	}
+
+	/* A stream was marked for stopping.*/
+	if (mark) {
+		param[0] = mrp;
+		param[1] = traffic;
+		param[2] = &port;
+		rc = stream_oper(&traffic->active, param, stream_stop);
+	}
+}  /* stop_active_reserv */
+
+static void start_passive_reserv(struct mrp_info *mrp, u8 port,
+				 struct mrp_port_info *info)
+{
+	int rc;
+	int tc;
+	struct mrp_traffic_info *traffic;
+	void *param[6];
+
+	param[0] = mrp;
+	param[1] = info;
+	param[3] = &port;
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		param[2] = traffic;
+		rc = stream_oper(&traffic->passive, param, stream_start);
+		traffic->bandwidth_left = traffic->bandwidth_max -
+			traffic->bandwidth_used;
+
+		/* Decrease Class B available bandwidth. */
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other =
+				*traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+	}
+	info->bandwidth_left = info->bandwidth_max - info->bandwidth_used;
+}  /* start_passive_reserv */
+
+static void mrp_combine_list(struct mrp_node_anchor *old_list,
+	struct mrp_node_anchor *new_list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &old_list->anchor;
+	next = prev->next;
+	while (next) {
+
+		/* Remove node from list. */
+		prev->next = next->next;
+		next->next = NULL;
+		mrp_insert_node(new_list, cmp_higher_stream, next);
+		next = prev->next;
+	}
+}  /* mrp_combine_list */
+
+static void chk_passive_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info)
+{
+	int rc;
+	int tc;
+	struct mrp_traffic_info *traffic;
+	void *param[6];
+
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		rc = stream_iter(&traffic->active, param, stream_set_mark);
+		rc = stream_iter(&traffic->passive, param, stream_reset);
+		traffic->bandwidth_used = 0;
+		mrp_combine_list(&traffic->active, &traffic->passive);
+#if 0
+		mrp_show_node(&traffic->passive, show_stream_info);
+#endif
+	}
+	info->bandwidth_used = 0;
+	start_passive_reserv(mrp, port, info);
+}
+
+static u32 chk_avail_tx_bandwidth(struct mrp_info *mrp, u8 port,
+				  struct mrp_port_info *info, int tc,
+				  struct SRP_reserv *t_reserv, bool drop)
+{
+	u32 bandwidth;
+	struct mrp_traffic_info *traffic;
+	char bw[20];
+	char bw_max[20];
+	char bw_left[20];
+	char tbw_max[20];
+	char tbw_left[20];
+	u32 code_bits = 0;
+	struct SRP_stream *stream = t_reserv->stream;
+
+	traffic = get_traffic_info(info, tc);
+	traffic->bandwidth_left = traffic->bandwidth_max -
+		traffic->bandwidth_used;
+	bandwidth = stream->bandwidth;
+	format_num(bw, (u32)bandwidth);
+	format_num(bw_max, info->bandwidth_max);
+	format_num(bw_left, info->bandwidth_left);
+	format_num(tbw_max, traffic->bandwidth_max);
+	format_num(tbw_left, traffic->bandwidth_left);
+#if 0
+dbg_msg("bw:%u  %s  %s  %s  %s\n", port, bw, bw_left, tbw_left, tbw_max);
+#endif
+
+	if (bandwidth > traffic->bandwidth_left)
+		code_bits |= RFC_NO_BANDWIDTH_TC_BIT;
+	if (bandwidth > info->bandwidth_left)
+		code_bits |= RFC_NO_BANDWIDTH_BIT;
+
+	/* See if other streams can be dropped to accommodate this one. */
+	if (code_bits && bandwidth <= traffic->bandwidth_max) {
+		bandwidth -= traffic->bandwidth_left;
+		if (drop_other_reserv(mrp, port, info, traffic, t_reserv,
+				      bandwidth, drop))
+			code_bits = 0;
+		else if (tc == SR_CLASS_A &&
+			 !(code_bits & RFC_NO_BANDWIDTH_TC_BIT)) {
+
+			/* Class A will have bandwidth from Class B. */
+			code_bits = 0;
+			if (drop) {
+				u32 left;
+				u32 used;
+				struct mrp_traffic_info *traffic_b;
+
+				traffic_b = get_traffic_info(info, SR_CLASS_B);
+				used = traffic->bandwidth_used + bandwidth;
+				left = traffic->bandwidth_max - used;
+				traffic_b->bandwidth_max =
+					traffic_b->bandwidth_delta + left;
+				stop_active_reserv(mrp, port, info, traffic_b);
+			}
+		}
+	}
+
+	return code_bits;
+}  /* chk_avail_tx_bandwidth */
+
+static int chk_avail_bandwidth(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			       struct SRP_reserv *t_reserv)
+{
+	u32 bandwidth;
+	int tc;
+	struct mrp_traffic_info *traffic;
+	int result;
+	struct SRP_stream *stream = t_reserv->stream;
+
+	tc = get_traffic_class(mrp, stream->priority);
+	traffic = get_traffic_info(info, tc);
+	bandwidth = stream->bandwidth;
+	t_reserv->code_bits = chk_avail_tx_bandwidth(mrp, port, info, tc,
+						     t_reserv, true);
+	t_reserv->code = msrp_failure_code(t_reserv->code_bits);
+
+	if (RFC_NO_ERROR == t_reserv->code) {
+		struct mrp_node *active;
+		u16 fid;
+		u16 ports = 1 << port;
+
+		/* Reduce bandwidth */
+		info->bandwidth_left -= bandwidth;
+		info->bandwidth_used += bandwidth;
+		traffic->bandwidth_left -= bandwidth;
+		traffic->bandwidth_used += bandwidth;
+		if (traffic->bandwidth_other) {
+			*traffic->bandwidth_other -= bandwidth;
+		}
+		if (stream->MaxFrameSize > traffic->max_frame_size)
+			traffic->max_frame_size = stream->MaxFrameSize;
+
+		active = create_stream_info(mrp, t_reserv);
+		if (!active)
+			return SRP_ASKING_FAILED;
+
+		add_reserv(traffic, true, active);
+
+		fid = mrp_get_fid(mrp, stream->vlan_id, stream->dest);
+		result = srp_update_mac(mrp, stream->dest, fid, ports, true);
+		if (result)
+			return SRP_ASKING_FAILED;
+#if 0
+		result = srp_update_vlan(mrp,
+			stream->vlan_id, stream->dest, ports, true);
+#endif
+		return SRP_READY;
+	} else {
+		struct mrp_node *passive;
+
+		t_reserv->declaration = SRP_FAILED;
+dbg_msg("no bw: %d\n", t_reserv->code);
+		if (!mrp->no_report)
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, port);
+		passive = create_stream_info(mrp, t_reserv);
+		if (passive)
+			add_reserv(traffic, false, passive);
+	}
+	return SRP_ASKING_FAILED;
+}  /* chk_avail_bandwidth */
+
+static void chk_talker_decl(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info)
+{
+	struct SRP_reserv *reserv;
+
+	reserv = info->declared.next;
+	while (reserv) {
+		/* talker not paired with a listener */
+		if (reserv->direction == SRP_TALKER && !reserv->pair &&
+		    ((reserv->code && (reserv->code_bits &
+		    (RFC_NO_BANDWIDTH_BIT | RFC_NO_BANDWIDTH_TC_BIT))) ||
+		    !reserv->code)) {
+			int tc;
+			u8 code = reserv->code;
+
+			tc = get_traffic_class(mrp, reserv->stream->priority);
+			reserv->code_bits =
+				chk_avail_tx_bandwidth(mrp, port, info, tc,
+						       reserv, false);
+			reserv->code = msrp_failure_code(reserv->code_bits);
+			if (code != reserv->code) {
+				if (reserv->code)
+					reserv->declaration = SRP_FAILED;
+				else
+					reserv->declaration = SRP_ADVERTISE;
+				add_attrib_report(mrp, reserv,
+					MRP_ACTION_TX_NEW, MRP_TYPE_TALKER,
+					port);
+			}
+		}
+		reserv = reserv->next;
+	}
+}  /* chk_talker_decl */
+
+static int update_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info)
+{
+	struct mrp_traffic_info *traffic;
+	int result = DEV_IOC_OK;
+	int tc;
+#ifdef DEBUG
+	char bw_used[20];
+	char bw_remain[20];
+#endif
+
+	if (!mrp->status.msrpEnabledStatus)
+		return result;
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		stop_active_reserv(mrp, port, info, traffic);
+#ifdef DEBUG
+		format_num(bw_used, traffic->bandwidth_used);
+		format_num(bw_remain, traffic->bandwidth_max);
+dbg_msg("used %d: %s %s\n", tc, bw_used, bw_remain);
+#endif
+	}
+	chk_passive_reserv(mrp, port, info);
+
+#ifdef DEBUG
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+dbg_msg("  active:\n");
+		mrp_show_node(&traffic->active, show_stream_info);
+dbg_msg("  passive:\n");
+		mrp_show_node(&traffic->passive, show_stream_info);
+		format_num(bw_used, traffic->bandwidth_used);
+dbg_msg("used %d: %s\n", tc, bw_used);
+	}
+#endif
+
+	srp_cfg_reserv(mrp, info);
+
+	chk_talker_decl(mrp, port, info);
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* update_reserv */
+
+static int proc_mrp_tx_listener(struct mrp_info *mrp, u8 port,
+				struct mrp_port_info *info,
+				struct SRP_reserv *l_reserv,
+				struct SRP_reserv *t_reserv,
+				struct SRP_stream *stream)
+{
+	struct mrp_traffic_info *traffic;
+	int tc;
+	int result = DEV_IOC_OK;
+	u8 declaration = l_reserv->declaration;
+
+	/* Talker is not ready. */
+	if (SRP_FAILED == stream->t_reserv->declaration) {
+dbg_msg("talker not ready\n");
+		declaration = SRP_ASKING_FAILED;
+	} else {
+
+		/* Find talker reservation if already created. */
+		if (!t_reserv)
+			t_reserv = srp_find_reserv(&info->declared,
+						   l_reserv->id, SRP_TALKER);
+		if (!t_reserv) {
+/* talker reservation may not be created yet because of link down. */
+dbg_msg("  no t_reserv\n");
+			declaration = SRP_ASKING_FAILED;
+		} else {
+			if (SRP_FAILED == t_reserv->declaration)
+				declaration = SRP_ASKING_FAILED;
+#ifdef DEBUG_MSRP_
+dbg_msg(" asked: %d\n", declaration);
+#endif
+			if (!t_reserv->streamAge) {
+				t_reserv->streamAge = get_stream_age(info);
+				t_reserv->pair = l_reserv;
+				l_reserv->pair = t_reserv;
+			}
+		}
+	}
+#ifdef DEBUG_MSRP
+	if (t_reserv)
+dbg_msg("  T:%d L:%d D:%d\n", t_reserv->code, l_reserv->code, declaration);
+	else
+dbg_msg("  L:%d D:%d\n", l_reserv->code, declaration);
+#endif
+
+	tc = get_traffic_class(mrp, stream->priority);
+	traffic = get_traffic_info(info, tc);
+
+	/* Check available bandwidth. */
+	if (declaration != SRP_ASKING_FAILED) {
+#ifdef DEBUG_MSRP_
+dbg_msg("not ask failed: %d\n", l_reserv->code);
+#endif
+
+		/* Actual reservation is not made yet. */
+		if (RFC_NO_RESOURCES == l_reserv->code) {
+#ifdef DEBUG_MSRP_
+dbg_msg("not yet\n");
+#endif
+			declaration = chk_avail_bandwidth(mrp, port, info,
+				t_reserv);
+			l_reserv->code = RFC_NO_BANDWIDTH;
+			if (declaration != SRP_ASKING_FAILED) {
+				l_reserv->code = RFC_NO_ERROR;
+				srp_cfg_reserv(mrp, info);
+				chk_talker_decl(mrp, port, info);
+			}
+		} else {
+			l_reserv->code = RFC_NO_ERROR;
+			start_passive_reserv(mrp, port, info);
+			srp_cfg_reserv(mrp, info);
+			chk_talker_decl(mrp, port, info);
+		}
+	} else if (RFC_NO_RESOURCES != l_reserv->code) {
+
+dbg_msg(" no lis: %d\n", l_reserv->code);
+		/* Listener is no longer ready to receive stream. */
+		if (l_reserv->code == RFC_NO_ERROR &&
+		    drop_active_reserv(mrp, port, info, traffic, t_reserv)) {
+
+			/* Used to indicate reservation is not done yet. */
+			l_reserv->code = RFC_NO_BANDWIDTH;
+			start_passive_reserv(mrp, port, info);
+			srp_cfg_reserv(mrp, info);
+			chk_talker_decl(mrp, port, info);
+		}
+	} else if (t_reserv) {
+		struct mrp_node *passive;
+
+		l_reserv->code = RFC_NO_BANDWIDTH;
+#ifdef DEBUG_MSRP
+dbg_msg("add pas\n");
+#endif
+		passive = create_stream_info(mrp, t_reserv);
+		if (passive)
+			add_reserv(traffic, false, passive);
+	}
+
+	/* Used to find out whether to send declaration back to talker. */
+	mrp->listeners++;
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_tx_listener */
+
+static int mrp_delete_listener(struct mrp_info *mrp, u8 nport,
+	struct mrp_port_info *info, struct SRP_reserv *l_reserv)
+{
+	struct SRP_reserv *t_reserv;
+	bool drop = false;
+	int result = DEV_IOC_OK;
+	int active = mrp->status.msrpEnabledStatus &&
+		     info->status.msrpPortEnabledStatus;
+
+	/* Check if a talker reservation is using bandwidth. */
+	t_reserv = srp_find_reserv(&info->declared, l_reserv->id, SRP_TALKER);
+	if (!t_reserv) {
+		struct SRP_stream *stream = l_reserv->stream;
+
+		/* Delete listener reservation when leaving. */
+		srp_remove_reserv(l_reserv, true);
+		if (stream)
+			result = mrp_update_listener(mrp, NULL,
+				stream, false);
+		return result;
+	}
+
+	if (t_reserv->declaration == SRP_ADVERTISE &&
+	    l_reserv->code == RFC_NO_ERROR) {
+		drop = drop_reserv(mrp, nport, info, t_reserv, true);
+	}
+	if (drop) {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop active\n");
+#endif
+#if 1
+		if (active)
+		start_passive_reserv(mrp, nport, info);
+#endif
+		srp_cfg_reserv(mrp, info);
+#if 1
+		if (active)
+		chk_talker_decl(mrp, nport, info);
+#endif
+	} else {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop passive\n");
+#endif
+		drop_reserv(mrp, nport, info, t_reserv, false);
+	}
+
+#if 0
+	/* Remember the listener state. */
+	declaration = l_reserv->declaration;
+#endif
+
+	t_reserv->pair = NULL;
+
+	/* Delete listener reservation when leaving. */
+	srp_remove_reserv(l_reserv, true);
+
+	/* Update the listener attribute reported back to talker. */
+	result = mrp_update_listener(mrp, NULL, t_reserv->stream, false);
+
+	/* Listener reservation is not using bandwidth if not ready. */
+
+	/* no reservation */
+	t_reserv->streamAge = 0;
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* mrp_delete_listener */
+
+static int proc_mrp_lv_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_listener *listener)
+{
+	struct SRP_reserv *l_reserv;
+	struct SRP_reserv *t_reserv;
+	struct mrp_port_info *info;
+	int declaration;
+	int result = DEV_IOC_OK;
+	bool drop = false;
+
+	info = get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x s:%u\n", __func__,
+port,
+listener->id[0],
+listener->id[1],
+listener->id[2],
+listener->id[3],
+listener->id[4],
+listener->id[5],
+listener->id[6],
+listener->id[7],
+listener->substate);
+#endif
+	l_reserv = srp_find_reserv(&info->registered, listener->id,
+				   SRP_LISTENER);
+	if (!l_reserv)
+{
+dbg_msg("  ! not found: %p %d\n", info, port);
+chk_reserv(info, port);
+}
+	if (!l_reserv)
+		return result;
+
+	/* Check if a talker reservation is using bandwidth. */
+	t_reserv = srp_find_reserv(&info->declared, listener->id, SRP_TALKER);
+	if (!t_reserv) {
+		struct SRP_stream *stream = l_reserv->stream;
+
+		/* Delete listener reservation when leaving. */
+		srp_remove_reserv(l_reserv, true);
+		if (stream)
+			result = mrp_update_listener(mrp, NULL,
+				stream, false);
+		return result;
+	}
+
+	if (t_reserv->declaration == SRP_ADVERTISE &&
+	    l_reserv->code == RFC_NO_ERROR) {
+		drop = drop_reserv(mrp, port, info, t_reserv, true);
+	}
+	if (drop) {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop active\n");
+#endif
+		start_passive_reserv(mrp, port, info);
+		srp_cfg_reserv(mrp, info);
+		chk_talker_decl(mrp, port, info);
+	} else {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop passive\n");
+#endif
+		drop_reserv(mrp, port, info, t_reserv, false);
+	}
+
+	/* Remember the listener state. */
+	declaration = l_reserv->declaration;
+
+	t_reserv->pair = NULL;
+
+	/* Delete listener reservation when leaving. */
+	srp_remove_reserv(l_reserv, true);
+
+	/* Update the listener attribute reported back to talker. */
+	result = mrp_update_listener(mrp, NULL, t_reserv->stream, false);
+
+	/* Listener reservation is not using bandwidth if not ready. */
+
+	/* no reservation */
+	t_reserv->streamAge = 0;
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv_listener */
+
+static int proc_mrp_rx_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_listener *listener, u8 new_decl)
+{
+	int result = DEV_IOC_OK;
+	u8 declaration = SRP_READY;
+	struct SRP_reserv *l_reserv;
+	struct SRP_stream *stream;
+	struct mrp_port_info *info;
+
+	info = get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x s:%u\n", __func__,
+port, new_decl,
+listener->id[0],
+listener->id[1],
+listener->id[2],
+listener->id[3],
+listener->id[4],
+listener->id[5],
+listener->id[6],
+listener->id[7],
+listener->substate);
+#endif
+	if (SRP_IGNORED == listener->substate)
+		return DEV_IOC_INVALID_CMD;
+
+	declaration = listener->substate;
+	stream = srp_find_stream_id(mrp, listener->id);
+
+	/* Create reservation if new. */
+	l_reserv = srp_find_reserv(&info->registered, listener->id,
+				   SRP_LISTENER);
+	if (!l_reserv) {
+		l_reserv = srp_create_reserv(listener->id, SRP_LISTENER,
+					     declaration, 0, mrp->id,
+					     RFC_NO_RESOURCES);
+		if (!l_reserv)
+			return -ENOMEM;
+
+		srp_insert_reserv(&info->registered, l_reserv);
+	} else {
+		l_reserv->declaration = declaration;
+	}
+
+	/* Stream may not exist yet when listener is received. */
+	l_reserv->stream = stream;
+
+	/* There is a talker. */
+	if (stream && stream->t_reserv && stream->in_port != port) {
+		int rc;
+		struct SRP_reserv *reserv;
+
+		mrp->listeners = 0;
+		result = proc_mrp_tx_listener(mrp, port, info, l_reserv, NULL,
+			stream);
+
+		/* Likely due to errors. */
+		if (!mrp->listeners)
+			return result;
+
+		info = get_port_info(mrp, stream->in_port);
+		reserv = mrp_link_listener(mrp, info, stream);
+		if (!reserv)
+			return -ENOMEM;
+
+		if (!new_decl) {
+			int q;
+			struct ksz_sw *sw =
+				container_of(mrp, struct ksz_sw, mrp);
+
+			q = get_actual_port(mrp, port);
+			if (sw->ops->get_tcDetected(sw, q))
+				new_decl = true;
+		}
+
+		/* Listener declaration will be updated in following call. */
+		rc = mrp_update_listener(mrp, reserv, stream, new_decl);
+		if (DEV_IOC_OK != rc)
+			result = rc;
+	}
+else {
+if (!stream)
+dbg_msg("no talker stream?\n");
+else if (!stream->t_reserv)
+dbg_msg("no talker reserv?\n");
+else
+dbg_msg("wrong port: %d %d\n", port, stream->in_port);
+}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_rx_listener */
+
+static u32 mrp_chk_as_port(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info, int tc)
+{
+	int avb_a;
+	int avb_b;
+	int asCapable;
+	int i;
+	u32 code_bits = 0;
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	cfg = &sw->info->port_cfg[get_actual_port(mrp, port)];
+	avb_a = cfg->avb_a;
+	avb_b = cfg->avb_b;
+	asCapable = cfg->asCapable | cfg->asCapable_set;
+	if (!asCapable)
+		code_bits |= RFC_ASCAPABLE_BIT;
+	if (tc != SR_CLASS_A && tc != SR_CLASS_B)
+		code_bits |= RFC_PRIORITY_BIT;
+	for (i = SR_CLASS_A; i >= SR_CLASS_B; i--) {
+		if (i != tc)
+			continue;
+		if (mrp->domain[tc - SR_CLASS_B].id &&
+		    info->priority[tc].SRPdomainBoundaryPort)
+			code_bits |= RFC_SRP_BOUNDARY_BIT;
+	}
+
+#if 1
+	if (fqtss_34_2_1b_hack && port < 3)
+		code_bits = 0;
+#endif
+
+	/* need at least 100 Mbps and full-duplex */
+	if (!info->duplex)
+		code_bits |= RFC_ASCAPABLE_BIT;
+	if (tc == SR_CLASS_A && mrp->domain[1].id)
+		cfg->avb_a = code_bits ? false : true;
+	else if (tc == SR_CLASS_B)
+		cfg->avb_b = code_bits ? false : true;
+if (port < 3 && (avb_a != cfg->avb_a || avb_b != cfg->avb_b))
+dbg_msg("  avb: %d=%d:%d %x\n", port, cfg->avb_a, cfg->avb_b, code_bits);
+	return code_bits;
+}  /* mrp_chk_as_port */
+
+static void mrp_chk_rx_talker(struct mrp_info *mrp, u8 port,
+				struct mrp_port_info *info,
+				  struct SRP_talker *talker,
+				  struct SRP_reserv *reserv)
+{
+	u8 code = talker->FailureCode;
+	u8 new_code = reserv->code;
+	u32 code_bits = 0;
+	const u8 *bridge_id = talker->bridge_id;
+	struct SRP_stream *stream_id = NULL;
+
+	stream_id = srp_find_stream_id(mrp, talker->id);
+
+	/* Stream should alway be found unless out of resource. */
+	if (!stream_id)
+		code_bits |= RFC_NO_RESOURCES_BIT;
+	if (memcmp(stream_id->dest, talker->dest, ETH_ALEN) &&
+	    (stream_id->MaxFrameSize != talker->MaxFrameSize ||
+	    stream_id->MaxIntervalFrames != talker->MaxIntervalFrames)) {
+dbg_msg(" used by other: %x\n", code_bits);
+		code_bits |= RFC_STREAM_ID_BIT;
+		code_bits |= RFC_FIRSTVALUE_CHANGED_BIT;
+	}
+	if (code_bits)
+		goto get_talker_done;
+
+	/* Check FirstValue. */
+	if (stream_id->latency != talker->AccumulatedLatency)
+{
+dbg_msg(" L: %x %x\n", stream_id->latency, talker->AccumulatedLatency);
+		code_bits |= RFC_LATENCY_CHANGED_BIT;
+}
+#if 1
+	if (msrp_35_1_14g_hack)
+		talker->reserved = 0;
+#endif
+	if (memcmp(stream_id->dest, talker->dest, ETH_ALEN) ||
+	    stream_id->vlan_id != talker->vlan_id ||
+	    stream_id->MaxFrameSize != talker->MaxFrameSize ||
+	    stream_id->MaxIntervalFrames != talker->MaxIntervalFrames ||
+	    stream_id->priority != talker->priority ||
+	    stream_id->rank != talker->rank ||
+	    stream_id->reserved != talker->reserved) {
+		code_bits |= RFC_FIRSTVALUE_CHANGED_BIT;
+		goto get_talker_done;
+	}
+
+	/* Use original code from talker. */
+	if (code)
+		goto get_talker_done;
+
+	code_bits |= mrp_chk_as_port(mrp, port, info,
+		get_traffic_class(mrp, talker->priority));
+	if (talker->MaxFrameSize > 1500)
+		code_bits |= RFC_MAXFRAMESIZE_BIT;
+	if (!talker->MaxIntervalFrames)
+		code_bits |= RFC_NO_BANDWIDTH_BIT;
+
+get_talker_done:
+	new_code = msrp_failure_code(code_bits);
+#ifdef DEBUG_MSRP
+if (port < 3 && new_code)
+dbg_msg(" new_code rx: %04x %d %d\n", code_bits, code, new_code);
+#endif
+
+	/* This bridge finds a problem. */
+	if (!code && new_code != code) {
+		bridge_id = mrp->id;
+		code = new_code;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+	}
+	reserv->code = code;
+	reserv->code_bits = code_bits;
+}  /* mrp_chk_rx_talker */
+
+static void mrp_chk_tx_talker(struct mrp_info *mrp, u8 port,
+				struct mrp_port_info *info,
+				  struct SRP_reserv *reserv)
+{
+	int tc;
+	u8 code = reserv->code;
+	u8 new_code = reserv->code;
+	u32 code_bits = 0;
+	const u8 *bridge_id = reserv->bridge_id;
+
+	tc = get_traffic_class(mrp, reserv->stream->priority);
+	code_bits |= mrp_chk_as_port(mrp, port, info, tc);
+	if (code || code_bits)
+		goto chk_tx_talker_done;
+
+	do {
+		int bit;
+		int index;
+		struct ksz_port_cfg *cfg;
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		cfg = &sw->info->port_cfg[get_actual_port(mrp, port)];
+		index = reserv->stream->vlan_id / VID_IN_DATA;
+		bit = reserv->stream->vlan_id % VID_IN_DATA;
+		if (cfg->untagged[index] & (1 << bit)) {
+			code_bits = RFC_VLAN_TAGGING_BIT;
+			goto chk_tx_talker_done;
+		}
+	} while (0);
+
+	code_bits = chk_avail_tx_bandwidth(mrp, port, info, tc, reserv, false);
+
+chk_tx_talker_done:
+	new_code = msrp_failure_code(code_bits);
+#ifdef DEBUG_MSRP
+if (new_code && port < 3)
+dbg_msg(" new_code tx: %d=%04x %d %d\n", port, code_bits, code, new_code);
+#endif
+
+	/* This bridge finds a problem. */
+	if (new_code != code) {
+		bridge_id = mrp->id;
+		code = new_code;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+	}
+	reserv->code = code;
+	reserv->code_bits = code_bits;
+
+	reserv->declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+
+#if 0
+	/* A listener is paired. */
+	if (reserv->pair) {
+		if (code || reserv->pair->declaration == SRP_ASKING_FAILED)
+			reserv->pair->code = RFC_NO_BANDWIDTH;
+		else
+			reserv->pair->code = RFC_NO_ERROR;
+	}
+#endif
+}  /* mrp_chk_tx_talker */
+
+static u32 mrp_max_latency(struct mrp_info *mrp, struct mrp_port_info *info,
+			int index,
+			struct SRP_talker *talker)
+{
+	u32 MaxPacketSize = 1542;
+	u32 MaxAllocBand = info->bandwidth[3].deltaBandwidth +
+		info->bandwidth[2].deltaBandwidth;
+	u32 portTransmitRate = info->speed;
+	u32 MaxFrameSize = talker->MaxFrameSize;
+	u32 t_Interval = 125000;
+	u64 t_AllStreams;
+	u32 t_MaxPacketSize;
+	u32 t_StreamPacket;
+	u32 t_Mbps = 1;
+	u32 t_IPG = 12;
+	u64 val;
+	u32 rem;
+
+if (!portTransmitRate)
+dbg_msg(" !! %s\n", __func__);
+	if (!portTransmitRate)
+		portTransmitRate = 100;
+if (!MaxAllocBand)
+dbg_msg(" !! %s\n", __func__);
+	if (!MaxAllocBand)
+		MaxAllocBand = 75;
+	if (!index)
+		t_Interval *= 2;
+	if (portTransmitRate != 1000) {
+		t_Mbps *= 10;
+		t_IPG *= 10;
+	}
+	MaxFrameSize += 22 + 8;
+	t_AllStreams = MaxAllocBand * t_Interval;
+	t_StreamPacket = MaxFrameSize * 8 * t_Mbps;
+	t_MaxPacketSize = MaxPacketSize * 8 * t_Mbps;
+	val = t_AllStreams;
+	val = div_u64_rem(val, portTransmitRate, &rem);
+	val -= t_StreamPacket;
+	val -= t_IPG;
+	val *= portTransmitRate;
+	val = div_u64_rem(val, MaxAllocBand, &rem);
+	val += t_StreamPacket;
+	val += t_MaxPacketSize;
+	rem = (u32) val;
+	rem += info->latency[index].portTcMaxLatency;
+	return rem;
+}
+
+static int proc_mrp_tx_talker(struct mrp_info *mrp, u8 port,
+			      struct SRP_talker *talker,
+			      struct SRP_reserv *reserv, u8 new_decl)
+{
+	int index;
+	u8 declaration;
+	u8 code = talker->FailureCode;
+	const u8 *bridge_id = talker->bridge_id;
+	u32 latency = talker->AccumulatedLatency;
+	struct SRP_stream *stream_id = NULL;
+	struct SRP_reserv *t_reserv;
+	struct SRP_reserv *l_reserv;
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+
+	info = get_port_info(mrp, port);
+	stream_id = srp_find_stream_id(mrp, talker->id);
+
+	/* Create reservation if new. */
+	t_reserv = srp_find_reserv(&info->declared, talker->id, SRP_TALKER);
+	code = reserv->code;
+	bridge_id = reserv->bridge_id;
+	declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+	index = get_traffic_index(get_traffic_class(mrp, talker->priority));
+#if 1
+	if (!mrp_10_5_1c_hack)
+#endif
+	latency += mrp_max_latency(mrp, info, index, talker);
+	if (!t_reserv) {
+		if (!code)
+			bridge_id = mrp->id;
+		t_reserv = srp_create_reserv(talker->id, SRP_TALKER,
+					     declaration, latency, bridge_id,
+					     code);
+		if (!t_reserv)
+			return -ENOMEM;
+
+		t_reserv->stream = stream_id;
+		srp_insert_reserv(&info->declared, t_reserv);
+	} else {
+		t_reserv->declaration = declaration;
+		memcpy(t_reserv->bridge_id, bridge_id, 8);
+		t_reserv->code = code;
+		t_reserv->rx_code = code;
+	}
+	t_reserv->code_bits = reserv->code_bits;
+
+	/* Check outgoing failures. */
+	if (!t_reserv->code)
+		mrp_chk_tx_talker(mrp, port, info, t_reserv);
+
+	if (!mrp->no_report) {
+		u8 action = MRP_ACTION_TX;
+
+		if (!new_decl && !code && t_reserv->code)
+			new_decl = true;
+		if (new_decl)
+			action = MRP_ACTION_TX_NEW;
+		add_attrib_report(mrp, t_reserv, action,
+			MRP_TYPE_TALKER, port);
+	}
+
+	l_reserv = srp_find_reserv(&info->registered, talker->id,
+				   SRP_LISTENER);
+	if (l_reserv) {
+		int rc;
+
+dbg_msg("have listen: %d=%p\n", port, l_reserv);
+		rc = proc_mrp_tx_listener(mrp, port, info, l_reserv, t_reserv,
+			t_reserv->stream);
+	}
+
+	/* Ask application to retrieve attributes. */
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_tx_talker */
+
+static int mrp_delete_talker(struct mrp_info *mrp, u8 nport,
+	struct mrp_port_info *info, struct SRP_reserv *reserv)
+{
+	struct SRP_reserv *l_reserv;
+	struct SRP_reserv *t_reserv;
+	u8 n;
+	int result = DEV_IOC_OK;
+	struct mrp_node *mac_node;
+#if 1
+	struct mrp_node *vlan_node;
+	struct mrp_vlan_info data;
+#endif
+	struct mrp_mac_info mac_data;
+	u16 fid = 0;
+	int free = true;
+	int active = mrp->status.msrpEnabledStatus &&
+		     info->status.msrpPortEnabledStatus;
+	int active_other;
+
+	/* No listener propagation if no talker. */
+	l_reserv = srp_find_reserv(&info->declared, reserv->id,
+		SRP_LISTENER);
+	if (l_reserv) {
+		srp_remove_reserv(l_reserv, true);
+	}
+
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == nport)
+			continue;
+
+		info = get_port_info(mrp, n);
+		active_other = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		t_reserv = srp_find_reserv(&info->declared, reserv->id,
+					   SRP_TALKER);
+		if (!t_reserv)
+			continue;
+
+		/* Unlink listener from talker. */
+		l_reserv = srp_find_reserv(&info->registered, reserv->id,
+					   SRP_LISTENER);
+		if (l_reserv) {
+			bool dropped = false;
+
+			l_reserv->pair = NULL;
+
+			/* stream will be deleted */
+			l_reserv->stream = NULL;
+
+			/* Drop reservation if listener is ready. */
+			if (l_reserv->declaration != SRP_ASKING_FAILED) {
+				dropped = drop_reserv(mrp, n, info, t_reserv,
+						      true);
+				if (dropped) {
+if (l_reserv->code != RFC_NO_ERROR)
+dbg_msg(" not ok? %d\n", l_reserv->code);
+#if 1
+					if (active_other)
+					start_passive_reserv(mrp, n, info);
+#endif
+				}
+#ifdef DEBUG_MSRP
+else
+dbg_msg(" not dropped\n");
+#endif
+			}
+			if (!dropped)
+				drop_reserv(mrp, n, info, t_reserv, false);
+			l_reserv->code = RFC_NO_RESOURCES;
+		}
+
+		free = true;
+#if 1
+		/* Reservation will be freed after reporting leaving. */
+		if (!mrp->no_report && active_other) {
+			free = false;
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_LV,
+				MRP_TYPE_TALKER, n);
+		}
+#endif
+		srp_remove_reserv(t_reserv, free);
+	}
+
+	srp_cfg_reserv(mrp, NULL);
+
+	free = true;
+#if 1
+	/* This is purely used to free the stream after reporting. */
+	if (!mrp->no_report && active) {
+		free = false;
+		add_attrib_report(mrp, reserv, MRP_ACTION_LV, MRP_TYPE_TALKER,
+			mrp->ports + 1);
+	}
+#endif
+
+#if 1
+	data.vid = reserv->stream->vlan_id;
+	memcpy(data.addr, reserv->stream->dest, ETH_ALEN);
+	vlan_node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (vlan_node) {
+#if 0
+		u16 ports;
+#endif
+		struct mrp_vlan_info *vlan = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+		vlan = vlan_node->data;
+		fid = vlan->fid;
+		vlan->ports &= SRP_PORT_AVAIL;
+#if 0
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL,
+			NULL);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+		if (!ports) {
+			mrp_free_fid(mrp, vlan->fid);
+			mrp_free_vlan(mrp, vlan->index);
+		}
+
+		/* Nobody is using the VLAN. */
+		if (!vlan->ports)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan, vlan_node);
+#endif
+	} else
+		fid = mrp_alloc_fid(mrp, data.vid);
+#endif
+	mac_data.fid = fid;
+	memcpy(mac_data.addr, reserv->stream->dest, ETH_ALEN);
+	mac_node = mrp_find_node(&mrp->mac_list, cmp_mac, &mac_data);
+#if 1
+	if (!mac_node) {
+dbg_msg(" not found! %d %02x:%02x\n", fid, mac_data.addr[4], mac_data.addr[5]);
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+	}
+#endif
+	if (mac_node) {
+		struct mrp_mac_info *mac = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+		mac = mac_node->data;
+		mac->srp_ports &= SRP_PORT_AVAIL;
+		mac->ports = mac->srp_ports | mac->mrp_ports;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+		if (!mac->ports) {
+			mrp_free_mac(mrp, mac->index);
+			mrp_delete_node(&mrp->mac_list, cmp_mac, mac_node);
+		}
+	}
+	srp_remove_stream(reserv->stream, free);
+	srp_remove_reserv(reserv, free);
+
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == nport)
+			continue;
+
+		info = get_port_info(mrp, n);
+		active_other = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!active_other)
+			continue;
+		chk_talker_decl(mrp, n, info);
+	}
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* mrp_delete_talker */
+
+static int proc_mrp_lv_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker)
+{
+	struct SRP_reserv *l_reserv;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *t_reserv;
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+	int n;
+	struct mrp_node *mac_node;
+#if 1
+	struct mrp_node *vlan_node;
+	struct mrp_vlan_info data;
+#endif
+	struct mrp_mac_info mac_data;
+	u16 fid = 0;
+	int free = true;
+
+	info = get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x\n", __func__,
+port,
+talker->id[0],
+talker->id[1],
+talker->id[2],
+talker->id[3],
+talker->id[4],
+talker->id[5],
+talker->id[6],
+talker->id[7]);
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x %03x %u:%u f:%u i:%u l:%u c:%u\n",
+talker->dest[0],
+talker->dest[1],
+talker->dest[2],
+talker->dest[3],
+talker->dest[4],
+talker->dest[5],
+talker->vlan_id, talker->priority, talker->rank,
+talker->MaxFrameSize, talker->MaxIntervalFrames, talker->AccumulatedLatency,
+talker->FailureCode);
+#endif
+	reserv = srp_find_reserv(&info->registered, talker->id, SRP_TALKER);
+	if (!reserv)
+		return result;
+
+	/* No listener propagation if no talker. */
+	l_reserv = srp_find_reserv(&info->declared, talker->id, SRP_LISTENER);
+	if (l_reserv) {
+		free = false;
+		add_attrib_report(mrp, l_reserv, MRP_ACTION_LV,
+			MRP_TYPE_LISTENER, port);
+		srp_remove_reserv(l_reserv, free);
+		free = true;
+	}
+
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == port)
+			continue;
+
+		info = get_port_info(mrp, n);
+#if 1
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+#endif
+		t_reserv = srp_find_reserv(&info->declared, talker->id,
+					   SRP_TALKER);
+		if (!t_reserv)
+			continue;
+
+		/* Unlink listener from talker. */
+		l_reserv = srp_find_reserv(&info->registered, talker->id,
+					   SRP_LISTENER);
+		if (l_reserv) {
+			bool dropped = false;
+
+			l_reserv->pair = NULL;
+
+			/* stream will be deleted */
+			l_reserv->stream = NULL;
+
+			/* Drop reservation if listener is ready. */
+			if (l_reserv->declaration != SRP_ASKING_FAILED) {
+				dropped = drop_reserv(mrp, n, info, t_reserv,
+						      true);
+				if (dropped) {
+if (l_reserv->code != RFC_NO_ERROR)
+dbg_msg(" not ok? %d\n", l_reserv->code);
+					start_passive_reserv(mrp, n, info);
+				}
+#ifdef DEBUG_MSRP
+else
+dbg_msg(" not dropped\n");
+#endif
+			}
+			if (!dropped)
+				drop_reserv(mrp, n, info, t_reserv, false);
+			l_reserv->code = RFC_NO_RESOURCES;
+		}
+
+		/* Reservation will be freed after reporting leaving. */
+		if (!mrp->no_report) {
+			free = false;
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_LV,
+				MRP_TYPE_TALKER, n);
+		}
+		srp_remove_reserv(t_reserv, free);
+	}
+
+	srp_cfg_reserv(mrp, NULL);
+
+	/* This is purely used to free the stream after reporting. */
+	if (!mrp->no_report) {
+		free = false;
+		add_attrib_report(mrp, reserv, MRP_ACTION_LV, MRP_TYPE_TALKER,
+			mrp->ports + 1);
+	}
+if (memcmp(reserv->stream->dest, talker->dest, ETH_ALEN))
+dbg_msg("  not same dest! %02x:%02x %02x:%02x\n",
+reserv->stream->dest[4], reserv->stream->dest[5],
+talker->dest[4], talker->dest[5]);
+#if 1
+	data.vid = reserv->stream->vlan_id;
+	memcpy(data.addr, reserv->stream->dest, ETH_ALEN);
+	vlan_node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (vlan_node) {
+#if 0
+		u16 ports;
+#endif
+		struct mrp_vlan_info *vlan = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+		vlan = vlan_node->data;
+		fid = vlan->fid;
+		vlan->ports &= SRP_PORT_AVAIL;
+#if 0
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL,
+			NULL);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+		if (!ports) {
+			mrp_free_fid(mrp, vlan->fid);
+			mrp_free_vlan(mrp, vlan->index);
+		}
+
+		/* Nobody is using the VLAN. */
+		if (!vlan->ports)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan, vlan_node);
+#endif
+	} else
+		fid = mrp_alloc_fid(mrp, data.vid);
+#endif
+	mac_data.fid = fid;
+	memcpy(mac_data.addr, reserv->stream->dest, ETH_ALEN);
+	mac_node = mrp_find_node(&mrp->mac_list, cmp_mac, &mac_data);
+#if 1
+	if (!mac_node) {
+dbg_msg(" not found! %d %02x:%02x\n", fid, mac_data.addr[4], mac_data.addr[5]);
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+	}
+#endif
+	if (mac_node) {
+		struct mrp_mac_info *mac = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+		mac = mac_node->data;
+		mac->srp_ports &= SRP_PORT_AVAIL;
+		mac->ports = mac->srp_ports | mac->mrp_ports;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+		if (!mac->ports) {
+			mrp_free_mac(mrp, mac->index);
+			mrp_delete_node(&mrp->mac_list, cmp_mac, mac_node);
+		}
+	}
+	srp_remove_stream(reserv->stream, free);
+	srp_remove_reserv(reserv, free);
+
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == port)
+			continue;
+
+		info = get_port_info(mrp, n);
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		chk_talker_decl(mrp, n, info);
+	}
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv_talker */
+
+static int proc_mrp_rx_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker, u8 new_decl)
+{
+	int n;
+	int result = DEV_IOC_OK;
+	u8 declaration;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_port_info *info;
+	u8 code = talker->FailureCode;
+	u32 code_bits = 0;
+
+	info = get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x\n", __func__,
+port,
+new_decl,
+talker->id[0],
+talker->id[1],
+talker->id[2],
+talker->id[3],
+talker->id[4],
+talker->id[5],
+talker->id[6],
+talker->id[7]);
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x %03x %u:%u f:%u i:%u l:%u c:%u\n",
+talker->dest[0],
+talker->dest[1],
+talker->dest[2],
+talker->dest[3],
+talker->dest[4],
+talker->dest[5],
+talker->vlan_id, talker->priority, talker->rank,
+talker->MaxFrameSize, talker->MaxIntervalFrames, talker->AccumulatedLatency,
+talker->FailureCode);
+#endif
+
+	/* Create stream if new. */
+	stream = srp_find_stream_id(mrp, talker->id);
+	if (!stream) {
+		stream = srp_find_dest_addr(mrp, talker->dest);
+		if (stream)
+			code_bits = RFC_DEST_ADDR_BIT;
+		stream = srp_create_stream(talker->id, talker->dest,
+			talker->vlan_id, talker->MaxFrameSize,
+			talker->MaxIntervalFrames, talker->priority,
+			talker->rank, talker->reserved,
+			talker->AccumulatedLatency);
+		if (stream) {
+#ifdef DEBUG_MSRP
+			char bw_str1[20];
+#endif
+			int tc = get_traffic_class(mrp, stream->priority);
+
+			stream->bandwidth =
+				calculate_bandwidth(stream->MaxFrameSize,
+						    stream->MaxIntervalFrames,
+						    frames_per_sec(tc));
+#ifdef DEBUG_MSRP
+		format_num(bw_str1, stream->bandwidth);
+dbg_msg("bw: %s\n", bw_str1);
+#endif
+			srp_insert_stream_by_id(mrp, stream);
+			srp_insert_stream_by_dest(mrp, stream);
+		}
+	}
+	if (!stream)
+		return -ENOMEM;
+
+	/* Create reservation if new. */
+	reserv = srp_find_reserv(&info->registered, talker->id, SRP_TALKER);
+	declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+	if (!reserv) {
+		struct mrp_node *node;
+		struct mrp_mac_info *mac;
+#if 0
+		struct mrp_vlan_info *vlan;
+		u8 index;
+#endif
+		u16 fid;
+		u16 ports = 1 << port;
+
+		reserv = srp_create_reserv(talker->id, SRP_TALKER,
+					   declaration,
+					   talker->AccumulatedLatency,
+					   talker->bridge_id, code);
+		if (!reserv)
+			return -ENOMEM;
+
+		/* Indicate registration source. */
+		reserv->streamAge = 0;
+		srp_insert_reserv(&info->registered, reserv);
+		stream->in_port = port;
+		stream->t_reserv = reserv;
+
+#if 0
+		/* Setup initial VLAN configuration. */
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, talker->vlan_id,
+			&index, &fid);
+		node = mrp_get_vlan_info(&mrp->vlan_list, talker->vlan_id,
+#if 0
+			talker->dest);
+#else
+			NULL);
+#endif
+		if (!node)
+			return -ENOMEM;
+		vlan = node->data;
+#if 0
+		vlan->ports = SRP_PORT_READY;
+#endif
+		ports = vlan->ports;
+
+		/* First time setting up VLAN table. */
+		if (!ports) {
+			vlan->index = mrp_alloc_vlan(mrp);
+			vlan->fid = mrp_alloc_fid(mrp, vlan->vid);
+#if 1
+			vlan->ports = SRP_PORT_READY;
+#endif
+			mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid,
+				vlan->ports);
+#if 0
+		} else {
+			vlan->index = index;
+			vlan->fid = fid;
+		}
+#endif
+		}
+#endif
+		fid = mrp_alloc_fid(mrp, talker->vlan_id);
+
+		/* Setup initial MAC configuration. */
+		node = mrp_get_mac_info(&mrp->mac_list, talker->dest, fid);
+		if (!node)
+			return -ENOMEM;
+		mac = node->data;
+		ports = mac->ports;
+		mac->rx_ports |= (1 << get_actual_port(mrp, port));
+		mac->srp_ports = SRP_PORT_READY;
+		mac->ports = mac->mrp_ports | mac->srp_ports;
+
+		/* First time setting up MAC table. */
+		if (!ports) {
+			int tc;
+			struct ksz_port_cfg *cfg;
+			struct ksz_sw *sw =
+				container_of(mrp, struct ksz_sw, mrp);
+
+			tc = get_traffic_class(mrp, stream->priority);
+			cfg = &sw->info->port_cfg[get_actual_port(mrp, port)];
+			if ((tc == SR_CLASS_A && cfg->avb_a) ||
+			    (tc == SR_CLASS_B && cfg->avb_b))
+				ports = mac->ports;
+			else
+				ports = SRP_PORT_OTHER;
+			mac->index = mrp_alloc_mac(mrp);
+			mrp_cfg_dest_addr(mrp, mac->index, mac->addr,
+				ports, mac->fid);
+		}
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	} else {
+		reserv->declaration = declaration;
+		reserv->latency = talker->AccumulatedLatency;
+		memcpy(reserv->bridge_id, talker->bridge_id, 8);
+		reserv->code = code;
+		reserv->rx_code = code;
+	}
+	reserv->stream = stream;
+	if (code_bits) {
+		reserv->code_bits = code_bits;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		memcpy(reserv->bridge_id, mrp->id, 8);
+	} else
+
+	/* Check incoming failures. */
+	mrp_chk_rx_talker(mrp, port, info, talker, reserv);
+
+	if (!new_decl) {
+		int q;
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		q = get_actual_port(mrp, port);
+		if (sw->ops->get_tcDetected(sw, q))
+			new_decl = true;
+		if (!new_decl && !talker->FailureCode && reserv->code)
+			new_decl = true;
+	}
+
+	mrp->listeners = 0;
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == port)
+			continue;
+		info = get_port_info(mrp, n);
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		if (!info->link)
+			continue;
+		result = proc_mrp_tx_talker(mrp, n, talker, reserv, new_decl);
+		if (result < 0)
+			return result;
+		reserv->tx_ports |= (1 << n);
+	}
+
+	/* There are no listeners. */
+	if (!mrp->listeners)
+		return result;
+
+	info = get_port_info(mrp, port);
+	reserv = mrp_link_listener(mrp, info, stream);
+	if (!reserv)
+		return -ENOMEM;
+
+	/* Listener declaration will be updated in following call. */
+	result = mrp_update_listener(mrp, reserv, stream, false);
+	return result;
+}  /* proc_mrp_rx_talker */
+
+static void mrp_change_tx_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info,
+	struct SRP_reserv *reserv, bool *drop)
+{
+	u8 action;
+	u8 code;
+	u8 decl;
+
+	if (SRP_TALKER != reserv->direction || reserv->rx_code)
+		return;
+
+	code = reserv->code;
+	decl = reserv->declaration;
+
+	mrp_chk_tx_talker(mrp, port, info, reserv);
+	if (code == reserv->code)
+		return;
+
+	action = MRP_ACTION_TX;
+
+	if (reserv->streamAge) {
+		if (reserv->code == RFC_PORT_IS_NOT_AVB) {
+			int tc = get_traffic_class(mrp,
+				reserv->stream->priority);
+
+			drop[tc] = true;
+		}
+	}
+	if (decl != reserv->declaration)
+		action = MRP_ACTION_TX_NEW;
+	add_attrib_report(mrp, reserv, action, MRP_TYPE_TALKER, port);
+}  /* mrp_change_tx_reserv */
+
+static void mrp_drop_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info, bool *drop)
+{
+	struct mrp_traffic_info *traffic;
+	int rc;
+	int tc;
+	void *param[6];
+
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		if (!drop[tc])
+			continue;
+
+		/* Mark all streams to stop. */
+		rc = stream_iter(&traffic->active, param, stream_set_mark);
+		info->bandwidth_used -= traffic->bandwidth_used;
+		traffic->bandwidth_used = 0;
+		traffic->bandwidth_left = traffic->bandwidth_max;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other = *traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+		param[0] = mrp;
+		param[1] = traffic;
+		param[2] = &port;
+		rc = stream_oper(&traffic->active, param, stream_stop);
+	}
+	info->bandwidth_left = info->bandwidth_max - info->bandwidth_used;
+	chk_passive_reserv(mrp, port, info);
+
+	srp_cfg_reserv(mrp, info);
+}  /* mrp_drop_reserv */
+
+static bool mrp_match_tx_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info,
+	struct SRP_reserv *reserv, struct SRP_reserv *t_reserv)
+{
+	bool drop[8];
+	u8 code;
+	u8 decl;
+
+	if (SRP_TALKER != t_reserv->direction ||
+	    t_reserv->stream != reserv->stream)
+		return false;
+
+	code = t_reserv->code;
+	decl = t_reserv->declaration;
+
+	/* use error codes from receive side */
+	t_reserv->code_bits = reserv->code_bits;
+	t_reserv->code = reserv->code;
+	t_reserv->rx_code = reserv->code;
+	t_reserv->declaration = t_reserv->code ? SRP_FAILED : SRP_ADVERTISE;
+	if (!reserv->code)
+		mrp_chk_tx_talker(mrp, port, info, t_reserv);
+	drop[SR_CLASS_A] = 0;
+	drop[SR_CLASS_B] = 0;
+	if (code != t_reserv->code) {
+		u8 action = MRP_ACTION_TX;
+
+dbg_msg(" co: %d %d\n", code, t_reserv->code);
+		if (t_reserv->streamAge) {
+			if (t_reserv->code == RFC_PORT_IS_NOT_AVB) {
+				int tc = get_traffic_class(mrp,
+					t_reserv->stream->priority);
+
+dbg_msg("drop\n");
+				drop[tc] = true;
+			}
+		}
+		if (decl != t_reserv->declaration)
+			action = MRP_ACTION_TX_NEW;
+		add_attrib_report(mrp, t_reserv, action, MRP_TYPE_TALKER, port);
+	}
+	mrp_drop_reserv(mrp, port, info, drop);
+	return true;
+}
+
+static void mrp_change_rx_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info,
+	struct SRP_reserv *reserv)
+{
+	struct SRP_reserv *t_reserv;
+	int n;
+	u8 code;
+
+	if (SRP_TALKER != reserv->direction || reserv->rx_code)
+		return;
+
+	code = reserv->code;
+
+	/* check if these bits are still there */
+	reserv->code_bits &= ~(
+		RFC_ASCAPABLE_BIT |
+		RFC_SRP_BOUNDARY_BIT |
+		RFC_PRIORITY_BIT);
+	reserv->code_bits |= mrp_chk_as_port(mrp, port, info,
+		get_traffic_class(mrp, reserv->stream->priority));
+	reserv->code = msrp_failure_code(reserv->code_bits);
+	if (code == reserv->code)
+		return;
+dbg_msg(" c: %d=%d %d\n", port, code, reserv->code);
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == port)
+			continue;
+		if (!(reserv->tx_ports & (1 << n)))
+			continue;
+		info = get_port_info(mrp, n);
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		t_reserv = info->declared.next;
+		while (t_reserv) {
+			if (mrp_match_tx_reserv(mrp, n, info, reserv,
+					     t_reserv))
+				break;
+			t_reserv = t_reserv->next;
+		}
+	}
+}  /* mrp_change_rx_reserv */
+
+static void mrp_fwd_addr(struct mrp_info *mrp, uint port, bool avb)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	port = get_actual_port(mrp, port);
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		if ((mac->rx_ports & (1 << port))) {
+			u16 ports;
+
+			if (avb)
+				ports = mac->ports;
+			else
+				ports = SRP_PORT_OTHER;
+			mrp_cfg_dest_addr(mrp, mac->index, mac->addr, ports,
+					  mac->fid);
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* mrp_fwd_addr */
+
+static int proc_mrp_chk_talker(struct mrp_info *mrp, u8 port, int tc)
+{
+	struct mrp_port_info *info;
+	struct SRP_reserv *reserv;
+	int avb_a;
+	int avb_b;
+	int result = DEV_IOC_OK;
+	bool drop[8];
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	cfg = &sw->info->port_cfg[get_actual_port(mrp, port)];
+	avb_a = cfg->avb_a;
+	avb_b = cfg->avb_b;
+	info = get_port_info(mrp, port);
+	if (!tc) {
+		mrp_chk_as_port(mrp, port, info, SR_CLASS_A);
+		tc = SR_CLASS_B;
+	}
+	mrp_chk_as_port(mrp, port, info, tc);
+	if (avb_b != cfg->avb_b) {
+		if (port < mrp->ports)
+			enable_acl_remap(mrp, port, !cfg->avb_b);
+		mrp_fwd_addr(mrp, port, cfg->avb_b);
+	} else if (!avb_b && avb_a != cfg->avb_a) {
+		if (port < mrp->ports)
+			enable_acl_remap(mrp, port, !cfg->avb_a);
+		mrp_fwd_addr(mrp, port, cfg->avb_a);
+	}
+	reserv = info->registered.next;
+	while (reserv) {
+		mrp_change_rx_reserv(mrp, port, info, reserv);
+		reserv = reserv->next;
+	}
+	drop[SR_CLASS_A] = 0;
+	drop[SR_CLASS_B] = 0;
+	reserv = info->declared.next;
+	while (reserv) {
+		mrp_change_tx_reserv(mrp, port, info, reserv, drop);
+		reserv = reserv->next;
+	}
+	mrp_drop_reserv(mrp, port, info, drop);
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_chk_talker */
+
+static int proc_mrp_lv_domain(struct mrp_info *mrp, u8 port,
+	struct SRP_domain_class *domain)
+{
+	int result = DEV_IOC_OK;
+	struct mrp_port_info *info;
+	struct SRP_domain_class *self = NULL;
+
+dbg_msg(" %s %d %d %d %d\n", __func__, port,
+	domain->id, domain->priority, domain->vlan_id);
+	if (is_host_port(mrp, port))
+		return result;
+	info = get_port_info(mrp, port);
+	if (SR_CLASS_B <= domain->id && domain->id <= SR_CLASS_A)
+		self = &mrp->domain[domain->id - SR_CLASS_B];
+	if (self && (self->id == domain->id &&
+	    self->priority == domain->priority)) {
+		if (!info->priority[domain->id].SRPdomainBoundaryPort) {
+			info->priority[domain->id].SRPdomainBoundaryPort = 1;
+			result = proc_mrp_chk_talker(mrp, port, domain->id);
+		}
+	}
+
+	return result;
+}  /* proc_mrp_lv_domain */
+
+static int proc_mrp_rx_domain(struct mrp_info *mrp, u8 port,
+	struct SRP_domain_class *domain)
+{
+	int result = DEV_IOC_OK;
+	struct mrp_port_info *info;
+	struct SRP_domain_class *self = NULL;
+
+dbg_msg(" %s %d %d %d %d\n", __func__, port,
+	domain->id, domain->priority, domain->vlan_id);
+	if (is_host_port(mrp, port))
+		return result;
+	if (domain->id >= SR_CLASS_NUM)
+		return result;
+	info = get_port_info(mrp, port);
+	if (SR_CLASS_B <= domain->id && domain->id <= SR_CLASS_A)
+		self = &mrp->domain[domain->id - SR_CLASS_B];
+	if (self && (self->id == domain->id &&
+	    self->priority == domain->priority)) {
+		if (info->priority[domain->id].SRPdomainBoundaryPort) {
+			info->priority[domain->id].SRPdomainBoundaryPort = 0;
+			result = proc_mrp_chk_talker(mrp, port, domain->id);
+		}
+	} else if (!info->priority[domain->id].SRPdomainBoundaryPort) {
+		info->priority[domain->id].SRPdomainBoundaryPort = 1;
+		if (self)
+			result = proc_mrp_chk_talker(mrp, port, domain->id);
+	}
+
+	return result;
+}  /* proc_mrp_rx_domain */
+
+static int proc_mrp_chk_registered(struct mrp_info *mrp, u8 port)
+{
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info;
+	int i;
+	int result = DEV_IOC_OK;
+
+dbg_msg("%s %d\n", __func__, port);
+	for (i = 0; i <= mrp->ports; i++) {
+		if (i == port)
+			continue;
+		info = get_port_info(mrp, i);
+		reserv = info->registered.next;
+		while (reserv) {
+if (SRP_TALKER == reserv->direction)
+dbg_msg(" t:%d %x\n", info->index, reserv->tx_ports);
+			if (SRP_TALKER == reserv->direction &&
+			    !(reserv->tx_ports & (1 << port))) {
+				struct SRP_talker talker;
+				struct SRP_reserv *l_reserv;
+				int result;
+
+dbg_msg(" need talker decl: %d\n", i);
+				memcpy(&talker, reserv->stream, 25);
+dbg_msg(" latency: %u %u\n", talker.AccumulatedLatency, reserv->stream->latency);
+				memcpy(talker.bridge_id, reserv->bridge_id, 8);
+				talker.FailureCode = reserv->code;
+				result = proc_mrp_tx_talker(mrp, port, &talker,
+							    reserv, true);
+				if (result < 0)
+					continue;
+				l_reserv = srp_find_reserv(&info->declared,
+							   reserv->stream->id,
+							   SRP_LISTENER);
+				if (l_reserv)
+					mrp_update_listener(mrp,
+							    l_reserv,
+							    reserv->stream,
+							    false);
+				chk_reserv(info, i);
+			}
+			reserv = reserv->next;
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_chk_registered */
+#endif
+
+static int proc_mrp_get_tx(struct mrp_info *mrp, int start,
+	struct mrp_cfg_options *cmd, int *output)
+{
+	struct mrp_report *attrib;
+
+#ifdef CONFIG_KSZ_MSRP
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+#endif
+	int result = DEV_IOC_MRP_REPORT;
+
+	attrib = mrp->report_head;
+	if (!attrib)
+		return DEV_IOC_INVALID_CMD;
+
+	*output = 0;
+	cmd->action = attrib->action;
+	cmd->type = attrib->type;
+	cmd->port = attrib->port + 1;
+	if (MRP_TYPE_MAC == attrib->type) {
+		struct MRP_mac *mac = &cmd->data.mac;
+		struct mrp_node *node = attrib->attrib;
+		struct mrp_mac_info *info = node->data;
+
+		memcpy(mac->addr, info->addr, ETH_ALEN);
+		*output = SIZEOF_MRP_mac;
+		if (MRP_ACTION_LV == attrib->action &&
+		    attrib->port == mrp->ports &&
+		    !info->ports) {
+			mrp_free_node(node);
+		}
+	} else if (MRP_TYPE_VLAN == attrib->type) {
+		struct MRP_vlan *vlan = &cmd->data.vlan;
+		struct mrp_node *node = attrib->attrib;
+		struct mrp_vlan_info *info = node->data;
+
+		vlan->id = info->vid;
+		*output = SIZEOF_MRP_vlan;
+		if (MRP_ACTION_LV == attrib->action &&
+		    attrib->port == mrp->ports &&
+		    !info->ports) {
+			mrp_free_node(node);
+		}
+
+#ifdef CONFIG_KSZ_MSRP
+	} else if (MRP_TYPE_DOMAIN == attrib->type) {
+		struct SRP_domain_class *domain = &cmd->data.domain;
+
+		*domain = mrp->domain[0];
+		*output = SIZEOF_SRP_domain_class;
+	} else if (MRP_TYPE_LISTENER == attrib->type ||
+		   MRP_TYPE_TALKER == attrib->type) {
+		reserv = attrib->attrib;
+		if (SRP_TALKER == reserv->direction) {
+			struct SRP_talker *talker = &cmd->data.talker;
+
+			stream = reserv->stream;
+			memcpy(talker->id, stream->id, 8);
+			memcpy(talker->dest, stream->dest, ETH_ALEN);
+			talker->vlan_id = stream->vlan_id;
+			talker->MaxFrameSize = stream->MaxFrameSize;
+			talker->MaxIntervalFrames = stream->MaxIntervalFrames;
+			talker->priority = stream->priority;
+			talker->rank = stream->rank;
+			talker->reserved = 0;
+			talker->AccumulatedLatency = reserv->latency;
+			memcpy(talker->bridge_id, reserv->bridge_id, 8);
+			talker->FailureCode = reserv->code;
+			*output = SIZEOF_SRP_talker;
+		} else {
+			memcpy(cmd->data.listener.id, reserv->id, 8);
+			cmd->data.listener.substate = reserv->declaration;
+			*output = SIZEOF_SRP_listener;
+		}
+
+		/* Did receive Leave indication. */
+		if (MRP_ACTION_LV == attrib->action) {
+			struct mrp_report *next = attrib->next;
+
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" r %s %p\n", __func__, reserv);
+#endif
+			kfree(reserv);
+
+			/* Special one to free the stream. */
+			if (next && next->port == mrp->ports + 1) {
+				reserv = next->attrib;
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" s %s %p\n", __func__, reserv->stream);
+dbg_msg(" r2 %s %p\n", __func__, reserv);
+#endif
+				kfree(reserv->stream);
+				kfree(reserv);
+
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" 1 %s %p\n", __func__, attrib);
+#endif
+				kfree(attrib);
+				attrib = next;
+			}
+		}
+#endif
+	}
+
+	/* Assume there are more attributes to report. */
+	mrp->report_head = attrib->next;
+	if (mrp->report_tail == attrib) {
+		mrp->report_tail = NULL;
+		result = DEV_IOC_OK;
+	}
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p\n", __func__, attrib);
+#endif
+	kfree(attrib);
+	return result;
+}  /* proc_mrp_get_tx */
+
+static int proc_mrp_get_attribute(struct mrp_info *mrp, int start, u8 *data,
+	int *output)
+{
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	int result = DEV_IOC_OK;
+
+	switch (cmd->type) {
+	case MRP_TYPE_UNKNOWN:
+		result = proc_mrp_get_tx(mrp, start, cmd, output);
+		break;
+	case MRP_TYPE_MAC:
+		break;
+	case MRP_TYPE_VLAN:
+		break;
+
+#ifdef CONFIG_KSZ_MSRP
+	case MRP_TYPE_DOMAIN:
+		break;
+	case MRP_TYPE_LISTENER:
+		break;
+	case MRP_TYPE_TALKER:
+		break;
+#endif
+	}
+	return result;
+}  /* proc_mrp_get_attribute */
+
+#ifdef CONFIG_KSZ_MSRP
+static int mrp_set_bandwidth(struct mrp_port_info *info)
+{
+	int q0;
+	int q1;
+	int tc;
+	u32 bandwidth;
+	struct mrp_traffic_info *traffic;
+	int check = false;
+
+	q0 = info->traffic[0].queue;
+	q1 = info->traffic[1].queue;
+	bandwidth = calculate_max_bandwidth(info->speed,
+		info->bandwidth[q1].deltaBandwidth +
+		info->bandwidth[q0].deltaBandwidth);
+
+	/* The link speed has changed. */
+	if (bandwidth != info->bandwidth_max) {
+		info->bandwidth_max = bandwidth;
+
+		/* Total bandwidth left will be set again if being used. */
+		if (info->bandwidth_used < info->bandwidth_max)
+			info->bandwidth_left = info->bandwidth_max -
+				info->bandwidth_used;
+		else
+			info->bandwidth_left = 0;
+	}
+
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		bandwidth = calculate_max_bandwidth(info->speed,
+			info->bandwidth[traffic->queue].deltaBandwidth);
+		traffic->bandwidth_delta = bandwidth;
+		traffic->bandwidth_max = bandwidth;
+	}
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		if (traffic->bandwidth_used < traffic->bandwidth_max)
+			traffic->bandwidth_left = traffic->bandwidth_max -
+				traffic->bandwidth_used;
+		else
+			/* Bandwidth left will be set after checking. */
+			traffic->bandwidth_left = 0;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other += traffic->bandwidth_left;
+		if (traffic->bandwidth_used || info->declared.next) {
+			check = true;
+		}
+	}
+dbg_msg("%s %d\n", __func__, check);
+	return check;
+}  /* mrp_set_bandwidth */
+
+static void mrp_event(struct mrp_info *mrp, uint p, enum mrp_event event);
+
+static int proc_mrp_set_speed(struct mrp_info *mrp, u8 port, u32 speed,
+			      bool duplex)
+{
+	struct mrp_port_info *info;
+	int link;
+	int result = DEV_IOC_OK;
+
+	if (speed != 1000 && speed != 100 && speed != 10 && speed != 0)
+		return DEV_IOC_INVALID_CMD;
+	info = get_port_info(mrp, port);
+	if (speed) {
+		if (speed >= 100 && duplex)
+			info->duplex = 1;
+		else
+			info->duplex = 0;
+	}
+	if (speed && speed != info->speed) {
+		info->speed = speed;
+		if (mrp_set_bandwidth(info)) {
+			result = update_reserv(mrp, port, info);
+		}
+	}
+	link = info->link;
+	info->link = speed != 0;
+if (port < 4)
+dbg_msg("%s %d=%d:%d %d %d\n", __func__, port, speed, duplex, info->link, link);
+	if (link != info->link && info->link && mrp->started) {
+		int rc;
+
+		rc = proc_mrp_chk_registered(mrp, port);
+		if (result == DEV_IOC_OK)
+			result = rc;
+		mrp_event(mrp, port, MRP_EVENT_REDECLARE);
+	}
+#if 0
+	if (link != info->link && !info->link && mrp->started) {
+		mrp_event(mrp, port, MRP_EVENT_FLUSH);
+	}
+#endif
+	return result;
+}  /* proc_mrp_set_speed */
+
+static int proc_mrp_set_delta(struct mrp_info *mrp, u8 port, u32 A, u32 B)
+{
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+
+dbg_msg("%s %d=%d %d\n", __func__, port, A, B);
+	if (A + B > 95)
+		return DEV_IOC_INVALID_CMD;
+	info = get_port_info(mrp, port);
+	info->bandwidth[3].deltaBandwidth = A;
+	info->bandwidth[2].deltaBandwidth = B;
+	if (mrp_set_bandwidth(info)) {
+	char bw_str1[20];
+	char bw_str2[20];
+	char bw_str3[20];
+	char bw_str4[20];
+	char bw_str5[20];
+		format_num(bw_str1, info->bandwidth_used);
+		format_num(bw_str2, info->traffic[1].bandwidth_max);
+		format_num(bw_str3, info->traffic[1].bandwidth_used);
+		format_num(bw_str4, info->traffic[0].bandwidth_max);
+		format_num(bw_str5, info->traffic[0].bandwidth_used);
+dbg_msg("bw used: %d %s; %s %s; %s %s\n", port,
+bw_str1, bw_str2, bw_str3, bw_str4, bw_str5);
+		result = update_reserv(mrp, port, info);
+	}
+	return result;
+}  /* proc_mrp_set_delta */
+#endif
+
+#ifdef PROC_MRP
+#ifndef MRP_PASSTHRU
+static int proc_mrp_xmit(struct mrp_info *mrp, uint p, struct sk_buff *skb);
+#endif
+
+#include "mrp.c"
+#include "mmrp.c"
+#include "mvrp.c"
+
+#ifdef CONFIG_KSZ_MSRP
+#include "msrp.c"
+
+static void mrp_get_talker(struct SRP_talker *talker,
+	struct srp_talker_failed *attr)
+{
+	memcpy(talker, attr, sizeof(struct srp_talker_failed));
+	talker->vlan_id = ntohs(talker->vlan_id);
+	talker->MaxFrameSize = ntohs(talker->MaxFrameSize);
+	talker->MaxIntervalFrames = ntohs(talker->MaxIntervalFrames);
+	talker->AccumulatedLatency = ntohl(talker->AccumulatedLatency);
+}  /* mrp_get_talker */
+
+static void mrp_set_talker(struct SRP_talker *talker,
+	struct srp_talker_failed *attr)
+{
+	memcpy(attr, talker, sizeof(struct srp_talker_failed));
+	attr->vlan_id = htons(attr->vlan_id);
+	attr->max_frame_size = htons(attr->max_frame_size);
+	attr->max_interval_frames = htons(attr->max_interval_frames);
+	attr->accumulated_latency = htonl(attr->accumulated_latency);
+}  /* mrp_set_talker */
+
+static void mrp_get_listener(struct SRP_listener *listener,
+	struct srp_listener *attr)
+{
+	memcpy(listener->id, attr->id, 8);
+	listener->substate = attr->substate;
+}  /* mrp_get_listener */
+
+static void mrp_set_listener(struct SRP_listener *listener,
+	struct srp_listener *attr)
+{
+	memcpy(attr->id, listener->id, 8);
+	attr->substate = listener->substate;
+}  /* mrp_set_listener */
+
+static void mrp_get_domain(struct SRP_domain_class *domain,
+	struct srp_domain *attr)
+{
+	domain->id = attr->class_id;
+	domain->priority = attr->class_priority;
+	domain->vlan_id = ntohs(attr->class_vid);
+}  /* mrp_get_domain */
+
+static void mrp_set_domain(struct SRP_domain_class *domain,
+	struct srp_domain *attr)
+{
+	attr->class_id = domain->id;
+	attr->class_priority = domain->priority;
+	attr->class_vid = htons(domain->vlan_id);
+}  /* mrp_set_domain */
+#endif
+
+static void proc_mrp_req_lv(struct mrp_info *mrp, uint port,
+			    struct mrp_application *appl)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[appl->type]);
+
+	mrp_join_timer_exec(app);
+}  /* proc_mrp_req_lv */
+
+static int proc_mrp_join_mac(struct mrp_info *mrp, uint port,
+	struct MRP_mac *mac, u8 new_decl)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[mac_mrp_app.type]);
+
+	mmrp_req_join_mac(app, mac->addr, new_decl);
+	return 0;
+}  /* proc_mrp_join_mac */
+
+static int proc_leave_mac(struct mrp_info *mrp, uint port,
+	struct MRP_mac *mac)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[mac_mrp_app.type]);
+
+	return mmrp_req_leave_mac(app, mac->addr);
+}  /* proc_leave_mac */
+
+static int proc_mrp_join_vlan(struct mrp_info *mrp, uint port,
+	struct MRP_vlan *vlan, u8 new_decl)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[vlan_mrp_app.type]);
+
+	mvrp_req_join(app, vlan->id, new_decl);
+	return 0;
+}  /* proc_mrp_join_vlan */
+
+static int proc_leave_vlan(struct mrp_info *mrp, uint port,
+	struct MRP_vlan *vlan)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[vlan_mrp_app.type]);
+
+	return mvrp_req_leave(app, vlan->id);
+}  /* proc_leave_vlan */
+
+#ifdef CONFIG_KSZ_MSRP
+static int proc_mrp_new_talker(struct mrp_info *mrp, uint port,
+	struct SRP_talker *talker, u8 new_decl)
+{
+	struct srp_talker_failed attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_talker(talker, &attr);
+	msrp_req_new_talker(app, &attr, new_decl);
+	return 0;
+}  /* proc_mrp_new_talker */
+
+static int proc_leave_talker(struct mrp_info *mrp, uint port,
+	struct SRP_talker *talker)
+{
+	struct srp_talker_failed attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_talker(talker, &attr);
+	return msrp_req_leave_talker(app, &attr);
+}  /* proc_leave_talker */
+
+static int proc_mrp_new_listener(struct mrp_info *mrp, uint port,
+	struct SRP_listener *listener, u8 new_decl)
+{
+	struct srp_listener attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_listener(listener, &attr);
+	msrp_req_new_listener(app, &attr, new_decl);
+	return 0;
+}  /* proc_mrp_new_listener */
+
+static int proc_leave_listener(struct mrp_info *mrp, uint port,
+	struct SRP_listener *listener)
+{
+	struct srp_listener attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_listener(listener, &attr);
+	return msrp_req_leave_listener(app, &attr);
+}  /* proc_leave_listener */
+
+static int proc_mrp_join_domain(struct mrp_info *mrp, uint port,
+	struct SRP_domain_class *domain, u8 new_decl)
+{
+	struct srp_domain attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_domain(domain, &attr);
+	msrp_req_join_domain(app, &attr, new_decl);
+	return 0;
+}  /* proc_mrp_join_domain */
+
+static int proc_leave_domain(struct mrp_info *mrp, uint port,
+	struct SRP_domain_class *domain)
+{
+	struct srp_domain attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_domain(domain, &attr);
+	return msrp_req_leave_domain(app, &attr);
+}  /* proc_leave_domain */
+#endif
+#endif
+
+static int proc_mrp_set_attribute(struct mrp_info *mrp, int start, u8 *data)
+{
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	u8 nport = cmd->port;
+	int result = DEV_IOC_OK;
+	int tx_leave = false;
+	int q;
+
+	if (!nport)
+		nport = mrp->ports;
+	else
+		--nport;
+	q = get_actual_port(mrp, nport);
+	switch (cmd->type) {
+	case MRP_TYPE_MAC:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_mac(mrp, nport, &cmd->data.mac,
+				cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_mac(mrp, nport, &cmd->data.mac);
+#ifdef PROC_MRP
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_join_mac(mrp, q, &cmd->data.mac,
+				cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_mac(mrp, q, &cmd->data.mac);
+#endif
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_mac(mrp, nport, &cmd->data.mac,
+				cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_mac(mrp, nport, &cmd->data.mac);
+			mrp->no_report = 0;
+		} else
+			mrp_show_node(&mrp->mac_list, show_mac_info);
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, q, &mac_mrp_app);
+		break;
+	case MRP_TYPE_VLAN:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_vlan(mrp, nport, &cmd->data.vlan,
+				cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_vlan(mrp, nport, &cmd->data.vlan);
+#ifdef PROC_MRP
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_join_vlan(mrp, q,
+				&cmd->data.vlan, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_vlan(mrp, q, &cmd->data.vlan);
+#endif
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_vlan(mrp, nport, &cmd->data.vlan,
+				cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_vlan(mrp, nport, &cmd->data.vlan);
+			mrp->no_report = 0;
+		} else
+			mrp_show_node(&mrp->vlan_list, show_vlan_info);
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, q, &vlan_mrp_app);
+		break;
+
+#ifdef CONFIG_KSZ_MSRP
+	case MRP_TYPE_DOMAIN:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_domain(mrp, nport,
+				&cmd->data.domain);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_domain(mrp, nport,
+				&cmd->data.domain);
+#ifdef PROC_MRP
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_join_domain(mrp, q,
+				&cmd->data.domain, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_domain(mrp, q, &cmd->data.domain);
+#endif
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_domain(mrp, nport,
+				&cmd->data.domain);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_domain(mrp, nport,
+				&cmd->data.domain);
+			mrp->no_report = 0;
+		}
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, q, &srp_mrp_app);
+		break;
+	case MRP_TYPE_LISTENER:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_listener(mrp, nport,
+				&cmd->data.listener, cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_listener(mrp, nport,
+				&cmd->data.listener);
+#ifdef PROC_MRP
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_new_listener(mrp, q,
+				&cmd->data.listener, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_listener(mrp, q,
+				&cmd->data.listener);
+#endif
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_listener(mrp, nport,
+				&cmd->data.listener, cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_listener(mrp, nport,
+				&cmd->data.listener);
+			mrp->no_report = 0;
+		}
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, q, &srp_mrp_app);
+		break;
+	case MRP_TYPE_TALKER:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_talker(mrp, nport,
+				&cmd->data.talker, cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_talker(mrp, nport,
+				&cmd->data.talker);
+#ifdef PROC_MRP
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_new_talker(mrp, q,
+				&cmd->data.talker, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_talker(mrp, q, &cmd->data.talker);
+#endif
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_talker(mrp, nport,
+				&cmd->data.talker, cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_talker(mrp, nport,
+				&cmd->data.talker);
+			mrp->no_report = 0;
+		} else {
+			int n;
+			struct mrp_port_info *info;
+
+			for (n = 0; n <= mrp->ports; n++) {
+				info = get_port_info(mrp, n);
+				if (!info->status.msrpPortEnabledStatus)
+					continue;
+				chk_reserv(info, n);
+			}
+		}
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, q, &srp_mrp_app);
+		break;
+#endif
+	case MRP_TYPE_PORT:
+
+#ifdef CONFIG_KSZ_MSRP
+		if (MRP_ACTION_SPEED == cmd->action) {
+			int speed = cmd->data.data[0];
+			bool duplex = cmd->data.data[1];
+
+			result = proc_mrp_set_speed(mrp, nport, speed, duplex);
+		} else if (MRP_ACTION_DELTA == cmd->action) {
+			u32 a = cmd->data.data[0];
+			u32 b = cmd->data.data[1];
+
+			result = proc_mrp_set_delta(mrp, nport, a, b);
+		} else if (MRP_ACTION_CHK_TALKER == cmd->action) {
+			result = proc_mrp_chk_talker(mrp, nport, 0);
+		} else if (MRP_ACTION_CHK_REG == cmd->action) {
+			result = proc_mrp_chk_registered(mrp, nport);
+		}
+#endif
+		break;
+	}
+	return result;
+}  /* proc_mrp_set_attribute */
+
+static void mrp_cfg_mcast_addr(struct mrp_info *mrp, u16 fid, u8 *dest,
+			       u16 ports)
+{
+	struct mrp_node *mac_node;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	sw->ops->cfg_mac(sw, 1, dest, ports, true, fid != 0, fid);
+	mac_node = mrp_get_mac_info(&mrp->mac_list, dest, fid);
+	if (mac_node) {
+		struct mrp_mac_info *mac;
+
+		mac = mac_node->data;
+
+		/* Indicate address is programmed in the lookup table. */
+		mac->srp_ports |= SRP_PORT_SET;
+	}
+}  /* mrp_cfg_mcast_addr */
+
+static void mrp_cfg_mac_work(struct work_struct *work)
+{
+	u16 *fid;
+	u16 *ports;
+	u8 addr[ETH_ALEN];
+	bool last;
+	struct sk_buff *skb;
+	struct mrp_info *mrp = container_of(work, struct mrp_info, cfg_mac);
+
+	last = skb_queue_empty(&mrp->macq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->macq);
+		last = skb_queue_empty(&mrp->macq);
+		if (!skb)
+			continue;
+		memcpy(addr, skb->data, ETH_ALEN);
+		fid = (u16 *)&skb->data[ETH_ALEN];
+		ports = (u16 *)&skb->data[ETH_ALEN + 2];
+		mrp_cfg_mcast_addr(mrp, *fid, addr, *ports);
+		kfree_skb(skb);
+	}
+	mrp->macq_sched = 0;
+}  /* mrp_cfg_mac_work */
+
+static int mrp_req_cfg_mac(struct mrp_info *mrp, u8 *addr, u16 fid, u16 ports)
+{
+	struct sk_buff *skb;
+	u16 *data;
+
+	skb = alloc_skb(64, GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, addr, ETH_ALEN);
+	data = (u16 *)&skb->data[ETH_ALEN];
+	*data = fid;
+	data++;
+	*data = ports;
+	skb_queue_tail(&mrp->macq, skb);
+	if (!mrp->macq_sched) {
+		mrp->macq_sched = 1;
+		schedule_work(&mrp->cfg_mac);
+	}
+	return 0;
+}  /* mrp_req_cfg_mac */
+
+static void mrp_rx_proc(struct work_struct *work)
+{
+	bool last;
+	struct sk_buff *skb;
+	struct mrp_applicant **data;
+	struct mrp_applicant *app;
+	struct mrp_info *mrp = container_of(work, struct mrp_info, rx_proc);
+
+	last = skb_queue_empty(&mrp->rxq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->rxq);
+		last = skb_queue_empty(&mrp->rxq);
+		if (!skb)
+			continue;
+		data = (struct mrp_applicant **)skb->cb;
+		app = *data;
+		app->rxpdu(app, &skb->data[sizeof(struct ethhdr)],
+			skb->len - sizeof(struct ethhdr));
+		app->cleanup(app);
+		kfree_skb(skb);
+	}
+}  /* mrp_rx_proc */
+
+#ifdef PROC_MRP
+static int proc_mrp_xmit(struct mrp_info *mrp, uint p, struct sk_buff *skb)
+{
+	int rc;
+	u16 tx_ports;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	const struct net_device_ops *ops = sw->main_dev->netdev_ops;
+	int result = DEV_IOC_OK;
+
+#ifdef DBG_MRP_
+dbg_msg("  T\n");
+#endif
+
+	/* Send to host port by simulating receiving. */
+	if (p == sw->HOST_PORT) {
+		skb->data[11] ^= 0x01;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		netif_rx_ni(skb);
+		return 0;
+	}
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(sw->main_dev) || !netif_carrier_ok(sw->main_dev)) {
+		kfree_skb(skb);
+		return 0;
+	}
+
+	tx_ports = sw->tx_ports[0] & mrp->tx_ports;
+#if 1
+if (!(tx_ports & (1 << p)) && sw->tx_ports[0] != mrp->tx_ports) {
+dbg_msg("  tx close: %d %x %x\n", p, sw->tx_ports[0], mrp->tx_ports);
+}
+#endif
+
+	/* Do not send to port if its link is lost. */
+	if (media_disconnected == sw->port_state[p].state ||
+	    !(tx_ports & (1 << p))) {
+		kfree_skb(skb);
+		return 0;
+	}
+
+	if (skb->len < 60) {
+		int len = 60 - skb->len;
+		memset(&skb->data[skb->len], 0, len);
+		skb_put(skb, len);
+	}
+	sw->net_ops->add_tail_tag(sw, skb, (1 << p));
+	do {
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(sw->main_dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return result;
+}  /* proc_mrp_mrp_xmit */
+#endif
+
+static void proc_mrp_cmd(struct mrp_info *mrp, struct mrp_work *parent)
+{
+	u8 *data = parent->param.data;
+	int result = DEV_IOC_OK;
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+			break;
+		case DEV_INFO_EXIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			result = proc_mrp_set_attribute(mrp, parent->option,
+				data);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			result = proc_mrp_get_attribute(mrp, parent->option,
+				data, &parent->output);
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_mrp_cmd */
+
+static void proc_mrp_work(struct work_struct *work)
+{
+	struct mrp_access *info =
+		container_of(work, struct mrp_access, work);
+	struct mrp_info *mrp =
+		container_of(info, struct mrp_info, hw_access);
+	struct mrp_work *cmd;
+
+	cmd = &info->works[info->head];
+	while (cmd->used) {
+		proc_mrp_cmd(mrp, cmd);
+		info->head++;
+		info->head &= MRP_WORK_LAST;
+		cmd = &info->works[info->head];
+	}
+}  /* proc_mrp_work */
+
+static int proc_mrp_hw_access(struct mrp_info *mrp, int cmd, int subcmd,
+	int option, void *data, size_t len, int *output, struct sk_buff *skb,
+	int wait)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int ret = 0;
+
+	access = &mrp->hw_access;
+	work = &access->works[access->tail];
+	if (work->used) {
+		pr_alert("work full\n");
+		return -EFAULT;
+	}
+	work->skb = skb;
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->used = true;
+	access->tail++;
+	access->tail &= MRP_WORK_LAST;
+	init_completion(&work->done);
+	schedule_work(&access->work);
+	if (!wait)
+		goto hw_access_end;
+	wait_for_completion(&work->done);
+
+	ret = work->result;
+	*output = work->output;
+	if (DEV_CMD_GET == work->cmd) {
+		int rc = ret;
+
+		if (DEV_IOC_MRP_REPORT == rc) {
+			rc = DEV_IOC_OK;
+			len = *output;
+		}
+		if (DEV_IOC_OK == rc)
+			memcpy(data, work->param.data, len);
+	}
+
+hw_access_end:
+	return ret;
+}  /* proc_mrp_hw_access */
+
+static void exit_mrp_work(struct mrp_info *mrp)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int i;
+
+	access = &mrp->hw_access;
+	for (i = 0; i < MRP_WORK_NUM; i++) {
+		work = &access->works[i];
+		flush_work(&work->work);
+	}
+	flush_work(&access->work);
+}  /* exit_mrp_work */
+
+static void init_mrp_work(struct mrp_info *mrp)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int i;
+
+	access = &mrp->hw_access;
+	for (i = 0; i < MRP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->mrp = mrp;
+		work->index = i;
+		work->prev = &access->works[(i - 1) & MRP_WORK_LAST];
+		INIT_WORK(&work->work, proc_mrp_work);
+		init_completion(&work->done);
+	}
+	access->head = access->tail = 0;
+	INIT_WORK(&access->work, proc_mrp_work);
+}  /* init_mrp_work */
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+static int mrp_chk_mcast(struct mrp_info *mrp, u8 *addr, u16 vid, u16 prio,
+			 u16 proto, uint port)
+{
+	struct mrp_node *mac_node;
+	struct mrp_mac_info *mac;
+	struct ksz_port_cfg *cfg;
+	struct maap_pdu *maap;
+#if 0
+	struct sk_buff *skb;
+	u16 *data;
+#endif
+	u16 ports;
+	u16 fid = 0;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) addr;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	int avb_a;
+	int avb_b;
+	int avb_0 = 0;
+	int *avb = &avb_0;
+	int ignore;
+	int result;
+
+	cfg = &sw->info->port_cfg[port];
+	avb_a = cfg->avb_a;
+	avb_b = cfg->avb_b;
+
+	maap = (struct maap_pdu *)&addr[14];
+	if (proto == 0x22F0) {
+		if (maap->subtype == AVTP_SUBTYPE_MAAP) {
+			if (memcmp(vlan->h_dest, maap_addr, ETH_ALEN))
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x  ",
+	vlan->h_dest[0], vlan->h_dest[1], vlan->h_dest[2],
+	vlan->h_dest[3], vlan->h_dest[4], vlan->h_dest[5]);
+dbg_msg(" maap: %d\n", maap->message_type);
+		}
+		if (maap->subtype == AVTP_SUBTYPE_MAAP ||
+		    !memcmp(vlan->h_dest, maap_addr, ETH_ALEN))
+			return 1;
+	}
+
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q))
+		maap = (struct maap_pdu *)(vlan + 1);
+
+	/* Priority is already remapped to 0 if port is not AVB. */
+
+	/* SR Class A supported */
+	if (prio == mrp->prio[SR_CLASS_A] && mrp->domain[1].id)
+		avb = &avb_a;
+	if (prio == mrp->prio[SR_CLASS_B])
+		avb = &avb_b;
+
+#if 0
+	/* XMOS sends this MAAP periodically. */
+	if (vlan->h_vlan_proto != htons(ETH_P_8021Q)) {
+		if (!(0x01 <= addr[15] && addr[15] <= 0x03))
+dbg_msg(" MAAP? %02X:%02X:%02X:%02X:%02X:%02X %02x\n",
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5], addr[15]);
+		if (0x01 <= addr[15] && addr[15] <= 0x03)
+			return 1;
+	}
+#endif
+	if (vid > 1) {
+#if 0
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vid, NULL,
+			&fid);
+		if (!ports)
+#endif
+			fid = sw->info->vid2fid[vid];
+	}
+
+	/* Check if the multicast address is registered. */
+	mac_node = mrp_get_mac_info(&mrp->mac_list, addr, fid);
+	if (!mac_node)
+		return -ENOMEM;
+
+	mac = mac_node->data;
+	ports = mac->srp_ports | mac->mrp_ports;
+
+	/* Address is used by MRP and already programmed into lookup table. */
+	if (ports & SRP_PORT_READY)
+		return 2;
+
+#if 1
+	if (!mac->jiffies) {
+dbg_msg(" drop: %d=%d=%02X:%02X:%02X:%02X:%02X:%02X  %02X\n", port, prio,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5], maap->subtype);
+	}
+#endif
+	mac->jiffies = jiffies + msecs_to_jiffies(10000);
+
+	ignore = (ports & SRP_PORT_BLACKLIST) &&
+		!(mac->rx_ports & SRP_PORT_IGNORE);
+
+	/* Address is programmed into lookup table. */
+	if (ports & SRP_PORT_SET)
+		result = 0;
+	else
+		result = !*avb;
+	mac->rx_ports &= ~SRP_PORT_IGNORE;
+
+	/* Address is already seen before. */
+	if (ignore)
+		return result;
+
+	mac->rx_ports |= (1 << port);
+	mac->srp_ports |= SRP_PORT_BLACKLIST;
+	if (!*avb)
+		ports = sw->PORT_MASK & ~sw->HOST_MASK;
+	else
+		ports = 0;
+
+	return mrp_req_cfg_mac(mrp, addr, fid, ports);
+}  /* mrp_chk_mcast */
+#endif
+
+static int mrp_dev_req(struct mrp_info *mrp, int start, char *arg)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	int err = 0;
+	int result = 0;
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) &req->param;
+	size_t param_size = 0;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+
+	if (DEV_MRP_ATTRIBUTE == subcmd) {
+		switch (cmd->type) {
+		case MRP_TYPE_PORT:
+			param_size = 4;
+			if (maincmd == DEV_CMD_PUT)
+				param_size += sizeof(u32) * 4;
+			break;
+		case MRP_TYPE_MAC:
+			param_size = SIZEOF_MRP_mac;
+			break;
+		case MRP_TYPE_VLAN:
+			param_size = SIZEOF_MRP_vlan;
+			break;
+
+#ifdef CONFIG_KSZ_MSRP
+		case MRP_TYPE_DOMAIN:
+			param_size = SIZEOF_SRP_domain_class;
+			break;
+		case MRP_TYPE_LISTENER:
+			param_size = SIZEOF_SRP_listener;
+			break;
+		case MRP_TYPE_TALKER:
+		default:
+			param_size = SIZEOF_SRP_talker;
+			break;
+#endif
+		}
+	}
+
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = mrp->version;
+				data[5] = mrp->ports;
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+						6) ||
+						copy_to_user(req->param.data,
+						data, 6)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+				result = proc_mrp_hw_access(mrp,
+					maincmd, subcmd, start,
+					data, 6, &output, NULL, true);
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, start,
+				data, 0, &output, NULL, true);
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			if (chk_ioctl_size(len, param_size,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, start,
+				data, len, &output, NULL, true);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			if (chk_ioctl_size(len, param_size,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+
+#ifdef CONFIG_KSZ_MSRP
+			if (MRP_ACTION_SPEED == cmd->action) {
+				uint port;
+				struct ksz_sw *sw =
+					container_of(mrp, struct ksz_sw, mrp);
+
+				port = sw_get_dev_port(sw, start, mrp->ports,
+					cmd->port);
+				__put_user(mrp->port_info[port].speed,
+					&req->output);
+				break;
+			}
+#endif
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, start,
+				data, len, &output, NULL, true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					param_size) ||
+					copy_to_user(req->param.data, data,
+					param_size)) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* mrp_dev_req */
+
+static void mrp_change_attr(struct mrp_applicant *app, void *value, u8 len,
+			    u8 type, enum mrp_event event)
+{
+	struct mrp_attr *attr;
+
+	attr = mrp_attr_lookup(app, value, len, type);
+	if (attr) {
+		u8 action;
+
+dbg_msg(" attr ");
+		/* Check the attribute is being declared. */
+		action = mrp_tx_action_table[attr->state];
+		if (action != MRP_TX_ACTION_S_JOIN_IN &&
+		    action != MRP_TX_ACTION_S_JOIN_IN_OPTIONAL)
+			action = mrp_tx_la_action_table[attr->state];
+
+		/* Change Join to New. */
+		if (action == MRP_TX_ACTION_S_JOIN_IN ||
+		    action == MRP_TX_ACTION_S_JOIN_IN_OPTIONAL)
+{
+dbg_msg(" event\n");
+			if (mrp_attr_event(app, attr, event))
+				mrp_join_timer_arm(app);
+}
+	}
+}
+
+static void mrp_update_event(struct mrp_applicant *app,
+			     struct mrp_applicant *to, enum mrp_event event)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+
+		if (attr->reg_state != MRP_REGISTRAR_IN)
+			continue;
+dbg_msg(" in %d:%02x%02x %d %d -> %d\n", app->port,
+attr->value[0], attr->value[1], attr->len, attr->type, to->port);
+		mrp_change_attr(to, attr->value, attr->len, attr->type, event);
+	}
+}
+
+static void mrp_event_port_app(struct mrp_port *port,
+	struct mrp_application *appl, enum mrp_event event)
+{
+	struct mrp_applicant *app;
+
+	app = rcu_dereference(port->applicants[appl->type]);
+	mrp_mad_event(app, event);
+	if (event == MRP_EVENT_LV && app->timer_arm.join)
+		mrp_join_timer_exec(app);
+}  /* mrp_event_port_app */
+
+static void mrp_new_event_port_app(struct mrp_info *mrp, uint p,
+				struct mrp_application *appl)
+{
+	struct mrp_applicant *app;
+	struct mrp_applicant *to;
+	int n;
+
+	app = rcu_dereference(mrp->mrp_ports[p].applicants[appl->type]);
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == p)
+			continue;
+		to = rcu_dereference(mrp->mrp_ports[n].applicants[appl->type]);
+		mrp_update_event(app, to, MRP_EVENT_NEW);
+	}
+}
+
+static void mrp_event(struct mrp_info *mrp, uint p, enum mrp_event event)
+{
+	if (skip_mrp_port(mrp, p))
+		return;
+	if (mrp->mmrp_rx_ports & (1 << p)) {
+		mrp_event_port_app(&mrp->mrp_ports[p], &mac_mrp_app, event);
+	}
+	if (mrp->mvrp_rx_ports & (1 << p)) {
+		mrp_event_port_app(&mrp->mrp_ports[p], &vlan_mrp_app, event);
+	}
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus &&
+	    mrp->port_info[p].status.msrpPortEnabledStatus) {
+		mrp_event_port_app(&mrp->mrp_ports[p], &srp_mrp_app, event);
+	}
+#endif
+}  /* mrp_event */
+
+static void mrp_setup_vlan(struct mrp_info *mrp, u16 vid,
+			   struct ksz_vlan_table *vlan)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct mrp_vlan_info data;
+	struct MRP_vlan mrp_vlan;
+	int cnt;
+	uint p;
+	uint q;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	if (!mrp->started)
+		return;
+	if (vid == 0 || vid == 1 || vid == 4095)
+		return;
+	cnt = 0;
+	q = 0;
+	if (vlan->valid) {
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (vlan->ports & (1 << p)) {
+				q = p;
+				cnt++;
+			}
+		}
+	}
+	if (cnt) {
+		int bit;
+		int index;
+		u16 ports;
+		int new_decl = false;
+
+#if 1
+		node = mrp_get_vlan_info(&mrp->vlan_list, vid, NULL);
+		if (!node)
+			return;
+
+		info = node->data;
+		ports = info->ports;
+		info->set_ports = vlan->ports;
+		info->ports = info->set_ports;
+#if 0
+			ports = mrp_find_vlan_ports(&mrp->vlan_list, vid,
+				&info->index, &info->fid);
+#endif
+		if (!ports) {
+			info->index = mrp_alloc_vlan(mrp);
+			info->fid = vlan->fid;
+dbg_msg(" index: %d %d\n", info->index, info->fid);
+		}
+#endif
+		index = vid / VID_IN_DATA;
+		bit = vid % VID_IN_DATA;
+		if (!(sw->info->vid[index] & (1 << bit)))
+			new_decl = true;
+		if (cnt > 1)
+			q = sw->mib_port_cnt + 1;
+		mrp_vlan.id = vid;
+dbg_msg(" q %d %x\n", q, info->tx_ports);
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (p == q) {
+				if (info->tx_ports & BIT(p)) {
+					info->tx_ports &= ~BIT(p);
+					if (proc_leave_vlan(mrp, p, &mrp_vlan))
+						proc_mrp_req_lv(mrp, p, &vlan_mrp_app);
+				}
+				continue;
+			}
+			if (!(info->tx_ports & BIT(p))) {
+				proc_mrp_join_vlan(mrp, p, &mrp_vlan, new_decl);
+				info->tx_ports |= BIT(p);
+			}
+		}
+	} else {
+		data.vid = vid;
+		memset(data.addr, 0xff, ETH_ALEN);
+		node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+		if (node) {
+			u16 ports;
+
+			info = node->data;
+			info->set_ports = 0;
+			info->ports = info->rx_ports;
+#if 0
+			ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid,
+						    NULL, NULL);
+#endif
+			ports = info->ports;
+			if (!ports) {
+				mrp_free_fid(mrp, info->fid);
+				mrp_free_vlan(mrp, info->index);
+			}
+dbg_msg(" delete: %x\n", info->ports);
+
+			/* Nobody is using the VLAN. */
+			if (!info->ports)
+				mrp_delete_node(&mrp->vlan_list, cmp_vlan,
+						node);
+		}
+		mrp_vlan.id = vid;
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (proc_leave_vlan(mrp, p, &mrp_vlan))
+				proc_mrp_req_lv(mrp, p, &vlan_mrp_app);
+		}
+	}
+}  /* mrp_setup_vlan */
+
+#ifdef PROC_MRP
+static void proc_mrp_rcv(struct mrp_info *mrp, struct mrp_applicant *app,
+			 struct sk_buff *skb)
+{
+	struct mrp_applicant **data = (struct mrp_applicant **)skb->cb;
+
+	*data = app;
+	skb_queue_tail(&mrp->rxq, skb);
+	schedule_work(&mrp->rx_proc);
+}  /* proc_mrp_rcv */
+
+static int mrp_rcv(struct mrp_info *mrp, struct sk_buff *skb, uint p)
+{
+	int mac_oper;
+	struct mrp_application *appl = NULL;
+	struct ethhdr *eth = (struct ethhdr *) skb->data;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	/* MAC_Operational. */
+	mac_oper = (sw->dev_ports & (1 << p));
+
+	if (eth->h_proto == htons(ETH_P_MVRP)) {
+/* MVRP.c.11.2.3a allows registration with non-forwarding ports. */
+#if 0
+		mac_oper &= sw->rx_ports[0];
+#endif
+		appl = &vlan_mrp_app;
+		if (!(mrp->mvrp_rx_ports & (1 << p)))
+			mac_oper = false;
+	} else if (eth->h_proto == htons(ETH_P_MMRP)) {
+		mac_oper &= sw->rx_ports[0];
+		appl = &mac_mrp_app;
+		if (!(mrp->mmrp_rx_ports & (1 << p)))
+			mac_oper = false;
+
+#ifdef CONFIG_KSZ_MSRP
+	} else if (eth->h_proto == htons(ETH_P_MSRP)) {
+		mac_oper &= sw->rx_ports[0];
+		appl = &srp_mrp_app;
+		if (!mrp->status.msrpEnabledStatus ||
+		    !mrp->port_info[p].status.msrpPortEnabledStatus)
+			mac_oper = false;
+	}
+#endif
+	if (appl) {
+		struct mrp_port *port;
+		struct mrp_applicant *app;
+		struct ksz_port_info *info;
+		int i;
+
+#ifdef DBG_MRP_
+
+dbg_msg("  R:\n");
+		for (i = 0; i < skb->len; i++) {
+			dbg_msg("%02x ", skb->data[i]);
+			if ((i % 16) == 15)
+				dbg_msg("\n");
+		}
+		if ((i % 16))
+			dbg_msg("\n");
+#endif
+		for (i = 0; i < sw->mib_port_cnt; i++) {
+			info = &sw->port_info[i];
+			if (!memcmp(eth->h_source, info->mac_addr, ETH_ALEN)) {
+				mac_oper = false;
+				break;
+			}
+		}
+		if (!mac_oper) {
+			kfree_skb(skb);
+			return 0;
+		}
+
+		port = &mrp->mrp_ports[p];
+		app = rcu_dereference(port->applicants[appl->type]);
+		if (!app) {
+dbg_msg(" no app! %d=%d\n", p, appl->type);
+			kfree_skb(skb);
+			return 0;
+		}
+
+		/* Destination MAC address needs to match MRP address. */
+		if (memcmp(app->group_address, eth->h_dest, ETH_ALEN))
+			goto done;
+		proc_mrp_rcv(mrp, app, skb);
+		return 0;
+	}
+
+done:
+	if (!memcmp(mrp->svlan_addr, eth->h_dest, ETH_ALEN)) {
+		/* Forward frame if port is C-VLAN. */
+	}
+
+	/* Known MRP type. */
+	if (appl) {
+		kfree_skb(skb);
+		return 0;
+	}
+	return 1;
+}  /* mrp_rcv */
+#endif
+
+#ifdef PROC_MRP
+static void mrp_delete_reserv(struct mrp_info *mrp, u8 nport, int direction)
+{
+	struct mrp_port_info *info;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *next;
+
+	info = get_port_info(mrp, nport);
+	reserv = info->registered.next;
+	while (reserv) {
+		next = reserv->next;
+		if (reserv->direction == direction) {
+			if (SRP_TALKER == direction)
+				mrp_delete_talker(mrp, nport, info, reserv);
+			else
+				mrp_delete_listener(mrp, nport, info, reserv);
+		}
+		reserv = next;
+	}
+}
+
+static void mrp_reset_reserv(struct mrp_info *mrp, u8 nport)
+{
+	struct mrp_port_info *info;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *next;
+	u8 n;
+	u8 first = nport;
+	u8 last = nport + 1;
+
+	if (nport == mrp->ports) {
+		first = 0;
+		last = mrp->ports + 1;
+	}
+	for (n = first; n < last; n++)
+		mrp_delete_reserv(mrp, n, SRP_LISTENER);
+	for (n = first; n < last; n++)
+		mrp_delete_reserv(mrp, n, SRP_TALKER);
+	for (n = first; n < last; n++) {
+		info = get_port_info(mrp, n);
+		reserv = info->declared.next;
+		while (reserv) {
+			next = reserv->next;
+			if (SRP_TALKER == reserv->direction &&
+			    reserv->stream) {
+				struct mrp_port_info *rx;
+				struct SRP_reserv *t_reserv;
+				struct SRP_stream *stream = reserv->stream;
+
+				rx = get_port_info(mrp, stream->in_port);
+				t_reserv = srp_find_reserv(&rx->registered,
+					stream->id, SRP_TALKER);
+				if (t_reserv) {
+					t_reserv->tx_ports &= ~BIT(n);
+				}
+			}
+			srp_remove_reserv(reserv, true);
+			reserv = next;
+		}
+	}
+}  /* mrp_reset_resev */
+
+static void proc_mrp_attribute(struct mrp_info *mrp, u8 *data)
+{
+	u8 q;
+	u8 in_port;
+	int output;
+	int result;
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+
+	in_port = cmd->port;
+	result = proc_mrp_set_attribute(mrp, 0, data);
+	while (DEV_IOC_MRP_REPORT == result) {
+		cmd->action = MRP_ACTION_TX;
+		cmd->type = MRP_TYPE_UNKNOWN;
+		result = proc_mrp_get_attribute(mrp, 0, data, &output);
+		if (cmd->action != MRP_ACTION_TX &&
+		    cmd->action != MRP_ACTION_TX_NEW &&
+		    cmd->action != MRP_ACTION_LV)
+			continue;
+
+		cmd->new_decl = false;
+		if (MRP_ACTION_TX_NEW == cmd->action) {
+			cmd->action = MRP_ACTION_TX;
+			cmd->new_decl = true;
+		}
+		q = get_actual_port(mrp, cmd->port - 1);
+		if (MRP_TYPE_MAC == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_join_mac(mrp, q, &cmd->data.mac,
+					cmd->new_decl);
+			else
+				mrp->mac_tx[q] |=
+					proc_leave_mac(mrp, q,
+						       &cmd->data.mac);
+		} else if (MRP_TYPE_VLAN == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_join_vlan(mrp, q, &cmd->data.vlan,
+					cmd->new_decl);
+			else
+				mrp->vlan_tx[q] |=
+					proc_leave_vlan(mrp, q,
+							&cmd->data.vlan);
+
+#ifdef CONFIG_KSZ_MSRP
+		} else if (MRP_TYPE_TALKER == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_new_talker(mrp, q,
+					&cmd->data.talker, cmd->new_decl);
+			else
+				mrp->srp_tx[q] |=
+					proc_leave_talker(mrp, q,
+							   &cmd->data.talker);
+		} else if (MRP_TYPE_LISTENER == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_new_listener(mrp, q,
+					&cmd->data.listener, cmd->new_decl);
+			else
+				mrp->srp_tx[q] |=
+					proc_leave_listener(mrp, q,
+							     &cmd->data.listener);
+		} else if (MRP_TYPE_DOMAIN == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_join_domain(mrp, q,
+					&cmd->data.domain, cmd->new_decl);
+			else
+				mrp->srp_tx[q] |=
+					proc_leave_domain(mrp, q,
+							   &cmd->data.domain);
+#endif
+		}
+	}
+}  /* proc_mrp_attribute */
+
+static void mmrp_acton(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	cmd->port = sw_get_net_port(sw, 0, mrp->ports, app->port);
+	if (MMRP_ATTR_MAC == attr->type) {
+		memcpy(cmd->data.mac.addr, attr->value, ETH_ALEN);
+		cmd->type = MRP_TYPE_MAC;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (!(app->normal & (1 << attr->type)))
+			cmd->new_decl |= 0x80;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+	}
+	attr->notify = MRP_NOTIFY_NONE;
+	if (cmd->type != MRP_TYPE_UNKNOWN)
+		proc_mrp_attribute(mrp, data);
+}  /* mmrp_acton */
+
+static void mmrp_cleanup(struct mrp_applicant *app)
+{
+	int q;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (q = 0; q < sw->mib_port_cnt; q++) {
+		if (mrp->mac_tx[q]) {
+			mrp->mac_tx[q] = 0;
+			proc_mrp_req_lv(mrp, q, &mac_mrp_app);
+		}
+	}
+}  /* mmrp_cleanup */
+
+static void mvrp_acton(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	cmd->port = sw_get_net_port(sw, 0, mrp->ports, app->port);
+	if (MVRP_ATTR_VID == attr->type) {
+		u16 *vid = (u16 *) attr->value;
+
+		cmd->data.vlan.id = ntohs(*vid);
+		cmd->type = MRP_TYPE_VLAN;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (!(app->normal & (1 << attr->type)))
+			cmd->new_decl |= 0x80;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+	}
+	attr->notify = MRP_NOTIFY_NONE;
+	if (cmd->type != MRP_TYPE_UNKNOWN)
+		proc_mrp_attribute(mrp, data);
+}  /* mvrp_acton */
+
+static void mvrp_cleanup(struct mrp_applicant *app)
+{
+	int q;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (q = 0; q < sw->mib_port_cnt; q++) {
+		if (mrp->vlan_tx[q]) {
+			mrp->vlan_tx[q] = 0;
+			proc_mrp_req_lv(mrp, q, &vlan_mrp_app);
+		}
+	}
+}  /* mvrp_cleanup */
+
+#ifdef CONFIG_KSZ_MSRP
+static void msrp_acton(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	cmd->port = sw_get_net_port(sw, 0, mrp->ports, app->port);
+	switch (attr->type) {
+	case MSRP_ATTR_TALKER_FAILED:
+	{
+		struct SRP_talker *talker = &cmd->data.talker;
+		struct srp_talker_failed *attr_talker =
+			(struct srp_talker_failed *) attr->value;
+
+		mrp_get_talker(talker, attr_talker);
+		cmd->type = MRP_TYPE_TALKER;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+		break;
+	}
+	case MSRP_ATTR_LISTENER:
+	{
+		struct SRP_listener *listener = &cmd->data.listener;
+		struct srp_listener *attr_listener =
+			(struct srp_listener *) attr->value;
+
+		mrp_get_listener(listener, attr_listener);
+		cmd->type = MRP_TYPE_LISTENER;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+		break;
+	}
+	case MSRP_ATTR_DOMAIN:
+	{
+		struct SRP_domain_class *domain = &cmd->data.domain;
+		struct srp_domain *attr_domain =
+			(struct srp_domain *) attr->value;
+
+		mrp_get_domain(domain, attr_domain);
+		cmd->type = MRP_TYPE_DOMAIN;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+		break;
+	}
+	default:
+		break;
+	}
+	attr->notify = MRP_NOTIFY_NONE;
+	if (cmd->type != MRP_TYPE_UNKNOWN)
+		proc_mrp_attribute(mrp, data);
+}  /* msrp_acton */
+
+static void msrp_cleanup(struct mrp_applicant *app)
+{
+	int q;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (q = 0; q < sw->mib_port_cnt; q++) {
+		if (mrp->srp_tx[q]) {
+			mrp->srp_tx[q] = 0;
+			proc_mrp_req_lv(mrp, q, &srp_mrp_app);
+		}
+	}
+}  /* msrp_cleanup */
+#endif
+#endif
+
+static void setup_mrp(struct mrp_info *mrp, struct net_device *dev)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+#ifdef PROC_MRP
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+	prandom32_seed(&rnd, get_random_int());
+#endif
+	memcpy(mrp->cvlan_addr, vlan_mrp_app.group_address, ETH_ALEN);
+	memcpy(mrp->svlan_addr, vlan_mrp_app.group_address, ETH_ALEN);
+	mrp->svlan_addr[5] = 0x0D;
+#endif
+
+	i = sw->info->multi_sys;
+
+	entry = &sw->info->mac_table[--i];
+	memcpy(entry->addr, maap_addr, ETH_ALEN);
+	entry->fid = 0;
+	entry->ports = sw->PORT_MASK;
+	alu = &sw->info->alu_table[i];
+	alu->forward = FWD_MAIN_DEV | FWD_MCAST | FWD_KNOWN;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+
+	sw->info->multi_sys = i;
+
+	sw->ops->cfg_mac(sw, 0, maap_addr, sw->PORT_MASK, false, false, 0);
+}  /* setup_mrp */
+
+static struct mrp_applicant *mrp_start_port_app(struct mrp_info *mrp,
+	u8 p, struct net_device *dev, struct mrp_application *appl)
+{
+	int err;
+	struct mrp_applicant *app;
+	struct mrp_port *port;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct ksz_port_info *info = &sw->port_info[p];
+
+	port = &mrp->mrp_ports[p];
+	err = mrp_init_applicant(port, mrp, p, dev, appl);
+	app = rcu_dereference(port->applicants[appl->type]);
+	memcpy(app->src_addr, info->mac_addr, ETH_ALEN);
+	return app;
+}  /* mrp_start_port_app */
+
+static void mrp_stop_port_app(struct mrp_port *port,
+	struct mrp_application *appl, int attrtype, bool rx)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	struct mrp_applicant *app;
+
+	app = rcu_dereference(port->applicants[appl->type]);
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+
+		if (rx || (attr->type == attrtype &&
+		    attr->reg_state == MRP_REGISTRAR_MT))
+			mrp_attr_destroy(app, attr);
+	}
+	if (rx)
+		mrp_uninit_applicant(port, appl);
+}  /* mrp_stop_port_app */
+
+static void mrp_start_mmrp_port_app(struct mrp_info *mrp, u8 p,
+				struct net_device *dev)
+{
+	struct mrp_applicant *app;
+
+	app = mrp_start_port_app(mrp, p, dev, &mac_mrp_app);
+	mmrp_init_application(app, mmrp_acton, mmrp_cleanup);
+	mrp_periodic_event(app, MRP_EVENT_PERIODIC_ENABLE);
+}  /* mrp_start_mmrp_port_app */
+
+static void mrp_start_mmrp_app(struct mrp_info *mrp)
+{
+	u8 p;
+	int err;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct net_device *dev = sw->main_dev;
+
+	err = dev_mc_add(dev, mac_mrp_app.group_address);
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_mrp_port(mrp, p))
+			continue;
+		if (!(mrp->mmrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_start_mmrp_port_app(mrp, p, dev);
+	}
+}  /* mrp_start_mmrp_app */
+
+static void mrp_stop_mmrp_app(struct mrp_info *mrp)
+{
+	u8 p;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct net_device *dev = sw->main_dev;
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_mrp_port(mrp, p))
+			continue;
+		if (!(mrp->mmrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_stop_port_app(&mrp->mrp_ports[p], &mac_mrp_app, 0, true);
+	}
+	dev_mc_del(dev, mac_mrp_app.group_address);
+}  /* mrp_stop_mmrp_app */
+
+static void mrp_start_mvrp_port_app(struct mrp_info *mrp, u8 p,
+				struct net_device *dev)
+{
+	struct mrp_applicant *app;
+
+	app = mrp_start_port_app(mrp, p, dev, &vlan_mrp_app);
+	mvrp_init_application(app, mvrp_acton, mvrp_cleanup);
+	app->group_address = mrp->cvlan_addr;
+	mrp_periodic_event(app, MRP_EVENT_PERIODIC_ENABLE);
+}  /* mrp_start_mvrp_port_app */
+
+static void mrp_start_mvrp_app(struct mrp_info *mrp)
+{
+	u8 p;
+	int err;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct net_device *dev = sw->main_dev;
+
+	err = dev_mc_add(dev, vlan_mrp_app.group_address);
+	err = dev_mc_add(dev, mrp->svlan_addr);
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_mrp_port(mrp, p))
+			continue;
+		if (!(mrp->mvrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_start_mvrp_port_app(mrp, p, dev);
+	}
+}  /* mrp_start_mvrp_app */
+
+static void mrp_stop_mvrp_app(struct mrp_info *mrp)
+{
+	u8 p;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct net_device *dev = sw->main_dev;
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_mrp_port(mrp, p))
+			continue;
+		if (!(mrp->mvrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_stop_port_app(&mrp->mrp_ports[p], &vlan_mrp_app, 0, true);
+	}
+	dev_mc_del(dev, vlan_mrp_app.group_address);
+	dev_mc_del(dev, mrp->svlan_addr);
+}  /* mrp_stop_mvrp_app */
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_start_msrp_port_app(struct mrp_info *mrp, u8 p,
+				struct net_device *dev)
+{
+	struct mrp_applicant *app;
+
+	app = mrp_start_port_app(mrp, p, dev, &srp_mrp_app);
+	msrp_init_application(app, msrp_acton, msrp_cleanup);
+	mrp_periodic_event(app, MRP_EVENT_PERIODIC_DISABLE);
+}  /* mrp_start_msrp_port_app */
+
+static void mrp_start_msrp_app(struct mrp_info *mrp)
+{
+	u8 p;
+	int err;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct net_device *dev = sw->main_dev;
+
+	err = dev_mc_add(dev, srp_mrp_app.group_address);
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_mrp_port(mrp, p))
+			continue;
+		if (!mrp->port_info[p].status.msrpPortEnabledStatus)
+			continue;
+
+		mrp_start_msrp_port_app(mrp, p, dev);
+	}
+}  /* mrp_start_msrp_app */
+
+static void mrp_stop_msrp_app(struct mrp_info *mrp)
+{
+	u8 p;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	struct net_device *dev = sw->main_dev;
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_mrp_port(mrp, p))
+			continue;
+		if (!mrp->port_info[p].status.msrpPortEnabledStatus)
+			continue;
+
+		mrp_stop_port_app(&mrp->mrp_ports[p], &srp_mrp_app, 0, true);
+	}
+	dev_mc_del(dev, srp_mrp_app.group_address);
+}  /* mrp_stop_msrp_app */
+#endif
+
+#if 0
+static void proc_mrp_chk_declared(struct mrp_info *mrp, uint port, int on)
+{
+	int result;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	struct mrp_vlan_info *vlan;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	u8 lp = sw_get_net_port(sw, 0, mrp->ports, port);
+
+#if 1
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info;
+
+	info = &mrp->port_info[port];
+		if (!mrp->port_info[p].status.msrpPortEnabledStatus)
+			continue;
+
+		mrp_start_msrp_port_decl(mrp, &mrp->mrp_ports[p]);
+	reserv = info->declared.next;
+	while (reserv) {
+		if (on) {
+			reserv->tx_ports |= BIT(lp);
+			if (reserv->direction == SRP_TALKER) {
+				struct SRP_talker talker;
+
+				memcpy(&talker, reserv->stream, 25);
+				memcpy(talker.bridge_id, reserv->bridge_id, 8);
+				talker.FailureCode = reserv->code;
+				result = proc_mrp_new_talker(mrp, port,
+					&talker, true);
+			} else if (reserv->direction == SRP_LISTENER) {
+				struct SRP_listener listener;
+
+				memcpy(&listener, reserv->stream, 8);
+				listener.substate = reserv->declaration;
+				result = proc_mrp_new_listener(mrp, port,
+					&listener, true);
+			}
+		} else {
+			reserv->tx_ports &= ~BIT(lp);
+		}
+		reserv = reserv->next;
+	}
+#endif
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+
+		if (mac->tx_ports & BIT(lp) && !on) {
+			mac->tx_ports &= ~BIT(lp);
+		} else if (!(mac->tx_ports & BIT(port)) && on) {
+			struct MRP_mac mrp_mac;
+
+			memcpy(mrp_mac.addr, mac->addr, ETH_ALEN);
+			mac->tx_ports |= BIT(lp);
+			result = proc_mrp_join_mac(mrp, port,
+				&mrp_mac, true);
+		}
+		prev = next;
+		next = prev->next;
+	}
+	prev = &mrp->vlan_list.anchor;
+	next = prev->next;
+	while (next) {
+		vlan = next->data;
+
+		if (vlan->tx_ports & BIT(port) && !on) {
+			vlan->tx_ports &= ~BIT(lp);
+		} else if (!(vlan->tx_ports & BIT(port)) && on) {
+			struct MRP_vlan mrp_vlan;
+
+			vlan->tx_ports |= BIT(lp);
+			mrp_vlan.id = vlan->vid;
+			result = proc_mrp_join_vlan(mrp, port,
+				&mrp_vlan, true);
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* proc_mrp_chk_declared */
+#endif
+
+static void mrp_start_app(struct mrp_info *mrp)
+{
+	mrp_start_mmrp_app(mrp);
+	mrp_start_mvrp_app(mrp);
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus)
+		mrp_start_msrp_app(mrp);
+#endif
+}  /* mrp_start_app */
+
+static void mrp_stop_app(struct mrp_info *mrp)
+{
+	mrp_stop_mmrp_app(mrp);
+	mrp_stop_mvrp_app(mrp);
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus)
+		mrp_stop_msrp_app(mrp);
+#endif
+}  /* mrp_stop_app */
+
+static void mrp_mmrp_decl(struct mrp_info *mrp, int q, u8 lp, int on)
+{
+	int result;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+
+		if ((mac->tx_ports & BIT(lp)) && !on) {
+			/* Leave request will be sent. */
+			mac->tx_ports &= ~BIT(lp);
+		} else if (!(mac->tx_ports & BIT(lp)) && on) {
+			struct MRP_mac mrp_mac;
+
+			memcpy(mrp_mac.addr, mac->addr, ETH_ALEN);
+			mac->tx_ports |= BIT(lp);
+			result = proc_mrp_join_mac(mrp, q, &mrp_mac, true);
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* mrp_mmrp_decl */
+
+static void mrp_mvrp_decl(struct mrp_info *mrp, int q, u8 lp, int on)
+{
+	int result;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *vlan;
+
+	prev = &mrp->vlan_list.anchor;
+	next = prev->next;
+	while (next) {
+		vlan = next->data;
+
+		if (vlan->ports & SRP_PORT_AVAIL) {
+			if ((vlan->tx_ports & BIT(lp)) && !on) {
+				/* Leave request will be sent. */
+				vlan->tx_ports &= ~BIT(lp);
+			} else if (!(vlan->tx_ports & BIT(lp)) && on) {
+				struct MRP_vlan mrp_vlan;
+
+				vlan->tx_ports |= BIT(lp);
+				mrp_vlan.id = vlan->vid;
+				result = proc_mrp_join_vlan(mrp, q, &mrp_vlan,
+							    true);
+			}
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* mrp_mvrp_decl */
+
+static void mrp_msrp_decl(struct mrp_info *mrp, int q, u8 lp, int on)
+{
+	int result;
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info;
+
+	info = &mrp->port_info[q];
+	reserv = info->declared.next;
+	while (reserv) {
+		if (on) {
+			reserv->tx_ports |= BIT(lp);
+			if (reserv->direction == SRP_TALKER) {
+				struct SRP_talker talker;
+
+				memcpy(&talker, reserv->stream, 25);
+				memcpy(talker.bridge_id, reserv->bridge_id, 8);
+				talker.FailureCode = reserv->code;
+				result = proc_mrp_new_talker(mrp, q,
+					&talker, true);
+			} else if (reserv->direction == SRP_LISTENER) {
+				struct SRP_listener listener;
+
+				memcpy(&listener, reserv->stream, 8);
+				listener.substate = reserv->declaration;
+				result = proc_mrp_new_listener(mrp, q,
+					&listener, true);
+			}
+		} else {
+			/* Leave request will be sent. */
+			reserv->tx_ports &= ~BIT(lp);
+		}
+		reserv = reserv->next;
+	}
+	if (on) {
+		proc_mrp_join_domain(mrp, q, &mrp->domain[0], true);
+		if (mrp->domain[1].id)
+			proc_mrp_join_domain(mrp, q, &mrp->domain[1], true);
+	}
+}  /* mrp_msrp_decl */
+
+static void mmrp_close_port(struct mrp_info *mrp, int q, u8 lp)
+{
+	/* MMRP is not enabled. */
+	if (!(mrp->mmrp_tx_ports & BIT(q)))
+		return;
+	mrp_mmrp_decl(mrp, q, lp, false);
+}  /* mmrp_close_port */
+
+static void mmrp_open_port(struct mrp_info *mrp, int q, u8 lp)
+{
+	struct mrp_applicant *app;
+	struct mrp_port *port = &mrp->mrp_ports[q];
+
+	/* MMRP is not enabled. */
+	if (!(mrp->mmrp_tx_ports & BIT(q)))
+		return;
+	app = rcu_dereference(port->applicants[mac_mrp_app.type]);
+if (!app)
+dbg_msg(" %s\n", __func__);
+	if (!app) {
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		mrp_start_mmrp_port_app(mrp, q, sw->main_dev);
+	}
+	mrp_mmrp_decl(mrp, q, lp, true);
+}  /* mmrp_open_port */
+
+static void mvrp_close_port(struct mrp_info *mrp, int q, u8 lp)
+{
+	/* MVRP is not enabled. */
+	if (!(mrp->mvrp_tx_ports & BIT(q)))
+		return;
+	mrp_mvrp_decl(mrp, q, lp, false);
+}  /* mvrp_close_port */
+
+static void mvrp_open_port(struct mrp_info *mrp, int q, u8 lp)
+{
+	struct mrp_applicant *app;
+	struct mrp_port *port = &mrp->mrp_ports[q];
+
+	/* MVRP is not enabled. */
+	if (!(mrp->mvrp_tx_ports & BIT(q)))
+		return;
+	app = rcu_dereference(port->applicants[vlan_mrp_app.type]);
+if (!app)
+dbg_msg(" %s\n", __func__);
+	if (!app) {
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		mrp_start_mvrp_port_app(mrp, q, sw->main_dev);
+	}
+	mrp_mvrp_decl(mrp, q, lp, true);
+}  /* mvrp_open_port */
+
+static void msrp_close_port(struct mrp_info *mrp, int q, u8 lp)
+{
+	/* MSRP is not enabled. */
+	if (!mrp->status.msrpEnabledStatus ||
+	    !mrp->port_info[q].status.msrpPortEnabledStatus)
+		return;
+	mrp_msrp_decl(mrp, q, lp, false);
+#if 0
+{
+	struct mrp_port *port = &mrp->mrp_ports[q];
+struct mrp_applicant *app;
+	app = rcu_dereference(port->applicants[srp_mrp_app.type]);
+dbg_msg(" timer: %d\n", app->timer_arm.join);
+}
+#endif
+}  /* msrp_close_port */
+
+static void msrp_open_port(struct mrp_info *mrp, int q, u8 lp)
+{
+	struct mrp_applicant *app;
+	struct mrp_port *port = &mrp->mrp_ports[q];
+
+dbg_msg("%s %d %d\n", __func__, q, lp);
+	/* MSRP is not enabled. */
+	if (!mrp->status.msrpEnabledStatus ||
+	    !mrp->port_info[q].status.msrpPortEnabledStatus)
+		return;
+	app = rcu_dereference(port->applicants[srp_mrp_app.type]);
+if (!app)
+dbg_msg(" %s\n", __func__);
+	if (!app) {
+		struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+		mrp_start_msrp_port_app(mrp, q, sw->main_dev);
+	}
+	mrp_msrp_decl(mrp, q, lp, true);
+}  /* msrp_open_port */
+
+static void mrp_close_port(struct mrp_info *mrp, int q)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	u8 lp = sw_get_net_port(sw, 0, mrp->ports, q) - 1;
+
+dbg_msg("%s %d %d\n", __func__, q, mrp->started);
+	if (!mrp->started || !(mrp->tx_ports & BIT(q)))
+		return;
+	if (skip_mrp_port(mrp, q))
+		return;
+	msrp_close_port(mrp, q, lp);
+	mvrp_close_port(mrp, q, lp);
+	mmrp_close_port(mrp, q, lp);
+
+	/* Every declaration leaves. */
+	mrp_event(mrp, q, MRP_EVENT_LV);
+#if 1
+	mrp_event(mrp, q, MRP_EVENT_R_LV);
+#endif
+	mrp->tx_ports &= ~BIT(q);
+	mrp->rx_ports = mrp->tx_ports;
+}  /* mrp_close_port */
+
+static void mrp_open_port(struct mrp_info *mrp, int q)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	u8 lp = sw_get_net_port(sw, 0, mrp->ports, q) - 1;
+
+dbg_msg("%s %d %d\n", __func__, q, mrp->started);
+	if (!mrp->started || (mrp->tx_ports & BIT(q)))
+		return;
+	if (skip_mrp_port(mrp, q))
+		return;
+	msrp_open_port(mrp, q, lp);
+	mvrp_open_port(mrp, q, lp);
+	mmrp_open_port(mrp, q, lp);
+	mrp->tx_ports |= BIT(q);
+	mrp->rx_ports = mrp->tx_ports;
+}  /* mrp_open_port */
+
+#if 0
+static void msrp_close_ports(struct mrp_info *mrp)
+{
+	u8 p;
+	u8 lp;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		lp = sw_get_net_port(sw, 0, mrp->ports, p) - 1;
+		msrp_close_port(mrp, p, lp);
+	}
+}  /* msrp_close_ports */
+#endif
+
+static void msrp_open_ports(struct mrp_info *mrp)
+{
+	u8 p;
+	u8 lp;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		lp = sw_get_net_port(sw, 0, mrp->ports, p) - 1;
+		if (p == sw->HOST_PORT ||
+		    sw->info->port_cfg[p].stp_state[0] == STP_STATE_FORWARDING)
+			msrp_open_port(mrp, p, lp);
+	}
+}  /* msrp_open_ports */
+
+static void mrp_close_ports(struct mrp_info *mrp)
+{
+	u8 p;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		mrp_close_port(mrp, p);
+	}
+}  /* mrp_close_ports */
+
+static void mrp_open_ports(struct mrp_info *mrp)
+{
+	u8 p;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (p == sw->HOST_PORT ||
+		    sw->info->port_cfg[p].stp_state[0] == STP_STATE_FORWARDING)
+			mrp_open_port(mrp, p);
+	}
+}  /* mrp_open_ports */
+
+static void mrp_from_backup(struct mrp_info *mrp, uint p)
+{
+dbg_msg("%s %d\n", __func__, p);
+	mrp->rx_ports |= (1 << p);
+
+	/* mrp_open_port will be called when the port is forwarding. */
+}  /* mrp_from_backup */
+
+static void mrp_to_backup(struct mrp_info *mrp, uint p)
+{
+dbg_msg("%s %d\n", __func__, p);
+	mrp_close_port(mrp, p);
+}  /* mrp_to_backup */
+
+static void mrp_from_designated(struct mrp_info *mrp, uint p, bool alt)
+{
+dbg_msg("%s %d=%d\n", __func__, p, alt);
+	mrp_event(mrp, p, MRP_EVENT_REDECLARE);
+	if (alt) {
+		mrp_close_port(mrp, p);
+	}
+}  /* mrp_from_designated */
+
+static void mrp_to_designated(struct mrp_info *mrp, uint p)
+{
+dbg_msg("%s %d\n", __func__, p);
+	mrp_event(mrp, p, MRP_EVENT_FLUSH);
+	if (!(mrp->rx_ports & (1 << p))) {
+dbg_msg("  fwd again\n");
+		mrp->rx_ports |= (1 << p);
+
+		/* mrp_open_port will be called when the port is forwarding. */
+	}
+}  /* mrp_to_designated */
+
+static void mrp_tc_detected(struct mrp_info *mrp, uint p)
+{
+dbg_msg("%s %d\n", __func__, p);
+	if (skip_mrp_port(mrp, p))
+		return;
+	if (mrp->mmrp_rx_ports & (1 << p)) {
+		mrp_new_event_port_app(mrp, p, &mac_mrp_app);
+	}
+	if (mrp->mvrp_rx_ports & (1 << p)) {
+		mrp_new_event_port_app(mrp, p, &vlan_mrp_app);
+	}
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus &&
+	    mrp->port_info[p].status.msrpPortEnabledStatus) {
+		mrp_new_event_port_app(mrp, p, &srp_mrp_app);
+	}
+#endif
+}  /* mrp_tc_detected */
+
+#if 0
+static const u8 test_id[] = {
+	0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x01,
+};
+
+static void mrp_validate(struct mrp_info *mrp)
+{
+	int i;
+	int result;
+	int inc = 1;
+
+#if 1
+	do {
+		struct SRP_listener listener;
+		u8 old;
+
+		listener.substate = 2;
+		memcpy(&listener.id, test_id, 8);
+#if 0
+		for (i = 0; i < 21; i++) {
+			result = proc_mrp_new_listener(mrp, 0, &listener, true);
+			listener.id[7] += inc;
+			if (!listener.id[7]) {
+				listener.id[6]++;
+				if (!listener.id[6]) {
+					listener.id[5]++;
+				}
+			}
+		}
+
+		memcpy(&listener.id, test_id, 8);
+		listener.id[2] += 1;
+		inc = 2;
+		for (i = 0; i < 2; i++) {
+			result = proc_mrp_new_listener(mrp, 0, &listener, true);
+			listener.id[7] += inc;
+			if (!listener.id[7]) {
+				listener.id[6]++;
+				if (!listener.id[6]) {
+					listener.id[5]++;
+				}
+			}
+		}
+
+#if 1
+		memcpy(&listener.id, test_id, 8);
+		listener.id[2] += 2;
+		inc = 19;
+		for (i = 0; i < 2; i++) {
+			result = proc_mrp_new_listener(mrp, 0, &listener, true);
+			listener.id[7] += inc;
+			if (!listener.id[7]) {
+				listener.id[6]++;
+				if (!listener.id[6]) {
+					listener.id[5]++;
+				}
+			}
+		}
+
+		memcpy(&listener.id, test_id, 8);
+		listener.id[2] += 3;
+		inc = 20;
+		for (i = 0; i < 2; i++) {
+			result = proc_mrp_new_listener(mrp, 0, &listener, true);
+			listener.id[7] += inc;
+			if (!listener.id[7]) {
+				listener.id[6]++;
+				if (!listener.id[6]) {
+					listener.id[5]++;
+				}
+			}
+		}
+
+		memcpy(&listener.id, test_id, 8);
+		listener.id[2] += 4;
+		inc = 21;
+		for (i = 0; i < 2; i++) {
+			result = proc_mrp_new_listener(mrp, 0, &listener, true);
+			listener.id[7] += inc;
+			if (!listener.id[7]) {
+				listener.id[6]++;
+				if (!listener.id[6]) {
+					listener.id[5]++;
+				}
+			}
+		}
+#endif
+#endif
+
+		memcpy(&listener.id, test_id, 8);
+		listener.id[2] += 4;
+		inc = 21;
+		for (i = 0; i < 130; i++) {
+			result = proc_mrp_new_listener(mrp, 0, &listener, true);
+			old = listener.id[7];
+			listener.id[7] += inc;
+			if (listener.id[7] < old) {
+				listener.id[6]++;
+				if (!listener.id[6]) {
+					listener.id[5]++;
+				}
+			}
+		}
+	} while (0);
+#endif
+#if 1
+	do {
+		struct MRP_vlan vlan;
+
+		vlan.id = 2;
+		inc = 10;
+		for (i = 0; i < 300; i++) {
+			result = proc_mrp_join_vlan(mrp, 0, &vlan, true);
+			vlan.id += inc;
+		}
+	} while (0);
+#endif
+}
+#endif
+
+static void mrp_start(struct mrp_info *mrp)
+{
+	mrp_start_app(mrp);
+	mrp->started = true;
+	mrp_open_ports(mrp);
+#if 0
+	mrp_validate(mrp);
+#endif
+}  /* mrp_start */
+
+static void mrp_stop(struct mrp_info *mrp)
+{
+	if (mrp->started)
+		mrp_stop_app(mrp);
+}  /* mrp_stop */
+
+static void mrp_open(struct mrp_info *mrp)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	struct mrp_vlan_info *vlan;
+
+#ifdef CONFIG_KSZ_MSRP
+	u8 n;
+	u8 p;
+	u8 tc;
+	struct mrp_port_info *info;
+	struct mrp_traffic_info *traffic;
+#endif
+	u16 ports = 0;
+	struct mrp_vlan_info *prev_vlan = NULL;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+		prev = next;
+		next = prev->next;
+	}
+
+	prev = &mrp->vlan_list.anchor;
+	next = prev->next;
+	while (next) {
+		vlan = next->data;
+
+		/* The very first one. */
+		if (!prev_vlan)
+			prev_vlan = vlan;
+		if (prev_vlan->vid == vlan->vid)
+			ports |= vlan->ports;
+		else {
+			mrp_cfg_vlan(mrp, prev_vlan->index, prev_vlan->vid,
+				prev_vlan->fid, ports);
+			prev_vlan = vlan;
+			ports = vlan->ports;
+		}
+		prev = next;
+		next = prev->next;
+	}
+	if (ports)
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+
+#ifdef CONFIG_KSZ_MSRP
+	for (n = 0; n <= mrp->ports; n++) {
+		info = get_port_info(mrp, n);
+		for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+			traffic = get_traffic_info(info, tc);
+			if (!traffic->bandwidth_used)
+				continue;
+			srp_cfg_credit_shaper(mrp, n, info, traffic);
+			traffic->bandwidth_set = traffic->bandwidth_used;
+		}
+		p = get_actual_port(mrp, n);
+		if (n < mrp->ports) {
+			setup_acl_drop(mrp, p);
+			setup_acl_remap(mrp, p);
+		}
+		for (tc = 0; tc < 4; tc++)
+			srp_cfg_idle_slope(mrp, p, tc, info,
+				info->bandwidth[tc].adminIdleSlope);
+	}
+#endif
+}  /* mrp_open */
+
+static void mrp_clr_blocked_addr(struct mrp_info *mrp, int hw_access)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		if ((mac->rx_ports & SRP_PORT_DROP) &&
+		    (mac->srp_ports & SRP_PORT_BLACKLIST)) {
+dbg_msg("  rmv: %02x:%02x:%02x:%02x:%02x:%02x %d\n",
+mac->addr[0], mac->addr[1], mac->addr[2], mac->addr[3], mac->addr[4], mac->addr[5], mac->fid);
+			if (hw_access)
+				mrp_cfg_dest_addr(mrp, mac->index, mac->addr,
+					0, mac->fid);
+
+			/* Remove node from list. */
+			prev->next = next->next;
+			mrp_free_node(next);
+		} else
+			prev = next;
+		next = prev->next;
+	}
+	mrp->mac_list.last = &mrp->mac_list.anchor;
+}  /* mrp_clr_blocked_addr */
+
+static void mrp_chk_blocked_addr(struct mrp_info *mrp)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		if ((mac->srp_ports & SRP_PORT_BLACKLIST) &&
+		    time_after_eq(jiffies, mac->jiffies)) {
+			uint p;
+			bool avb;
+			u16 ports;
+			struct ksz_port_cfg *cfg;
+
+			if (mac->rx_ports & SRP_PORT_IGNORE)
+				mac->rx_ports |= SRP_PORT_DROP;
+			mac->rx_ports |= SRP_PORT_IGNORE;
+			avb = true;
+			for (p = 0; p < sw->mib_port_cnt; p++) {
+				if (mac->rx_ports & (1 << p)) {
+					cfg = &sw->info->port_cfg[p];
+					if (!cfg->avb_b) {
+						avb = false;
+						break;
+					}
+				}
+			}
+			if (avb)
+				ports = sw->HOST_MASK;
+			else
+				ports = sw->PORT_MASK;
+			sw->ops->cfg_mac(sw, mac->index, mac->addr, ports,
+					 false, mac->fid != 0, mac->fid);
+			mac->jiffies = jiffies + msecs_to_jiffies(10000);
+		}
+		prev = next;
+		next = prev->next;
+	}
+	mrp_clr_blocked_addr(mrp, true);
+}  /* mrp_chk_blocked_addr */
+
+static void mrp_close(struct mrp_info *mrp, int hw_access)
+{
+	if (mrp->started) {
+		mrp_clr_blocked_addr(mrp, hw_access);
+		mrp_close_ports(mrp);
+		mrp->started = false;
+	}
+}  /* mrp_close */
+
+static ssize_t sysfs_mrp_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct mrp_info *mrp = &sw->mrp;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & AVB_SUPPORT))
+		return 0;
+	switch (proc_num) {
+	case PROC_SET_MRP_SRC_ADDR:
+		chk = 0;
+		break;
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_GET_MSRP_INFO:
+		len += sprintf(buf + len,
+			"max interference size %u\n",
+			mrp->max_interference_size);
+		if (regeneration_hack)
+			len += sprintf(buf + len,
+				"regeneration\n");
+		if (ba_hack)
+			len += sprintf(buf + len,
+				"BA\n");
+		if (mrp_10_1_2f_hack)
+			len += sprintf(buf + len,
+				"MRP.10.1.2F\n");
+		if (mrp_10_1_8a_hack)
+			len += sprintf(buf + len,
+				"MRP.10.1.8A\n");
+		if (mrp_10_5_1_hack)
+			len += sprintf(buf + len,
+				"MRP.10.5.1\n");
+		if (mrp_10_5_1c_hack)
+			len += sprintf(buf + len,
+				"MRP.10.5.1C\n");
+		if (mrp_10_5_1d_hack)
+			len += sprintf(buf + len,
+				"MRP.10.5.1D\n");
+		if (msrp_35_1_14g_hack)
+			len += sprintf(buf + len,
+				"MSRP.35.1.14G\n");
+		if (fqtss_hack)
+			len += sprintf(buf + len,
+				"FQTSS\n");
+		if (fqtss_34_2_3_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.3\n");
+		if (fqtss_34_2_1b_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.1B\n");
+		if (fqtss_34_2_5b_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.5B\n");
+		if (fqtss_34_2_9b_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.9B\n");
+		break;
+	case PROC_SET_MSRP_ENABLED:
+		chk = mrp->status.msrpEnabledStatus;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_MSRP_SR_A:
+		chk = mrp->domain[1].id != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+#endif
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_mrp_read */
+
+static int sysfs_mrp_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct mrp_info *mrp = &sw->mrp;
+	int processed = true;
+
+	if (!(sw->features & AVB_SUPPORT))
+		return false;
+	switch (proc_num) {
+	case PROC_SET_MRP_SRC_ADDR:
+	{
+		struct mrp_applicant *app;
+		u8 p;
+		u8 *mac_addr;
+		struct ksz_port_info *info;
+
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (skip_mrp_port(mrp, p))
+				continue;
+
+			info = &sw->port_info[p];
+			if (num)
+				mac_addr = info->mac_addr;
+			else
+				mac_addr = sw->info->mac_addr;
+			app = rcu_dereference(mrp->mrp_ports[p].applicants[
+					      mac_mrp_app.type]);
+			memcpy(app->src_addr, mac_addr, ETH_ALEN);
+
+			app = rcu_dereference(mrp->mrp_ports[p].applicants[
+					      vlan_mrp_app.type]);
+			memcpy(app->src_addr, mac_addr, ETH_ALEN);
+
+#ifdef CONFIG_KSZ_MSRP
+			app = rcu_dereference(mrp->mrp_ports[p].applicants[
+					      srp_mrp_app.type]);
+			memcpy(app->src_addr, mac_addr, ETH_ALEN);
+#endif
+		}
+		break;
+	}
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_GET_MSRP_INFO:
+		if (!strncmp(buf, "mrp_10.1.2f", 11))
+			mrp_10_1_2f_hack = true;
+		else if (!strncmp(buf, "mrp_10.1.8a", 11))
+			mrp_10_1_8a_hack = true;
+		else if (!strncmp(buf, "mrp_10.5.1c", 11))
+			mrp_10_5_1c_hack = true;
+		else if (!strncmp(buf, "mrp_10.5.1d", 11))
+			mrp_10_5_1d_hack = true;
+		else if (!strncmp(buf, "mrp_10.5.1", 10))
+			mrp_10_5_1_hack = true;
+		else if (!strncmp(buf, "msrp_35.1.14g", 13))
+			msrp_35_1_14g_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.3", 12))
+			fqtss_34_2_3_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.1b", 13))
+			fqtss_34_2_1b_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.5b", 13))
+			fqtss_34_2_5b_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.9b", 13))
+			fqtss_34_2_9b_hack = true;
+		else if (!strncmp(buf, "fqtss", 5))
+			fqtss_hack = true;
+		else if (!strncmp(buf, "ba", 2))
+			ba_hack = true;
+		else if (!strncmp(buf, "regeneration", 12)) {
+			if (!regeneration_hack) {
+				uint n;
+				uint p;
+
+				regeneration_hack = true;
+				for (n = 0; n < mrp->ports; n++) {
+					p = get_actual_port(mrp, n);
+					setup_acl_remap(mrp, p);
+				}
+			}
+		} else if (!strncmp(buf, "reset", 5)) {
+			ba_hack = false;
+			mrp_10_1_2f_hack = false;
+			mrp_10_1_8a_hack = false;
+			mrp_10_5_1_hack = false;
+			mrp_10_5_1c_hack = false;
+			mrp_10_5_1d_hack = false;
+			msrp_35_1_14g_hack = false;
+			fqtss_hack = false;
+			fqtss_34_2_3_hack = false;
+			fqtss_34_2_1b_hack = false;
+			fqtss_34_2_5b_hack = false;
+			fqtss_34_2_9b_hack = false;
+			if (regeneration_hack) {
+				uint n;
+				uint p;
+
+				regeneration_hack = false;
+				for (n = 0; n < mrp->ports; n++) {
+					p = get_actual_port(mrp, n);
+					setup_acl_remap(mrp, p);
+				}
+			}
+		}
+else
+dbg_msg(" ?? %s]\n", buf);
+		break;
+	case PROC_SET_MSRP_ENABLED:
+		if (mrp->status.msrpEnabledStatus != !!num) {
+			if (!mrp->status.msrpEnabledStatus) {
+				mrp_reset_reserv(mrp, mrp->ports);
+				mrp->status.msrpEnabledStatus = true;
+				mrp_start_msrp_app(mrp);
+				msrp_open_ports(mrp);
+			} else {
+#if 0
+				msrp_close_ports(mrp);
+#endif
+				mrp_stop_msrp_app(mrp);
+				mrp->status.msrpEnabledStatus = false;
+			}
+		}
+		break;
+	case PROC_SET_MSRP_SR_A:
+	{
+		struct srp_domain domain;
+		struct mrp_applicant *app;
+		struct mrp_port_info *info;
+		u32 delta_a;
+		u32 delta_b;
+		u8 p;
+		u8 lp;
+		int join = 0;
+
+		if (num && !mrp->domain[1].id) {
+			mrp->domain[1].id = SR_CLASS_A;
+			join = 2;
+		} else if (!num && mrp->domain[1].id) {
+			join = 1;
+		}
+		if (!join)
+			break;
+		domain.class_id = mrp->domain[1].id;
+		domain.class_priority = mrp->domain[1].priority;
+		domain.class_vid = htons(mrp->domain[1].vlan_id);
+		--join;
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (skip_mrp_port(mrp, p))
+				continue;
+
+			app = rcu_dereference(mrp->mrp_ports[p].applicants[
+					      srp_mrp_app.type]);
+			if (join)
+				msrp_req_join_domain(app, &domain, true);
+			else
+				msrp_req_leave_domain(app, &domain);
+		}
+		if (!join) {
+			mrp->domain[1].id = 0;
+		}
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (skip_mrp_port(mrp, p))
+				continue;
+			info = &mrp->port_info[p];
+			delta_a = info->bandwidth[3].deltaBandwidth;
+			delta_b = info->bandwidth[2].deltaBandwidth;
+			if (join) {
+				if (delta_a > 95)
+					delta_a = 95;
+				if (delta_a + delta_b > 95)
+					delta_b = 95 - delta_a;
+			} else {
+				delta_b += delta_a;
+				delta_a = 0;
+				if (delta_b > 95)
+					delta_b = 95;
+			}
+			lp = sw_get_net_port(sw, 0, mrp->ports, p);
+			mrp_set_delta(mrp, lp, delta_a, delta_b);
+		}
+		break;
+	}
+#endif
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mrp_write */
+
+static ssize_t sysfs_mrp_port_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct mrp_info *mrp = &sw->mrp;
+#ifdef CONFIG_KSZ_MSRP
+	struct mrp_port_info *info;
+	int index;
+	char per_str[20];
+#endif
+	struct ksz_port_cfg *cfg;
+	int chk = 0;
+	int type = SHOW_HELP_NUM;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & AVB_SUPPORT))
+		return 0;
+	if (skip_mrp_port(mrp, port))
+		return 0;
+	cfg = &sw->info->port_cfg[port];
+#ifdef CONFIG_KSZ_MSRP
+	index = cfg->q_index;
+	info = &mrp->port_info[port];
+#endif
+	switch (proc_num) {
+	case PROC_SET_PORT_MMRP_ENABLED:
+		chk = !!(mrp->mmrp_rx_ports & (1 << port));
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MMRP_MAC:
+		chk = 0;
+		if (mrp->mmrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[mac_mrp_app.type]);
+			chk = !!(app->normal & (1 << MMRP_ATTR_MAC));
+		}
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MMRP_SVC:
+		chk = 0;
+		if (mrp->mmrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[mac_mrp_app.type]);
+			chk = !!(app->normal & (1 << MMRP_ATTR_SVC));
+		}
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MMRP_REG:
+		if (mrp->mmrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+			struct mrp_attr *attr;
+			char *str;
+			struct ksz_mac_table *entry;
+
+			entry = &sw->info->mac_entry;
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[mac_mrp_app.type]);
+			attr = mrp_attr_lookup(app, entry->addr, ETH_ALEN,
+					MMRP_ATTR_MAC);
+			str = "unregistered";
+			if (attr) {
+				if (attr->fix_state != MRP_REGISTRAR_LV) {
+					if (attr->fix_state == MRP_REGISTRAR_IN)
+						str = "fixed";
+					else
+						str = "forbidden";
+				} else if (attr->reg_state == MRP_REGISTRAR_IN)
+					str = "registered";
+			}
+			len += sprintf(buf + len,
+				"%s\n", str);
+		}
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_PORT_MVRP_ENABLED:
+		chk = !!(mrp->mvrp_rx_ports & (1 << port));
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MVRP_VID:
+		chk = 0;
+		if (mrp->mvrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[vlan_mrp_app.type]);
+			chk = !!(app->normal & (1 << MVRP_ATTR_VID));
+		}
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MVRP_REG:
+		if (mrp->mvrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+			struct mrp_attr *attr;
+			char *str;
+			__be16 vlan_id = htons(sw->vlan_index);
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[vlan_mrp_app.type]);
+			attr = mrp_attr_lookup(app, &vlan_id, sizeof(vlan_id),
+					MVRP_ATTR_VID);
+			str = "unregistered";
+			if (attr) {
+				if (attr->fix_state != MRP_REGISTRAR_LV) {
+					if (attr->fix_state == MRP_REGISTRAR_IN)
+						str = "fixed";
+					else
+						str = "forbidden";
+				} else if (attr->reg_state == MRP_REGISTRAR_IN)
+					str = "registered";
+			}
+			len += sprintf(buf + len,
+				"%s\n", str);
+		}
+		type = SHOW_HELP_NONE;
+		break;
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_SET_PORT_ASCAPABLE:
+		chk = cfg->asCapable;
+		if (!chk && cfg->asCapable_set)
+			chk = 2;
+		break;
+	case PROC_SET_PORT_MSRP_ENABLED:
+		chk = info->status.msrpPortEnabledStatus;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TC_DELTA_BANDWIDTH:
+		chk = info->bandwidth[index].deltaBandwidth;
+		break;
+	case PROC_SET_TC_ADMIN_IDLE_SLOPE:
+		format_per(per_str, info->bandwidth[index].adminIdleSlope);
+		len += sprintf(buf + len,
+			"%s\n", per_str);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_TC_OPER_IDLE_SLOPE:
+		format_per(per_str, info->bandwidth[index].operIdleSlope);
+		len += sprintf(buf + len,
+			"%s\n", per_str);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_TC_ALGORITHM:
+		chk = info->algorithm[index].algorithm;
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (strict priority)");
+				break;
+			case 1:
+				strcpy(note, " (credit-based shaping)");
+				break;
+			case 3:
+				strcpy(note, " (weighted round robin)");
+				break;
+			default:
+				strcpy(note, " (reserved)");
+				break;
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_GET_SR_0_RX_PRIO:
+		chk = info->priority[SR_CLASS_B].received_priority;
+		break;
+	case PROC_SET_SR_0_TX_PRIO:
+		chk = info->priority[SR_CLASS_B].regenerated_priority;
+		break;
+	case PROC_GET_SR_0_SRP_DOMAIN_BOUNDARY:
+		chk = info->priority[SR_CLASS_B].SRPdomainBoundaryPort;
+		break;
+	case PROC_GET_SR_1_RX_PRIO:
+		chk = info->priority[SR_CLASS_A].received_priority;
+		break;
+	case PROC_SET_SR_1_TX_PRIO:
+		chk = info->priority[SR_CLASS_A].regenerated_priority;
+		break;
+	case PROC_GET_SR_1_SRP_DOMAIN_BOUNDARY:
+		chk = info->priority[SR_CLASS_A].SRPdomainBoundaryPort;
+		break;
+#endif
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_mrp_port_read */
+
+static int sysfs_mrp_port_write(struct ksz_sw *sw, int proc_num, uint port,
+	int num, const char *buf)
+{
+	struct mrp_info *mrp = &sw->mrp;
+#ifdef CONFIG_KSZ_MSRP
+	struct mrp_port_info *info;
+	int index;
+#endif
+	u8 lp;
+	struct ksz_port_cfg *cfg;
+	int processed = true;
+
+	if (!(sw->features & AVB_SUPPORT))
+		return false;
+	if (skip_mrp_port(mrp, port))
+		return false;
+	cfg = &sw->info->port_cfg[port];
+#ifdef CONFIG_KSZ_MSRP
+	index = cfg->q_index;
+	info = &mrp->port_info[port];
+#endif
+	switch (proc_num) {
+	case PROC_SET_PORT_MMRP_ENABLED:
+		if (!!(mrp->mmrp_rx_ports & (1 << port)) != !!num) {
+			if (num) {
+				mrp->mmrp_rx_ports |= (1 << port);
+				mrp->mmrp_tx_ports |= (1 << port);
+				mrp_start_mmrp_port_app(mrp, port,
+							sw->main_dev);
+			} else {
+				mrp->mmrp_rx_ports &= ~(1 << port);
+				mrp_stop_port_app(&mrp->mrp_ports[port],
+						  &mac_mrp_app, 0, true);
+			}
+		}
+		break;
+	case PROC_SET_PORT_MMRP_MAC:
+		if (mrp->mvrp_rx_ports & (1 << port)) {
+			int on;
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[mac_mrp_app.type]);
+			on = !!(app->normal & (1 << MMRP_ATTR_MAC));
+			if (on != !!num) {
+				if (num) {
+					app->normal |= (1 << MMRP_ATTR_MAC);
+				} else {
+					app->normal &= ~(1 << MMRP_ATTR_MAC);
+					mrp_stop_port_app(&mrp->mrp_ports[port],
+							  &mac_mrp_app,
+							  MMRP_ATTR_MAC, false);
+				}
+			}
+		}
+		break;
+	case PROC_SET_PORT_MMRP_SVC:
+		if (mrp->mvrp_rx_ports & (1 << port)) {
+			int on;
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[mac_mrp_app.type]);
+			on = !!(app->normal & (1 << MMRP_ATTR_SVC));
+			if (on != !!num) {
+				if (num) {
+					app->normal |= (1 << MMRP_ATTR_SVC);
+				} else {
+					app->normal &= ~(1 << MMRP_ATTR_SVC);
+					mrp_stop_port_app(&mrp->mrp_ports[port],
+							  &mac_mrp_app,
+							  MMRP_ATTR_SVC, false);
+				}
+			}
+		}
+		break;
+	case PROC_SET_PORT_MMRP_REG:
+		if (mrp->mmrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+			enum mrp_registrar_state state;
+			struct ksz_mac_table *entry;
+
+			entry = &sw->info->mac_entry;
+			if (num == 2)
+				state = MRP_REGISTRAR_MT;
+			else if (num == 1)
+				state = MRP_REGISTRAR_IN;
+			else
+				state = MRP_REGISTRAR_LV;
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[mac_mrp_app.type]);
+			mmrp_req_set_mac(app, entry->addr, state);
+		}
+		break;
+	case PROC_SET_PORT_MVRP_ENABLED:
+		if (!!(mrp->mvrp_rx_ports & (1 << port)) != !!num) {
+			if (num) {
+				mrp->mvrp_rx_ports |= (1 << port);
+				mrp->mvrp_tx_ports |= (1 << port);
+				mrp_start_mvrp_port_app(mrp, port,
+							sw->main_dev);
+			} else {
+				mrp->mvrp_rx_ports &= ~(1 << port);
+				mrp_stop_port_app(&mrp->mrp_ports[port],
+						  &vlan_mrp_app, 0, true);
+			}
+		}
+		break;
+	case PROC_SET_PORT_MVRP_VID:
+		if (mrp->mvrp_rx_ports & (1 << port)) {
+			int on;
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[vlan_mrp_app.type]);
+			on = !!(app->normal & (1 << MVRP_ATTR_VID));
+			if (on != !!num) {
+				if (num) {
+					app->normal |= (1 << MVRP_ATTR_VID);
+				} else {
+					app->normal &= ~(1 << MVRP_ATTR_VID);
+					mrp_stop_port_app(&mrp->mrp_ports[port],
+							  &vlan_mrp_app,
+							  MVRP_ATTR_VID, false);
+				}
+			}
+		}
+		break;
+	case PROC_SET_PORT_MVRP_REG:
+		if (mrp->mvrp_rx_ports & (1 << port)) {
+			struct mrp_applicant *app;
+			enum mrp_registrar_state state;
+
+			if (num == 2)
+				state = MRP_REGISTRAR_MT;
+			else if (num == 1)
+				state = MRP_REGISTRAR_IN;
+			else
+				state = MRP_REGISTRAR_LV;
+			app = rcu_dereference(mrp->mrp_ports[port].
+					      applicants[vlan_mrp_app.type]);
+			mvrp_req_set(app, sw->vlan_index, state);
+		}
+		break;
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_SET_PORT_ASCAPABLE:
+		cfg->asCapable_set = !!num;
+		break;
+	case PROC_SET_PORT_MSRP_ENABLED:
+	{
+		if (info->status.msrpPortEnabledStatus == !!num)
+			break;
+
+		lp = sw_get_net_port(sw, 0, mrp->ports, port) - 1;
+		if (!info->status.msrpPortEnabledStatus) {
+			mrp_reset_reserv(mrp, lp);
+			info->status.msrpPortEnabledStatus = true;
+			mrp_start_msrp_port_app(mrp, port, sw->main_dev);
+			msrp_open_port(mrp, port, lp);
+			mrp_chk_registered(mrp, lp);
+		} else {
+#if 0
+			msrp_close_port(mrp, port, lp);
+#endif
+			mrp_stop_port_app(&mrp->mrp_ports[port],
+					  &srp_mrp_app, 0, true);
+			info->status.msrpPortEnabledStatus = false;
+		}
+		break;
+	}
+	case PROC_SET_TC_DELTA_BANDWIDTH:
+		lp = sw_get_net_port(sw, 0, mrp->ports, port);
+		if (index == 2 && num >= 0 &&
+		    num + info->deltaBandwidth <= 95 &&
+		    num + info->deltaBandwidth > 0)
+			mrp_set_delta(mrp, lp, info->deltaBandwidth, num);
+		if (index == 3 && num >= 0 && num <= 95)
+			info->deltaBandwidth = num;
+		break;
+	case PROC_SET_TC_ADMIN_IDLE_SLOPE:
+		if (num >= 0 && num < 10000) {
+			u32 rem;
+			u32 slope;
+			u64 val = num;
+
+			val <<= CREDIT_PERCENTAGE_S;
+			val += 50;
+			val = div_u64_rem(val, 100, &rem);
+			slope = (u32)val;
+			info->bandwidth[index].adminIdleSlope = slope;
+			srp_cfg_idle_slope(mrp, port, index, info, slope);
+		}
+		break;
+	case PROC_SET_TC_ALGORITHM:
+		if (0 <= num && num < 2)
+			info->algorithm[index].algorithm = num;
+		else if (3 == num)
+			info->algorithm[index].algorithm = num;
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+		sw->reg->lock(sw);
+		if (0 == num || 1 == num) {
+			port_set_schedule_mode(sw, port, index,
+				MTI_SCHEDULE_STRICT_PRIO);
+			port_set_shaping(sw, port, index,
+				1 == num ? MTI_SHAPING_SRP : MTI_SHAPING_OFF);
+		} else if (3 == num) {
+			port_set_schedule_mode(sw, port, index,
+				MTI_SCHEDULE_WRR);
+			port_set_shaping(sw, port, index,
+				MTI_SHAPING_OFF);
+		}
+		sw->reg->unlock(sw);
+#endif
+		break;
+	case PROC_SET_SR_0_TX_PRIO:
+		if (0 <= num && num < 8)
+			info->priority[SR_CLASS_B].regenerated_priority = num;
+		break;
+	case PROC_SET_SR_1_TX_PRIO:
+		if (0 <= num && num < 8)
+			info->priority[SR_CLASS_A].regenerated_priority = num;
+		break;
+#endif
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mrp_port_write */
+
+static void leave_mrp(struct mrp_info *mrp)
+{
+}  /* leave_mrp */
+
+static void mrp_init(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = container_of(mrp, struct ksz_sw, mrp);
+	u8 port;
+
+#ifdef CONFIG_KSZ_MSRP
+	int index;
+	int tc;
+	struct mrp_port_info *info;
+	struct mrp_traffic_info *traffic;
+	char bw_str1[20];
+	char bw_str2[20];
+	char bw_str3[20];
+	char bw_str4[20];
+	char bw_str5[20];
+#endif
+
+	mrp->access = create_singlethread_workqueue("mrp_access");
+	init_mrp_work(mrp);
+	mutex_init(&mrp->lock);
+	skb_queue_head_init(&mrp->rxq);
+	skb_queue_head_init(&mrp->macq);
+	skb_queue_head_init(&mrp->vlanq);
+	INIT_WORK(&mrp->cfg_mac, mrp_cfg_mac_work);
+	INIT_WORK(&mrp->cfg_vlan, mrp_cfg_vlan_work);
+	INIT_WORK(&mrp->rx_proc, mrp_rx_proc);
+
+	/* Number of ports can be capped for testing purpose. */
+	mrp->ports = sw->mib_port_cnt - 1;
+#if 0
+	mrp->tx_ports = sw->HOST_MASK;
+	mrp->rx_ports = 0;
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		lp = sw_get_net_port(sw, 0, mrp->ports, port);
+		if (lp > mrp->ports && port != sw->HOST_PORT)
+			continue;
+		mrp->tx_ports |= 1 << port;
+	}
+	mrp->rx_ports = mrp->tx_ports;
+	mrp->mmrp_ports = mrp->mvrp_ports = mrp->tx_ports;
+	sw->on_ports = mrp->rx_ports;
+#endif
+	mrp->mmrp_rx_ports = mrp->mmrp_tx_ports = sw->PORT_MASK;
+	mrp->mvrp_rx_ports = mrp->mvrp_tx_ports = sw->PORT_MASK;
+
+	mrp_init_list(&mrp->mac_list);
+	mrp_init_list(&mrp->vlan_list);
+
+#ifdef CONFIG_KSZ_MSRP
+	mrp->status.msrpEnabledStatus = true;
+	mrp->id = sw->ops->get_br_id(sw);
+	mrp->tc[2] = SR_CLASS_B;
+	mrp->tc[3] = SR_CLASS_A;
+	mrp->prio[SR_CLASS_B] = 2;
+	mrp->prio[SR_CLASS_A] = 3;
+	mrp->domain[0].id = SR_CLASS_B;
+	mrp->domain[0].priority = mrp->prio[SR_CLASS_B];
+	mrp->domain[0].vlan_id = 2;
+	mrp->domain[1].id = SR_CLASS_A;
+#if 0
+	mrp->domain[1].id = 0;
+#endif
+	mrp->domain[1].priority = mrp->prio[SR_CLASS_A];
+	mrp->domain[1].vlan_id = 2;
+	mrp->max_interference_size = 1500;
+
+	mrp_init_list(&mrp->mac_down);
+	mrp_init_list(&mrp->mac_up);
+	mrp_init_list(&mrp->vlan_down);
+	mrp_init_list(&mrp->vlan_up);
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (skip_mrp_port(mrp, port))
+			continue;
+		info = &mrp->port_info[port];
+		info->index = port;
+		info->status.msrpPortEnabledStatus = true;
+		info->speed = 100;
+		if (sw->port_info[port].tx_rate)
+			info->speed = sw->port_info[port].tx_rate /
+				TX_RATE_UNIT;
+
+		info->bandwidth[3].deltaBandwidth = 50;
+#if 0
+		info->bandwidth[3].deltaBandwidth = 75;
+#endif
+		info->bandwidth[3].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[3].operIdleSlope =
+			info->bandwidth[3].adminIdleSlope;
+
+		info->bandwidth[2].deltaBandwidth = 25;
+#if 0
+		info->bandwidth[2].deltaBandwidth = 0;
+#endif
+		info->bandwidth[2].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[2].operIdleSlope =
+			info->bandwidth[2].adminIdleSlope;
+
+		info->bandwidth[1].deltaBandwidth = 0;
+		info->bandwidth[0].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[1].operIdleSlope =
+			info->bandwidth[1].adminIdleSlope;
+
+		info->bandwidth[0].deltaBandwidth = 0;
+		info->bandwidth[0].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[0].operIdleSlope =
+			info->bandwidth[0].adminIdleSlope;
+
+		for (tc = 0; tc < 4; tc++) {
+			info->bandwidth[tc].traffic_class = tc;
+			info->algorithm[tc].traffic_class = tc;
+		}
+		info->algorithm[0].algorithm = 3;
+		info->algorithm[1].algorithm = 3;
+		info->algorithm[2].algorithm = 1;
+		info->algorithm[3].algorithm = 1;
+
+		for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+			index = get_traffic_index(tc);
+			info->latency[index].traffic_class = tc;
+			info->latency[index].portTcMaxLatency = 1000;
+			if (tc == SR_CLASS_A)
+				info->latency[index].portTcMaxLatency = 20000;
+			if (tc == SR_CLASS_B)
+				info->latency[index].portTcMaxLatency = 20000;
+
+			info->priority[tc].received_priority =
+				mrp->prio[tc];
+			info->priority[tc].regenerated_priority = 0;
+			if (port != sw->HOST_PORT)
+				info->priority[tc].SRPdomainBoundaryPort = 1;
+
+			traffic = get_traffic_info(info, tc);
+			traffic->queue = get_queue_priority(mrp, tc);
+
+			mrp_init_list(&traffic->active);
+			mrp_init_list(&traffic->passive);
+		}
+		info->traffic[1].bandwidth_avail =
+			&info->traffic[0].bandwidth_delta;
+		info->traffic[1].bandwidth_other =
+			&info->traffic[0].bandwidth_max;
+		info->traffic[0].bandwidth_avail = NULL;
+		info->traffic[0].bandwidth_other = NULL;
+		mrp_set_bandwidth(info);
+
+		format_num(bw_str1, info->bandwidth_left);
+		format_num(bw_str2, info->traffic[1].bandwidth_max);
+		format_num(bw_str3, info->traffic[1].bandwidth_left);
+		format_num(bw_str4, info->traffic[0].bandwidth_max);
+		format_num(bw_str5, info->traffic[0].bandwidth_left);
+dbg_msg("bw: %d %s; %s %s; %s %s\n", port,
+bw_str1, bw_str2, bw_str3, bw_str4, bw_str5);
+	}
+#if 0
+	do {
+		struct SRP_talker talker;
+
+		talker.MaxFrameSize = 70;
+		info = get_port_info(mrp, 0);
+		mrp_max_latency(mrp, info, 0, &talker);
+		talker.MaxFrameSize = 42;
+		mrp_max_latency(mrp, info, 0, &talker);
+	} while (0);
+#endif
+#if 0
+	for (port = 3; port < sw->mib_port_cnt; port++) {
+		lp = sw_get_net_port(sw, 0, mrp->ports, port);
+		if (lp > mrp->ports && port != sw->HOST_PORT)
+			continue;
+		info = &mrp->port_info[port];
+		info->status.msrpPortEnabledStatus = false;
+	}
+#endif
+#endif
+}  /* mrp_init */
+
+static void mrp_exit(struct mrp_info *mrp)
+{
+	bool last;
+	struct sk_buff *skb;
+
+	last = skb_queue_empty(&mrp->rxq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->rxq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&mrp->rxq);
+	}
+	last = skb_queue_empty(&mrp->macq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->macq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&mrp->macq);
+	}
+	last = skb_queue_empty(&mrp->vlanq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->vlanq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&mrp->vlanq);
+	}
+	exit_mrp_work(mrp);
+	if (mrp->access) {
+		destroy_workqueue(mrp->access);
+		mrp->access = NULL;
+	}
+}  /* mrp_exit */
+
+static struct mrp_ops mrp_ops = {
+	.init			= mrp_init,
+	.exit			= mrp_exit,
+
+	.dev_req		= mrp_dev_req,
+
+	.from_backup		= mrp_from_backup,
+	.to_backup		= mrp_to_backup,
+	.from_designated	= mrp_from_designated,
+	.to_designated		= mrp_to_designated,
+	.tc_detected		= mrp_tc_detected,
+
+	.chk_talker		= mrp_chk_talker,
+	.setup_vlan		= mrp_setup_vlan,
+};
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_mrp.h b/drivers/net/ethernet/micrel/ksz9897/ksz_mrp.h
new file mode 100644
index 0000000..799ee2e
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_mrp.h
@@ -0,0 +1,360 @@
+/**
+ * Microchip MRP driver header
+ *
+ * Copyright (c) 2015-2017 Microchp Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MRP_H
+#define KSZ_MRP_H
+
+#include "ksz_mrp_api.h"
+
+#if 1
+/* Process MRP in driver. */
+#define PROC_MRP
+#endif
+
+#ifdef PROC_MRP
+#if 0
+#define DBG_MRP
+#endif
+#if 0
+#define DBG_MRP_APP
+#endif
+#if 0
+#define DBG_MRP_REG
+#endif
+#if 0
+#define DBG_MRP_RX
+#endif
+#if 0
+#define DBG_MRP_TX
+#endif
+#if 0
+#define MRP_BASIC
+#endif
+#if 0
+#define MRP_PASSTHRU
+#endif
+#include "mrp.h"
+#endif
+
+
+struct mrp_node {
+	void *data;
+	struct mrp_node *next;
+};
+
+struct mrp_node_anchor {
+	struct mrp_node anchor;
+	struct mrp_node *last;
+	int cnt;
+};
+
+#ifdef CONFIG_KSZ_MSRP
+struct SRP_bridge_base {
+	uint msrpEnabledStatus:1;
+	uint msrpTalkerPruning:1;
+	uint msrpMaxFanInPorts;
+	uint msrpLatencyMaxFrameSize;
+};
+
+struct SRP_bridge_port {
+	uint msrpPortEnabledStatus:1;
+	uint Failed_Registrations;
+	u8 Last_PDU_Origin[ETH_ALEN];
+	u16 SR_PVID;
+};
+
+struct SRP_latency_parameter {
+	u8 traffic_class;
+	u32 portTcMaxLatency;
+};
+
+struct SRP_reserv;
+
+struct SRP_stream {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 MaxFrameSize;
+	u16 MaxIntervalFrames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 latency;
+	u32 bandwidth;
+
+	u8 in_port;
+	struct SRP_reserv *t_reserv;
+
+	struct SRP_stream *id_prev;
+	struct SRP_stream *id_next;
+	struct SRP_stream *dest_prev;
+	struct SRP_stream *dest_next;
+} __packed;
+
+enum {
+	SRP_TALKER,
+	SRP_LISTENER,
+};
+
+#define SRP_ASKING_FAILED_SCALE		(1 << SRP_ASKING_FAILED)
+#define SRP_READY_SCALE			(1 << SRP_READY)
+#define SRP_READY_FAILED_SCALE		(1 << SRP_READY_FAILED)
+
+struct SRP_reserv {
+	u8 id[8];
+	u8 direction;
+	u8 declaration;
+	u32 latency;
+	u8 bridge_id[8];
+	u8 rx_code;
+	u8 code;
+	u32 code_bits;
+	u16 tx_ports;
+
+	uint dropped_frames;
+	u64 streamAge;
+
+	struct SRP_stream *stream;
+	struct SRP_reserv *pair;
+
+	struct SRP_reserv *next;
+	struct SRP_reserv *prev;
+};
+
+struct SRP_bandwidth_availability_parameter {
+	u8 traffic_class;
+	int deltaBandwidth;
+	u64 adminIdleSlope;
+	u64 operIdleSlope;
+};
+
+struct SRP_transmission_selection_algorithm {
+	u8 traffic_class;
+	int algorithm;
+};
+
+struct SRP_priority_regeneration_override {
+	u8 received_priority;
+	u8 regenerated_priority;
+	int SRPdomainBoundaryPort;
+};
+
+struct mrp_traffic_info {
+	u32 bandwidth_delta;
+	u32 bandwidth_max;
+	u32 bandwidth_left;
+	u32 bandwidth_used;
+	u32 bandwidth_set;
+	u32 *bandwidth_avail;
+	u32 *bandwidth_other;
+	u32 max_frame_size;
+	u8 queue;
+
+	struct mrp_node_anchor active;
+	struct mrp_node_anchor passive;
+};
+#endif
+
+struct mrp_report {
+	void *attrib;
+	u8 action;
+	u8 type;
+	u8 port;
+	struct mrp_report *next;
+};
+
+#define SRP_PORT_AVAIL			((1 << (mrp->ports + 1)) - 1)
+#define SRP_PORT_OTHER			((1 << mrp->ports) - 1)
+#define SRP_PORT_SET			(1 << 11)
+#define SRP_PORT_DROP			(1 << 12)
+#define SRP_PORT_IGNORE			(1 << 13)
+#define SRP_PORT_BLACKLIST		(1 << 14)
+#define SRP_PORT_READY			(1 << 15)
+
+struct mrp_mac_info {
+	u16 fid;
+	u8 addr[ETH_ALEN];
+	u16 ports;
+	u16 set_ports;
+	u16 mrp_ports;
+	u16 srp_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 index;
+	unsigned long jiffies;
+};
+
+struct mrp_vlan_info {
+	u16 vid;
+	u16 fid;
+	u8 addr[ETH_ALEN];
+	u16 ports;
+	u16 set_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 index;
+};
+
+struct srp_stream_info {
+	struct SRP_reserv *reserv;
+	u8 *id;
+	u64 age;
+	u8 rank;
+	u8 mark:1;
+};
+
+#ifdef CONFIG_KSZ_MSRP	
+struct mrp_port_info {
+	u32 bandwidth_max;
+	u32 bandwidth_left;
+	u32 bandwidth_used;
+	u32 speed;
+	u64 age;
+	u32 link:1;
+	u32 duplex:1;
+	u8 index;
+	u16 credit[4];
+	int deltaBandwidth;
+	struct mrp_traffic_info traffic[2];
+
+	struct SRP_bridge_port status;
+	struct SRP_latency_parameter latency[2];
+	struct SRP_bandwidth_availability_parameter bandwidth[PRIO_QUEUES];
+	struct SRP_transmission_selection_algorithm algorithm[PRIO_QUEUES];
+	struct SRP_priority_regeneration_override priority[SR_CLASS_NUM];
+
+	struct SRP_reserv declared;
+	struct SRP_reserv registered;
+};
+#endif
+
+struct mrp_info;
+
+struct mrp_work {
+	struct work_struct work;
+	struct completion done;
+	struct mrp_info *mrp;
+	struct sk_buff *skb;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	int index;
+	union {
+		struct mrp_cfg_options cfg;
+		u8 data[8];
+	} param;
+	struct mrp_work *prev;
+};
+
+#define MRP_WORK_NUM			(1 << 4)
+#define MRP_WORK_LAST			(MRP_WORK_NUM - 1)
+
+struct mrp_access {
+	struct work_struct work;
+	int index;
+	int head;
+	int tail;
+	struct mrp_work works[MRP_WORK_NUM];
+};
+
+struct mrp_ops {
+	void (*init)(struct mrp_info *mrp);
+	void (*exit)(struct mrp_info *mrp);
+	int (*dev_req)(struct mrp_info *mrp, int start, char *arg);
+
+	void (*from_backup)(struct mrp_info *mrp, uint p);
+	void (*to_backup)(struct mrp_info *mrp, uint p);
+	void (*from_designated)(struct mrp_info *mrp, uint p, bool alt);
+	void (*to_designated)(struct mrp_info *mrp, uint p);
+	void (*tc_detected)(struct mrp_info *mrp, uint p);
+
+	void (*chk_talker)(struct mrp_info *mrp, u8 port);
+
+	void (*setup_vlan)(struct mrp_info *mrp, u16 vid,
+			   struct ksz_vlan_table *vlan);
+};
+
+struct mrp_info {
+	struct mutex lock;
+	struct mrp_access hw_access;
+	struct workqueue_struct *access;
+	struct sk_buff_head rxq;
+	struct sk_buff_head txq;
+	struct sk_buff_head macq;
+	struct sk_buff_head vlanq;
+	int macq_sched;
+	int vlanq_sched;
+	struct work_struct cfg_mac;
+	struct work_struct cfg_vlan;
+	struct work_struct rx_proc;
+	u8 version;
+	u8 ports;
+	u8 started;
+	u32 rx_ports;
+	u32 tx_ports;
+	u32 mmrp_rx_ports;
+	u32 mmrp_tx_ports;
+	u32 mvrp_rx_ports;
+	u32 mvrp_tx_ports;
+	uint no_report;
+	int listeners;
+
+	const struct mrp_ops *ops;
+
+#ifdef CONFIG_KSZ_MSRP
+	const u8 *id;
+	u8 tc[8];
+	u8 prio[SR_CLASS_NUM];
+	u32 max_interference_size;
+	u16 mcast_ports;
+	int mcast_port_cnt;
+
+	struct SRP_bridge_base status;
+	struct mrp_port_info port_info[TOTAL_PORT_NUM];
+
+	struct SRP_domain_class domain[2];
+
+	struct SRP_stream stream_by_id;
+	struct SRP_stream stream_by_dest;
+
+	struct mrp_node_anchor mac_down;
+	struct mrp_node_anchor mac_up;
+	struct mrp_node_anchor vlan_down;
+	struct mrp_node_anchor vlan_up;
+#endif
+	struct mrp_node_anchor mac_list;
+	struct mrp_node_anchor vlan_list;
+
+	struct mrp_report *report_head;
+	struct mrp_report *report_tail;
+
+#ifdef PROC_MRP
+	struct mrp_port mrp_ports[TOTAL_PORT_NUM];
+	int mac_tx[TOTAL_PORT_NUM];
+	int vlan_tx[TOTAL_PORT_NUM];
+	int srp_tx[TOTAL_PORT_NUM];
+	u8 cvlan_addr[ETH_ALEN];
+	u8 svlan_addr[ETH_ALEN];
+#endif
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_mrp_api.h b/drivers/net/ethernet/micrel/ksz9897/ksz_mrp_api.h
new file mode 100644
index 0000000..491f311
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_mrp_api.h
@@ -0,0 +1,174 @@
+/**
+ * Microchip MRP driver API header
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MRP_API_H
+#define KSZ_MRP_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN			6
+#endif
+
+
+enum {
+	DEV_IOC_MRP_REPORT = DEV_IOC_LAST,
+};
+
+enum {
+	DEV_MRP_ATTRIBUTE,
+};
+
+struct MRP_mac {
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct MRP_vlan {
+	u16 id;
+} __packed;
+
+enum {
+	SRP_IGNORED,
+	SRP_ASKING_FAILED,
+	SRP_READY,
+	SRP_READY_FAILED,
+	SRP_ADVERTISE,
+	SRP_FAILED,
+};
+
+struct SRP_listener {
+	u8 id[8];
+	u8 substate;
+} __packed;
+
+struct SRP_talker {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 MaxFrameSize;
+	u16 MaxIntervalFrames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 AccumulatedLatency;
+	u8 bridge_id[8];
+	u8 FailureCode;
+} __packed;
+
+enum {
+	RFC_NO_ERROR,
+	RFC_NO_BANDWIDTH,
+	RFC_NO_RESOURCES,
+	RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS,
+	RFC_STREAM_ID_USED,
+	RFC_DEST_ADDR_USED,
+	RFC_PREEMPTED_BY_RANK,
+	RFC_LATENCY_CHANGED,
+	RFC_PORT_IS_NOT_AVB,
+	RFC_USE_DIFF_DEST_ADDR,
+	RFC_OUT_OF_MSRP_RESOURCE,
+	RFC_OUT_OF_MMRP_RESOURCE,
+	RFC_CANNOT_STORE_DEST_ADDR,
+	RFC_PRIORITY_IS_NOT_SR_CLASS,
+	RFC_MAXFRAMESIZE_TOO_LARGE,
+	RFC_MAXFANINPORTS_LIMIT_REACHED,
+	RFC_FIRSTVALUE_CHANGED,
+	RFC_VLAN_BLOCKED,
+	RFC_VLAN_TAGGING_DISABLED,
+	RFC_SR_CLASS_PRIORITY_MISMATCHED,
+};
+
+enum {
+	SR_CLASS_G,
+	SR_CLASS_F,
+	SR_CLASS_E,
+	SR_CLASS_D,
+	SR_CLASS_C,
+	SR_CLASS_B,
+	SR_CLASS_A,
+	SR_CLASS_NUM
+};
+
+#define SR_CLASS_A_ID	6
+#define SR_CLASS_B_ID	5
+
+struct SRP_domain_class {
+	u8 id;
+	u8 priority;
+	u16 vlan_id;
+} __packed;
+
+union mrp_data {
+	struct MRP_mac mac;
+	struct MRP_vlan vlan;
+	struct SRP_talker talker;
+	struct SRP_listener listener;
+	struct SRP_domain_class domain;
+	u32 data[4];
+} __packed;
+
+enum {
+	MRP_ACTION_RX,
+	MRP_ACTION_TX,
+	MRP_ACTION_TX_NEW,
+	MRP_ACTION_LV,
+	MRP_ACTION_ON,
+	MRP_ACTION_OFF,
+	MRP_ACTION_DECL,
+	MRP_ACTION_DROP,
+
+	MRP_ACTION_DBG,
+	MRP_ACTION_SPEED,
+	MRP_ACTION_DELTA,
+	MRP_ACTION_CHK_TALKER,
+	MRP_ACTION_CHK_REG,
+};
+
+enum {
+	MRP_TYPE_UNKNOWN,
+	MRP_TYPE_MAC,
+	MRP_TYPE_VLAN,
+	MRP_TYPE_TALKER,
+	MRP_TYPE_LISTENER,
+	MRP_TYPE_DOMAIN,
+	MRP_TYPE_PORT,
+};
+
+struct mrp_cfg_options {
+	u8 action;
+	u8 type;
+	u8 port;
+	u8 new_decl;
+	union mrp_data data;
+} __packed;
+
+#define SIZEOF_MRP_mac	\
+	(sizeof(struct MRP_mac) + sizeof(u8) * 4)
+
+#define SIZEOF_MRP_vlan	\
+	(sizeof(struct MRP_vlan) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_talker	\
+	(sizeof(struct SRP_talker) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_listener	\
+	(sizeof(struct SRP_listener) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_domain_class	\
+	(sizeof(struct SRP_domain_class) + sizeof(u8) * 4)
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_9897.c b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_9897.c
new file mode 100644
index 0000000..ed8a0eb
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_9897.c
@@ -0,0 +1,6957 @@
+/**
+ * Microchip PTP common code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#define CURRENT_UTC_OFFSET  37 /* 1 Jan 2017 */
+
+
+#if 0
+#define ENABLE_10_MHZ_CLK
+#endif
+
+
+#define FMT_NSEC_SIZE			12
+
+static char *format_nsec(char *str, u32 nsec)
+{
+	u32 nsec0;
+	u32 nsec1;
+	u32 nsec2;
+	char str0[4];
+
+	nsec0 = nsec % 1000;
+	nsec1 = (nsec / 1000) % 1000;
+	nsec2 = (nsec / 1000000) % 1000;
+	sprintf(str0, "%03u", nsec0);
+	if (nsec2)
+		sprintf(str, "%3u.%03u.%s", nsec2, nsec1, str0);
+	else if (nsec1)
+		sprintf(str, "    %3u.%s", nsec1, str0);
+	else
+		sprintf(str, "        %3u", nsec0);
+	return str;
+}  /* format_nsec */
+
+struct pseudo_iphdr {
+	__u8 ttl;
+	__u8 protocol;
+	__be16 tot_len;
+	__be32 saddr;
+	__be32 daddr;
+};
+
+struct pseudo_ip6hdr {
+	__be16 payload_len;
+	__u8 hop_limit;
+	__u8 nexthdr;
+	struct in6_addr saddr;
+	struct in6_addr daddr;
+};
+
+static u32 timestamp_val(u32 timestamp, u8 *sec)
+{
+	*sec = timestamp >> 30;
+	timestamp <<= 2;
+	timestamp >>= 2;
+	return timestamp;
+}  /* timestamp_val */
+
+static void calc_diff(struct ksz_ptp_time *prev, struct ksz_ptp_time *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time diff;
+	int prev_nsec = prev->nsec;
+	int cur_nsec = cur->nsec;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= NANOSEC_IN_SEC) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	} else if (diff.nsec <= -NANOSEC_IN_SEC) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff */
+
+static void calc_udiff(struct ptp_utime *prev, struct ptp_utime *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time t1;
+	struct ksz_ptp_time t2;
+
+	if (prev->sec > (1UL << 31) || cur->sec > (1UL << 31)) {
+		s64 t3;
+		s64 t4;
+		s64 diff;
+		s32 rem;
+
+		t3 = (u64) prev->sec * NANOSEC_IN_SEC + prev->nsec;
+		t4 = (u64) cur->sec * NANOSEC_IN_SEC + cur->nsec;
+		diff = t4 - t3;
+		t3 = div_s64_rem(diff, NSEC_PER_SEC, &rem);
+		result->sec = (s32) t3;
+		result->nsec = rem;
+		return;
+	}
+	t1.sec = prev->sec;
+	t1.nsec = prev->nsec;
+	t2.sec = cur->sec;
+	t2.nsec = cur->nsec;
+	calc_diff(&t1, &t2, result);
+}  /* calc_udiff */
+
+#ifdef PTP_PROCESS
+static void calc_diff64(struct ptp_ltime *prev, struct ptp_ltime *cur,
+	struct ptp_ltime *result)
+{
+	struct ptp_ltime diff;
+	s64 prev_nsec = prev->nsec;
+	s64 cur_nsec = cur->nsec;
+	s64 scaled_nsec = (s64) NANOSEC_IN_SEC << SCALED_NANOSEC_S;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= scaled_nsec) {
+		diff.nsec -= scaled_nsec;
+		diff.sec++;
+	} else if (diff.nsec <= -scaled_nsec) {
+		diff.nsec += scaled_nsec;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += scaled_nsec;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= scaled_nsec;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff64 */
+#endif
+
+static void ptp_write_index(struct ptp_info *ptp, int shift, u8 unit)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+#ifndef USE_OLD_PTP_UNIT_INDEX
+	u32 index = sw->cached.ptp_unit_index;
+
+	index &= ~(PTP_UNIT_M << shift);
+	index |= (u32) unit << shift;
+	sw->cached.ptp_unit_index = index;
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, index);
+#else
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, unit);
+#endif
+}  /* ptp_write_index */
+
+static void add_nsec(struct ptp_utime *t, u32 nsec)
+{
+	t->nsec += nsec;
+	if (t->nsec >= NANOSEC_IN_SEC) {
+		t->nsec -= NANOSEC_IN_SEC;
+		t->sec++;
+	}
+}  /* add_nsec */
+
+static void sub_nsec(struct ptp_utime *t, u32 nsec)
+{
+	if (t->nsec < nsec) {
+		t->nsec += NANOSEC_IN_SEC;
+		t->sec--;
+	}
+	t->nsec -= nsec;
+}  /* sub_nsec */
+
+static void update_ts(struct ptp_ts *ts, u32 cur_sec)
+{
+	int sec;
+	u8 sec_chk;
+
+	ts->t.nsec = timestamp_val(ts->timestamp, &sec_chk);
+	if (ts->timestamp)
+		sec = (cur_sec - sec_chk) & 3;
+	else
+		sec = 0;
+	if (sec >= 3)
+		sec -= 4;
+	ts->t.sec = cur_sec - sec;
+}  /* update_ts */
+
+#define INIT_NSEC			40
+#define MIN_CYCLE_NSEC			8
+#define MIN_GAP_NSEC			120
+#define PULSE_NSEC			8
+
+static int check_cascade(struct ptp_info *ptp, int first, int total,
+	u16 *repeat, u32 sec, u32 nsec)
+{
+	struct ptp_output *cur;
+	struct ptp_output *next;
+	struct ptp_output *prev;
+	int diff;
+	int i;
+	int tso;
+	int min_cnt;
+	int cnt;
+
+	tso = first;
+	cur = &ptp->outputs[tso];
+	next = &ptp->outputs[first + total];
+	next->start = cur->start;
+	add_nsec(&next->start, cur->iterate);
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->stop = cur->start;
+		add_nsec(&cur->stop, cur->len);
+		next = &ptp->outputs[tso + 1];
+		calc_udiff(&cur->stop, &next->start, &cur->gap);
+		if ((cur->gap.sec < 0 || (!cur->gap.sec && cur->gap.nsec < 0))
+				&& (i < total - 1 || 1 != *repeat)) {
+			dbg_msg("gap too small: %d=%d\n", i, cur->gap.nsec);
+			return 1;
+		}
+	}
+	if (1 == *repeat)
+		goto check_cascade_done;
+
+	min_cnt = *repeat;
+	tso = first + 1;
+	for (i = 1; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		prev = &ptp->outputs[tso - 1];
+		if (cur->iterate < prev->iterate) {
+			diff = prev->iterate - cur->iterate;
+			cnt = prev->gap.nsec / diff + 1;
+		} else if (cur->iterate > prev->iterate) {
+			diff = cur->iterate - prev->iterate;
+			cnt = cur->gap.nsec / diff + 1;
+		} else
+			cnt = *repeat;
+		if (min_cnt > cnt)
+			min_cnt = cnt;
+	}
+	if (*repeat > min_cnt)
+		*repeat = min_cnt;
+	prev = &ptp->outputs[first + tso];
+	for (cnt = 0; cnt < *repeat; cnt++) {
+		tso = first;
+		for (i = 0; i < total; i++, tso++) {
+			cur = &ptp->outputs[tso];
+			next = &ptp->outputs[tso + 1];
+			dbg_msg("%d: %d:%9d %d %d:%9d %d: %d:%9d\n",
+				i, cur->start.sec, cur->start.nsec, cur->len,
+				cur->gap.sec, cur->gap.nsec, cur->iterate,
+				cur->stop.sec, cur->stop.nsec);
+			if (cur->stop.sec > next->start.sec ||
+					(cur->stop.sec == next->start.sec &&
+					cur->stop.nsec > next->stop.nsec))
+				dbg_msg("> %d %d:%9d %d:%9d\n", i,
+					cur->stop.sec, cur->stop.nsec,
+					next->start.sec, next->start.nsec);
+			add_nsec(&cur->start, cur->iterate);
+			cur->stop = cur->start;
+			add_nsec(&cur->stop, cur->len);
+			if (!i)
+				prev->start = cur->start;
+		}
+		dbg_msg("%d:%9d\n", prev->start.sec, prev->start.nsec);
+	}
+
+check_cascade_done:
+	tso = first;
+	cur = &ptp->outputs[tso];
+	if (cur->trig.sec >= sec)
+		return 0;
+
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->trig.sec += sec;
+		add_nsec(&cur->trig, nsec);
+	}
+	return 0;
+}
+
+#define MAX_DRIFT_CORR			6250000
+#define LOW_DRIFT_CORR			2499981
+#define MAX_U32_S			32
+#define MAX_DIVIDER_S			31
+
+static u32 drift_in_sec(u32 abs_offset, u64 interval64)
+{
+	u64 drift64;
+
+	drift64 = abs_offset;
+	drift64 *= NANOSEC_IN_SEC;
+	drift64 = div64_u64(drift64, interval64);
+	return (u32) drift64;
+}
+
+static u32 clk_adjust_val(int diff, u32 interval)
+{
+	u32 adjust;
+	u32 rem;
+	u64 adjust64;
+
+	if (0 == diff)
+		return 0;
+	if (diff < 0)
+		adjust = -diff;
+	else
+		adjust = diff;
+
+	/* 2^32 * adjust * 1000000000 / interval / 25000000 */
+	if (interval != NANOSEC_IN_SEC)
+		adjust = drift_in_sec(adjust, interval);
+
+	if (adjust >= MAX_DRIFT_CORR)
+		adjust = 0x3fffffff;
+	else {
+		adjust64 = 1LL << 32;
+		adjust64 *= adjust;
+		adjust64 = div_u64_rem(adjust64, 25000000, &rem);
+		adjust = (u32) adjust64;
+		if (adjust >= 0x40000000)
+			adjust = 0x3fffffff;
+	}
+	if (diff < 0)
+		adjust |= PTP_RATE_DIR;
+	return adjust;
+}  /* clk_adjust_val */
+
+static void ptp_tso_off(struct ptp_info *ptp, u8 tso, u16 tso_bit)
+{
+	ptp->reg->tx_off(ptp, tso);
+	ptp->tso_intr &= ~tso_bit;
+	ptp->tso_used &= ~tso_bit;
+	if (ptp->tso_sys & tso_bit) {
+		printk(KERN_INFO "tso %d off!\n", tso);
+		ptp->tso_sys &= ~tso_bit;
+	}
+	ptp->tso_dev[tso] = NULL;
+}  /* ptp_tso_off */
+
+static inline void ptp_tx_reset(struct ptp_info *ptp, u8 tso, u32 *ctrl_ptr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (!ctrl_ptr) {
+		ctrl_ptr = &ctrl;
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		*ctrl_ptr = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	}
+	*ctrl_ptr &= ~TS_RESET;
+	*ctrl_ptr |= TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+	*ctrl_ptr &= ~TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+}  /* ptp_tx_reset */
+
+static inline void ptp_gpo_reset(struct ptp_info *ptp, int gpo, u8 tso,
+	u32 *ctrl_ptr)
+{
+	ptp_tx_reset(ptp, tso, ctrl_ptr);
+	ptp->cascade_gpo[gpo].tso &= ~(1 << tso);
+}  /* ptp_gpo_reset */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_acquire(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->ops->acquire(sw);
+}  /* ptp_acquire */
+
+static void ptp_release(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->ops->release(sw);
+}  /* ptp_release */
+
+static void get_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	u16 subnsec;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	data = sw->cached.ptp_clk_ctrl;
+	data |= PTP_READ_TIME;
+	sw_w16(sw, REG_PTP_CLK_CTRL, data);
+	do {
+		u8 buf[12];
+		u16 *w_ptr;
+		u32 *d_ptr;
+
+		sw_r(sw, REG_PTP_RTC_SUB_NANOSEC__2, buf, 10);
+		w_ptr = (u16 *) buf;
+		subnsec = be16_to_cpu(*w_ptr);
+		++w_ptr;
+		d_ptr = (u32 *) w_ptr;
+		t->nsec = be32_to_cpu(*d_ptr);
+		++d_ptr;
+		t->sec = be32_to_cpu(*d_ptr);
+	} while (0);
+#if 1
+if (subnsec > PTP_RTC_SUB_NANOSEC_M)
+printk(" ?%x ", subnsec);
+#endif
+	subnsec &= PTP_RTC_SUB_NANOSEC_M;
+	add_nsec(t, subnsec * 8);
+}  /* get_ptp_time */
+
+static void set_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	data = sw->cached.ptp_clk_ctrl;
+	sw_w16(sw, REG_PTP_RTC_SUB_NANOSEC__2, 0);
+	sw_w32(sw, REG_PTP_RTC_NANOSEC, t->nsec);
+	sw_w32(sw, REG_PTP_RTC_SEC, t->sec);
+	data |= PTP_LOAD_TIME;
+	sw_w16(sw, REG_PTP_CLK_CTRL, data);
+}  /* set_ptp_time */
+
+static void adjust_ptp_time(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+	int adj_hack)
+{
+	u16 ctrl;
+	u16 adj = 0;
+	u32 val = nsec;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_ADJ;
+	sw_w32(sw, REG_PTP_RTC_SEC, sec);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		sw_w32(sw, REG_PTP_RTC_NANOSEC, nsec);
+		sw_w16(sw, REG_PTP_CLK_CTRL, ctrl);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		sw_w16(sw, REG_PTP_CLK_CTRL, adj);
+}  /* adjust_ptp_time */
+
+static void adjust_sync_time(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	u32 adjust;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE;
+	sw_w32(sw, REG_PTP_RATE_DURATION, duration);
+	sw_w32(sw, REG_PTP_SUBNANOSEC_RATE, adjust);
+}  /* adjust_sync_time */
+
+static void ptp_rx_reset(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (!ctrl_ptr) {
+		ctrl_ptr = &ctrl;
+		ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+		*ctrl_ptr = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	}
+	*ctrl_ptr &= ~TRIG_RESET;
+	*ctrl_ptr |= TS_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+	*ctrl_ptr &= ~TS_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+}  /* ptp_rx_reset */
+
+static void ptp_rx_off(struct ptp_info *ptp, u8 tsi)
+{
+	u32 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	u32 ts_intr = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		ctrl &= ~TS_INT_ENABLE;
+		ts_intr = tsi_bit;
+	}
+
+	/* Disable previous timestamp detection. */
+	ctrl &= ~TS_ENABLE;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		ptp_rx_reset(ptp, tsi, &ctrl);
+		sw_w32(sw, REG_TS_CTRL_STAT__4, 0);
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+	if (ts_intr)
+		sw_w32(sw, REG_PTP_INT_STATUS__4, ts_intr);
+}  /* ptp_rx_off */
+
+static inline void ptp_rx_intr(struct ptp_info *ptp, u16 tsi_bit, u32 *ctrl)
+{
+	ptp->ts_intr |= tsi_bit;
+	*ctrl |= TS_INT_ENABLE;
+}  /* ptp_rx_intr */
+
+static inline void ptp_rx_on(struct ptp_info *ptp, u8 tsi, int intr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+
+	ctrl |= TS_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_rx_on */
+
+static void ptp_rx_restart(struct ptp_info *ptp, u8 tsi)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	ctrl &= ~TS_ENABLE;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	ctrl |= TS_ENABLE;
+	sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_rx_restart */
+
+static u32 ts_event_gpi(u8 gpi, u8 event)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = event;
+	ctrl <<= TS_DETECT_S;
+	data = gpi & TS_GPI_M;
+	data <<= TS_GPI_S;
+	ctrl |= data;
+	return ctrl;
+}
+
+static u32 ts_cascade(int prev)
+{
+	u32 ctrl;
+
+	ctrl = prev & TS_CASCADE_UPS_M;
+	ctrl <<= TS_CASCADE_UPS_S;
+	ctrl |= TS_CASCADE_ENABLE;
+	return ctrl;
+}
+
+static void ptp_rx_event(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Config pattern. */
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = ts_event_gpi(gpi, event);
+	sw_w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, tsi, intr);
+}  /* ptp_rx_event */
+
+static void ptp_rx_cascade_event(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	int last;
+	int tsi;
+	u32 ctrl;
+	u32 tail;
+	int i;
+	int prev;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = ts_event_gpi(gpi, event);
+		ctrl |= ts_cascade(prev);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		sw_w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+		/* Enable timestamp interrupt. */
+		if (intr) {
+			ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+			ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+			sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		}
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, first);
+	ctrl = ts_event_gpi(gpi, event);
+	ctrl |= ts_cascade(last);
+	ptp->cascade_rx |= (1 << first);
+	sw_w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, first, intr);
+}  /* ptp_rx_cascade_event */
+
+static u32 ptp_get_event_cnt(struct ptp_info *ptp, u8 tsi, void *ptr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	return sw_r32(sw, REG_TS_CTRL_STAT__4);
+}  /* ptp_get_event_cnt */
+
+static void ptp_get_events(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr)
+{
+	int i;
+	u32 *data = buf;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_r(sw, reg_ns, buf, len);
+	for (i = 0; i < len / sizeof(u32); i++)
+		data[i] = be32_to_cpu(data[i]);
+}  /* ptp_get_events */
+
+static void ptp_read_event_func(struct ptp_info *ptp, u8 tsi, void *ptr,
+	u32 (*get_event_cnt)(struct ptp_info *ptp, u8 tsi, void *ptr),
+	void (*get_events)(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr))
+{
+	u32 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	u8 buf[96];
+	u32 *d_ptr;
+	u32 reg_ns;
+	struct ptp_utime t;
+	u32 sub;
+	int max_ts;
+	int num;
+	int i;
+	int edge;
+	struct ptp_event *event = &ptp->events[tsi];
+	int last = event->num;
+
+	ctrl = get_event_cnt(ptp, tsi, ptr);
+	num = (ctrl >> TS_EVENT_DETECT_S) & TS_EVENT_DETECT_M;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (event->num >= max_ts)
+		return;
+	i = event->num;
+
+	reg_ns = REG_TS_EVENT_0_NANOSEC + TS_EVENT_SAMPLE * i;
+	get_events(ptp, reg_ns, 12 * (max_ts - event->num), buf, ptr);
+	d_ptr = (u32 *) buf;
+	for (; i < max_ts; i++) {
+		t.nsec = (*d_ptr);
+		++d_ptr;
+		t.sec = (*d_ptr);
+		++d_ptr;
+		sub = (*d_ptr) & TS_EVENT_SUB_NANOSEC_M;
+		++d_ptr;
+		edge = ((t.nsec >> TS_EVENT_EDGE_S) & TS_EVENT_EDGE_M);
+#if 0
+printk("edge: %x %x\n", event->event, edge);
+#endif
+		t.nsec &= TS_EVENT_NANOSEC_M;
+		add_nsec(&t, sub * 8);
+#if 1
+/*
+ * THa  2011/10/06
+ * Unit sometimes detects rising edge when it is configured to detect falling
+ * edge only.  This happens in the case of hooking up the output pin to an
+ * input pin and using two units running opposite cycle in cascade mode.  The
+ * 8 ns switch pulse before the cycle is too short to detect properly,
+ * resulting in missing edges.
+ * When detecting events directly from the output pin, the minimum pulse time
+ * is 24 ns for proper detection without missing any edge.
+ */
+		if (event->event < 2 && edge != event->event)
+			edge = event->event;
+#endif
+		event->edge |= edge << i;
+		event->t[i] = t;
+	}
+	event->num = max_ts;
+
+	/* Indicate there is new event. */
+	if (event->num > last)
+		ptp->ts_status |= tsi_bit;
+}  /* ptp_read_event_func */
+
+static void ptp_read_event(struct ptp_info *ptp, u8 tsi)
+{
+	ptp_read_event_func(ptp, tsi, NULL, ptp_get_event_cnt,
+		ptp_get_events);
+}  /* ptp_read_event */
+
+static u32 trig_cascade(int prev)
+{
+	u32 ctrl;
+
+	ctrl = prev & TRIG_CASCADE_UPS_M;
+	ctrl <<= TRIG_CASCADE_UPS_S;
+	return ctrl;
+}
+
+static void ptp_tx_off(struct ptp_info *ptp, u8 tso)
+{
+	u32 ctrl;
+	u16 tso_bit = (1 << tso);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+
+	/* Disable previous trigger out if not already completed. */
+	ctrl = sw_r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	if (ctrl & TRIG_ENABLE) {
+		ctrl &= ~TRIG_ENABLE;
+		sw_w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	}
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		ptp_gpo_reset(ptp, ptp->outputs[tso].gpo, tso, &ctrl);
+		ptp->cascade_tx &= ~tso_bit;
+	} else {
+		ctrl = sw_r32(sw, REG_TRIG_CTRL__4);
+		if (ctrl & TRIG_CASCADE_ENABLE) {
+			ctrl &= ~TRIG_CASCADE_ENABLE;
+			ctrl &= ~TRIG_CASCADE_TAIL;
+			ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+			sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+		}
+	}
+}  /* ptp_tx_off */
+
+static void ptp_tx_on(struct ptp_info *ptp, u8 tso)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	ctrl |= TRIG_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_tx_on */
+
+static void ptp_tx_trigger_time(struct ptp_info *ptp, u8 tso, u32 sec, u32 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_w32(sw, REG_TRIG_TARGET_SEC, sec);
+	sw_w32(sw, REG_TRIG_TARGET_NANOSEC, nsec);
+}  /* ptp_tx_trigger_time */
+
+static u32 trig_event_gpo(u8 gpo, u8 event)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = event & TRIG_PATTERN_M;
+	ctrl <<= TRIG_PATTERN_S;
+	data = gpo & TRIG_GPO_M;
+	data <<= TRIG_GPO_S;
+	ctrl |= data;
+	return ctrl;
+}
+
+static void ptp_tx_event(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+	int intr, int now, int opt)
+{
+	u32 ctrl;
+	u32 pattern = 0;
+	u16 tso_bit = (1 << tso);
+	struct ptp_output *cur = &ptp->outputs[tso];
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Hardware immediately keeps level high on new GPIO if not reset. */
+	if (cur->level && gpo != cur->gpo)
+		ptp_gpo_reset(ptp, cur->gpo, tso, NULL);
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	if (intr)
+		ctrl |= TRIG_NOTIFY;
+	if (now)
+		ctrl |= TRIG_NOW;
+	if (opt)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (TRIG_REG_OUTPUT == event) {
+		pattern = pulse & TRIG_BIT_PATTERN_M;
+		cur->level = 0;
+		if (cnt) {
+			u32 reg;
+
+			reg = cnt - 1;
+			reg %= 16;
+			while (reg) {
+				pulse >>= 1;
+				reg--;
+			}
+			if (pulse & 1)
+				cur->level = 1;
+		}
+		pulse = 0;
+	} else if (event >= TRIG_NEG_PULSE) {
+		if (0 == pulse)
+			pulse = 1;
+		else if (pulse > TRIG_PULSE_WIDTH_M)
+			pulse = TRIG_PULSE_WIDTH_M;
+		sw_w24(sw, REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+	}
+
+	/* Config cycle width. */
+	if (event >= TRIG_NEG_PERIOD) {
+		u32 data = cnt;
+		int min_cycle = pulse * PULSE_NSEC + MIN_CYCLE_NSEC;
+
+		if (cycle < min_cycle)
+			cycle = min_cycle;
+		sw_w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+		/* Config trigger count. */
+		data <<= TRIG_CYCLE_CNT_S;
+		pattern |= data;
+		sw_w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+	}
+
+	cur->len = 0;
+	if (event >= TRIG_NEG_PERIOD) {
+		if (cnt)
+			cur->len += cycle * cnt;
+		else
+			cur->len += 0xF0000000;
+	} else if (event >= TRIG_NEG_PULSE)
+		cur->len += pulse * PULSE_NSEC;
+	else
+		cur->len += MIN_CYCLE_NSEC;
+
+	cur->start.sec = sec;
+	cur->start.nsec = nsec;
+	cur->iterate = iterate;
+	cur->trig = cur->start;
+	cur->stop = cur->start;
+	add_nsec(&cur->stop, cur->len);
+	cur->gpo = gpo;
+
+	switch (event) {
+	case TRIG_POS_EDGE:
+	case TRIG_NEG_PULSE:
+	case TRIG_NEG_PERIOD:
+		cur->level = 1;
+		break;
+	case TRIG_REG_OUTPUT:
+		break;
+	default:
+		cur->level = 0;
+		break;
+	}
+
+	if (ptp->cascade)
+		return;
+
+	/*
+	 * Need to reset after completion.  Otherwise, this output pattern
+	 * does not behave consistently in cascade mode.
+	 */
+	if (TRIG_NEG_EDGE == event)
+		ptp->cascade_tx |= tso_bit;
+
+	ptp->cascade_gpo[gpo].total = 0;
+	if (cur->level)
+		ptp->cascade_gpo[gpo].tso |= tso_bit;
+	else
+		ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+
+	/* Config trigger time. */
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_tx_event */
+
+static void ptp_pps_event(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	u32 pattern;
+	u32 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 tso = ptp->pps_tso;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_tx_off(ptp, tso);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	sw_w24(sw, REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+
+	/* Config cycle width. */
+	sw_w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	sw_w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_pps_event */
+
+static void cfg_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec, u32 nsec)
+{
+	u32 pattern;
+	u32 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	if (1 == tso)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	sw_w32(sw, REG_TRIG_PULSE_WIDTH__4 + 0, pulse);
+
+	/* Config cycle width. */
+	sw_w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	sw_w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+}  /* cfg_10MHz */
+
+static void ptp_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	int i;
+	u32 nsec;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	for (i = 0; i < 2; i++) {
+		ptp_tx_off(ptp, tso);
+
+		cfg_10MHz(ptp, tso, gpo, sec, nsec);
+
+		/* Enable trigger. */
+		ptp_tx_on(ptp, tso);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz */
+
+static void ptp_tx_cascade_cycle(struct ptp_info *ptp, u8 tso, u32 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_w32(sw, REG_TRIG_ITERATE_TIME, nsec);
+}  /* ptp_tx_cascade_cycle */
+
+static void ptp_tx_cascade_on(struct ptp_info *ptp, u8 tso, u8 first, u8 last,
+	u16 repeat)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw_r32(sw, REG_TRIG_CTRL__4);
+	ctrl |= TRIG_CASCADE_ENABLE;
+	ctrl &= ~trig_cascade(TRIG_CASCADE_UPS_M);
+	if (tso == first)
+		ctrl |= trig_cascade(last);
+	else
+		ctrl |= trig_cascade(tso - 1);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		ctrl |= repeat - 1;
+	}
+	sw_w32(sw, REG_TRIG_CTRL__4, ctrl);
+}  /* ptp_tx_cascade_on */
+
+static int ptp_tx_cascade(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	int i;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	if (check_cascade(ptp, first, total, &repeat, sec, nsec)) {
+		dbg_msg("cascade repeat timing is not right\n");
+		return 1;
+	}
+	tso = last;
+	for (i = 0; i < total; i++, tso--) {
+		cur = &ptp->outputs[tso];
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		ptp_tx_trigger_time(ptp, tso, cur->trig.sec,
+			cur->trig.nsec);
+		ptp_tx_cascade_cycle(ptp, tso, cur->iterate);
+		ptp_tx_cascade_on(ptp, tso, first, last, repeat);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	/* Do not reset last unit to keep level high. */
+	if (ptp->outputs[last].level) {
+		ptp->cascade_tx &= ~(1 << last);
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso |= (1 << last);
+	} else
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso &= ~(1 << last);
+	ptp_tx_on(ptp, first);
+	return 0;
+}  /* ptp_tx_cascade */
+
+/* -------------------------------------------------------------------------- */
+
+static void set_ptp_domain(struct ptp_info *ptp, u8 domain)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) & ~PTP_DOMAIN_M;
+	ctrl |= domain;
+	sw->reg->w16(sw, REG_PTP_DOMAIN_VERSION, ctrl);
+}  /* set_ptp_domain */
+
+static void set_ptp_mode(struct ptp_info *ptp, u16 mode)
+{
+	u16 val;
+	u16 sav;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	val = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	sav = val;
+	val &= ~(PTP_1STEP | PTP_TC_P2P | PTP_MASTER);
+	val |= mode;
+	if (val != sav)
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, val);
+}  /* set_ptp_mode */
+
+static void synchronize_clk(struct ptp_info *ptp)
+{
+	u32 sec;
+	int inc;
+
+	if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+		ptp->adjust_offset = -ptp->adjust_offset;
+		ptp->adjust_sec = -ptp->adjust_sec;
+		inc = false;
+	} else
+		inc = true;
+	sec = (u32) ptp->adjust_sec;
+	ptp->reg->adjust_time(ptp, inc, sec, ptp->adjust_offset,
+		ptp->features & PTP_ADJ_HACK);
+	ptp->offset_changed = ptp->adjust_offset;
+	ptp->adjust_offset = 0;
+	ptp->adjust_sec = 0;
+}  /* synchronize_clk */
+
+static void set_ptp_adjust(struct ptp_info *ptp, u32 adjust)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, adjust);
+}  /* set_ptp_adjust */
+
+static inline void unsyntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+}  /* unsyntonize_clk */
+
+static void syntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl |= PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+}  /* syntonize_clk */
+
+static u16 get_ptp_delay(struct ptp_info *ptp, uint port, u32 reg)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = PORT_CTRL_ADDR(port, reg);
+	return sw->reg->r16(sw, reg);
+}  /* get_ptp_delay */
+
+static void set_ptp_delay(struct ptp_info *ptp, uint port, u32 reg, u16 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = PORT_CTRL_ADDR(port, reg);
+	sw->reg->w16(sw, reg, nsec);
+}  /* set_ptp_delay */
+
+static u16 get_ptp_ingress(struct ptp_info *ptp, uint port)
+{
+	return get_ptp_delay(ptp, port, REG_PTP_PORT_RX_DELAY__2);
+}
+
+static u16 get_ptp_egress(struct ptp_info *ptp, uint port)
+{
+	return get_ptp_delay(ptp, port, REG_PTP_PORT_TX_DELAY__2);
+}
+
+static short get_ptp_asym(struct ptp_info *ptp, uint port)
+{
+	short val;
+
+	val = get_ptp_delay(ptp, port, REG_PTP_PORT_ASYM_DELAY__2);
+	if (val & 0x8000)
+		val = -(val & ~0x8000);
+	return val;
+}
+
+static u32 get_ptp_link(struct ptp_info *ptp, uint port)
+{
+	u32 reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_LINK_DELAY__4);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	return sw->reg->r32(sw, reg);
+}
+
+static void set_ptp_ingress(struct ptp_info *ptp, uint port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, REG_PTP_PORT_RX_DELAY__2, nsec);
+}
+
+static void set_ptp_egress(struct ptp_info *ptp, uint port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, REG_PTP_PORT_TX_DELAY__2, nsec);
+}
+
+static void set_ptp_asym(struct ptp_info *ptp, uint port, short nsec)
+{
+	if (nsec < 0)
+		nsec = -nsec | 0x8000;
+	set_ptp_delay(ptp, port, REG_PTP_PORT_ASYM_DELAY__2, nsec);
+}
+
+static void set_ptp_link(struct ptp_info *ptp, uint port, u32 nsec)
+{
+	u32 reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_LINK_DELAY__4);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->reg->w32(sw, reg, nsec);
+}
+
+static inline void dbp_tx_ts(char *name, u8 port, u32 timestamp)
+{
+	u8 overflow;
+	char ts[FMT_NSEC_SIZE];
+
+	timestamp = timestamp_val(timestamp, &overflow);
+	format_nsec(ts, timestamp);
+	dbg_msg("%s p:%d c:%u %08x:%s\n", name, port, overflow, timestamp, ts);
+}  /* dbp_tx_ts */
+
+static void ptp_setup_udp_msg(struct ptp_dev_info *info, u8 *data, int len,
+	void (*func)(u8 *data, void *param), void *param)
+{
+	u8 buf[MAX_TSM_UDP_LEN];
+	int in_intr = in_interrupt();
+
+	if (len > MAX_TSM_UDP_LEN)
+		len = MAX_TSM_UDP_LEN;
+	if (!in_intr)
+		mutex_lock(&info->lock);
+	memcpy(buf, data, len);
+	if (func)
+		func(buf, param);
+	len += 2;
+	if (info->read_len + len <= info->read_max) {
+		u16 *udp_len = (u16 *) &info->read_buf[info->read_len];
+
+		*udp_len = len;
+		udp_len++;
+		memcpy(udp_len, buf, len - 2);
+		info->read_len += len;
+	}
+	if (!in_intr)
+		mutex_unlock(&info->lock);
+	wake_up_interruptible(&info->wait_udp);
+}  /* ptp_setup_udp_msg */
+
+static void ptp_tsm_resp(u8 *data, void *param)
+{
+	struct tsm_db *db = (struct tsm_db *) data;
+	struct ptp_ts *ts = param;
+	u32 timestamp;
+	u8 sec_chk;
+
+	db->cmd |= TSM_CMD_RESP;
+	db->cur_sec = htonl(ts->t.sec);
+	db->cur_nsec = htonl(ts->t.nsec);
+	timestamp = timestamp_val(ts->timestamp, &sec_chk);
+	db->timestamp = htonl(timestamp);
+	db->cur_nsec = db->timestamp;
+}  /* ptp_tsm_resp */
+
+static void ptp_tsm_get_time_resp(u8 *data, void *param)
+{
+	struct tsm_get_time *get = (struct tsm_get_time *) data;
+	struct ptp_utime *t = param;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->sec = htonl(t->sec);
+	get->nsec = htonl(t->nsec);
+}  /* ptp_tsm_get_time_resp */
+
+/**
+ * ptp_get_dest_port - Convert virtual port mask to physical port mask
+ * @ptp:	The PTP instance.
+ * @start:	0-based starting port of the network device.
+ * @port:	1-based virtual port of the network device.
+ *
+ * This function converts virtual ports used inside the network device to
+ * actual ports in the switch.
+ */
+static u32 ptp_get_dest_port(struct ptp_info *ptp, int start, uint port)
+{
+	int c;
+	int i;
+	int j;
+	u32 dest = 0;
+	int num = ptp->ports;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	--num;
+	for (c = 0, i = start, j = start; c < num; c++, i++, j++) {
+		if (i == sw->HOST_PORT)
+			j++;
+		if (port & (1 << c))
+			dest |= 1 << j;
+	}
+	return dest;
+}  /* ptp_get_dest_port */
+
+/**
+ * ptp_get_dev_port - Convert virtual port to physical port
+ * @ptp:	The PTP instance.
+ * @start:	0-based starting port of the network device.
+ * @port:	0-based virtual port of the network device.
+ *
+ * This function converts virtual port used inside the network device to
+ * actual port in the switch.
+ */
+static u32 ptp_get_dev_port(struct ptp_info *ptp, int start, uint port)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	port += start;
+	if (port >= sw->HOST_PORT)
+		++port;
+	return port;
+}  /* ptp_get_dev_port */
+
+/**
+ * ptp_get_net_port - Convert physical port to virtual port
+ * @ptp:	The PTP instance.
+ * @start:	0-based starting port of the network device.
+ * @port:	0-based physical port of the switch.
+ *
+ * This function converts physical port used in the switch to virtual port used
+ * by the network device.
+ */
+static u32 ptp_get_net_port(struct ptp_info *ptp, int start, uint port)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	port += start;
+	if (port == sw->HOST_PORT)
+		return ptp->ports - start;
+	if (port < sw->HOST_PORT || start > sw->HOST_PORT)
+		++port;
+	return port - start;
+}  /* ptp_get_net_port */
+
+static void add_tx_delay(struct ptp_ts *ts, int delay, u32 cur_sec)
+{
+	update_ts(ts, cur_sec);
+
+	/*
+	 * Save timestamp without transmit latency for PTP stack that adjusts
+	 * transmit latency itself.
+	 */
+	ts->r = ts->t;
+	add_nsec(&ts->t, delay);
+	ts->timestamp = ts->t.nsec;
+}  /* add_tx_delay */
+
+static void save_tx_ts(struct ptp_info *ptp, struct ptp_tx_ts *tx,
+	struct ptp_hw_ts *htx, int delay, uint port)
+{
+	unsigned long diff = 0;
+
+	add_tx_delay(&htx->ts, delay, ptp->cur_time.sec);
+	if (ptp->overrides & PTP_CHECK_PATH_DELAY) {
+		if (ptp->last_rx_ts.t.sec) {
+			struct ksz_ptp_time diff;
+
+			calc_udiff(&htx->ts.t, &ptp->last_rx_ts.t, &diff);
+			dbg_msg("pd: %d\n", diff.nsec);
+		} else
+			ptp->last_tx_ts = htx->ts;
+	}
+	if (!htx->sim_2step) {
+		struct tsm_db *db = (struct tsm_db *) tx->data.buf;
+		u8 msg = tx->data.buf[0] & 3;
+
+		tx->ts = htx->ts;
+		tx->resp_time = jiffies;
+		if (tx->req_time)
+			diff = tx->resp_time - tx->req_time;
+		if (diff < 4 * ptp->delay_ticks) {
+			if (tx->missed) {
+				if (diff > 2 * ptp->delay_ticks)
+					dbg_msg("  caught: %d, %lu; %x=%04x\n",
+						port, diff, msg,
+						ntohs(db->seqid));
+				if (tx->dev) {
+					ptp_setup_udp_msg(tx->dev,
+						tx->data.buf, tx->data.len,
+						ptp_tsm_resp, &tx->ts);
+					tx->dev = NULL;
+				}
+
+				/* Invalidate the timestamp. */
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		} else {
+			dbg_msg("  new: %d, %lu; %x=%04x\n", port, diff,
+				msg, ntohs(db->seqid));
+		}
+		tx->missed = false;
+		if (tx->skb) {
+			int len;
+			u64 ns;
+			struct skb_shared_hwtstamps shhwtstamps;
+
+			if (ptp->tx_en & (1 << 8))
+				ns = (u64) tx->ts.t.sec * NANOSEC_IN_SEC +
+					tx->ts.t.nsec;
+			else
+				ns = (u64) tx->ts.r.sec * NANOSEC_IN_SEC +
+					tx->ts.r.nsec;
+			memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+			shhwtstamps.hwtstamp = ns_to_ktime(ns);
+
+			/* Indicate which port message is sent out.
+			 * Can only report physical port.
+			 */
+			tx->msg->hdr.reserved2 = port + 1;
+			len = (unsigned char *) tx->msg - tx->skb->data;
+			__skb_pull(tx->skb, len);
+			skb_tstamp_tx(tx->skb, &shhwtstamps);
+
+			/* buffer not released yet. */
+			if (skb_shinfo(tx->skb)->tx_flags & SKBTX_HW_TSTAMP)
+				skb_shinfo(tx->skb)->tx_flags &=
+					~SKBTX_IN_PROGRESS;
+			else
+				dev_kfree_skb_irq(tx->skb);
+			tx->skb = NULL;
+		}
+	}
+	htx->sending = false;
+}  /* save_tx_ts */
+
+static int get_speed_index(struct ptp_info *ptp, uint port)
+{
+	int index;
+
+	if (1000 == ptp->linked[port])
+		index = 0;
+	else if (100 == ptp->linked[port])
+		index = 1;
+	else
+		index = 2;
+	return index;
+}  /* get_speed_index */
+
+static int get_tx_time(struct ptp_info *ptp, uint port, u16 status)
+{
+	int delay;
+	int index;
+	u32 reg;
+	struct ptp_tx_ts *tx = NULL;
+	struct ptp_hw_ts *htx = NULL;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	index = get_speed_index(ptp, port);
+	while (status) {
+		delay = ptp->tx_latency[port][index];
+		if (status & PTP_PORT_XDELAY_REQ_INT) {
+			reg = REG_PTP_PORT_XDELAY_TS;
+			tx = &ptp->tx_dreq[port];
+			htx = &ptp->hw_dreq[port];
+			status &= ~PTP_PORT_XDELAY_REQ_INT;
+		} else if (status & PTP_PORT_SYNC_INT) {
+			reg = REG_PTP_PORT_SYNC_TS;
+			tx = &ptp->tx_sync[port];
+			htx = &ptp->hw_sync[port];
+			status &= ~PTP_PORT_SYNC_INT;
+		} else {
+			reg = REG_PTP_PORT_PDRESP_TS;
+			tx = &ptp->tx_resp[port];
+			htx = &ptp->hw_resp[port];
+			status &= ~PTP_PORT_PDELAY_RESP_INT;
+		}
+		htx->ts.timestamp = sw->reg->r32(sw,
+			PORT_CTRL_ADDR(port, reg));
+		save_tx_ts(ptp, tx, htx, delay, port);
+	}
+	if (!htx)
+		return false;
+
+	return true;
+}  /* get_tx_time */
+
+static void generate_tx_event(struct ptp_info *ptp, int gpo)
+{
+	struct ptp_utime t;
+
+	if (gpo >= MAX_GPIO)
+		return;
+	ptp->first_sec = 0;
+	ptp->intr_sec = 0;
+	ptp->update_sec_jiffies = jiffies;
+	ptp->reg->get_time(ptp, &t);
+	t.sec += 1;
+	if (t.nsec >= (NANOSEC_IN_SEC - ptp->delay_ticks * 50000000))
+		t.sec += 1;
+	ptp->reg->pps_event(ptp, gpo, t.sec);
+#ifdef ENABLE_10_MHZ_CLK
+	ptp->reg->ptp_10MHz(ptp, ptp->mhz_tso, ptp->mhz_gpo, t.sec);
+#endif
+	schedule_delayed_work(&ptp->update_sec, (1000000 - t.nsec / 1000) * HZ
+		/ 1000000);
+}  /* generate_tx_event */
+
+static void ptp_check_pps(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, check_pps);
+
+	if (ptp->update_sec_jiffies) {
+		ptp->ops->acquire(ptp);
+		generate_tx_event(ptp, ptp->pps_gpo);
+		ptp->ops->release(ptp);
+	}
+}  /* ptp_check_pps */
+
+static void prepare_gps(struct ptp_info *ptp)
+{
+	ptp->ops->acquire(ptp);
+	ptp->tsi_used |= (1 << ptp->gps_tsi);
+	ptp->events[ptp->gps_tsi].event = 1;
+	ptp->events[ptp->gps_tsi].timeout = 0;
+	ptp->reg->rx_event(ptp, ptp->gps_tsi, ptp->gps_gpi, DETECT_RISE, true);
+	ptp->ops->release(ptp);
+}  /* prepare_gps */
+
+static void prepare_pps(struct ptp_info *ptp)
+{
+	if (ptp->pps_gpo >= MAX_GPIO)
+		return;
+	ptp->ops->acquire(ptp);
+	ptp->tso_used |= (1 << ptp->pps_tso);
+	ptp->tso_sys |= (1 << ptp->pps_tso);
+#ifdef ENABLE_10_MHZ_CLK
+	ptp->tso_used |= (1 << ptp->mhz_tso);
+	ptp->tso_used |= (1 << 1);
+	ptp->tso_sys |= (1 << ptp->mhz_tso);
+	ptp->tso_sys |= (1 << 1);
+#endif
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->tsi_used |= (1 << ptp->pps_tsi);
+	ptp->tsi_sys |= (1 << ptp->pps_tsi);
+	ptp->events[ptp->pps_tsi].num = 0;
+	ptp->events[ptp->pps_tsi].event = 1;
+	ptp->events[ptp->pps_tsi].edge = 0;
+	ptp->events[ptp->pps_tsi].expired = 0;
+	ptp->reg->rx_event(ptp, ptp->pps_tsi, ptp->pps_gpo, DETECT_RISE, true);
+	ptp->ops->release(ptp);
+}  /* prepare_pps */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_tx_intr_enable(struct ptp_info *ptp)
+{
+	u32 reg;
+	int i;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	for (i = 0; i < ptp->ports; i++) {
+		i = chk_last_port(sw, i);
+		reg = PORT_CTRL_ADDR(i, REG_PTP_PORT_TX_INT_STATUS__2);
+		sw->reg->w16(sw, reg, 0xffff);
+		reg = PORT_CTRL_ADDR(i, REG_PTP_PORT_TX_INT_ENABLE__2);
+		sw->reg->w16(sw, reg, ptp->tx_intr);
+	}
+}  /* ptp_tx_intr_enable */
+
+/* -------------------------------------------------------------------------- */
+
+static int ptp_poll_event(struct ptp_info *ptp, u8 tsi)
+{
+	int max_ts;
+	int num;
+	u32 status;
+	u16 tsi_bit = (1 << tsi);
+	struct ptp_event *event = &ptp->events[tsi];
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	status = sw->reg->r32(sw, REG_TS_CTRL_STAT__4);
+	ptp->ops->release(ptp);
+	num = (status >> TS_EVENT_DETECT_S) & TS_EVENT_DETECT_M;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (max_ts > event->num) {
+		ptp->ops->acquire(ptp);
+		status = sw->reg->r32(sw, REG_PTP_INT_STATUS__4);
+		if (status & tsi_bit)
+			sw->reg->w32(sw, REG_PTP_INT_STATUS__4, tsi_bit);
+		ptp->reg->read_event(ptp, tsi);
+		ptp->ts_status = 0;
+		ptp->ops->release(ptp);
+		return true;
+	}
+	return false;
+}  /* ptp_poll_event */
+
+static void convert_scaled_nsec(s64 scaled_nsec, int s, s64 *sec, int *nsec)
+{
+	int sign;
+	u64 quot;
+	u32 rem;
+
+	/* Convert to positive number first. */
+	if (scaled_nsec < 0) {
+		sign = -1;
+		scaled_nsec = -scaled_nsec;
+	} else
+		sign = 1;
+	scaled_nsec >>= s;
+	quot = div_u64_rem(scaled_nsec, NSEC_PER_SEC, &rem);
+	*sec = quot;
+	*nsec = (int) rem;
+
+	/* Positive number means clock is faster. */
+	if (1 == sign) {
+		*sec = -*sec;
+		*nsec = -*nsec;
+	}
+}  /* convert_scaled_nsec */
+
+static void adj_cur_time(struct ptp_info *ptp)
+{
+	if (ptp->adjust_offset || ptp->adjust_sec) {
+		synchronize_clk(ptp);
+		if (ptp->sec_changed)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else {
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+	}
+	if (ptp->sec_changed) {
+		struct timespec ts;
+		struct ptp_utime cur;
+
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ts = ktime_to_timespec(ktime_get_real());
+		cur.sec = ts.tv_sec;
+		cur.nsec = ts.tv_nsec;
+		calc_udiff(&ptp->cur_time, &cur, &ptp->time_diff);
+		ptp->sec_changed = 0;
+	}
+}  /* adj_cur_time */
+
+static void set_before_adj(struct ptp_info *ptp, struct ptp_utime *cur)
+{
+	ptp->adjust_offset += cur->nsec;
+	ptp->adjust_offset += ptp->set_delay;
+	ptp->adjust_offset += ptp->get_delay;
+	cur->nsec = 0;
+	if (ptp->adjust_offset > NANOSEC_IN_SEC) {
+		ptp->adjust_offset -= NANOSEC_IN_SEC;
+		cur->sec++;
+	}
+	ptp->reg->set_time(ptp, cur);
+}   /* set_before_adj */
+
+static void set_cur_time(struct ptp_info *ptp, struct ptp_ts *ts)
+{
+	struct ptp_utime cur;
+	s64 diff_sec;
+	int diff_nsec;
+
+	ptp->adjust_offset = ts->t.nsec - ts->timestamp;
+	ptp->reg->get_time(ptp, &cur);
+	diff_nsec = ts->t.nsec - ts->timestamp;
+	diff_sec = (s64) ts->t.sec - cur.sec;
+	if (ptp->features & PTP_ADJ_SEC) {
+		if (diff_sec) {
+			s64 nsec;
+
+			nsec = diff_sec;
+			nsec *= NANOSEC_IN_SEC;
+			nsec += diff_nsec;
+			convert_scaled_nsec(-nsec, 0, &ptp->adjust_sec,
+				&ptp->adjust_offset);
+		} else {
+			ptp->adjust_offset = diff_nsec;
+			ptp->adjust_sec = 0;
+		}
+		ptp->sec_changed = ptp->adjust_sec;
+	} else {
+		if (abs(diff_sec) <= 1) {
+			diff_nsec += diff_sec * NANOSEC_IN_SEC;
+			if (abs(diff_nsec) < NANOSEC_IN_SEC) {
+				ptp->adjust_offset = diff_nsec;
+				diff_sec = 0;
+			}
+		}
+		if (diff_sec) {
+			cur.sec = ts->t.sec;
+			set_before_adj(ptp, &cur);
+			ptp->sec_changed = diff_sec;
+		}
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+}  /* set_cur_time */
+
+static void adj_clock(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, adj_clk);
+	struct ptp_utime cur;
+
+	ptp->ops->acquire(ptp);
+
+	ptp->sec_changed = ptp->adjust_sec;
+	if (!(ptp->features & PTP_ADJ_SEC)) {
+
+		/* Need to adjust second. */
+		if (abs(ptp->adjust_sec) > 1) {
+			ptp->reg->get_time(ptp, &cur);
+			cur.sec += ptp->adjust_sec;
+			set_before_adj(ptp, &cur);
+		} else
+			ptp->adjust_offset += ptp->adjust_sec * NANOSEC_IN_SEC;
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+	ptp->ops->release(ptp);
+}  /* adj_clock */
+
+static void set_latency(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, set_latency);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int index;
+	uint p;
+
+	ptp->ops->acquire(ptp);
+	for (p = 0; p < ptp->ports; p++) {
+		p = chk_last_port(sw, p);
+		if (ptp->linked[p] & 0x80000000) {
+			ptp->linked[p] &= ~0x80000000;
+			index = get_speed_index(ptp, p);
+			set_ptp_ingress(ptp, p, ptp->rx_latency[p][index]);
+			set_ptp_egress(ptp, p, ptp->tx_latency[p][index]);
+		}
+	}
+	ptp->ops->release(ptp);
+}  /* set_latency */
+
+static void execute(struct ptp_info *ptp, struct work_struct *work)
+{
+	queue_work(ptp->access, work);
+}  /* execute */
+
+static void ptp_hw_disable(struct ptp_info *ptp)
+{
+	int i;
+	u32 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	for (i = 0; i < 2; i++) {
+		sw->cached.ptp_unit_index =
+			(i << PTP_TSI_INDEX_S) |
+			(i << PTP_TOU_INDEX_S);
+		sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4,
+			sw->cached.ptp_unit_index);
+		ctrl = TS_RESET | TRIG_RESET;
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, 0);
+		sw->reg->w32(sw, REG_TRIG_CTRL__4, 0);
+		sw->reg->w32(sw, REG_TS_CTRL_STAT__4, 0);
+	}
+	sw->cached.ptp_unit_index = (i << PTP_TOU_INDEX_S);
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, sw->cached.ptp_unit_index);
+	ctrl = TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, 0);
+	sw->reg->w32(sw, REG_TRIG_CTRL__4, 0);
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, 0);
+}  /* ptp_hw_disable */
+
+static void ptp_hw_enable(struct ptp_info *ptp)
+{
+	int i;
+	u16 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp_hw_disable(ptp);
+#ifndef ACL_TEST
+	data = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+dbg_msg("msg_conf1: %x\n", data);
+	data = ptp->mode;
+	sw->reg->w16(sw, REG_PTP_MSG_CONF1, data);
+	data = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	if ((sw->overrides & TAIL_TAGGING) && (data & PTP_ENABLE))
+		sw->overrides |= PTP_TAG;
+#endif
+	data = sw->reg->r16(sw, REG_PTP_MSG_CONF2);
+dbg_msg("msg_conf2: %x\n", data);
+	sw->reg->w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	data = sw->reg->r16(sw, REG_PTP_CLK_CTRL);
+dbg_msg("clk_ctrl: %x\n", data);
+	data |= PTP_CLK_ENABLE;
+	data &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = data;
+#if 1
+	if (!(sw->features & NEW_CAP))
+		data |= PTP_CLK_RESET;
+#endif
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+
+	sw->reg->w16(sw, REG_SW_HSR_TPID__2, 0x892F);
+
+	for (i = 0; i < ptp->ports; i++) {
+		i = chk_last_port(sw, i);
+		set_ptp_ingress(ptp, i, ptp->rx_latency[i][0]);
+		set_ptp_egress(ptp, i, ptp->tx_latency[i][0]);
+	}
+
+	/* PTP stack is still running while device is reset. */
+	if (ptp->drift_set) {
+		ptp->drift = ptp->drift_set;
+		ptp->adjust = clk_adjust_val(ptp->drift, NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+
+	ptp->ops->release(ptp);
+}  /* ptp_hw_enable */
+
+static void init_tx_ts(struct ptp_tx_ts *ts)
+{
+	ts->ts.timestamp = 0;
+	ts->req_time = 0;
+	ts->resp_time = 0;
+	ts->missed = false;
+	ts->hdr.messageType = 7;
+}  /* init_tx_ts */
+
+static void ptp_init_hw(struct ptp_info *ptp)
+{
+	uint port;
+	u32 reg;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	for (port = 0; port < ptp->ports; port++) {
+		int index;
+
+		port = chk_last_port(sw, port);
+		ptp->hw_sync[port].ts.timestamp = 0;
+		ptp->hw_sync[port].sending = false;
+		ptp->hw_dreq[port].ts.timestamp = 0;
+		ptp->hw_dreq[port].sending = false;
+		ptp->hw_resp[port].ts.timestamp = 0;
+		ptp->hw_resp[port].sending = false;
+		init_tx_ts(&ptp->tx_sync[port]);
+		init_tx_ts(&ptp->tx_dreq[port]);
+		init_tx_ts(&ptp->tx_resp[port]);
+		reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_XDELAY_TS);
+		ptp->xdelay_ts[port] = sw->reg->r32(sw, reg);
+		reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_PDRESP_TS);
+		ptp->pdresp_ts[port] = sw->reg->r32(sw, reg);
+		index = get_speed_index(ptp, port);
+		ptp->rx_latency[port][index] = get_ptp_ingress(ptp, port);
+		ptp->tx_latency[port][index] = get_ptp_egress(ptp, port);
+		ptp->asym_delay[port][index] = get_ptp_asym(ptp, port);
+		ptp->peer_delay[port] = get_ptp_link(ptp, port);
+		dbg_msg("%d = %d %d %d; %u\n", port,
+			ptp->rx_latency[port][index],
+			ptp->tx_latency[port][index],
+			ptp->asym_delay[port][index],
+			ptp->peer_delay[port]);
+		set_ptp_link(ptp, port, 0);
+		ptp->peer_delay[port] = 0;
+	}
+	ptp->ops->release(ptp);
+}  /* ptp_init_hw */
+
+static void ptp_check(struct ptp_info *ptp)
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->features |= PTP_ADJ_HACK;
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &cur);
+	ptp->reg->adjust_time(ptp, true, 10, 0, true);
+	ptp->reg->get_time(ptp, &now);
+dbg_msg("%08x:%08x %08x:%08x\n", cur.sec, cur.nsec, now.sec, now.nsec);
+	if (now.sec - cur.sec >= 10) {
+		ptp->features &= ~PTP_ADJ_HACK;
+		ptp->features |= PTP_ADJ_SEC;
+		ptp->reg->adjust_time(ptp, false, 10, 0, true);
+		ptp->version = 1;
+	}
+/*
+ * THa  2013/01/08
+ * The Rev. D chip has a problem of decrementing nanosecond that is bigger than
+ * the current nanosecond when continual clock adjustment is enabled.  The
+ * workaround is to use the PTP_ADJ_HACK code although the actual problem
+ * avoided is now different.
+ */
+	if (!(ptp->features & PTP_ADJ_HACK)) {
+		u16 data;
+
+		data = sw->cached.ptp_clk_ctrl;
+		sw->cached.ptp_clk_ctrl |= PTP_CLK_ADJ_ENABLE;
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, sw->cached.ptp_clk_ctrl);
+		if (cur.sec < 1)
+			cur.sec = 1;
+		cur.nsec = 0;
+		ptp->reg->set_time(ptp, &cur);
+		ptp->reg->adjust_time(ptp, false, 0, 800000000, false);
+		ptp->reg->get_time(ptp, &now);
+		dbg_msg("%x:%u %x:%u\n", cur.sec, cur.nsec, now.sec, now.nsec);
+		if (abs(now.sec - cur.sec) > 2) {
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("! %x:%u\n", now.sec, now.nsec);
+			ptp->features |= PTP_ADJ_HACK;
+			sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+
+			sw->reg->w16(sw, REG_PTP_CLK_CTRL,
+				data | PTP_CLK_ADJ_ENABLE);
+			ptp->reg->set_time(ptp, &cur);
+			ptp->reg->adjust_time(ptp, false, 0, 800000000, true);
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("ok %x:%u\n", now.sec, now.nsec);
+		}
+		sw->cached.ptp_clk_ctrl = data;
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+	}
+	ptp->version = 2;
+	ptp->ops->release(ptp);
+}  /* ptp_check */
+
+static void ptp_start(struct ptp_info *ptp, int init)
+{
+	u32 ctrl;
+	struct timespec ts;
+	struct ptp_utime t;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (!ptp->version) {
+		ptp_hw_enable(ptp);
+		ptp_check(ptp);
+		if (ptp->test_access_time)
+			ptp->test_access_time(ptp);
+		ptp_init_hw(ptp);
+	} else
+	if (init && (sw->features & NEW_CAP))
+		ptp_hw_enable(ptp);
+	ptp->ops->acquire(ptp);
+	ctrl = sw_r16(sw, REG_PTP_MSG_CONF1);
+	if (ctrl == ptp->mode) {
+		ptp->cfg = sw_r16(sw, REG_PTP_MSG_CONF2);
+		ptp->domain = sw_r16(sw, REG_PTP_DOMAIN_VERSION) &
+			PTP_DOMAIN_M;
+		if (!init) {
+			ptp->ops->release(ptp);
+			return;
+		}
+	} else if (!init)
+		ptp->mode = ctrl;
+	if (ptp->mode != ptp->def_mode) {
+		dbg_msg("mode changed: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->ptp_synt = false;
+	}
+	dbg_msg("ptp_start: %04x %04x\n",
+		ptp->mode, ptp->cfg);
+	sw_w16(sw, REG_PTP_MSG_CONF1, ptp->mode);
+	sw_w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	sw_w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	if (sw->overrides & TAIL_TAGGING)
+		sw->overrides |= PTP_TAG;
+	ptp->tx_intr = PTP_PORT_XDELAY_REQ_INT;
+#if 0
+	ptp->tx_intr = PTP_PORT_XDELAY_REQ_INT | PTP_PORT_PDELAY_RESP_INT |
+		PTP_PORT_SYNC_INT;
+#endif
+	ptp_tx_intr_enable(ptp);
+	ptp->ops->release(ptp);
+
+	ts = ktime_to_timespec(ktime_get_real());
+	t.sec = ts.tv_sec;
+	t.nsec = ts.tv_nsec;
+
+	/* Adjust for leap seconds. */
+	t.sec += ptp->utc_offset;
+	ptp->ops->acquire(ptp);
+	set_ptp_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->ops->release(ptp);
+
+	prepare_pps(ptp);
+	ptp->started = true;
+}  /* ptp_start */
+
+static void save_msg_info(struct ptp_info *ptp, struct ptp_msg_info *info,
+	struct ptp_msg_hdr *hdr, u32 port, u32 timestamp)
+{
+	struct ptp_msg_options *data = &info->data;
+
+#if 0
+printk("save %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x.%04x %x %04x %x\n",
+hdr->sourcePortIdentity.clockIdentity.addr[0],
+hdr->sourcePortIdentity.clockIdentity.addr[1],
+hdr->sourcePortIdentity.clockIdentity.addr[2],
+hdr->sourcePortIdentity.clockIdentity.addr[3],
+hdr->sourcePortIdentity.clockIdentity.addr[4],
+hdr->sourcePortIdentity.clockIdentity.addr[5],
+hdr->sourcePortIdentity.clockIdentity.addr[6],
+hdr->sourcePortIdentity.clockIdentity.addr[7],
+hdr->sourcePortIdentity.port,
+hdr->messageType, hdr->sequenceId, hdr->domainNumber);
+#endif
+	memcpy(&data->id, &hdr->sourcePortIdentity,
+		sizeof(struct ptp_port_identity));
+	data->seqid = hdr->sequenceId;
+	data->domain = hdr->domainNumber;
+	data->port = port;
+	data->ts.timestamp = timestamp;
+	update_ts(&data->ts, ptp->cur_time.sec);
+	info->sec = ptp->sec_lo;
+}  /* save_msg_info */
+
+static void exit_msg_info(struct ptp_msg_info info[])
+{
+	struct ptp_msg_info *msg;
+	struct ptp_msg_info *prev;
+	int msg_type;
+
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		prev = &info[msg_type];
+		msg = prev->next;
+		while (msg) {
+			prev->next = msg->next;
+			kfree(msg);
+			msg = prev->next;
+		}
+		info[msg_type].data.port = 0;
+	}
+}  /* exit_msg_info */
+
+static void init_msg_info(struct ptp_msg_info info[], spinlock_t *lock)
+{
+	int msg_type;
+
+	spin_lock_init(lock);
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		info[msg_type].data.port = 0;
+		info[msg_type].next = NULL;
+	}
+}  /* init_msg_info */
+
+static void check_expired_msg(struct ptp_info *ptp, struct ptp_msg_info info[],
+	spinlock_t *lock, int *cnt)
+{
+	struct ptp_msg_info *msg;
+	struct ptp_msg_info *prev;
+	int msg_type;
+	int diff;
+
+	spin_lock(lock);
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		prev = &info[msg_type];
+		msg = prev->next;
+		while (msg) {
+			diff = abs(ptp->sec_lo - msg->sec);
+			if (diff >= 4) {
+				if (cnt && *cnt > 0)
+					(*cnt)--;
+				prev->next = msg->next;
+				kfree(msg);
+				msg = prev;
+			}
+			prev = msg;
+			msg = msg->next;
+		}
+	}
+	spin_unlock(lock);
+}  /* check_expired_msg */
+
+static int find_msg_info(struct ptp_msg_info *msg_info, spinlock_t *lock,
+	struct ptp_msg_hdr *hdr, struct ptp_port_identity *id, int remove,
+	struct ptp_msg_options *info)
+{
+	struct ptp_msg_info *rx_msg = msg_info;
+	struct ptp_msg_info *prev;
+	struct ptp_msg_options *data;
+	int ret = false;
+
+	spin_lock(lock);
+	prev = rx_msg;
+	rx_msg = rx_msg->next;
+	while (rx_msg) {
+		data = &rx_msg->data;
+		if (!memcmp(&data->id, id, sizeof(struct ptp_port_identity))
+				&& data->seqid == hdr->sequenceId &&
+				data->domain == hdr->domainNumber) {
+			info->port = data->port;
+			info->ts = data->ts;
+			if (remove) {
+				prev->next = rx_msg->next;
+				kfree(rx_msg);
+			}
+			ret = true;
+			break;
+		}
+		prev = rx_msg;
+		rx_msg = rx_msg->next;
+	}
+	spin_unlock(lock);
+	return ret;
+}  /* find_msg_info */
+
+static int ptp_stop(struct ptp_info *ptp, int hw_access)
+{
+	flush_work(&ptp->adj_clk);
+	flush_work(&ptp->set_latency);
+	cancel_delayed_work_sync(&ptp->check_pps);
+	cancel_delayed_work_sync(&ptp->update_sec);
+	flush_workqueue(ptp->access);
+	ptp->update_sec_jiffies = 0;
+	exit_msg_info(ptp->rx_msg_info);
+	exit_msg_info(ptp->tx_msg_info);
+
+	/* S2 chip can be reset. */
+	ptp->ptp_synt = false;
+
+	/* Stop processing PTP interrupts. */
+	ptp->started = false;
+	ptp->first_drift = ptp->drift = 0;
+	ptp->tx_intr = 0;
+
+	/* Stop triggered outputs and timestamp inputs. */
+	if (hw_access) {
+		ptp->ops->acquire(ptp);
+		ptp_hw_disable(ptp);
+		ptp_tx_intr_enable(ptp);
+		ptp->ops->release(ptp);
+	}
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+	return false;
+}  /* ptp_stop */
+
+static struct ptp_dev_info *find_minor_dev(struct ptp_dev_info *info)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_dev_info *dev;
+	struct ptp_dev_info *prev;
+
+	dev = ptp->dev[info->minor ^ 1];
+	prev = ptp->dev[info->minor];
+	while (prev != info && prev && dev) {
+		prev = prev->next;
+		dev = dev->next;
+	}
+	if (prev != info)
+		dev = NULL;
+	return dev;
+}  /* find_minor_dev */
+
+static void ptp_init_state(struct ptp_info *ptp)
+{
+	u32 reg;
+	struct ptp_utime t;
+	struct ptp_msg_info *tx_msg;
+
+	if (ptp->op_state) {
+		ptp->op_state++;
+		return;
+	}
+	mutex_lock(&ptp->lock);
+	ptp->udp_head = ptp->udp_tail = 0;
+	for (reg = 0; reg < MAX_TSM_UDP_CNT; reg++)
+		ptp->udp[reg].len = 0;
+	tx_msg = &ptp->tx_msg_info[7];
+	tx_msg->data.port = 0;
+	tx_msg->data.ts.timestamp = 0;
+	ptp->tx_msg_cnt = 0;
+	ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+	mutex_unlock(&ptp->lock);
+
+	if (!ptp->started)
+		return;
+	ptp->reg->start(ptp, false);
+
+	ptp_init_hw(ptp);
+
+	ptp->ops->acquire(ptp);
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	memset(&ptp->last_rx_ts, 0, sizeof(struct ptp_ts));
+	memset(&ptp->last_tx_ts, 0, sizeof(struct ptp_ts));
+
+	if (!ptp->ptp_synt) {
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+	ptp->reg->get_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->op_state = 1;
+
+	/* Do not try to adjust drift automatically. */
+	if (!ptp->first_drift)
+		ptp->first_drift = 1;
+	ptp->ops->release(ptp);
+}  /* ptp_init_state */
+
+static void ptp_exit_state(struct ptp_info *ptp)
+{
+	if (ptp->op_state > 1) {
+		ptp->op_state--;
+		return;
+	}
+	if (ptp->mode & PTP_MASTER) {
+		u16 data;
+		struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+		ptp->ops->acquire(ptp);
+		data = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+		data &= ~PTP_MASTER;
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, data);
+		ptp->ops->release(ptp);
+		ptp->mode &= ~PTP_MASTER;
+		ptp->def_mode &= ~PTP_MASTER;
+	}
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	ptp->tx_msg_cnt = 0;
+	ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+	ptp->tx_en = ptp->rx_en = 0;
+	ptp->tx_en_ports = ptp->rx_en_ports = 0;
+	ptp->cap = 0;
+	ptp->op_mode = 0;
+	ptp->op_state = 0;
+	ptp->forward = FWD_MAIN_DEV;
+
+	/* Indicate drift is not being set by PTP stack. */
+	ptp->drift_set = 0;
+}  /* ptp_exit_state */
+
+static struct ptp_msg *check_ptp_msg(u8 *data, u16 **udp_check_ptr)
+{
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct iphdr *iph = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	struct udphdr *udp;
+	int ipv6;
+	struct ptp_msg *msg;
+
+	if (eth->h_proto == htons(0x88F7)) {
+		msg = (struct ptp_msg *)(eth + 1);
+		goto check_ptp_version;
+	}
+
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+
+			ptr += VLAN_HLEN;
+			vlan = (struct vlan_ethhdr *) ptr;
+		}
+		if (vlan->h_vlan_encapsulated_proto == htons(0x88F7)) {
+			msg = (struct ptp_msg *)(vlan + 1);
+			goto check_ptp_version;
+		}
+		ipv6 = vlan->h_vlan_encapsulated_proto == htons(ETH_P_IPV6);
+		if (vlan->h_vlan_encapsulated_proto != htons(ETH_P_IP) &&
+				!ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(vlan + 1);
+		iph = (struct iphdr *)(vlan + 1);
+	} else {
+		ipv6 = eth->h_proto == htons(ETH_P_IPV6);
+		if (eth->h_proto != htons(ETH_P_IP) && !ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(eth + 1);
+		iph = (struct iphdr *)(eth + 1);
+	}
+
+	if (ipv6) {
+		if (ip6h->nexthdr != IPPROTO_UDP)
+			return NULL;
+
+		udp = (struct udphdr *)(ip6h + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	} else {
+		if (iph->protocol != IPPROTO_UDP)
+			return NULL;
+		if (ntohs(iph->frag_off) & IP_OFFSET)
+			return NULL;
+
+		udp = (struct udphdr *)(iph + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	}
+
+	if (udp->dest != htons(319) && udp->dest != htons(320))
+		return NULL;
+
+	msg = (struct ptp_msg *)(udp + 1);
+
+check_ptp_version:
+	if (msg->hdr.versionPTP >= 2)
+		return msg;
+	return NULL;
+}  /* check_ptp_msg */
+
+static struct ptp_msg *check_ptp_event(u8 *data)
+{
+	struct ptp_msg *msg;
+
+	msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return NULL;
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		break;
+	case DELAY_REQ_MSG:
+		break;
+	case PDELAY_REQ_MSG:
+		break;
+	case PDELAY_RESP_MSG:
+		break;
+	default:
+		msg = NULL;
+		break;
+	}
+	return msg;
+}
+
+/**
+ * update_ptp_msg - Update PTP message
+ * @data:	The PTP frame data.
+ * @port:	Buffer to hold the tx port, or hoding the rx port.
+ * @timestamp:	Buffer to hold the tx timestamp, or holding the rx timestamp.
+ * @overrides:	Parameter to do something with the reserved fields.
+ *
+ * This function serves two purposes so that driver code is compatible to the
+ * PTP stack using the first generation PTP operation behavior.
+ * The PTP header reserved fieids were used to store the port and timestamp.
+ * When receiving and if necessary those field are filled with the rx port
+ * and timestamp so that old PTP stack can retrieve those information.
+ * When transmiting the tx port and timestamp are retrieved before those fields
+ * are filled with zero.
+ */
+static struct ptp_msg *update_ptp_msg(u8 *data, u8 *port, u32 *timestamp,
+	u32 overrides)
+{
+	struct ptp_msg *msg;
+	u16 *udp_check_loc = NULL;
+	int udp_check = 0;
+
+	msg = check_ptp_msg(data, &udp_check_loc);
+	if (!msg)
+		return NULL;
+	if (msg->hdr.reserved2 != *port) {
+		u8 data = msg->hdr.reserved2;
+
+		/*
+		 * Hardware automatically updates the port number to the
+		 * actual port sending the Pdelay_Req message.
+		 */
+		if ((overrides & PTP_UPDATE_PDELAY_RESP_PORT) &&
+		    PDELAY_RESP_MSG == msg->hdr.messageType) {
+			u16 reply = ntohs(msg->data.pdelay_resp.
+				requestingPortIdentity.port);
+
+			if (reply - 1 == *port) {
+				udp_check += reply;
+				msg->data.pdelay_resp.requestingPortIdentity.
+					port = htons(*port);
+				udp_check -= *port;
+			}
+		}
+		if (overrides & PTP_ZERO_RESERVED_FIELD) {
+			udp_check += data;
+			msg->hdr.reserved2 = *port;
+			udp_check -= *port;
+		}
+		*port = data;
+	}
+	if (msg->hdr.reserved3 != htonl(*timestamp)) {
+		u32 tmp = ntohl(msg->hdr.reserved3);
+
+		if (overrides & PTP_ZERO_RESERVED_FIELD) {
+			int i;
+			u16 *data = (u16 *) &msg->hdr.reserved3;
+
+			for (i = 0; i < 2; i++)
+				udp_check += ntohs(data[i]);
+			msg->hdr.reserved3 = htonl(*timestamp);
+			for (i = 0; i < 2; i++)
+				udp_check -= ntohs(data[i]);
+		}
+		*timestamp = tmp;
+	}
+	if ((overrides & PTP_VERIFY_TIMESTAMP) &&
+			PDELAY_RESP_MSG == msg->hdr.messageType &&
+			msg->hdr.flagField.flag.twoStepFlag) {
+		struct ptp_utime rx;
+
+		rx.nsec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			nsec);
+		rx.sec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			sec.lo);
+		*timestamp = (rx.sec << 30) | rx.nsec;
+	}
+	if (udp_check && udp_check_loc) {
+		u16 check;
+
+		check = ntohs(*udp_check_loc);
+		udp_check += check;
+		udp_check = (udp_check >> 16) + (udp_check & 0xffff);
+		udp_check += (udp_check >> 16);
+		check = (u16) udp_check;
+		if (!check)
+			check = -1;
+		*udp_check_loc = htons(check);
+	}
+	return msg;
+}  /* update_ptp_msg */
+
+static void get_rx_tstamp(void *ptr, struct sk_buff *skb)
+{
+	struct ptp_info *ptp = ptr;
+	struct ptp_msg *msg;
+	struct ptp_msg_options *rx_msg;
+	struct ptp_ts ts;
+	u64 ns;
+	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+
+	if (!shhwtstamps)
+		return;
+
+	/* Received PTP messages are saved in database. */
+	if (ptp->op_mode > 1) {
+
+		/* Use previously parsed PTP message if available. */
+		msg = ptp->rx_msg;
+		if (!msg)
+			msg = check_ptp_msg(skb->data, NULL);
+		if (!msg || msg->hdr.messageType & 0x8)
+			return;
+	}
+
+	rx_msg = &ptp->rx_msg_info[7].data;
+	ts = rx_msg->ts;
+
+	ns = (u64) ts.t.sec * NANOSEC_IN_SEC + ts.t.nsec;
+	memset(shhwtstamps, 0, sizeof(*shhwtstamps));
+	shhwtstamps->hwtstamp = ns_to_ktime(ns);
+}  /* get_rx_tstamp */
+
+static void get_tx_tstamp(struct ptp_info *ptp, struct sk_buff *skb)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int cnt;
+	uint p;
+	struct ptp_msg *msg;
+	u32 port;
+	u32 intr;
+	struct ptp_tx_ts *tx;
+	struct sk_buff *orig_skb = skb;
+
+	if (ptp->tx_msg_parsed)
+		msg = ptp->tx_msg;
+	else
+		msg = check_ptp_msg(skb->data, NULL);
+	ptp->tx_msg_parsed = false;
+	if (!msg || msg->hdr.messageType & 0x8)
+		return;
+
+	if (ptp->tx_ports & sw->TAIL_TAG_LOOKUP)
+		port = (1 << ptp->ports) - 1;
+	else
+		port = ptp->tx_ports & ((1 << ptp->ports) - 1);
+	if (SYNC_MSG == msg->hdr.messageType) {
+		tx = ptp->tx_sync;
+		intr = PTP_PORT_SYNC_INT;
+	} else if (PDELAY_RESP_MSG == msg->hdr.messageType) {
+		tx = ptp->tx_resp;
+		intr = PTP_PORT_PDELAY_RESP_INT;
+	} else {
+		tx = ptp->tx_dreq;
+		intr = PTP_PORT_XDELAY_REQ_INT;
+	}
+	if (!(ptp->tx_intr & intr))
+		return;
+	cnt = 0;
+	for (p = 0; p < ptp->ports; p++, tx++) {
+		if (!(port & (1 << p)))
+			continue;
+		if (tx->skb) {
+			if (skb_shinfo(tx->skb)->tx_flags & SKBTX_HW_TSTAMP)
+				skb_shinfo(tx->skb)->tx_flags &=
+					~SKBTX_IN_PROGRESS;
+			else {
+				dev_kfree_skb_irq(tx->skb);
+				tx->skb = NULL;
+			}
+		}
+
+		/* Need to create socket buffer for more than 1 port. */
+		if (cnt++) {
+			skb = skb_copy(orig_skb, GFP_ATOMIC);
+			if (!skb)
+				break;
+			skb->sk = orig_skb->sk;
+			msg = check_ptp_event(skb->data);
+		}
+		tx->skb = skb;
+		tx->msg = msg;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+	}
+}  /* get_tx_tstamp */
+
+static int ptp_hwtstamp_ioctl(struct ptp_info *ptp, struct ifreq *ifr,
+			      u16 ports)
+{
+	struct hwtstamp_config config;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	/* reserved for future extensions */
+	if (config.flags)
+		return -EINVAL;
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		ptp->tx_en_ports &= ~ports;
+		if (!ptp->tx_en_ports)
+			ptp->tx_en &= ~1;
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+	case HWTSTAMP_TX_ON:
+		ptp->tx_en_ports |= ports;
+		ptp->tx_en |= 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		if (!ptp->cap && (ptp->rx_en & 1) && (ptp->rx_en & (1 << 8))) {
+			ptp->tx_en &= ~(1 << 8);
+			ptp->rx_en &= ~(1 << 8);
+		}
+		ptp->rx_en_ports &= ~ports;
+		if (!ptp->rx_en_ports)
+			ptp->rx_en &= ~1;
+		break;
+	case HWTSTAMP_FILTER_ALL:
+#if 0
+		ptp->rx_en |= 1;
+		break;
+#endif
+	default:
+		if (!ptp->cap && !(ptp->rx_en & 1) && (ptp->tx_en & 1)) {
+			ptp->tx_en |= (1 << 8);
+			ptp->rx_en |= (1 << 8);
+		}
+		ptp->rx_en_ports |= ports;
+		ptp->rx_en |= 1;
+		break;
+	}
+
+	return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
+		-EFAULT : 0;
+}
+
+static int ptp_chk_rx_msg(struct ptp_info *ptp, u8 *data, uint port)
+{
+	struct ptp_msg *msg;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Use previously parsed PTP message if available. */
+	msg = ptp->rx_msg;
+	if (!msg)
+		msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return false;
+	ptp->rx_msg = msg;
+
+	if ((sw->features & USE_802_1X_AUTH) &&
+	    !(sw->on_ports & (1 << port))) {
+		return true;
+	}
+	return false;
+}  /* ptp_chk_rx_msg */
+
+static int ptp_drop_pkt(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+	int *tag, int *ptp_tag, int *forward)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Not PTP message. */
+	if (!(sw->tag.ports & 0x80))
+		return false;
+	do {
+		u16 vid;
+		u16 *protocol;
+
+		if (!(ptp->vid))
+			break;
+		if (vlan_get_tag(skb, &vid))
+			break;
+		vid &= VLAN_VID_MASK;
+		protocol = (u16 *) &skb->data[VLAN_ETH_HLEN - 2];
+
+		if (!vid)
+			break;
+		if (*protocol == ntohs(0x88F7) && vid != ptp->vid)
+			return true;
+	} while (0);
+	if (!(vlan_id & (1 << *tag)))
+		*tag = 0;
+	*ptp_tag = sw->tag.ports & ~0x80;
+	ptp->ops->get_rx_info(ptp, skb->data, *ptp_tag, sw->tag.timestamp);
+	*forward = ptp->forward;
+	if (!ptp->op_state) {
+		*ptp_tag = 0;
+		return false;
+	}
+	if (ptp_chk_rx_msg(ptp, skb->data, *ptp_tag))
+		return true;
+	if (ptp->rx_en & 1)
+		ptp->ops->get_rx_tstamp(ptp, skb);
+	(*ptp_tag)++;
+	return false;
+}  /* ptp_drop_pkt */
+
+static void set_msg_info(struct ptp_info *ptp, struct ptp_msg_hdr *hdr,
+	u32 port, u32 timestamp)
+{
+	struct ptp_msg_info *tx_msg;
+	struct ptp_msg_info *info;
+
+	tx_msg = &ptp->tx_msg_info[hdr->messageType];
+	info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+	if (info) {
+		spin_lock(&ptp->tx_msg_lock);
+		save_msg_info(ptp, info, hdr, port, timestamp);
+		info->next = tx_msg->next;
+		tx_msg->next = info;
+		if (ptp->tx_msg_cnt >= 0)
+			ptp->tx_msg_cnt++;
+		spin_unlock(&ptp->tx_msg_lock);
+	}
+}  /* set_msg_info */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct ptp_dev_info *info,
+	int *output, int wait);
+
+static struct ptp_utime last_rcv;
+static s64 first_sync;
+static s64 first_recv;
+
+static void handle_sync(struct ptp_info *ptp, struct ptp_msg *msg)
+{
+	struct ptp_utime recv;
+	struct ptp_utime sync;
+	struct ksz_ptp_time drift;
+	struct ksz_ptp_time interval;
+	struct ksz_ptp_time offset;
+	s64 corr;
+	u64 nsec;
+	s64 drift_per_sec;
+	s64 avg;
+	s64 cur_recv;
+	s64 cur_sync;
+static s64 sync_corr;
+static struct ptp_ts ts;
+static struct ptp_utime last_sync;
+static struct ksz_ptp_time last_offset;
+
+	sync.sec = sync.nsec = 0;
+	if (SYNC_MSG == msg->hdr.messageType) {
+		struct ptp_msg_info *rx_msg;
+
+		rx_msg = &ptp->rx_msg_info[7];
+		ts = rx_msg->data.ts;
+
+		corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+		corr <<= 32;
+		corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+		corr >>= 16;
+		sync_corr = corr;
+
+		/* This is one-step Sync. */
+		if (!msg->hdr.flagField.flag.twoStepFlag) {
+			sync.sec = ntohl(msg->data.sync.
+				originTimestamp.sec.lo);
+			sync.nsec = ntohl(msg->data.sync.
+				originTimestamp.nsec);
+		}
+	} else if (FOLLOW_UP_MSG == msg->hdr.messageType) {
+		corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+		corr <<= 32;
+		corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+		corr >>= 16;
+		sync_corr += corr;
+		sync.sec = ntohl(msg->data.follow_up.
+			preciseOriginTimestamp.sec.lo);
+		sync.nsec = ntohl(msg->data.follow_up.
+			preciseOriginTimestamp.nsec);
+	}
+
+	/* Sync transmit timestamp not received. */
+	if (!sync.sec)
+		return;
+	if (sync_corr) {
+		u32 rem;
+
+		corr = ts.t.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += ts.t.nsec;
+		corr -= sync_corr;
+		corr = div_s64_rem(corr, NANOSEC_IN_SEC, &rem);
+		ts.t.sec = (u32) corr;
+		ts.t.nsec = rem;
+	}
+#if 0
+	ts.t.nsec += 5;
+	ts.t.nsec /= 10;
+	ts.t.nsec *= 10;
+	sync.nsec += 5;
+	sync.nsec /= 10;
+	sync.nsec *= 10;
+#endif
+	calc_udiff(&sync, &ts.t, &offset);
+	cur_recv = cur_sync = avg = 0;
+	if (first_sync) {
+		nsec = sync.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += sync.nsec;
+		cur_sync = nsec;
+		nsec = ts.t.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += ts.t.nsec;
+		cur_recv = nsec;
+		cur_sync -= first_sync;
+		cur_recv -= first_recv;
+		cur_recv -= cur_sync;
+		cur_recv = abs(cur_recv);
+		cur_recv *= NANOSEC_IN_SEC;
+	} else {
+		nsec = sync.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += sync.nsec;
+		first_sync = nsec;
+		nsec = ts.t.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += ts.t.nsec;
+		first_recv = nsec;
+	}
+	nsec = 0;
+	drift_per_sec = 0;
+	if (last_rcv.sec) {
+		calc_udiff(&last_sync, &sync, &interval);
+		calc_diff(&last_offset, &offset, &drift);
+		nsec = interval.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += interval.nsec;
+		drift_per_sec = abs(drift.sec);
+		drift_per_sec *= NANOSEC_IN_SEC;
+		drift_per_sec += abs(drift.nsec);
+		drift_per_sec *= NANOSEC_IN_SEC;
+		drift_per_sec = div64_s64(drift_per_sec, nsec);
+		avg = div64_s64(cur_recv, cur_sync);
+	}
+	if (sync_corr) {
+		u32 rem;
+
+		cur_recv = avg * nsec;
+		cur_recv = div_s64_rem(cur_recv, NANOSEC_IN_SEC, &rem);
+		cur_recv += nsec;
+		corr = last_rcv.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += last_rcv.nsec;
+		cur_recv += corr;
+		corr = recv.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += recv.nsec;
+		corr -= cur_recv;
+printk(" corr: %lld %lld %lld\n", sync_corr, corr, corr - sync_corr);
+		sync_corr = 0;
+	}
+dbg_msg("sync: %x:%9u %x:%9u p:%10lld d:%lld a:%lld %lld\n",
+ts.t.sec, ts.t.nsec,
+sync.sec, sync.nsec, nsec, drift_per_sec, avg, drift_per_sec - avg);
+dbg_msg("o: %d\n", offset.nsec);
+#if 1
+	if (nsec && nsec < 5000000000) {
+		struct ptp_clk_options clk_opt;
+		int drift_set;
+		int output;
+		int err;
+
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+
+		drift_set = (int) drift_per_sec;
+		if (drift_set < 5 && abs(offset.nsec) > 1000) {
+			clk_opt.sec = offset.sec;
+			clk_opt.nsec = offset.nsec;
+			if (offset.nsec < 0)
+				output = 2;
+			last_rcv.sec = 0;
+			ts.t.sec = 0;
+			first_recv = 0;
+			first_sync = 0;
+		}
+
+		if (drift.nsec < 0)
+			drift_set = -drift_set;
+		drift_set += ptp->drift;
+
+		clk_opt.drift = drift_set;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			false);
+	}
+#endif
+	last_sync = sync;
+	last_rcv = ts.t;
+	last_offset = offset;
+}  /* handle_sync */
+
+static struct ptp_msg *ptp_set_rx_info(struct ptp_info *ptp, u8 *data, u8 port,
+	u32 timestamp)
+{
+	struct ptp_msg *msg;
+	u32 overrides = ptp->overrides;
+
+	/* Do not need to parse PTP message. */
+	if (1 == ptp->op_mode)
+		return NULL;
+
+	/* Set receive port and timestamp inside the PTP message. */
+	if (0 == ptp->op_mode && ptp->op_state) {
+		overrides |= PTP_ZERO_RESERVED_FIELD;
+		overrides |= PTP_UPDATE_PDELAY_RESP_PORT;
+	} else
+		overrides &= ~PTP_ZERO_RESERVED_FIELD;
+	msg = update_ptp_msg(data, &port, &timestamp, overrides);
+
+#if 1
+	if (ptp->overrides & PTP_CHECK_SYNC_TIME)
+		handle_sync(ptp, msg);
+#endif
+
+	/* Do not need to save PTP messages. */
+	if (0 == ptp->op_mode)
+		msg = NULL;
+	return msg;
+}  /* ptp_set_rx_info */
+
+#if 1
+/* Used for 802.1BA test tool to accept Announce messages. */
+static int ba_hack;
+#endif
+
+static struct ptp_msg *ptp_get_tx_info(struct ptp_info *ptp, u8 *data,
+	u32 *tx_port, u32 *tx_timestamp)
+{
+	struct ptp_msg *msg;
+	u32 overrides = ptp->overrides;
+	u32 timestamp = 0;
+	u8 port = 0;
+
+#if 1
+	if (!ba_hack)
+#endif
+	if ((1 == ptp->op_mode || 2 == ptp->op_mode) && !ptp->tx_msg_cnt) {
+		/*
+		 * This packet is not parsed and will be checked again if
+		 * necessary.
+		 */
+		ptp->tx_msg_parsed = false;
+		return NULL;
+	}
+
+	/* Get receive port and timestamp inside the PTP message. */
+	if (0 == ptp->op_mode)
+		overrides |= PTP_ZERO_RESERVED_FIELD;
+	msg = update_ptp_msg(data, &port, &timestamp, overrides);
+	if (msg) {
+		port = ptp_get_dest_port(ptp, 0, port);
+
+		/* Get transmit port and timestamp inside the PTP message. */
+		if (0 == ptp->op_mode) {
+			if (port)
+				*tx_port = port;
+			if (timestamp)
+				*tx_timestamp = timestamp;
+		}
+
+		/* Simulate passing transmit information from application. */
+		if (ptp->overrides & PTP_TEST_TX_INFO) {
+			u32 tx_ports = 0;
+
+			if (port)
+				tx_ports = port;
+			set_msg_info(ptp, &msg->hdr, tx_ports, timestamp);
+		}
+	}
+	return msg;
+}  /* ptp_get_tx_info */
+
+static void ptp_get_rx_info(struct ptp_info *ptp, u8 *data, u8 port,
+	u32 timestamp)
+{
+	int index;
+	struct ptp_msg *msg;
+	struct ptp_msg_info *rx_msg;
+	struct ptp_msg_info *info = NULL;
+
+	/* Indicate PTP message is not parsed yet. */
+	ptp->rx_msg = NULL;
+
+	/* Entry is not used for PTP message. */
+	rx_msg = &ptp->rx_msg_info[7];
+	rx_msg->data.port = port;
+	rx_msg->data.ts.timestamp = timestamp;
+	update_ts(&rx_msg->data.ts, ptp->cur_time.sec);
+
+	index = get_speed_index(ptp, port);
+	sub_nsec(&rx_msg->data.ts.t, ptp->rx_latency[port][index]);
+	timestamp = (rx_msg->data.ts.t.sec << 30) | rx_msg->data.ts.t.nsec;
+	if (ptp->overrides & PTP_CHECK_PATH_DELAY) {
+		if (ptp->last_tx_ts.t.sec) {
+			struct ksz_ptp_time diff;
+
+			calc_udiff(&ptp->last_tx_ts.t, &rx_msg->data.ts.t,
+				&diff);
+			dbg_msg("pd: %d\n", diff.nsec);
+		} else
+			ptp->last_rx_ts = rx_msg->data.ts;
+	}
+#if 0
+	if (ptp->overrides & PTP_CHECK_SYNC_TIME)
+dbg_msg(" %x; %08x; %x:%09u\n", ptp->cur_time.sec, timestamp,
+rx_msg->data.ts.t.sec, rx_msg->data.ts.t.nsec);
+#endif
+
+	msg = ptp_set_rx_info(ptp, data, port, timestamp);
+	if (!msg)
+		return;
+
+	/* Indicate PTP message is parsed. */
+	ptp->rx_msg = msg;
+
+	rx_msg = &ptp->rx_msg_info[msg->hdr.messageType];
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		if (!memcmp(&msg->hdr.sourcePortIdentity, &ptp->masterIdentity,
+		    sizeof(struct ptp_clock_identity))) {
+			rx_msg->data.port = port;
+			rx_msg->data.ts.timestamp = timestamp;
+		}
+		info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+		break;
+
+	/* General messages that do not need tracking. */
+	case DELAY_RESP_MSG:
+	case ANNOUNCE_MSG:
+	case SIGNALING_MSG:
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		break;
+	case MANAGEMENT_MSG:
+	{
+		u8 action = msg->data.management.b.actionField;
+
+		/* No need to track management response message. */
+		if (MANAGEMENT_RESPONSE == action ||
+				MANAGEMENT_ACKNOWLEDGE == action)
+			break;
+	}
+	default:
+		info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+		break;
+	}
+	if (info) {
+		spin_lock(&ptp->rx_msg_lock);
+		save_msg_info(ptp, info, &msg->hdr, port, timestamp);
+		info->next = rx_msg->next;
+		rx_msg->next = info;
+		spin_unlock(&ptp->rx_msg_lock);
+	}
+}  /* ptp_get_rx_info */
+
+static void ptp_set_tx_info(struct ptp_info *ptp, u8 *data, void *ptr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int found;
+	struct ptp_msg *msg;
+	struct ptp_msg_options tx_msg;
+	struct ksz_sw_tx_tag *tag = ptr;
+
+	tx_msg.port = 0;
+	tx_msg.ts.timestamp = 0;
+
+	/* Assume packet will be parsed to determine PTP message type. */
+	ptp->tx_msg_parsed = true;
+	ptp->tx_msg = ptp_get_tx_info(ptp, data, &tx_msg.port,
+		&tx_msg.ts.timestamp);
+	if (!ptp->tx_msg) {
+
+		/* Block PTP messages for blocked ports. */
+		if ((sw->features & USE_802_1X_AUTH) &&
+		    (tag->ports & sw->TAIL_TAG_LOOKUP)) {
+			if (!ptp->tx_msg_parsed) {
+				ptp->tx_msg = check_ptp_msg(data, NULL);
+				ptp->tx_msg_parsed = true;
+			}
+			if (ptp->tx_msg)
+				tag->ports = sw->on_ports;
+		}
+
+		/* Remember transmit ports for transmit timestamp report. */
+		ptp->tx_ports = tag->ports;
+		return;
+	}
+	msg = ptp->tx_msg;
+
+	/* Check whether application sets transmit information using API. */
+	found = find_msg_info(&ptp->tx_msg_info[msg->hdr.messageType],
+		&ptp->tx_msg_lock, &msg->hdr, &msg->hdr.sourcePortIdentity,
+		true, &tx_msg);
+	if (found) {
+		uint port;
+		u32 bits;
+		struct ptp_tx_ts *tx = NULL;
+
+		port = 0;
+		bits = tx_msg.port;
+		while (bits) {
+			if ((bits & 1) && bits != 1) {
+				port = 0;
+				break;
+			}
+			++port;
+			bits >>= 1;
+		}
+		switch (msg->hdr.messageType) {
+		case SYNC_MSG:
+			if (port)
+				tx = &ptp->tx_sync[port - 1];
+			break;
+		case DELAY_REQ_MSG:
+			if (port)
+				tx = &ptp->tx_dreq[port - 1];
+			break;
+		case PDELAY_REQ_MSG:
+			if (port)
+				tx = &ptp->tx_dreq[port - 1];
+			break;
+		case PDELAY_RESP_MSG:
+			if (port)
+				tx = &ptp->tx_resp[port - 1];
+			break;
+		default:
+			tx = NULL;
+		}
+		if (tx) {
+			tx->ts.timestamp = 0;
+			tx->req_time = 0;
+			tx->hdr.messageType = 7;
+		}
+		found = 2;
+		if (ptp->tx_msg_cnt > 0)
+			ptp->tx_msg_cnt--;
+	}
+
+	/* Check whether a default port is set.  Only used in testing. */
+	if (!found && (ptp->overrides & PTP_USE_DEFAULT_PORT)) {
+		tx_msg.port = ptp->tx_msg_info[7].data.port;
+		tx_msg.ts.timestamp = ptp->tx_msg_info[7].data.ts.timestamp;
+		found = 2;
+	}
+
+	/* Only PDELAY_RESP_MSG requires timestamp in transmission. */
+	if (!found && PDELAY_RESP_MSG == msg->hdr.messageType) {
+		int two_step = msg->hdr.flagField.flag.twoStepFlag;
+		struct ptp_msg_pdelay_resp *resp = &msg->data.pdelay_resp;
+
+		found = find_msg_info(&ptp->rx_msg_info[PDELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr,
+			&resp->requestingPortIdentity, !two_step, &tx_msg);
+
+		/* Need to specify timestamp in 1-step mode. */
+		if (!two_step && found) {
+			int delay;
+			int index;
+			struct ptp_ts ts;
+
+			/* Calculate timestamp automatically. */
+			ts = tx_msg.ts;
+			index = get_speed_index(ptp, tx_msg.port);
+			delay = ptp->rx_latency[tx_msg.port][index];
+			sub_nsec(&ts.t, delay);
+			tx_msg.ts.timestamp = (ts.t.sec << 30) | ts.t.nsec;
+		}
+	}
+	if (ba_hack) {
+		if (msg->hdr.messageType == ANNOUNCE_MSG &&
+		    memcmp(&data[6], sw->info->mac_addr, ETH_ALEN))
+			memcpy(&data[6], sw->info->mac_addr, ETH_ALEN);
+	}
+
+	tag->timestamp = tx_msg.ts.timestamp;
+
+	/* Specific ports are specified. */
+	if (!(tag->ports & sw->TAIL_TAG_LOOKUP))
+		goto set_tx_info_done;
+
+	/* No need to set outgoing port for unicast message. */
+	if (msg->hdr.flagField.flag.unicastFlag)
+		goto set_tx_info_done;
+
+	if (found || tx_msg.port) {
+		if (tx_msg.port) {
+			if (1 == found)
+				tag->ports = (1 << tx_msg.port);
+			else
+				tag->ports = tx_msg.port;
+		}
+		goto set_tx_info_done;
+	} else if (ptp->op_mode != 3)
+		goto set_tx_info_done;
+
+	/* Automatically find a port to send. */
+	switch (msg->hdr.messageType) {
+	case DELAY_REQ_MSG:
+		if (ptp->rx_msg_info[SYNC_MSG].data.ts.timestamp) {
+			tx_msg.port = ptp->rx_msg_info[SYNC_MSG].data.port;
+			found = true;
+		}
+		break;
+	case PDELAY_RESP_MSG:
+		/* Already determined from previous code. */
+		break;
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		found = find_msg_info(&ptp->rx_msg_info[PDELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			pdelay_resp_follow_up.requestingPortIdentity,
+			true, &tx_msg);
+		break;
+	case DELAY_RESP_MSG:
+		found = find_msg_info(&ptp->rx_msg_info[DELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			delay_resp.requestingPortIdentity,
+			true, &tx_msg);
+		break;
+	case MANAGEMENT_MSG:
+	{
+		u8 action = msg->data.management.b.actionField;
+
+		/* Not response to management request message. */
+		if (MANAGEMENT_GET == action || MANAGEMENT_SET == action ||
+		    MANAGEMENT_COMMAND == action)
+			break;
+
+		found = find_msg_info(&ptp->rx_msg_info[MANAGEMENT_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			management.b.targetPortIdentity,
+			true, &tx_msg);
+		break;
+	}
+	default:
+		break;
+	}
+
+	if (found)
+		tag->ports = (1 << tx_msg.port);
+dbg_msg("  tx m:%x f:%d p:%x\n", msg->hdr.messageType, found, tag->ports);
+
+set_tx_info_done:
+	if ((sw->features & USE_802_1X_AUTH) &&
+	    (tag->ports & sw->TAIL_TAG_LOOKUP)) {
+		tag->ports = sw->on_ports;
+	}
+
+	/* Remember transmit ports for transmit timestamp report. */
+	ptp->tx_ports = tag->ports;
+
+	do {
+		uint p;
+		u32 port;
+		u32 intr;
+		struct ptp_tx_ts *tx;
+
+		/* Not PTP event message. */
+		if (msg->hdr.messageType & 0x8)
+			break;
+		if (ptp->tx_ports & sw->TAIL_TAG_LOOKUP)
+			port = (1 << ptp->ports) - 1;
+		else
+			port = ptp->tx_ports & ((1 << ptp->ports) - 1);
+		if (SYNC_MSG == msg->hdr.messageType) {
+			tx = ptp->tx_sync;
+			intr = PTP_PORT_SYNC_INT;
+		} else if (PDELAY_RESP_MSG == msg->hdr.messageType) {
+			tx = ptp->tx_resp;
+			intr = PTP_PORT_PDELAY_RESP_INT;
+		} else {
+			tx = ptp->tx_dreq;
+			intr = PTP_PORT_XDELAY_REQ_INT;
+		}
+
+		/* Transmit timestamps are not retrieved. */
+		if (!(ptp->tx_intr & intr))
+			break;
+		for (p = 0; p < ptp->ports; p++, tx++) {
+			if (!(port & (1 << p)))
+				continue;
+			memcpy(&tx->hdr, &msg->hdr,
+				sizeof(struct ptp_msg_hdr));
+		}
+	} while (0);
+}  /* ptp_set_tx_info */
+
+static void proc_ptp_get_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp->mode = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	ptp->cfg = sw->reg->r16(sw, REG_PTP_MSG_CONF2);
+	ptp->domain = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) & PTP_DOMAIN_M;
+	ptp->ops->release(ptp);
+
+	/* Check mode in case the switch is reset outside of driver control. */
+	if (ptp->mode != ptp->def_mode && ptp->started) {
+		dbg_msg("mode mismatched: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->reg->start(ptp, false);
+	}
+	cmd->two_step = (ptp->mode & PTP_1STEP) ? 0 : 1;
+	cmd->master = (ptp->mode & PTP_MASTER) ? 1 : 0;
+	cmd->p2p = (ptp->mode & PTP_TC_P2P) ? 1 : 0;
+	cmd->as = (ptp->mode & PTP_802_1AS) ? 1 : 0;
+	cmd->unicast = (ptp->cfg & PTP_UNICAST_ENABLE) ? 1 : 0;
+	cmd->alternate = (ptp->cfg & PTP_ALTERNATE_MASTER) ? 1 : 0;
+	cmd->domain_check = (ptp->cfg & PTP_DOMAIN_CHECK) ? 1 : 0;
+	cmd->udp_csum = (ptp->cfg & PTP_UDP_CHECKSUM) ? 1 : 0;
+	cmd->delay_assoc = (ptp->cfg & PTP_DELAY_CHECK) ? 1 : 0;
+	cmd->pdelay_assoc = (ptp->cfg & PTP_PDELAY_CHECK) ? 1 : 0;
+	cmd->sync_assoc = (ptp->cfg & PTP_SYNC_CHECK) ? 1 : 0;
+	cmd->drop_sync = (ptp->cfg & PTP_DROP_SYNC_DELAY_REQ) ? 1 : 0;
+	cmd->priority = (ptp->cfg & PTP_ALL_HIGH_PRIO) ? 1 : 0;
+	cmd->reserved = ptp->started;
+	cmd->domain = ptp->domain;
+	cmd->access_delay = ptp->get_delay;
+}  /* proc_ptp_get_cfg */
+
+static int proc_ptp_set_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	u16 cfg;
+	u16 mode;
+	u8 domain;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	mode = ptp->mode;
+	cfg = ptp->cfg;
+	domain = ptp->domain;
+	if (cmd->domain_set) {
+		domain = cmd->domain;
+	} else {
+		if (cmd->two_step_set) {
+			if (cmd->two_step)
+				ptp->mode &= ~PTP_1STEP;
+			else
+				ptp->mode |= PTP_1STEP;
+		}
+		if (cmd->master_set) {
+			if (cmd->master)
+				ptp->mode |= PTP_MASTER;
+			else
+				ptp->mode &= ~PTP_MASTER;
+		}
+		if (cmd->p2p_set) {
+			if (cmd->p2p)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+		}
+		if (cmd->as_set) {
+			if (cmd->as)
+				ptp->mode |= PTP_802_1AS;
+			else
+				ptp->mode &= ~PTP_802_1AS;
+		}
+		if (cmd->unicast_set) {
+			if (cmd->unicast)
+				ptp->cfg |= PTP_UNICAST_ENABLE;
+			else
+				ptp->cfg &= ~PTP_UNICAST_ENABLE;
+		}
+		if (cmd->alternate_set) {
+			if (cmd->alternate)
+				ptp->cfg |= PTP_ALTERNATE_MASTER;
+			else
+				ptp->cfg &= ~PTP_ALTERNATE_MASTER;
+		}
+		if (cmd->domain_check_set) {
+			if (cmd->domain_check)
+				ptp->cfg |= PTP_DOMAIN_CHECK;
+			else
+				ptp->cfg &= ~PTP_DOMAIN_CHECK;
+		}
+		if (cmd->udp_csum_set) {
+			if (cmd->udp_csum)
+				ptp->cfg |= PTP_UDP_CHECKSUM;
+			else
+				ptp->cfg &= ~PTP_UDP_CHECKSUM;
+		}
+		if (cmd->delay_assoc_set) {
+			if (cmd->delay_assoc)
+				ptp->cfg |= PTP_DELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_DELAY_CHECK;
+		}
+		if (cmd->pdelay_assoc_set) {
+			if (!(ptp->mode & PTP_1STEP))
+				cmd->pdelay_assoc = 0;
+			if (cmd->pdelay_assoc)
+				ptp->cfg |= PTP_PDELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_PDELAY_CHECK;
+		}
+		if (cmd->sync_assoc_set) {
+			if (cmd->sync_assoc)
+				ptp->cfg |= PTP_SYNC_CHECK;
+			else
+				ptp->cfg &= ~PTP_SYNC_CHECK;
+		}
+		if (cmd->drop_sync_set) {
+			if (cmd->drop_sync)
+				ptp->cfg |= PTP_DROP_SYNC_DELAY_REQ;
+			else
+				ptp->cfg &= ~PTP_DROP_SYNC_DELAY_REQ;
+		}
+		if (cmd->priority_set) {
+			if (cmd->priority)
+				ptp->cfg |= PTP_ALL_HIGH_PRIO;
+			else
+				ptp->cfg &= ~PTP_ALL_HIGH_PRIO;
+		}
+	}
+	ptp->ops->acquire(ptp);
+	if (mode != ptp->mode) {
+		u16 tx_intr = ptp->tx_intr;
+
+		/* For efficiency. */
+		if ((ptp->mode & PTP_1STEP) &&
+		    !(ptp->overrides & PTP_VERIFY_TIMESTAMP))
+			ptp->tx_intr &= ~
+				(PTP_PORT_SYNC_INT | PTP_PORT_PDELAY_RESP_INT);
+		else
+			ptp->tx_intr |=
+				(PTP_PORT_SYNC_INT | PTP_PORT_PDELAY_RESP_INT);
+		dbg_msg("mode: %x %x\n", mode, ptp->mode);
+		mode = ptp->mode;
+		if (ptp->overrides & PTP_VERIFY_TIMESTAMP)
+			mode |= PTP_1STEP;
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, mode);
+		ptp->def_mode = mode;
+		if (tx_intr != ptp->tx_intr)
+			ptp_tx_intr_enable(ptp);
+	}
+	if (cfg != ptp->cfg) {
+		dbg_msg("cfg: %x %x\n", cfg, ptp->cfg);
+		sw->reg->w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	}
+	if (domain != ptp->domain) {
+		ptp->domain = domain;
+		set_ptp_domain(ptp, ptp->domain);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_cfg */
+
+static void cancel_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	ptp->ops->acquire(ptp);
+	first = tsi;
+	events = &ptp->events[tsi];
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	tsi = first;
+	ptp->events[tsi].timeout = 0;
+	do {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+		events = &ptp->events[tsi];
+		events->first = 0;
+		events->last = 0;
+		tsi_bit = 1 << tsi;
+		if (ptp->tsi_used & tsi_bit) {
+			if (events->num < events->max) {
+				ptp->reg->read_event(ptp, tsi);
+				ptp->ts_status = 0;
+			}
+			ptp->reg->rx_off(ptp, tsi);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			if (ptp->tsi_sys & tsi_bit) {
+				printk(KERN_INFO "tsi %d off!\n", tsi);
+				ptp->tsi_sys &= ~tsi_bit;
+				ptp->update_sec_jiffies = jiffies;
+				schedule_delayed_work(&ptp->update_sec,
+					1000 * HZ / 1000);
+			}
+			ptp->tsi_dev[tsi] = NULL;
+		}
+		++tsi;
+	} while (tsi != last);
+	ptp->ops->release(ptp);
+}  /* cancel_rx_unit */
+
+static int check_expired_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	u32 expired;
+	struct ptp_event *events;
+	struct ksz_ptp_time diff;
+	struct ptp_utime t;
+
+	events = &ptp->events[tsi];
+	first = tsi;
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	events = &ptp->events[first];
+	if (events->num && events->timeout) {
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &t);
+		ptp->ops->release(ptp);
+		calc_udiff(events->t, &t, &diff);
+		if (diff.sec >= 0 && diff.nsec >= 0) {
+			expired = diff.sec * 1000 + diff.nsec / 1000000;
+			expired = expired * HZ / 1000;
+			if (expired > events->timeout) {
+				cancel_rx_unit(ptp, first);
+				return 1;
+			}
+		}
+	}
+	return 0;
+}  /* check_expired_rx_unit */
+
+static int proc_dev_rx_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_tsi_options *cmd = (struct ptp_tsi_options *) data;
+	u8 event;
+	int first;
+	int i;
+	int intr;
+	int tsi;
+	int avail;
+	int total;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	tsi = cmd->tsi;
+	total = cmd->total;
+	if (!total)
+		total = 1;
+	first = tsi;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		cancel_rx_unit(ptp, tsi);
+		goto proc_ptp_rx_cascade_event_done;
+	}
+	if (tsi >= MAX_TIMESTAMP_UNIT) {
+		first = 0;
+		do {
+			for (tsi = first; tsi < MAX_TIMESTAMP_UNIT; tsi++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last)
+					break;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			first = tsi;
+			avail = 1;
+			for (i = 1; i < total; i++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last) {
+					++avail;
+					++tsi;
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+				} else {
+					++first;
+					break;
+				}
+		} while (avail < total);
+	} else {
+		for (i = 0; i < total; i++) {
+			if (ptp->tsi_used & (1 << tsi) ||
+					ptp->events[tsi].last)
+				if (!check_expired_rx_unit(ptp, tsi))
+					return DEV_IOC_UNIT_USED;
+			++tsi;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				tsi = 0;
+		}
+	}
+	if (cmd->gpi >= MAX_GPIO)
+		return -EINVAL;
+	if (0 == cmd->event)
+		event = DETECT_FALL;
+	else if (1 == cmd->event)
+		event = DETECT_RISE;
+	else {
+		event = DETECT_RISE | DETECT_FALL;
+		cmd->event = 2;
+	}
+	tsi = first;
+	last = first + total;
+	if (last > MAX_TIMESTAMP_UNIT)
+		last -= MAX_TIMESTAMP_UNIT;
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < total; i++) {
+		tsi_bit = 1 << tsi;
+		ptp->tsi_used |= tsi_bit;
+		if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+			ptp->tsi_intr |= tsi_bit;
+			ptp->tsi_dev[tsi] = info;
+		}
+		events = &ptp->events[tsi];
+		events->num = 0;
+		events->event = cmd->event;
+		events->edge = 0;
+		events->expired = 0;
+		if (total > 1) {
+			events->first = first;
+			events->last = last;
+		}
+		++tsi;
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+	}
+	tsi = first;
+	ptp->events[tsi].timeout = cmd->timeout * HZ / 1000;
+
+	/* Zero timeout means repeatable. */
+	if (!ptp->events[tsi].timeout && cmd->timeout)
+		ptp->events[tsi].timeout = 1;
+	if (total > 1)
+		ptp->reg->rx_cascade_event(ptp, tsi, total, cmd->gpi, event,
+			intr);
+	else
+		ptp->reg->rx_event(ptp, tsi, cmd->gpi, event, intr);
+	ptp->ops->release(ptp);
+
+proc_ptp_rx_cascade_event_done:
+	*data = tsi;
+	return 0;
+}  /* proc_dev_rx_event */
+
+static int find_avail_tx_unit(struct ptp_info *ptp, int total, int *unit)
+{
+	int avail;
+	int first;
+	int i;
+	int tso;
+
+	first = 0;
+	do {
+		for (tso = first; tso < MAX_TRIG_UNIT; tso++)
+			if (!(ptp->tso_used & (1 << tso)))
+				break;
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		first = tso++;
+		avail = 1;
+		for (i = 1; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (!(ptp->tso_used & (1 << tso))) {
+				++avail;
+				++tso;
+			} else {
+				++first;
+				break;
+			}
+		}
+	} while (avail < total);
+	*unit = first;
+	return 0;
+}  /* find_avail_tx_unit */
+
+static int proc_dev_tx_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int gpo;
+	int intr;
+	int tso;
+	int tso_bit;
+	struct ptp_utime t;
+	u16 active;
+	u32 status;
+	int err = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	if (cmd->event > TRIG_REG_OUTPUT)
+		return -EINVAL;
+	tso = cmd->tso;
+	tso_bit = 1 << tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		ptp->ops->acquire(ptp);
+
+		/* Reset the tso. */
+		ptp->cascade_tx |= tso_bit;
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+		goto proc_dev_tx_event_done;
+	}
+	if (ptp->cascade && (tso < ptp->cascade_gpo[gpo].first ||
+			tso >= ptp->cascade_gpo[gpo].first +
+			ptp->cascade_gpo[gpo].total))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available unit for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, 1, &tso);
+
+		if (rc)
+			return rc;
+	} else if (!ptp->cascade && (ptp->tso_used & tso_bit)) {
+
+		/* See whether previous operation is completed. */
+		ptp->ops->acquire(ptp);
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		active = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+		status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+		ptp->ops->release(ptp);
+		if (active & TRIG_ACTIVE) {
+			u16 error = (status >> TRIG_ERROR_S) &
+				PTP_TRIG_UNIT_M;
+
+			if (!(error & tso_bit))
+				return DEV_IOC_UNIT_USED;
+			dbg_msg("trig err: %d\n", tso);
+		}
+		if (!(active & TRIG_ACTIVE)) {
+			u16 done = status & PTP_TRIG_UNIT_M;
+
+			if (!(done & tso_bit)) {
+				/* Reset the unit. */
+				ptp->cascade_tx |= tso_bit;
+				dbg_msg(" !? trig done: %d\n", tso);
+			}
+		}
+		ptp->ops->acquire(ptp);
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+	}
+	ptp->ops->acquire(ptp);
+	if (!ptp->cascade && (cmd->flags & PTP_CMD_REL_TIME) &&
+			cmd->sec < 100) {
+		ptp->reg->get_time(ptp, &t);
+		if (0 == cmd->sec) {
+			cmd->nsec += t.nsec;
+			cmd->nsec += 500;
+			cmd->nsec /= 1000;
+			cmd->nsec *= 1000;
+			if (cmd->nsec >= NANOSEC_IN_SEC) {
+				cmd->nsec -= NANOSEC_IN_SEC;
+				cmd->sec++;
+			}
+		}
+		cmd->sec += t.sec;
+	}
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+		ptp->tso_intr |= tso_bit;
+		ptp->tso_dev[tso] = info;
+	}
+	ptp->tso_used |= tso_bit;
+	ptp->reg->tx_event(ptp, tso, cmd->gpo, cmd->event, cmd->pulse,
+		cmd->cycle, cmd->cnt, cmd->sec, cmd->nsec, cmd->iterate, intr,
+		!(cmd->flags & PTP_CMD_ON_TIME),
+		(cmd->flags & PTP_CMD_CLK_OPT));
+	if (cmd->flags & PTP_CMD_ON_TIME) {
+		status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+		status = (status >> TRIG_ERROR_S) & PTP_TRIG_UNIT_M;
+		if (status & tso_bit)
+			err = DEV_IOC_UNIT_ERROR;
+	}
+	ptp->ops->release(ptp);
+
+proc_dev_tx_event_done:
+	*data = tso;
+	return err;
+}  /* proc_dev_tx_event */
+
+static int proc_ptp_tx_cascade_init(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int i;
+	int tso;
+	int total;
+	u32 status;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	tso = cmd->tso;
+	gpo = cmd->gpo;
+	total = cmd->total;
+	if (!total)
+		return -EINVAL;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	first = tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		if (first != ptp->cascade_gpo[gpo].first ||
+				total != ptp->cascade_gpo[gpo].total) {
+			first = ptp->cascade_gpo[gpo].first;
+			total = ptp->cascade_gpo[gpo].total;
+		}
+
+		/* Reset the last unit in case it is used to raise the level. */
+		first = first + total - 1;
+		if (ptp->outputs[first].level) {
+			ptp->cascade_tx |= (1 << first);
+			ptp->tso_used |= (1 << first);
+		}
+		ptp->ops->acquire(ptp);
+		for (i = 0; i < total; i++, tso++) {
+			if (ptp->tso_used & (1 << tso))
+				ptp_tso_off(ptp, tso, (1 << tso));
+		}
+		tso = total;
+		ptp->cascade = false;
+		ptp->ops->release(ptp);
+		goto proc_ptp_tx_cascade_init_done;
+	}
+
+	if (ptp->cascade)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available units for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, total, &first);
+
+		if (rc)
+			return rc;
+	} else {
+		for (i = 0; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (ptp->tso_used & (1 << tso))
+				return DEV_IOC_UNIT_USED;
+			++tso;
+		}
+	}
+
+	if ((cmd->flags & PTP_CMD_CASCADE_RESET_OPER))
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* Last operation was not in cascade mode. */
+	if (!ptp->cascade_gpo[gpo].total)
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* previous last unit. */
+	i = ptp->cascade_gpo[gpo].first + ptp->cascade_gpo[gpo].total - 1;
+
+	/* current last unit. */
+	tso = first + total - 1;
+
+	/* Last operation not ended high. */
+	if (tso == i || !ptp->outputs[i].level)
+		goto proc_ptp_tx_cascade_init_set;
+
+	ptp->ops->acquire(ptp);
+	ptp_write_index(ptp, PTP_GPIO_INDEX_S, gpo);
+	status = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+
+	/* Current level is high. */
+	if (status & GPIO_IN) {
+
+		/* Set unit to hold the level high. */
+		ptp->reg->tx_event(ptp, tso, gpo, TRIG_POS_EDGE, 0, 0, 1, 0, 1,
+			0, PTP_CMD_INTR_OPER, 1, 0);
+
+		/* Release the signal from the previous last unit. */
+		ptp_gpo_reset(ptp, ptp->outputs[i].gpo, i, NULL);
+	}
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_init_set:
+	ptp->cascade = true;
+	ptp->cascade_gpo[gpo].first = first;
+	ptp->cascade_gpo[gpo].total = total;
+	tso = first;
+
+proc_ptp_tx_cascade_init_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade_init */
+
+static int proc_ptp_tx_cascade(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int tso;
+	int total;
+	struct ptp_utime t;
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	tso = cmd->tso;
+	total = cmd->total;
+	first = tso;
+	if (!ptp->cascade || tso != ptp->cascade_gpo[gpo].first ||
+			total != ptp->cascade_gpo[gpo].total)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		proc_ptp_tx_cascade_init(ptp, data);
+		goto proc_ptp_tx_cascade_done;
+	}
+	ptp->ops->acquire(ptp);
+	if ((cmd->flags & PTP_CMD_REL_TIME) && cmd->sec < 100) {
+		ptp->reg->get_time(ptp, &t);
+		cmd->sec += t.sec;
+	}
+	total = ptp->reg->tx_cascade(ptp, tso, total, cmd->cnt, cmd->sec,
+		cmd->nsec, cmd->flags & PTP_CMD_INTR_OPER);
+	if (!total)
+		ptp->cascade = false;
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade */
+
+static void proc_tsm_get_gps(struct ptp_info *ptp, u8 *data)
+{
+	struct tsm_get_gps *get = (struct tsm_get_gps *) data;
+
+	if (!ptp->gps_dev)
+		return;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->seqid = htons(ptp->gps_seqid);
+	get->sec = htonl(ptp->gps_time.sec);
+	get->nsec = htonl(ptp->gps_time.nsec);
+	ptp_setup_udp_msg(ptp->gps_dev, data, sizeof(struct tsm_get_gps),
+		NULL, NULL);
+	ptp->gps_dev = NULL;
+}  /* proc_tsm_get_gps */
+
+static int proc_dev_get_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) * MAX_TIMESTAMP_EVENT_UNIT +
+		sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp->events[in->unit].num)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	out->event = ptp->events[in->unit].event;
+	out->num = ptp->events[in->unit].num;
+	out->edge = ptp->events[in->unit].edge;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, ptp->events[in->unit].t, len);
+	len += sizeof(struct ptp_tsi_info);
+	ptp_setup_udp_msg(info, buf, len, NULL, NULL);
+	return 0;
+}  /* proc_dev_get_event */
+
+static int proc_ptp_get_event(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	int ret = -1;
+
+	if (ptp->tsi_dev[in->unit])
+		ret = proc_dev_get_event(ptp->tsi_dev[in->unit], data);
+	return ret;
+}  /* proc_ptp_get_event */
+
+static int proc_ptp_get_trig(struct ptp_info *ptp, u8 *data, u16 done,
+	u16 error)
+{
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) + sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+	struct ptp_output *cur;
+	int tso = in->unit;
+	int tso_bit = (1 << tso);
+
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	cur = &ptp->outputs[tso];
+	if (error & tso_bit)
+		out->event = 1;
+	else if (!(done & tso_bit))
+		out->event = 2;
+	else
+		out->event = 0;
+	out->num = 1;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, &cur->trig, len);
+	len += sizeof(struct ptp_tsi_info);
+	if (ptp->tso_dev[tso]) {
+		ptp_setup_udp_msg(ptp->tso_dev[tso], buf, len, NULL, NULL);
+		return 0;
+	}
+	return -1;
+}  /* proc_ptp_get_trig */
+
+static int proc_dev_poll_event(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->ptp;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp_poll_event(ptp, in->unit))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	return proc_dev_get_event(info, data);
+}  /* proc_dev_poll_event */
+
+static int proc_dev_get_event_info(struct ptp_dev_info *info, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	in->unit = MAX_TIMESTAMP_UNIT;
+	in->event = MAX_TIMESTAMP_EVENT_UNIT;
+	in->num = 0;
+	return 0;
+}  /* proc_dev_get_event_info */
+
+static int proc_ptp_get_output(struct ptp_info *ptp, u8 *data)
+{
+	int *output = (int *) data;
+	struct ptp_tso_options *in = (struct ptp_tso_options *) data;
+
+	if (in->gpo >= MAX_GPIO)
+		return -EINVAL;
+	*output = ptp->cascade_gpo[in->gpo].tso;
+	return 0;
+}  /* proc_ptp_get_output */
+
+static void proc_ptp_get_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &t);
+	ptp->ops->release(ptp);
+	cmd->sec = t.sec;
+	cmd->nsec = t.nsec;
+}  /* proc_ptp_get_clk */
+
+static int proc_ptp_set_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+	struct timespec ts;
+	struct ptp_utime sys_time;
+
+	t.sec = cmd->sec;
+	t.nsec = cmd->nsec;
+	ptp->ops->acquire(ptp);
+	ts = ktime_to_timespec(ktime_get_real());
+	sys_time.sec = ts.tv_sec;
+	sys_time.nsec = ts.tv_nsec;
+	ptp->reg->set_time(ptp, &t);
+	ptp->cur_time = t;
+	calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->ops->release(ptp);
+	dbg_msg(" set clk: %x:%09u\n", cmd->sec, cmd->nsec);
+	return 0;
+}  /* proc_ptp_set_clk */
+
+static int proc_ptp_adj_clk(struct ptp_info *ptp, u8 *data, int adjust)
+{
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	adjust--;
+	if (cmd->sec > 1) {
+		ptp->adjust_sec = cmd->sec;
+		ptp->adjust_offset = cmd->nsec;
+		if (!adjust) {
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		}
+		dbg_msg("adj clk: %d %u:%09u\n", adjust, cmd->sec, cmd->nsec);
+		cmd->sec = cmd->nsec = 0;
+		ptp->adj_clk.func(&ptp->adj_clk);
+	}
+	ptp->ops->acquire(ptp);
+	if (cmd->nsec || cmd->sec) {
+		ptp->sec_changed = cmd->sec;
+		if (!(ptp->features & PTP_ADJ_SEC)) {
+			cmd->nsec += cmd->sec * NANOSEC_IN_SEC;
+			cmd->sec = 0;
+		}
+		ptp->reg->adjust_time(ptp, adjust, cmd->sec, cmd->nsec,
+			ptp->features & PTP_ADJ_HACK);
+		ptp->offset_changed = cmd->nsec;
+		if (!adjust)
+			ptp->offset_changed = -cmd->nsec;
+		if (ptp->sec_changed)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else {
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+		if (ptp->sec_changed) {
+			if (adjust)
+				ptp->cur_time.sec += cmd->sec;
+			else
+				ptp->cur_time.sec -= cmd->sec;
+			ptp->sec_changed = 0;
+		}
+		if (adjust)
+			add_nsec(&ptp->cur_time, cmd->nsec);
+		else
+			sub_nsec(&ptp->cur_time, cmd->nsec);
+		if (cmd->sec)
+		dbg_msg(" adj clk: %d %u:%09u\n", adjust, cmd->sec, cmd->nsec);
+	}
+	if (cmd->interval) {
+		ptp->drift = cmd->drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift, cmd->interval);
+		set_ptp_adjust(ptp, ptp->adjust);
+		if (!ptp->ptp_synt) {
+			syntonize_clk(ptp);
+			ptp->ptp_synt = true;
+		}
+if (!ptp->first_drift && ptp->drift_set)
+dbg_msg("first drift: %d\n", ptp->drift_set);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+#if 0
+		dbg_msg(" adj drift: %d\n", cmd->drift);
+#endif
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_adj_clk */
+
+static int proc_ptp_get_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = chk_last_port(sw, port);
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = get_ptp_ingress(ptp, port);
+	delay->tx_latency = get_ptp_egress(ptp, port);
+	delay->asym_delay = get_ptp_asym(ptp, port);
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_delay */
+
+static int proc_ptp_set_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int index;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = chk_last_port(sw, port);
+	ptp->ops->acquire(ptp);
+	set_ptp_ingress(ptp, port, delay->rx_latency);
+	set_ptp_egress(ptp, port, delay->tx_latency);
+	set_ptp_asym(ptp, port, delay->asym_delay);
+	index = get_speed_index(ptp, port);
+	ptp->rx_latency[port][index] = delay->rx_latency;
+	ptp->tx_latency[port][index] = delay->tx_latency;
+	ptp->asym_delay[port][index] = delay->asym_delay;
+	ptp->ops->release(ptp);
+	dbg_msg("set delay: %d = %d %d %d\n", port,
+		ptp->rx_latency[port][index],
+		ptp->tx_latency[port][index],
+		ptp->asym_delay[port][index]);
+	return 0;
+}  /* proc_ptp_set_delay */
+
+static int proc_ptp_get_peer_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	u32 link;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = chk_last_port(sw, port);
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = 0;
+	delay->tx_latency = 0;
+	delay->asym_delay = 0;
+	link = get_ptp_link(ptp, port);
+	delay->reserved = link;
+	delay->rx_latency = link >> 16;
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_peer_delay */
+
+static int proc_ptp_set_peer_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	u32 link;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = chk_last_port(sw, port);
+	ptp->ops->acquire(ptp);
+	link = delay->rx_latency;
+	link <<= 16;
+	link |= delay->reserved;
+	if (link != ptp->peer_delay[port]) {
+		set_ptp_link(ptp, port, link);
+		ptp->peer_delay[port] = link;
+		if (abs(link - ptp->peer_delay[port]) > 5)
+			dbg_msg("set delay: %d = %d\n", port,
+				ptp->peer_delay[port]);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_peer_delay */
+
+static int proc_ptp_get_port_cfg(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_port_cfg *cfg;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = chk_last_port(sw, port);
+	cfg = &sw->info->port_cfg[port];
+	delay->reserved = 0;
+	if (cfg->ptp_enabled)
+		delay->reserved |= PTP_PORT_ENABLED;
+	if (cfg->asCapable)
+		delay->reserved |= PTP_PORT_ASCAPABLE;
+	return 0;
+}  /* proc_ptp_get_port_cfg */
+
+static int proc_ptp_set_port_cfg(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_port_cfg *cfg;
+
+	if (port >= ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = chk_last_port(sw, port);
+	cfg = &sw->info->port_cfg[port];
+	cfg->ptp_enabled = !!(delay->reserved & PTP_PORT_ENABLED);
+	if (cfg->ptp_enabled)
+		cfg->asCapable = !!(delay->reserved & PTP_PORT_ASCAPABLE);
+	else
+		cfg->asCapable = false;
+
+#ifdef CONFIG_KSZ_MSRP
+	do {
+		struct mrp_info *mrp = &sw->mrp;
+
+dbg_msg("as: %d=%d\n", port, cfg->asCapable);
+		mrp->ops->chk_talker(mrp,
+				     sw_get_net_port(sw, 0, mrp->ports, port));
+	} while (0);
+#endif
+	return 0;
+}  /* proc_ptp_set_port_cfg */
+
+static void ptp_tx_done(struct ptp_info *ptp, int tso)
+{
+	int first;
+	int last;
+	int prev;
+	u32 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+	data = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+	if (data & TRIG_CASCADE_ENABLE) {
+		last = tso;
+		do {
+			--tso;
+			ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+			data = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+			prev = (data >> TRIG_CASCADE_UPS_S) &
+				TRIG_CASCADE_UPS_M;
+			if (prev == last)
+				break;
+		} while (tso > 0);
+		first = tso;
+		for (tso = last; tso > first; tso--)
+			ptp_tso_off(ptp, tso, (1 << tso));
+	}
+	ptp_tso_off(ptp, tso, (1 << tso));
+}  /* ptp_tx_done */
+
+static struct ptp_tx_ts *proc_get_ts(struct ptp_info *ptp, u8 port, u8 msg,
+	u16 seqid, u8 *mac, struct ptp_dev_info *info, int len)
+{
+	struct ptp_tx_ts *tx;
+	int from_stack = false;
+	u8 *data = NULL;
+
+	if (info)
+		data = info->write_buf;
+	if (SYNC_MSG == msg)
+		tx = &ptp->tx_sync[port];
+	else if (PDELAY_RESP_MSG == msg)
+		tx = &ptp->tx_resp[port];
+	else
+		tx = &ptp->tx_dreq[port];
+	if (seqid || mac[0] || mac[1])
+		from_stack = true;
+	if (data && tx->req_time && ptp->linked[port] && port < 5)
+		dbg_msg("  last %x=%04x: p=%d, j=%lu\n", msg, seqid, port,
+			jiffies - tx->req_time);
+	tx->missed = false;
+	tx->req_time = jiffies;
+	if (tx->ts.timestamp && from_stack) {
+		unsigned long diff = tx->req_time - tx->resp_time;
+
+		/* The timestamp is not valid. */
+		if (diff >= 4 * ptp->delay_ticks) {
+			dbg_msg("  invalid: %x=%04x: %d, %lu\n",
+				msg, seqid, port, diff);
+			tx->ts.timestamp = 0;
+		} else if (diff > 2 * ptp->delay_ticks)
+			dbg_msg("  ready? %x=%04x: %d, %lu\n",
+				msg, seqid, port, diff);
+	}
+	if (!tx->ts.timestamp && ptp->linked[port] && data) {
+		int rc = wait_event_interruptible_timeout(ptp->wait_ts[port],
+			0 != tx->ts.timestamp, ptp->delay_ticks);
+
+		if (rc < 0)
+			return NULL;
+	}
+	if (!tx->ts.timestamp) {
+		if (from_stack && data) {
+			tx->missed = true;
+			memcpy(tx->data.buf, data, len);
+			tx->data.len = len;
+			tx->dev = info;
+		}
+		if (ptp->linked[port] && port < 5)
+			dbg_msg("  missed %x=%04x: p=%d, j=%lu\n",
+				msg, seqid, port, jiffies - tx->req_time);
+		tx = NULL;
+	}
+	return tx;
+}  /* proc_get_ts */
+
+static int proc_ptp_get_timestamp(struct ptp_info *ptp, u8 *data,
+	struct ptp_dev_info *info)
+{
+	struct ptp_ts_options *opt = (struct ptp_ts_options *) data;
+
+	if (opt->timestamp) {
+		struct ptp_ts ts;
+
+		ts.timestamp = opt->timestamp;
+		update_ts(&ts, ptp->cur_time.sec);
+		opt->sec = ts.t.sec;
+		opt->nsec = ts.t.nsec;
+	} else {
+		struct ptp_tx_ts *tx;
+		struct tsm_db *db;
+		u8 port;
+
+		if (opt->port >= ptp->ports)
+			return DEV_IOC_INVALID_CMD;
+
+		/* Save timestamp information for later reporting. */
+		if (info) {
+			db = (struct tsm_db *) info->write_buf;
+			db->cmd = opt->msg;
+			db->cmd |= TSM_CMD_DB_GET;
+			db->index = opt->port << 1;
+			db->seqid = htons(opt->seqid);
+			db->mac[0] = opt->mac[0];
+			db->mac[1] = opt->mac[1];
+		}
+		port = ptp_get_dev_port(ptp, 0, opt->port);
+		tx = proc_get_ts(ptp, port, opt->msg,
+			opt->seqid, opt->mac, info, sizeof(struct tsm_db));
+		if (!tx)
+			return -EAGAIN;
+		if (ptp->cap & PTP_KNOW_ABOUT_LATENCY) {
+			opt->sec = tx->ts.r.sec;
+			opt->nsec = tx->ts.r.nsec;
+		} else {
+			opt->sec = tx->ts.t.sec;
+			opt->nsec = tx->ts.t.nsec;
+		}
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+	}
+	return 0;
+}  /* proc_ptp_get_timestamp */
+
+static struct ptp_tx_ts *proc_get_ts_port(struct ptp_info *ptp, u8 msg,
+	uint *port, int *more)
+{
+	uint p;
+	struct ptp_tx_ts *tx;
+	struct ptp_tx_ts *xts = NULL;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	*more = false;
+	for (p = 0; p < ptp->ports; p++) {
+		p = chk_last_port(sw, p);
+		if (SYNC_MSG == msg)
+			tx = &ptp->tx_sync[p];
+		else if (PDELAY_RESP_MSG == msg)
+			tx = &ptp->tx_resp[p];
+		else
+			tx = &ptp->tx_dreq[p];
+
+		/* Same type of event message has been sent. */
+		if (tx->hdr.messageType != msg)
+			continue;
+
+		*more = true;
+
+		/* Have all information ready. */
+		if (xts)
+			break;
+
+		if (!tx->ts.timestamp && ptp->linked[p]) {
+			int rc = wait_event_interruptible_timeout(
+				ptp->wait_ts[p], 0 != tx->ts.timestamp,
+				ptp->delay_ticks);
+
+			if (rc < 0)
+				return NULL;
+		}
+
+		/* Transmit timestamp ready. */
+		if (tx->ts.timestamp) {
+			xts = tx;
+			*port = p;
+			*more = false;
+			tx->ts.timestamp = 0;
+			tx->req_time = 0;
+			tx->hdr.messageType = 7;
+		}
+	}
+	return xts;
+}  /* proc_get_ts_port */
+
+static int proc_ptp_get_msg_info(struct ptp_info *ptp, int start, u8 *data,
+	struct ptp_dev_info *info, int *tx)
+{
+	struct ptp_msg_options *opt = (struct ptp_msg_options *) data;
+
+	if (!*tx) {
+		struct ptp_msg_hdr hdr;
+		struct ptp_msg_options tx_msg;
+		int found;
+
+		memcpy(&hdr.sourcePortIdentity, &opt->id,
+			sizeof(struct ptp_port_identity));
+		hdr.messageType = opt->msg;
+		hdr.sequenceId = opt->seqid;
+		hdr.domainNumber = opt->domain;
+		found = find_msg_info(&ptp->rx_msg_info[hdr.messageType],
+			&ptp->rx_msg_lock, &hdr, &hdr.sourcePortIdentity,
+			true, &tx_msg);
+		if (found) {
+			opt->port = tx_msg.port + 1;
+			opt->ts = tx_msg.ts;
+		} else
+			return DEV_IOC_UNIT_UNAVAILABLE;
+	}
+	else {
+		struct ptp_tx_ts *xts;
+		uint port = 0;
+
+		/* Not event message. */
+		if (opt->msg & 0x8)
+			return DEV_IOC_INVALID_CMD;
+
+		xts = proc_get_ts_port(ptp, opt->msg, &port, tx);
+		if (xts) {
+			if (ptp->cap & PTP_KNOW_ABOUT_LATENCY) {
+				opt->ts.t.sec = xts->ts.r.sec;
+				opt->ts.t.nsec = xts->ts.r.nsec;
+			} else {
+				opt->ts.t.sec = xts->ts.t.sec;
+				opt->ts.t.nsec = xts->ts.t.nsec;
+			}
+			opt->port = port + 1;
+		} else
+			return -EAGAIN;
+	}
+	return 0;
+}  /* proc_ptp_get_msg_info */
+
+static int proc_ptp_set_msg_info(struct ptp_info *ptp, int start, u8 *data,
+	struct ptp_dev_info *info)
+{
+	struct ptp_msg_options *opt = (struct ptp_msg_options *) data;
+
+	/* Used for testing. */
+	if (7 == opt->msg) {
+		struct ptp_msg_info *tx_msg;
+
+		tx_msg = &ptp->tx_msg_info[opt->msg];
+		tx_msg->data.port = opt->port;
+		tx_msg->data.ts.timestamp = opt->ts.timestamp;
+		if (opt->port) {
+			ptp->tx_msg_cnt = -1;
+			ptp->overrides |= PTP_USE_DEFAULT_PORT;
+		} else {
+			ptp->tx_msg_cnt = 0;
+			ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+		}
+	} else {
+		struct ptp_msg_hdr hdr;
+		u32 port = opt->port;
+
+		memcpy(&hdr.sourcePortIdentity, &opt->id,
+			sizeof(struct ptp_port_identity));
+		hdr.messageType = opt->msg;
+		hdr.sequenceId = opt->seqid;
+		hdr.domainNumber = opt->domain;
+		opt->ts.timestamp = (opt->ts.t.sec << 30) |
+			opt->ts.t.nsec;
+		set_msg_info(ptp, &hdr, port, opt->ts.timestamp);
+	}
+	return 0;
+}  /* proc_ptp_set_msg_info */
+
+static int parse_tsm_msg(struct ptp_dev_info *info, int len)
+{
+	struct ptp_info *ptp = info->ptp;
+	u8 *data = info->write_buf;
+	u8 cmd = data[0] & 0xf0;
+	u8 msg = data[0] & 0x03;
+	int result = 0;
+
+	switch (cmd) {
+	case TSM_CMD_DB_GET_TIME:
+	{
+		struct tsm_get_time *get = (struct tsm_get_time *) data;
+		struct ptp_ts ts;
+
+		ts.timestamp = ntohl(get->nsec);
+		if (ts.timestamp) {
+			update_ts(&ts, ptp->cur_time.sec);
+		} else {
+			ptp->ops->acquire(ptp);
+			ptp->reg->get_time(ptp, &ts.t);
+			ptp->ops->release(ptp);
+		}
+		ptp_setup_udp_msg(info, data, len, ptp_tsm_get_time_resp,
+			&ts.t);
+		break;
+	}
+	case TSM_CMD_DB_GET:
+	{
+		struct tsm_db *db = (struct tsm_db *) data;
+
+		if (db->index <= (1 << 7)) {
+			struct ptp_tx_ts *tx;
+			uint port = db->index >> 1;
+
+			if (port > ptp->ports)
+				break;
+			port = ptp_get_dev_port(ptp, 0, port);
+			tx = proc_get_ts(ptp, port, msg, ntohs(db->seqid),
+				db->mac, info, len);
+			if (tx) {
+				ptp_setup_udp_msg(info, data, len,
+					ptp_tsm_resp, &tx->ts);
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		}
+		break;
+	}
+	case TSM_CMD_GET_GPS_TS:
+	{
+		/* First time to get GPS timestamp. */
+		if (MAX_TIMESTAMP_UNIT == ptp->gps_tsi) {
+			ptp->gps_tsi = DEFAULT_GPS_TSI;
+			if (ptp->tsi_used & (1 << ptp->gps_tsi))
+				ptp->reg->rx_off(ptp, ptp->gps_tsi);
+			prepare_gps(ptp);
+			ptp->gps_seqid = 0;
+		}
+		ptp->gps_req_time = jiffies;
+		ptp->gps_dev = info;
+		if (ptp->gps_resp_time) {
+			unsigned long diff = ptp->gps_req_time -
+				ptp->gps_resp_time;
+
+			/* The timestamp is not valid. */
+			if (diff >= 2 * ptp->delay_ticks) {
+				dbg_msg("  invalid gps: %lu\n", diff);
+				ptp->gps_time.sec = 0;
+			}
+		}
+		if (ptp->gps_time.sec) {
+			proc_tsm_get_gps(ptp, data);
+			ptp->gps_time.sec = 0;
+			ptp->gps_req_time = 0;
+		} else
+			dbg_msg("  missed gps\n");
+		break;
+	}
+	case TSM_CMD_CNF_SET:
+	{
+		struct tsm_cfg *cfg = (struct tsm_cfg *) data;
+		u32 ingress = htonl(cfg->ingress_delay);
+
+		ptp->ops->acquire(ptp);
+		if (0xFF == cfg->port) {
+			u16 mode;
+
+			if (cfg->enable & 0x04)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+			mode = ptp->mode;
+			if (ptp->overrides & PTP_VERIFY_TIMESTAMP)
+				mode |= PTP_1STEP;
+			set_ptp_mode(ptp, mode);
+			ptp->def_mode = ptp->mode;
+		} else {
+			u8 port = cfg->port - 1;
+
+			port = ptp_get_dev_port(ptp, 0, port);
+			if ((cfg->enable & 0x10) && port < ptp->ports &&
+					ptp->peer_delay[port] != ingress) {
+				ptp->peer_delay[port] = ingress;
+				set_ptp_link(ptp, port, ingress);
+			}
+		}
+		ptp->ops->release(ptp);
+		break;
+	}
+	case TSM_CMD_CLOCK_SET:
+	{
+		struct tsm_clock_set *clk = (struct tsm_clock_set *) data;
+		struct ptp_ts ts;
+
+		ts.t.sec = ntohl(clk->sec);
+		ts.t.nsec = ntohl(clk->nsec);
+		ts.timestamp = ntohl(clk->timestamp);
+		ptp->ops->acquire(ptp);
+		set_cur_time(ptp, &ts);
+		ptp->ops->release(ptp);
+		ptp->state = 2;
+		break;
+	}
+	case TSM_CMD_CLOCK_CORRECT:
+	{
+		struct tsm_clock_correct *clk = (struct tsm_clock_correct *)
+			data;
+		u32 drift;
+		u32 nsec;
+		int ptp_offset;
+
+		drift = ntohl(clk->drift);
+		nsec = ntohl(clk->nsec);
+		ptp_offset = ntohl(clk->offset);
+		if (2 == (clk->add >> 4))
+			break;
+
+		ptp->ops->acquire(ptp);
+		if (nsec) {
+			ptp->reg->adjust_time(ptp, !ptp_offset, 0, nsec,
+				ptp->features & PTP_ADJ_HACK);
+			ptp->offset_changed = nsec;
+			if (ptp_offset)
+				ptp->offset_changed = -nsec;
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+		if (clk->add & 1)
+			ptp->drift = drift;
+		else
+			ptp->drift = -drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift,
+			NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+if (!ptp->first_drift)
+dbg_msg("  first drift: %d\n", ptp->drift_set);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+		ptp->ops->release(ptp);
+		break;
+	}
+	default:
+		dbg_msg("tsm cmd: %02X, %d\n", cmd, len);
+	}
+	return result;
+}  /* parse_tsm_msg */
+
+static struct ptp_info *ptp_priv;
+
+static struct ptp_dev_info *alloc_dev_info(unsigned int minor)
+{
+	struct ptp_dev_info *info;
+
+	info = kzalloc(sizeof(struct ptp_dev_info), GFP_KERNEL);
+	if (info) {
+		info->ptp = ptp_priv;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_udp);
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+		info->read_max = 60000;
+		info->read_buf = kzalloc(info->read_max, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = ptp_priv->dev[minor];
+		ptp_priv->dev[minor] = info;
+	}
+	return info;
+}  /* alloc_dev_info */
+
+static void free_dev_info(struct ptp_dev_info *info)
+{
+	if (info) {
+		struct ptp_info *ptp = info->ptp;
+		unsigned int minor = info->minor;
+		struct ptp_dev_info *prev = ptp->dev[minor];
+
+		if (prev == info) {
+			ptp->dev[minor] = info->next;
+		} else {
+			while (prev && prev->next != info)
+				prev = prev->next;
+			if (prev)
+				prev->next = info->next;
+		}
+		kfree(info->read_buf);
+		kfree(info->write_buf);
+		kfree(info);
+	}
+}  /* free_dev_info */
+
+static int ptp_dev_open(struct inode *inode, struct file *filp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	unsigned int minor = MINOR(inode->i_rdev);
+
+	if (minor > 1)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_dev_info(minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* ptp_dev_open */
+
+static int ptp_dev_release(struct inode *inode, struct file *filp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+
+	free_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* ptp_dev_release */
+
+static int ptp_get_port_info(struct ptp_info *ptp, u8 *data, int *output)
+{
+	struct net_device *netdev;
+	struct ksz_port *netport;
+	u32 mask;
+	int len;
+	int c;
+	int n;
+	int p;
+	int phys_port;
+	char devname[40];
+	char *dot;
+	char *dot2;
+	char *name = (char *)data;
+	bool matched = false;
+	unsigned int vlan = 0;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int dev_count = sw->dev_count + sw->dev_offset;
+
+	len = strnlen(name, sizeof(devname));
+	strncpy(devname, name, len);
+	devname[len] = '\0';
+	dot = strchr(devname, '.');
+	if (dot) {
+		++dot;
+		n = sscanf(dot, "%u", &vlan);
+		dot2 = strchr(dot, '.');
+		if (dot2) {
+			*dot2 = '\0';
+			len = strnlen(devname, sizeof(devname));
+		}
+	}
+
+	/* Check real network device. */
+	for (n = 0; n < dev_count; n++) {
+		netdev = sw->netdev[n];
+		if (!strncmp(netdev->name, devname, len) &&
+		    (sw->net_ops->get_priv_port)) {
+			netport = sw->net_ops->get_priv_port(netdev);
+			phys_port = netport->first_port;
+			mask = 0;
+			for (c = 0, p = phys_port;
+			     c < netport->port_cnt; c++, p++) {
+				if (p == sw->HOST_PORT)
+					continue;
+				mask |= (1 << p);
+			}
+			matched = true;
+			break;
+		}
+	}
+
+	/* Check virtual VLAN device. */
+	if (!matched && vlan > VLAN_PORT_START &&
+	    vlan <= sw->mib_port_cnt + VLAN_PORT_START) {
+		--dot;
+		*dot = '\0';
+		--vlan;
+		for (n = 0; n < dev_count; n++) {
+			netdev = sw->netdev[n];
+			if (!strncmp(netdev->name, devname, len) &&
+			    (sw->net_ops->get_priv_port)) {
+				netport = sw->net_ops->get_priv_port(netdev);
+				phys_port = netport->first_port;
+				phys_port += vlan - VLAN_PORT_START;
+				mask = (1 << phys_port);
+				matched = true;
+				break;
+			}
+		}
+	}
+	if (matched) {
+		data[0] = 'M';
+		data[1] = 'i';
+		data[2] = 'c';
+		data[3] = 'r';
+		data[4] = phys_port + 1;
+		data[5] = 0;
+		*output = mask;
+	} else {
+		result = DEV_IOC_INVALID_CMD;
+	}
+	return result;
+}  /* ptp_get_port_info */
+
+static int execute_wait(struct ptp_work *work)
+{
+	int rc = 0;
+
+	execute(work->ptp, &work->work);
+	wait_for_completion(&work->done);
+	return rc;
+}  /* execute_wait */
+
+static void proc_ptp_work(struct work_struct *work)
+{
+	struct ptp_work *parent =
+		container_of(work, struct ptp_work, work);
+	struct ptp_info *ptp = parent->ptp;
+	struct ptp_dev_info *info = parent->dev_info;
+	u8 *data = parent->param.data;
+	uint port;
+	u32 reg;
+	u32 val;
+	size_t width;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+/* ptp_hwstamp_ctl is called first in Linuxptp, and it crashes occasionally. */
+#if 0
+			if (ptp->op_mode)
+				ptp->tx_en = ptp->rx_en = 0;
+#endif
+			if (ptp->op_state)
+				goto skip;
+			ptp->cap = ptp->op_mode = 0;
+			if ('1' == data[0] &&
+                            '5' == data[1] &&
+                            '8' == data[2] &&
+                            '8' == data[3] &&
+                            'v' == data[4] &&
+                            '2' == data[5]) {
+				int cap = parent->option;
+
+/*
+ * op_mode 0	Original mode of using reserved fields to specify port.
+ * op_mode 1	Have multiple devices and use standard Linux PTP API to receive
+ * 		timestamps.  Do not need to know about PTP messages except the
+ * 		only case of sending 1-step Pdelay_Resp timestamp.  The
+ * 		destination port is already known by other means.
+ * op_mode 2	The destination port is communicated well before sending the
+ * 		PTP message.
+ * op_mode 3	Use single device and do not know about multiple ports.
+ */
+				ptp->forward = FWD_MAIN_DEV;
+				if (cap & PTP_HAVE_MULT_DEVICES)
+					ptp->forward = FWD_VLAN_DEV | FWD_STP_DEV;
+				if ((cap & PTP_HAVE_MULT_DEVICES) &&
+				    (cap & PTP_CAN_RX_TIMESTAMP)) {
+					ptp->op_mode = 1;
+				} else if (cap & PTP_HAVE_MULT_DEVICES) {
+					ptp->op_mode = 2;
+				} else if (!(cap & (PTP_KNOW_ABOUT_MULT_PORTS |
+						    PTP_HAVE_MULT_PORTS))) {
+					ptp->op_mode = 3;
+				} else if (cap & PTP_USE_RESERVED_FIELDS) {
+					ptp->op_mode = 0;
+				} else {
+					ptp->op_mode = 2;
+				}
+				if (cap & PTP_CAN_RX_TIMESTAMP) {
+					if (cap & PTP_KNOW_ABOUT_LATENCY) {
+						ptp->rx_en &= ~(1 << 8);
+						ptp->tx_en &= ~(1 << 8);
+					} else {
+						ptp->rx_en |= (1 << 8);
+						ptp->tx_en |= (1 << 8);
+					}
+				}
+				ptp->cap = cap;
+dbg_msg("op_mode: %x %d %x:%x\n", ptp->cap, ptp->op_mode, ptp->rx_en, ptp->tx_en);
+			}
+
+skip:
+			ptp_init_state(ptp);
+			parent->output = ptp->drift;
+			break;
+		case DEV_INFO_EXIT:
+			ptp_exit_state(ptp);
+			break;
+		case DEV_INFO_RESET:
+			reg = parent->option;
+			break;
+		case DEV_INFO_PORT:
+			result = ptp_get_port_info(ptp, data, &parent->output);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			result = proc_ptp_set_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			result = proc_dev_rx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_TOUT:
+			result = proc_dev_tx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CASCADE:
+			if (parent->option)
+				result = proc_ptp_tx_cascade(ptp, data);
+			else
+				result = proc_ptp_tx_cascade_init(ptp, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CLK:
+			if (parent->option)
+				result = proc_ptp_adj_clk(ptp, data,
+					parent->option);
+			else
+				result = proc_ptp_set_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			if (ptp->op_mode > 0 || ptp->cap) {
+				u32 *param = (u32 *) data;
+
+				switch (param[0]) {
+				case 4:
+					width = 4;
+					break;
+				case 1:
+					width = 1;
+					break;
+				default:
+					width = 2;
+					break;
+				}
+				reg = param[1];
+				val = param[2];
+			} else {
+				val = reg >> 16;
+				reg &= 0xffff;
+				width = 2;
+			}
+			ptp->ops->acquire(ptp);
+			switch (width) {
+			case 4:
+				sw->reg->w32(sw, reg, val);
+				break;
+			case 1:
+				sw->reg->w8(sw, reg, val);
+				break;
+			default:
+				sw->reg->w16(sw, reg, val);
+			}
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_peer_delay(ptp, port, data);
+			break;
+		case DEV_PTP_PORT_CFG:
+			port = parent->option;
+			result = proc_ptp_set_port_cfg(ptp, port, data);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			proc_ptp_get_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			if (2 == parent->option)
+				result = proc_dev_get_event_info(info, data);
+			else if (1 == parent->option)
+				result = proc_dev_poll_event(info, data);
+
+			/* Not actually used. */
+			else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			break;
+		case DEV_PTP_CLK:
+			if ((ptp->op_mode > 0 || ptp->cap) && parent->option) {
+				parent->output = ptp->drift;
+				break;
+			}
+			proc_ptp_get_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			width = 2;
+			if (ptp->op_mode > 0 || ptp->cap) {
+				u32 *param = (u32 *) data;
+
+				switch (param[0]) {
+				case 4:
+					width = 4;
+					break;
+				case 1:
+					width = 1;
+					break;
+				default:
+					width = 2;
+					break;
+				}
+				reg = param[1];
+				val = param[2];
+			}
+			ptp->ops->acquire(ptp);
+			switch (width) {
+			case 4:
+				parent->output = sw->reg->r32(sw, reg);
+				break;
+			case 1:
+				parent->output = sw->reg->r8(sw, reg);
+				break;
+			default:
+				parent->output = sw->reg->r16(sw, reg);
+			}
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_peer_delay(ptp, port, data);
+			break;
+		case DEV_PTP_PORT_CFG:
+			port = parent->option;
+			result = proc_ptp_get_port_cfg(ptp, port, data);
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_ptp_work */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct ptp_dev_info *info,
+	int *output, int wait)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int ret = 0;
+
+	access = &ptp->hw_access;
+	mutex_lock(&access->lock);
+	work = &access->works[access->index];
+	if (work->used) {
+		pr_alert("work full\n");
+		mutex_unlock(&access->lock);
+		return -EFAULT;
+	}
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->dev_info = info;
+	work->wait = wait;
+	work->used = true;
+	access->index++;
+	access->index &= PTP_WORK_LAST;
+	init_completion(&work->done);
+	if (!wait) {
+		execute(ptp, &work->work);
+		goto hw_access_end;
+	}
+	ret = execute_wait(work);
+
+	/* Cannot continue if ERESTARTSYS. */
+	if (ret < 0)
+		goto hw_access_end;
+
+	ret = work->result;
+	if (DEV_IOC_OK == ret && (DEV_CMD_GET == work->cmd ||
+	    (work->cmd == DEV_CMD_INFO && work->subcmd == DEV_INFO_PORT)))
+		memcpy(data, work->param.data, len);
+	*output = work->output;
+
+hw_access_end:
+	mutex_unlock(&access->lock);
+	return ret;
+}  /* proc_ptp_hw_access */
+
+static void exit_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		flush_work(&work->work);
+	}
+}  /* exit_ptp_work */
+
+static void init_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	mutex_init(&access->lock);
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->ptp = ptp;
+		INIT_WORK(&work->work, proc_ptp_work);
+		init_completion(&work->done);
+	}
+}  /* init_ptp_work */
+
+#ifdef CONFIG_PTP_1588_CLOCK
+#include "micrel_ptp.c"
+#endif
+
+static void ptp_chk_rx_events(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	u16 tsi_bit;
+	int i;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		int stop;
+
+		stop = false;
+		tsi_bit = 1 << i;
+		event = &ptp->events[i];
+		if (ptp->tsi_used & tsi_bit) {
+
+			/* At least one event. */
+			if (event->num || event->expired) {
+				if (event->num >= event->max)
+					stop = true;
+				else if (event->expired &&
+					 time_after_eq(jiffies,
+					 event->expired)) {
+					ptp->reg->read_event(ptp, i);
+					stop = true;
+				}
+			}
+		}
+		if ((ptp->ts_status & ptp->ts_intr) & tsi_bit) {
+			u8 data[24];
+
+			if (ptp->tsi_intr & tsi_bit) {
+				data[0] = PTP_CMD_GET_EVENT;
+				data[1] = i;
+				proc_ptp_get_event(ptp, data);
+			}
+			if (i == ptp->gps_tsi && ptp->gps_req_time) {
+				unsigned long diff = jiffies -
+					ptp->gps_req_time;
+
+				if (diff < 2 * ptp->delay_ticks) {
+					data[0] = TSM_CMD_GET_GPS_TS;
+					proc_tsm_get_gps(ptp, data);
+					ptp->gps_time.sec = 0;
+				}
+				ptp->gps_req_time = 0;
+			}
+
+			/* Not used in cascade mode. */
+			if (!event->timeout && !event->last) {
+				event->num = 0;
+				ptp->reg->rx_restart(ptp, i);
+				stop = false;
+			}
+		}
+		if (stop) {
+			ptp->reg->rx_off(ptp, i);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			ptp->tsi_dev[i] = NULL;
+			ptp->events[i].timeout = 0;
+			if (i + 1 == event->last) {
+				int tsi;
+				int last;
+
+				tsi = event->first;
+				last = event->last;
+				do {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].first = 0;
+					ptp->events[tsi].last = 0;
+					++tsi;
+				} while (tsi != last);
+			}
+		}
+	}
+}  /* ptp_chk_rx_events */
+
+static void ptp_update_sec(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, update_sec);
+
+	if (ptp->update_sec_jiffies) {
+#ifndef NO_SEC_TIMESTAMP
+		ptp->cur_time.sec++;
+#else
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ptp->ops->release(ptp);
+#endif
+		ptp->sec_lo++;
+		if (!(ptp->sec_lo & 3)) {
+			check_expired_msg(ptp, ptp->rx_msg_info,
+				&ptp->rx_msg_lock, NULL);
+			check_expired_msg(ptp, ptp->tx_msg_info,
+				&ptp->tx_msg_lock, &ptp->tx_msg_cnt);
+		}
+		ptp->ops->acquire(ptp);
+		ptp_chk_rx_events(ptp);
+		ptp->ops->release(ptp);
+		schedule_delayed_work(&ptp->update_sec, 1000 * HZ / 1000);
+	}
+}  /* ptp_update_sec */
+
+static u32 _get_clk_cnt(void)
+{
+	return 0;
+}
+
+#define ACCESS_VAL			1000
+
+static void test_get_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	ptp->reg->get_time(ptp, cur);
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+	*now_cnt = ptp->get_clk_cnt();
+}
+
+static void test_set_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->set_time(ptp, cur);
+	*now_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+}
+
+static u32 test_avg_time(struct ptp_info *ptp,
+	void (*test_time)(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt))
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_ptp_time diff;
+	int i;
+	int clk_delay[6];
+	u32 cur_cnt;
+	u32 now_cnt;
+	u32 hw_delay[6];
+	u64 clk;
+	u32 rem;
+
+	cur.sec = 5;
+	cur.nsec = 0x12345678;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < 5; i++) {
+		test_time(ptp, &cur, &now, &cur_cnt, &now_cnt);
+		calc_udiff(&cur, &now, &diff);
+		clk_delay[i] = (diff.nsec + (ACCESS_VAL / 2)) / ACCESS_VAL *
+			ACCESS_VAL;
+		hw_delay[i] = now_cnt - cur_cnt;
+	}
+	ptp->ops->release(ptp);
+	clk_delay[5] = 20000000;
+	hw_delay[5] = 50000000;
+	for (i = 0; i < 5; i++) {
+		clk = hw_delay[i];
+		clk *= 1000000;
+		if (ptp->clk_divider)
+			clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+		dbg_msg(" %u %u=%llu\n", clk_delay[i], hw_delay[i], clk);
+		if (clk_delay[i] < clk_delay[5])
+			clk_delay[5] = clk_delay[i];
+		if (hw_delay[i] < hw_delay[5])
+			hw_delay[5] = hw_delay[i];
+	}
+	clk = hw_delay[5];
+	clk *= 1000000;
+	if (ptp->clk_divider)
+		clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+	dbg_msg("%u %llu\n", clk_delay[5], clk);
+	return clk_delay[5];
+}
+
+static void _test_access_time(struct ptp_info *ptp)
+{
+	ptp->get_delay = test_avg_time(ptp, test_get_time);
+	ptp->set_delay = test_avg_time(ptp, test_set_time);
+	if (ptp->get_delay < 10000)
+		ptp->delay_ticks = 10 * HZ / 1000;
+	else if (ptp->get_delay < 12000000)
+		ptp->delay_ticks = 20 * HZ / 1000;
+	else
+		ptp->delay_ticks = 30 * HZ / 1000;
+	dbg_msg("delay_ticks: %lu\n", ptp->delay_ticks);
+}  /* test_access_time */
+
+static void set_ptp_drift(struct ptp_info *ptp, int drift)
+{
+	drift /= 100;
+	drift *= 100;
+	drift = -drift;
+	ptp->first_drift = ptp->drift = drift;
+	ptp->first_sec = 0;
+	ptp->adjust = clk_adjust_val(drift, NANOSEC_IN_SEC);
+	set_ptp_adjust(ptp, ptp->adjust);
+	syntonize_clk(ptp);
+	ptp->ptp_synt = true;
+	dbg_msg("drift: %d\n", drift);
+}  /* set_ptp_drift */
+
+static void check_sys_time(struct ptp_info *ptp, unsigned long cur_jiffies,
+	union ktime cur_ktime)
+{
+	int diff;
+	int interval;
+	u32 cur_clk_cnt;
+
+	cur_clk_cnt = ptp->get_clk_cnt();
+	if (!ptp->first_drift) {
+		interval = 8;
+		diff = ptp->cur_time.sec - ptp->intr_sec;
+
+		/*
+		 * The second interval is not accurate after first setting up
+		 * the clock until later.
+		 */
+		if (diff < 6)
+			ptp->first_sec = 0;
+	} else
+		interval = 10;
+
+	if (!ptp->first_sec) {
+		ptp->last_clk_cnt = cur_clk_cnt;
+		ptp->total_clk_cnt = 0;
+		ptp->last_jiffies = cur_jiffies;
+		ptp->total_jiffies = 0;
+		ptp->first_ktime = cur_ktime;
+		ptp->first_sec = ptp->cur_time.sec;
+		return;
+	}
+
+	diff = ptp->cur_time.sec - ptp->first_sec;
+
+	if (diff >= 1 && !(diff % interval)) {
+		u32 rem;
+		u64 clk;
+		u64 clk_cnt;
+		s64 drift_clk;
+		s64 drift_jiffies;
+		s64 drift_ktime;
+		u32 passed_sec;
+		u64 passed_usec;
+		u64 passed_nsec;
+		u32 cnt;
+
+		cnt = cur_clk_cnt - ptp->last_clk_cnt;
+		ptp->total_clk_cnt += cnt;
+		ptp->last_clk_cnt = cur_clk_cnt;
+
+		passed_sec = ptp->cur_time.sec - ptp->first_sec;
+		passed_usec = passed_sec;
+		passed_usec *= 1000000;
+		passed_nsec = passed_usec;
+		passed_nsec *= 1000;
+
+		cnt = cur_jiffies - ptp->last_jiffies;
+		ptp->total_jiffies += cnt;
+		ptp->last_jiffies = cur_jiffies;
+
+		clk = ptp->total_jiffies * (1000000 / HZ);
+		drift_jiffies = clk - passed_usec;
+		drift_jiffies *= 1000;
+		drift_jiffies = div_s64_rem(drift_jiffies, passed_sec, &rem);
+
+		cur_ktime.tv64 -= ptp->first_ktime.tv64;
+		drift_ktime = cur_ktime.tv64 - passed_nsec;
+		drift_ktime = div_s64_rem(drift_ktime, passed_sec, &rem);
+
+		if (!ptp->clk_divider) {
+			if (!ptp->first_drift)
+				set_ptp_drift(ptp, (int) drift_ktime);
+			else
+				printk(KERN_INFO "%lld %lld\n",
+					drift_jiffies, drift_ktime);
+			return;
+		}
+
+		clk_cnt = div_u64_rem(ptp->total_clk_cnt, passed_sec, &rem);
+
+		clk = ptp->total_clk_cnt * 1000000;
+		clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+		drift_clk = clk;
+		if (drift_clk < 0)
+			ptp->overrides &= ~PTP_CHECK_SYS_TIME;
+		drift_clk -= passed_nsec;
+		drift_clk = div_s64_rem(drift_clk, passed_sec, &rem);
+
+		if (!ptp->first_drift)
+			set_ptp_drift(ptp, (int) drift_clk);
+		else
+			printk(KERN_INFO "%10llu %lld %lld %lld\n",
+				clk_cnt, drift_clk, drift_jiffies, drift_ktime);
+	}
+}  /* check_sys_time */
+
+static int dbg_ts_intr;
+static void proc_ptp_intr(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	u16 done;
+	u16 error;
+	u16 status;
+	u32 trig_status;
+	u32 int_status;
+	u16 tsi_bit;
+	u8 data[24];
+	int i;
+	int tsi;
+	union ktime cur_ktime;
+	struct timespec ts;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	cur_ktime = ktime_get_real();
+	ts = ktime_to_timespec(cur_ktime);
+
+proc_chk_trig_intr:
+	int_status = sw->reg->r32(sw, REG_PTP_INT_STATUS__4);
+	if (!int_status)
+		goto proc_ptp_intr_done;
+
+	sw->reg->w32(sw, REG_PTP_INT_STATUS__4, int_status);
+
+	status = (int_status >> TRIG_INT_S) & PTP_TRIG_UNIT_M;
+	if (!status)
+		goto proc_chk_ts_intr;
+
+	trig_status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+	error = (trig_status >> TRIG_ERROR_S) & PTP_TRIG_UNIT_M;
+	done = trig_status & PTP_TRIG_UNIT_M;
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+		if (status & (1 << i)) {
+			if (ptp->tso_intr & (1 << i)) {
+				data[0] = PTP_CMD_GET_OUTPUT;
+				data[1] = i;
+				proc_ptp_get_trig(ptp, data, done, error);
+			}
+			ptp_tx_done(ptp, i);
+		}
+	}
+
+proc_chk_ts_intr:
+	status = (int_status >> TS_INT_S) & PTP_TS_UNIT_M;
+	if (!status)
+		goto proc_ptp_port_intr;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		tsi_bit = 1 << i;
+		if (!(status & tsi_bit))
+			continue;
+if (!(status & ptp->ts_intr)) {
+printk("  !!\n");
+ptp->reg->rx_reset(ptp, i, NULL);
+}
+		ptp->reg->read_event(ptp, i);
+		event = &ptp->events[i];
+		if (event->timeout &&
+		    (event->num < event->max || event->last)) {
+			unsigned long expired;
+
+			expired = jiffies + event->timeout;
+			if (0 == expired)
+				expired = 1;
+			event->expired = expired;
+			if (event->last) {
+				tsi = i + 1;
+				while (tsi != event->last) {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].expired = expired;
+
+					/* Extend timeout for next unit. */
+					ptp->events[tsi].timeout =
+						event->timeout;
+					++tsi;
+				}
+			}
+		}
+		if (event->last && i != event->first) {
+			tsi = i - 1;
+			if (tsi < 0)
+				tsi = MAX_TIMESTAMP_UNIT - 1;
+			if (ptp->tsi_used & (1 << tsi))
+				ptp->events[tsi].expired = jiffies;
+		}
+
+		/* For system use only. */
+		if (!(ptp->tsi_sys & tsi_bit))
+			continue;
+		if (i == ptp->pps_tsi) {
+			struct ptp_utime sys_time;
+
+			if (event->num > 1)
+if (dbg_ts_intr < 20) {
+dbg_msg(" events %d %x:%9u %x:%9u; %x\n", event->num,
+event->t[0].sec, event->t[0].nsec,
+event->t[event->num - 1].sec, event->t[event->num - 1].nsec,
+ptp->cur_time.sec);
+++dbg_ts_intr;
+dbg_msg(" %d %d; %x %x %x\n", event->timeout, event->last,
+ptp->ts_status, ptp->ts_intr, ptp->tsi_used);
+}
+#ifndef NO_SEC_TIMESTAMP
+			ptp->cur_time.sec = event->t[0].sec;
+			ptp->cur_time.nsec = event->t[0].nsec;
+if (0 == ptp->cur_time.sec)
+printk("  ???  ");
+#if 1
+if (dbg_ts_intr < 3) {
+dbg_msg("%x:%9u\n", ptp->cur_time.sec, ptp->cur_time.nsec);
+++dbg_ts_intr;
+}
+#endif
+#ifndef NO_SEC_TIMESTAMP
+			ptp->update_sec_jiffies = 0;
+#endif
+#endif
+			ptp->sec_lo++;
+			sys_time.sec = ts.tv_sec;
+			sys_time.nsec = ts.tv_nsec;
+			calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+			if (!ptp->intr_sec)
+				ptp->intr_sec = ptp->cur_time.sec;
+#if 1
+			if ((ptp->overrides & PTP_CHECK_SYS_TIME) ||
+					!ptp->first_drift)
+				check_sys_time(ptp, jiffies, cur_ktime);
+#endif
+#ifdef CONFIG_PTP_1588_CLOCK
+			if (ptp->clock_events & (1 << 0))
+				ptp_event_trigger(ptp->clock_info, 0,
+					ptp->cur_time.sec, ptp->cur_time.nsec);
+			if (ptp->clock_events & (1 << 31))
+				ptp_event_pps(ptp->clock_info);
+#endif
+			if (!(ptp->sec_lo & 3)) {
+				check_expired_msg(ptp, ptp->rx_msg_info,
+					&ptp->rx_msg_lock, NULL);
+				check_expired_msg(ptp, ptp->tx_msg_info,
+					&ptp->tx_msg_lock, &ptp->tx_msg_cnt);
+			}
+		} else if (i == ptp->gps_tsi) {
+			ptp->gps_time.sec = event->t[0].sec;
+			ptp->gps_time.nsec = event->t[0].nsec;
+			++ptp->gps_seqid;
+			ptp->gps_resp_time = jiffies;
+		}
+	}
+	ptp_chk_rx_events(ptp);
+	ptp->ts_status = 0;
+
+proc_ptp_port_intr:
+
+	goto proc_chk_trig_intr;
+
+proc_ptp_intr_done:
+	return;
+}  /* proc_ptp_intr */
+
+#ifdef ETHTOOL_GET_TS_INFO
+static int ptp_get_ts_info(struct ptp_info *ptp, struct net_device *dev,
+	struct ethtool_ts_info *info)
+{
+	int ptp_clock = false;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int ret = -1;
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	ptp_clock = true;
+#endif
+	if (!(sw->features & PTP_HW) || !ptp_clock)
+		return ethtool_op_get_ts_info(dev, info);
+#ifdef CONFIG_PTP_1588_CLOCK
+	ret = micrel_ptp_get_ts_info(ptp, info);
+#endif
+	return ret;
+}  /* ptp_get_ts_info */
+#endif
+
+static void proc_ptp_tx_intr(struct ptp_info *ptp, uint port)
+{
+	u32 reg;
+	u16 status;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = PORT_CTRL_ADDR(port, REG_PTP_PORT_TX_INT_STATUS__2);
+	status = sw->reg->r16(sw, reg);
+	if (status) {
+		sw->reg->w16(sw, reg, status);
+		status &= ptp->tx_intr;
+		if (get_tx_time(ptp, port, status))
+			wake_up_interruptible(&ptp->wait_ts[port]);
+	}
+}  /* proc_ptp_tx_intr */
+
+#define PTP_ENABLE_TXTS		SIOCDEVPRIVATE
+#define PTP_DISABLE_TXTS	(SIOCDEVPRIVATE + 1)
+#define PTP_ENABLE_RXTS		(SIOCDEVPRIVATE + 2)
+#define PTP_DISABLE_RXTS	(SIOCDEVPRIVATE + 3)
+#define PTP_GET_TX_TIMESTAMP	(SIOCDEVPRIVATE + 4)
+#define PTP_GET_RX_TIMESTAMP	(SIOCDEVPRIVATE + 5)
+#define PTP_SET_TIME		(SIOCDEVPRIVATE + 6)
+#define PTP_GET_TIME		(SIOCDEVPRIVATE + 7)
+#define PTP_SET_FIPER_ALARM	(SIOCDEVPRIVATE + 8)
+#define PTP_SET_ADJ		(SIOCDEVPRIVATE + 9)
+#define PTP_GET_ADJ		(SIOCDEVPRIVATE + 10)
+#define PTP_CLEANUP_TS		(SIOCDEVPRIVATE + 11)
+#define PTP_ADJ_TIME		(SIOCDEVPRIVATE + 12)
+
+struct ixxat_ptp_time {
+	/* just 48 bit used */
+	u64 sec;
+	u32 nsec;
+};
+
+struct ixxat_ptp_ident {
+	u8 vers;
+	u8 mType;
+	u16 netwProt;
+	u16 seqId;
+	struct ptp_port_identity portId;
+} __packed;
+
+/* needed for timestamp data over ioctl */
+struct ixxat_ptp_data {
+	struct ixxat_ptp_ident ident;
+	struct ixxat_ptp_time ts;
+};
+
+static int ixxat_ptp_ioctl(struct ptp_info *ptp, unsigned int cmd,
+	struct ifreq *ifr)
+{
+	struct ixxat_ptp_time ptp_time;
+	struct ixxat_ptp_data ptp_data;
+	struct ptp_clk_options clk_opt;
+	int output;
+	s64 scaled_nsec;
+	struct ptp_ts ts;
+	struct ptp_tx_ts *tx;
+	int drift;
+	int err = 0;
+	uint port;
+
+	switch (cmd) {
+	case PTP_ENABLE_TXTS:
+		ptp->tx_en |= 2;
+		break;
+	case PTP_DISABLE_TXTS:
+		ptp->tx_en &= ~2;
+		break;
+	case PTP_ENABLE_RXTS:
+		ptp->rx_en |= 2;
+		break;
+	case PTP_DISABLE_RXTS:
+		ptp->rx_en &= ~2;
+		break;
+	case PTP_GET_TX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		err = -EINVAL;
+		port = htons(ptp_data.ident.portId.port);
+		if (port < 1 || port > ptp->ports)
+			break;
+		port--;
+		port = ptp_get_dev_port(ptp, 0, port);
+		tx = proc_get_ts(ptp, port, ptp_data.ident.mType,
+			ptp_data.ident.seqId,
+			ptp_data.ident.portId.clockIdentity.addr,
+			NULL, 0);
+		if (!tx)
+			break;
+		ptp_data.ts.sec = tx->ts.r.sec;
+		ptp_data.ts.nsec = tx->ts.r.nsec;
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_RX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		ts.timestamp = ptp_data.ts.nsec;
+		if (ts.timestamp)
+			update_ts(&ts, ptp->cur_time.sec);
+		else {
+			struct ptp_msg_hdr hdr;
+			struct ptp_msg_options tx_msg;
+			int found;
+
+			ts.t.sec = ts.t.nsec = 0;
+			memcpy(&hdr.sourcePortIdentity, &ptp_data.ident.portId,
+				sizeof(struct ptp_port_identity));
+			hdr.messageType = ptp_data.ident.mType;
+			hdr.sequenceId = htons(ptp_data.ident.seqId);
+			hdr.domainNumber = ptp_data.ident.vers;
+#if 0
+printk("%02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x.%04x %x %04x %x\n",
+hdr.sourcePortIdentity.clockIdentity.addr[0],
+hdr.sourcePortIdentity.clockIdentity.addr[1],
+hdr.sourcePortIdentity.clockIdentity.addr[2],
+hdr.sourcePortIdentity.clockIdentity.addr[3],
+hdr.sourcePortIdentity.clockIdentity.addr[4],
+hdr.sourcePortIdentity.clockIdentity.addr[5],
+hdr.sourcePortIdentity.clockIdentity.addr[6],
+hdr.sourcePortIdentity.clockIdentity.addr[7],
+hdr.sourcePortIdentity.port,
+hdr.messageType, hdr.sequenceId, hdr.domainNumber);
+#endif
+			found = find_msg_info(&ptp->rx_msg_info[
+				hdr.messageType],
+				&ptp->rx_msg_lock, &hdr,
+				&hdr.sourcePortIdentity,
+				PDELAY_REQ_MSG != hdr.messageType, &tx_msg);
+			if (found) {
+				ts.t = tx_msg.ts.t;
+			}
+		}
+#if 0
+printk("%u:%09u\n", ts.t.sec, ts.t.nsec);
+#endif
+		ptp_data.ts.sec = ts.t.sec;
+		ptp_data.ts.nsec = ts.t.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_TIME:
+	{
+		struct timespec ts;
+		struct ksz_ptp_time cur_time;
+		struct ksz_ptp_time sys_time;
+
+		ts = ktime_to_timespec(ktime_get_real());
+		sys_time.sec = ts.tv_sec;
+		sys_time.nsec = ts.tv_nsec;
+		calc_diff(&ptp->time_diff, &sys_time, &cur_time);
+		ptp_time.sec = cur_time.sec;
+		ptp_time.nsec = cur_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_GET, DEV_PTP_CLK, 0,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		if (err)
+			break;
+		ptp_time.sec = clk_opt.sec;
+		ptp_time.nsec = clk_opt.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_time, sizeof(ptp_time));
+		break;
+	}
+	case PTP_SET_TIME:
+		if (copy_from_user(&ptp_time, ifr->ifr_data, sizeof(ptp_time)))
+			return -EFAULT;
+		output = 0;
+		clk_opt.sec = (u32) ptp_time.sec;
+		clk_opt.nsec = ptp_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_ADJ_TIME:
+		if (copy_from_user(&scaled_nsec, ifr->ifr_data, sizeof(s64)))
+			return -EFAULT;
+		convert_scaled_nsec(scaled_nsec, SCALED_NANOSEC_S,
+			&ptp->adjust_sec, &ptp->adjust_offset);
+		if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+			output = 1;
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		} else
+			output = 2;
+		clk_opt.sec = (u32) ptp->adjust_sec;
+		clk_opt.nsec = ptp->adjust_offset;
+		clk_opt.interval = 0;
+		ptp->adjust_sec = 0;
+		ptp->adjust_offset = 0;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_SET_ADJ:
+		if (copy_from_user(&drift, ifr->ifr_data, sizeof(drift)))
+			return -EFAULT;
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+		clk_opt.drift = drift;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_GET_ADJ:
+		drift = ptp->drift;
+		err = copy_to_user(ifr->ifr_data, &drift, sizeof(drift));
+		break;
+	case PTP_CLEANUP_TS:
+		break;
+	case PTP_SET_FIPER_ALARM:
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+	return err;
+}
+
+static int ptp_dev_req(struct ptp_info *ptp, int start, char *arg,
+	struct ptp_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	struct ptp_dev_info *dev;
+	int err = 0;
+	int result = 0;
+	int v2 = 0;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+		{
+			struct ksz_sw *sw = container_of(ptp,
+				struct ksz_sw, ptp_hw);
+
+			if (chk_ioctl_size(len, 6,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+
+			if (len >= 8 &&
+			    'v' == data[4] && '2' == data[5])
+				v2 = 1;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			data[0] = 'M';
+			data[1] = 'i';
+			data[2] = 'c';
+			data[3] = 'r';
+			data[4] = ptp->version;
+			data[5] = ptp->ports;
+			if (v2) {
+				data[6] = sw->HOST_PORT + 1;
+				data[7] = 0;
+			} else
+				data[5] = ptp->ports - 1;
+			if (!access_ok(VERIFY_WRITE, req->param.data, len) ||
+			    copy_to_user(req->param.data, data, len)) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		case DEV_INFO_EXIT:
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, 0, info, &output,
+				true);
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+			if (!info)
+				break;
+			data[0] = 0xF0;
+			dev = find_minor_dev(info);
+			if (dev)
+				ptp_setup_udp_msg(dev, data, 4, NULL, NULL);
+			ptp_setup_udp_msg(info, data, 4, NULL, NULL);
+			break;
+		case DEV_INFO_RESET:
+			if (output < 3) {
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, 0, info, &output,
+					false);
+			} else
+				result = -EINVAL;
+			break;
+		case DEV_INFO_PORT:
+			if (len < 6) {
+				result = DEV_IOC_INVALID_LEN;
+				break;
+			}
+			if (chk_ioctl_size(len, len,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!result) {
+				len = 6;
+				__put_user(output, &req->output);
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+				    len) ||
+				    copy_to_user(req->param.data, data, len)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CASCADE:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_REG:
+			if ((ptp->op_mode > 0 || ptp->cap) &&
+			    chk_ioctl_size(len,
+					sizeof(u32) * 3,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_IDENTITY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_clock_identity),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if ((ptp->op_mode > 0 || ptp->cap) && output) {
+				memcpy(&ptp->masterIdentity, data,
+					sizeof(struct ptp_clock_identity));
+				break;
+			}
+			memcpy(&ptp->clockIdentity, data,
+				sizeof(struct ptp_clock_identity));
+			break;
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			ptp->utc_offset = output;
+			break;
+		case DEV_PTP_MSG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_msg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_set_msg_info(ptp, start, data, info);
+			break;
+		case DEV_PTP_PORT_CFG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_cfg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_cfg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_info),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			if (output) {
+				int wait = false;
+
+				if (2 == output)
+					wait = true;
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, len, info, &output,
+					wait);
+				if (!wait)
+					break;
+				if (!access_ok(VERIFY_WRITE, req->param.data,
+				    len) || copy_to_user(req->param.data,
+				    data, len)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+			} else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_output(ptp, data);
+			output = *((int *) data);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			if (0 == ptp->op_mode && !ptp->cap)
+				output = 0;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_clk_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_clk_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_REG:
+			if ((ptp->op_mode > 0 || ptp->cap) &&
+			    chk_ioctl_size(len,
+					sizeof(u32) * 3,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_IDENTITY:
+		{
+			struct ptp_clock_identity *id = &ptp->clockIdentity;
+
+			if ((ptp->op_mode > 0 || ptp->cap) && output)
+				id = &ptp->masterIdentity;
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_clock_identity)) ||
+					copy_to_user(req->param.data, id,
+					sizeof(struct ptp_clock_identity))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			__put_user(ptp->utc_offset, &req->output);
+			break;
+		case DEV_PTP_TIMESTAMP:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_ts_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_timestamp(ptp, data, info);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_ts_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_ts_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_MSG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_msg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_msg_info(ptp, start, data, info,
+				&output);
+			__put_user(output, &req->output);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_msg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_msg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_PORT_CFG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(VERIFY_WRITE, req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* ptp_dev_req */
+
+#ifdef HAVE_UNLOCKED_IOCTL
+static long ptp_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+#else
+static int ptp_dev_ioctl(struct inode *inode, struct file *filp,
+	unsigned int cmd, unsigned long arg)
+#endif
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	struct ptp_info *ptp = info->ptp;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok(VERIFY_WRITE, (void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok(VERIFY_READ, (void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = ptp_dev_req(ptp, 0, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* ptp_dev_ioctl */
+
+static ssize_t ptp_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_udp,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_read */
+
+static ssize_t ptp_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct ptp_dev_info *info = (struct ptp_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+	u8 cmd;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	cmd = info->write_buf[0] & 0xf0;
+	switch (cmd) {
+	case TSM_CMD_GET_GPS_TS:
+		size = sizeof(struct tsm_get_gps);
+		break;
+	case TSM_CMD_DB_GET_TIME:
+		size = sizeof(struct tsm_get_time);
+		break;
+	case TSM_CMD_DB_GET:
+		size = sizeof(struct tsm_db);
+		break;
+	case TSM_CMD_CNF_SET:
+		size = sizeof(struct tsm_cfg);
+		break;
+	case TSM_CMD_CLOCK_SET:
+		size = sizeof(struct tsm_clock_set);
+		break;
+	case TSM_CMD_CLOCK_CORRECT:
+		size = sizeof(struct tsm_clock_correct);
+		break;
+	default:
+		dbg_msg("tsm: %x\n", info->write_buf[0]);
+		result = count;
+		goto dev_write_done;
+	}
+	if (count < size) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	result = size;
+	rc = parse_tsm_msg(info, count);
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_write */
+
+static const struct file_operations ptp_dev_fops = {
+	.read		= ptp_dev_read,
+	.write		= ptp_dev_write,
+#ifdef HAVE_UNLOCKED_IOCTL
+	.unlocked_ioctl	= ptp_dev_ioctl,
+#else
+	.ioctl		= ptp_dev_ioctl,
+#endif
+	.open		= ptp_dev_open,
+	.release	= ptp_dev_release,
+};
+
+static struct class *ptp_class;
+
+static int init_ptp_device(int dev_major, char *dev_name, char *minor_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &ptp_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	ptp_class = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(ptp_class)) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(ptp_class, NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	device_create(ptp_class, NULL, MKDEV(dev_major, 1), NULL, minor_name);
+	return dev_major;
+}  /* init_ptp_device */
+
+static void exit_ptp_device(int dev_major, char *dev_name)
+{
+	device_destroy(ptp_class, MKDEV(dev_major, 1));
+	device_destroy(ptp_class, MKDEV(dev_major, 0));
+	class_destroy(ptp_class);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_ptp_device */
+
+static void ptp_set_identity(struct ptp_info *ptp, u8 *addr)
+{
+	memcpy(&ptp->clockIdentity.addr[0], &addr[0], 3);
+	ptp->clockIdentity.addr[3] = 0xFF;
+	ptp->clockIdentity.addr[4] = 0xFE;
+	memcpy(&ptp->clockIdentity.addr[5], &addr[3], 3);
+}  /* ptp_set_identity */
+
+#if 0
+static void ptp_validate(struct ptp_info *ptp)
+{
+	int n;
+	int p;
+	int q;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int save_port = sw->HOST_PORT;
+	int *host_port = &sw->HOST_PORT;
+
+	dbg_msg("ptp validate:\n");
+	for (*host_port = 0; *host_port < sw->mib_port_cnt; (*host_port)++) {
+		for (n = 0; n < ptp->ports; n++) {
+			p = ptp_get_dev_port(ptp, 0, n);
+			q = ptp_get_net_port(ptp, 0, n);
+			dbg_msg(" %d=%d %d\n", n, p, q);
+		}
+		dbg_msg("%04x\n",
+			ptp_get_dest_port(ptp, 0, (1 << ptp->ports) - 1));
+	}
+	*host_port = save_port;
+}  /* ptp_validate */
+#endif
+
+static void ptp_init(struct ptp_info *ptp, u8 *mac_addr)
+{
+	int i;
+	int latency[2][3];
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->utc_offset = CURRENT_UTC_OFFSET;
+	ptp->get_delay = 100000;
+	ptp->set_delay = 100000;
+	ptp->delay_ticks = 2;
+	ptp->access = create_singlethread_workqueue("ptp_access");
+	init_ptp_work(ptp);
+	mutex_init(&ptp->lock);
+	for (i = 0; i < MAX_PTP_PORT; i++)
+		init_waitqueue_head(&ptp->wait_ts[i]);
+	init_waitqueue_head(&ptp->wait_intr);
+	INIT_WORK(&ptp->adj_clk, adj_clock);
+	INIT_WORK(&ptp->set_latency, set_latency);
+	INIT_DELAYED_WORK(&ptp->check_pps, ptp_check_pps);
+	INIT_DELAYED_WORK(&ptp->update_sec, ptp_update_sec);
+	ptp_set_identity(ptp, mac_addr);
+
+#ifndef ACL_TEST
+	ptp->mode = PTP_ENABLE |
+		PTP_IPV4_UDP_ENABLE |
+		PTP_1STEP;
+	ptp->mode |= PTP_IPV6_UDP_ENABLE;
+	ptp->mode |= PTP_ETH_ENABLE;
+#else
+	ptp->mode = PTP_1STEP;
+#endif
+	ptp->cfg = 0;
+	ptp->cfg |= PTP_UNICAST_ENABLE;
+	ptp->cfg |= PTP_UDP_CHECKSUM;
+#if 1
+	ptp->cfg |= PTP_DOMAIN_CHECK;
+
+	/* Pdelay_Req association does not work in 2-step mode anymore. */
+	ptp->cfg |= PTP_DELAY_CHECK;
+	ptp->cfg |= PTP_SYNC_CHECK;
+#endif
+	ptp->def_mode = ptp->mode;
+	ptp->def_cfg = ptp->cfg;
+	ptp->trig_intr = PTP_TRIG_UNIT_M;
+
+	if (!ptp->ports)
+		ptp->ports = MAX_PTP_PORT;
+
+	/* KSZ8463: 574 = 415 + 45 + 114 */
+	/* KSZ9567 S1: 1246; 616 */
+	/* KSZ8463: 70m=908, 100m=1090; 1m=6.07 ns, 483; 582 */
+	/* KSZ9567 S1: 70m=1211, 100m = 1369; 1m=5.27 ns, 842; 866 */
+	/* KSZ9567 S1: 360 ns */
+	/* KSZ9567 S1: 70m=921, 100m = 1072; 1m=5.03 ns, 569; 569 */
+	/* KSZ9567 S1: 72 ns */
+	/* KSZ9567 S3: 70m=1968, 100m = 2124; 1m=5.20 ns, 1604; 1622 */
+	/* KSZ9567 S3: 1080 ns */
+	/* KSZ9567 S3: 70m=1012, 100m = 1164; 1m=5.07 ns, 657; 660 */
+	/* KSZ9567 S3: 152 ns */
+	latency[0][0] = 390;
+	latency[1][0] = 140;
+	latency[0][1] = 580;
+	latency[0][1] = 620;
+	latency[1][1] = 140;
+	if (sw->revision >= 2) {
+		latency[0][1] = 980;
+		latency[1][1] = 540;
+	}
+	latency[0][2] = latency[0][1];
+	latency[1][2] = latency[1][1];
+	for (i = 0; i < ptp->ports; i++) {
+		i = chk_last_port(sw, i);
+		cfg = &sw->info->port_cfg[i];
+		cfg->ptp_enabled = true;
+#if 0
+		cfg->asCapable_set = true;
+#endif
+		ptp->rx_latency[i][0] = latency[0][0];
+		ptp->tx_latency[i][0] = latency[1][0];
+		ptp->rx_latency[i][1] = latency[0][1];
+		ptp->tx_latency[i][1] = latency[1][1];
+		ptp->rx_latency[i][2] = latency[0][2];
+		ptp->tx_latency[i][2] = latency[1][2];
+	}
+
+	if (!ptp->get_clk_cnt)
+		ptp->get_clk_cnt = _get_clk_cnt;
+	if (!ptp->test_access_time)
+		ptp->test_access_time = _test_access_time;
+
+	ptp->gps_tsi = MAX_TIMESTAMP_UNIT;
+	ptp->gps_gpi = DEFAULT_GPS_GPI;
+	ptp->pps_gpo = DEFAULT_PPS_GPO;
+	ptp->pps_tsi = DEFAULT_PPS_TSI;
+	ptp->pps_tso = DEFAULT_PPS_TSO;
+	ptp->mhz_gpo = DEFAULT_MHZ_GPO;
+	ptp->mhz_tso = DEFAULT_MHZ_TSO;
+#if 0
+	ptp->pps_gpo = MAX_GPIO;
+#endif
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++)
+		ptp->events[i].max = MAX_TIMESTAMP_EVENT_UNIT;
+
+	init_msg_info(ptp->rx_msg_info, &ptp->rx_msg_lock);
+	init_msg_info(ptp->tx_msg_info, &ptp->tx_msg_lock);
+	for (i = sw->phy_port_cnt; i < ptp->ports; i++) {
+		i = chk_last_port(sw, i);
+		ptp->linked[i] = 1000;
+	}
+
+	ptp_priv = ptp;
+	sprintf(ptp->dev_name[0], "ptp_dev");
+	sprintf(ptp->dev_name[1], "ptp_event");
+	ptp->dev_major = init_ptp_device(0, ptp->dev_name[0],
+		ptp->dev_name[1]);
+
+#if 0
+	ptp_validate(ptp);
+#endif
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	micrel_ptp_probe(ptp);
+#endif
+}  /* ptp_init */
+
+static void ptp_exit(struct ptp_info *ptp)
+{
+	exit_ptp_work(ptp);
+	flush_work(&ptp->adj_clk);
+	flush_work(&ptp->set_latency);
+	cancel_delayed_work_sync(&ptp->check_pps);
+	cancel_delayed_work_sync(&ptp->update_sec);
+	if (ptp->access) {
+		destroy_workqueue(ptp->access);
+		ptp->access = NULL;
+	}
+	if (ptp->dev_major >= 0)
+		exit_ptp_device(ptp->dev_major, ptp->dev_name[0]);
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	micrel_ptp_remove(ptp);
+#endif
+}  /* ptp_exit */
+
+enum {
+	PROC_SET_PTP_FEATURES,
+	PROC_SET_PTP_OVERRIDES,
+	PROC_SET_PTP_VID,
+};
+
+static ssize_t sysfs_ptp_read(struct ptp_info *ptp, int proc_num, ssize_t len,
+	char *buf)
+{
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", ptp->features);
+		len += sprintf(buf + len, "\t%08x = adjust hack\n",
+			PTP_ADJ_HACK);
+		len += sprintf(buf + len, "\t%08x = adjust sec\n",
+			PTP_ADJ_SEC);
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", ptp->overrides);
+		len += sprintf(buf + len, "\t%08x = PTP port forwarding\n",
+			PTP_PORT_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP port TX forwarding\n",
+			PTP_PORT_TX_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP check path delay\n",
+			PTP_CHECK_PATH_DELAY);
+		len += sprintf(buf + len, "\t%08x = PTP verify timestamp\n",
+			PTP_VERIFY_TIMESTAMP);
+		len += sprintf(buf + len, "\t%08x = PTP zero reserved field\n",
+			PTP_ZERO_RESERVED_FIELD);
+		len += sprintf(buf + len, "\t%08x = PTP check system time\n",
+			PTP_CHECK_SYS_TIME);
+		len += sprintf(buf + len, "\t%08x = PTP check sync time\n",
+			PTP_CHECK_SYNC_TIME);
+		break;
+	case PROC_SET_PTP_VID:
+		len += sprintf(buf + len, "0x%04x\n", ptp->vid);
+		break;
+	}
+	return len;
+}
+
+static void sysfs_ptp_write(struct ptp_info *ptp, int proc_num, int num,
+	const char *buf)
+{
+	int changes;
+
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->features ^ num;
+		ptp->features = num;
+#ifdef PTP_PROCESS
+		if ((changes & (PTP_SYNT | PTP_SIM_2_STEP))) {
+#ifdef PTP_2_STEP
+			if (num & PTP_SIM_2_STEP) {
+				ptp->sim_2_step = true;
+				ptp->mode &= ~PTP_1STEP;
+			} else {
+				ptp->sim_2_step = false;
+				ptp->mode |= PTP_1STEP;
+			}
+#endif
+			if (num & (PTP_SYNT | PTP_SIM_2_STEP)) {
+				ptp_init_state(ptp);
+				if (num & PTP_SYNT) {
+					ptp->sim = 1;
+					ptp->I = 0;
+					ptp->KP = 50;
+					ptp->KI = 5;
+				}
+			} else {
+				ptp_exit_state(ptp);
+				dbg_msg("exit ptp\n");
+			}
+		}
+#endif
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->overrides ^ num;
+		if ((changes & PTP_CHECK_SYS_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYS_TIME))
+			ptp->first_sec = 0;
+#if 1
+		if ((changes & PTP_CHECK_SYNC_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYNC_TIME)) {
+			last_rcv.sec = 0;
+			first_recv = 0;
+			first_sync = 0;
+		}
+#endif
+		ptp->overrides = num;
+		break;
+	case PROC_SET_PTP_VID:
+		ptp->vid = num;
+		break;
+	}
+}
+
+static struct ptp_reg_ops ptp_reg_ops = {
+	.get_time		= get_ptp_time,
+	.set_time		= set_ptp_time,
+	.adjust_time		= adjust_ptp_time,
+	.adjust_sync_time	= adjust_sync_time,
+
+	.rx_off			= ptp_rx_off,
+	.rx_reset		= ptp_rx_reset,
+	.rx_restart		= ptp_rx_restart,
+	.rx_event		= ptp_rx_event,
+	.rx_cascade_event	= ptp_rx_cascade_event,
+	.read_event		= ptp_read_event,
+
+	.tx_off			= ptp_tx_off,
+	.tx_event		= ptp_tx_event,
+	.pps_event		= ptp_pps_event,
+	.ptp_10MHz		= ptp_10MHz,
+	.tx_cascade		= ptp_tx_cascade,
+
+	.start			= ptp_start,
+};
+
+static struct ptp_ops ptp_ops = {
+	.acquire		= ptp_acquire,
+	.release		= ptp_release,
+
+	.init			= ptp_init,
+	.exit			= ptp_exit,
+
+	.stop			= ptp_stop,
+	.set_identity		= ptp_set_identity,
+
+	.check_msg		= check_ptp_msg,
+	.get_rx_tstamp		= get_rx_tstamp,
+	.get_tx_tstamp		= get_tx_tstamp,
+	.hwtstamp_ioctl		= ptp_hwtstamp_ioctl,
+	.ixxat_ioctl		= ixxat_ptp_ioctl,
+	.dev_req		= ptp_dev_req,
+	.proc_intr		= proc_ptp_intr,
+#ifdef ETHTOOL_GET_TS_INFO
+	.get_ts_info		= ptp_get_ts_info,
+#endif
+
+	.sysfs_read		= sysfs_ptp_read,
+	.sysfs_write		= sysfs_ptp_write,
+
+	.drop_pkt		= ptp_drop_pkt,
+	.get_rx_info		= ptp_get_rx_info,
+	.set_tx_info		= ptp_set_tx_info,
+};
+
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_9897.h b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_9897.h
new file mode 100644
index 0000000..16dd573
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_9897.h
@@ -0,0 +1,1135 @@
+/**
+ * Microchip PTP common header
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_PTP_H
+#define KSZ_PTP_H
+
+#ifndef __KERNEL__
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef short s16;
+typedef long long s64;
+typedef unsigned long long u64;
+#endif
+
+struct ksz_ptp_time {
+	int sec;
+	int nsec;
+};
+
+struct ptp_utime {
+	u32 sec;
+	u32 nsec;
+};
+
+struct ptp_ts {
+	struct ptp_utime r;
+	struct ptp_utime t;
+	u32 timestamp;
+};
+
+struct ptp_second {
+	u16 hi;
+	u32 lo;
+} __packed;
+
+struct ptp_timestamp {
+	struct ptp_second sec;
+	u32 nsec;
+} __packed;
+
+#define SCALED_NANOSEC_S		16
+#define SCALED_NANOSEC_MULT		(1 << SCALED_NANOSEC_S)
+
+struct ptp_scaled_ns {
+	int hi;
+	s64 lo;
+} __packed;
+
+struct ptp_correction {
+	int scaled_nsec_hi;
+	int scaled_nsec_lo;
+} __packed;
+
+struct ptp_clock_identity {
+	u8 addr[8];
+};
+
+struct ptp_port_identity {
+	struct ptp_clock_identity clockIdentity;
+	u16 port;
+} __packed;
+
+struct ptp_clock_quality {
+	u8 clockClass;
+	u8 clockAccuracy;
+	u16 offsetScaledLogVariance;
+} __packed;
+
+struct ptp_port_address {
+	u16 networkProtocol;
+	u16 addressLength;
+	u8 addressField[1];
+} __packed;
+
+struct ptp_text {
+	u8 lengthField;
+	u8 textField[1];
+} __packed;
+
+#define SYNC_MSG			0x0
+#define DELAY_REQ_MSG			0x1
+#define PDELAY_REQ_MSG			0x2
+#define PDELAY_RESP_MSG			0x3
+#define FOLLOW_UP_MSG			0x8
+#define DELAY_RESP_MSG			0x9
+#define PDELAY_RESP_FOLLOW_UP_MSG	0xA
+#define ANNOUNCE_MSG			0xB
+#define SIGNALING_MSG			0xC
+#define MANAGEMENT_MSG			0xD
+
+struct ptp_msg_hdr {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 transportSpecific:4;
+	u8 messageType:4;
+	u8 reserved1:4;
+	u8 versionPTP:4;
+#else
+	u8 messageType:4;
+	u8 transportSpecific:4;
+	u8 versionPTP:4;
+	u8 reserved1:4;
+#endif
+	u16 messageLength;
+	u8 domainNumber;
+	u8 reserved2;
+	union {
+		struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+			u8 reservedFlag7:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag4:1;
+			u8 reservedFlag3:1;
+			u8 unicastFlag:1;
+			u8 twoStepFlag:1;
+			u8 alternateMasterFlag:1;
+			u8 reservedFlag6:1;
+			u8 reservedFlag5:1;
+			u8 frequencyTraceable:1;
+			u8 timeTraceable:1;
+			u8 ptpTimescale:1;
+			u8 utcOffsetValid:1;
+			u8 leap59:1;
+			u8 leap61:1;
+#else
+			u8 alternateMasterFlag:1;
+			u8 twoStepFlag:1;
+			u8 unicastFlag:1;
+			u8 reservedFlag3:1;
+			u8 reservedFlag4:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag7:1;
+			u8 leap61:1;
+			u8 leap59:1;
+			u8 utcOffsetValid:1;
+			u8 ptpTimescale:1;
+			u8 timeTraceable:1;
+			u8 frequencyTraceable:1;
+			u8 reservedFlag5:1;
+			u8 reservedFlag6:1;
+#endif
+		} __packed flag;
+		u16 data;
+	} __packed flagField;
+	struct ptp_correction correctionField;
+	u32 reserved3;
+	struct ptp_port_identity sourcePortIdentity;
+	u16 sequenceId;
+	u8 controlField;
+	char logMessageInterval;
+} __packed;
+
+struct ptp_msg_sync {
+	struct ptp_timestamp originTimestamp;
+} __packed;
+
+struct ptp_msg_follow_up {
+	struct ptp_timestamp preciseOriginTimestamp;
+} __packed;
+
+struct ptp_msg_delay_resp {
+	struct ptp_timestamp receiveTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_req {
+	struct ptp_timestamp originTimestamp;
+	struct ptp_port_identity reserved;
+} __packed;
+
+struct ptp_msg_pdelay_resp {
+	struct ptp_timestamp requestReceiptTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_resp_follow_up {
+	struct ptp_timestamp responseOriginTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+#define TLV_MANAGEMENT					0x0001
+#define TLV_MANAGEMENT_ERROR_STATUS			0x0002
+#define TLV_ORGANIZATION_EXTENSION			0x0003
+#define TLV_REQUEST_UNICAST_TRANSMISSION		0x0004
+#define TLV_GRANT_UNICAST_TRANSMISSION			0x0005
+#define TLV_CANCEL_UNICAST_TRANSMISSION			0x0006
+#define TLV_ACKNOWLEDGE_CANCEL_UNICAST_TRANSMISSION	0x0007
+#define TLV_PATH_TRACE					0x0008
+#define TLV_ALTERNATE_TIME_OFFSET_INDICATOR		0x0009
+
+struct ptp_tlv {
+	u16 tlvType;
+	u16 lengthField;
+} __packed;
+
+struct ptp_organization_ext_tlv {
+	struct ptp_tlv tlv;
+	u8 organizationId[3];
+	u8 organizationSubType[3];
+	u8 dataField[1];
+} __packed;
+
+struct IEEE_C37_238_data {
+	u16 grandmasterID;
+	u32 grandmasterTimeInaccuracy;
+	u32 networkTimeInaccuracy;
+	u16 reserved;
+} __packed;
+
+struct IEEE_802_1AS_data_1 {
+	int cumulativeScaledRateOffset;
+	u16 gmTimeBaseIndicator;
+	struct ptp_scaled_ns lastGmPhaseChange;
+	int scaledLastGmFreqChange;
+} __packed;
+
+struct IEEE_802_1AS_data_2 {
+	char linkDelayInterval;
+	char timeSyncInterval;
+	char announceInterval;
+	u8 flags;
+	u16 reserved;
+} __packed;
+
+struct ptp_request_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+} __packed;
+
+struct ptp_grant_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+	u8 reserved2;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved3:7;
+	u8 renewal:1;
+#else
+	u8 renewal:1;
+	u8 reserved3:7;
+#endif
+} __packed;
+
+struct ptp_cancel_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_alternate_time_offset_tlv {
+	struct ptp_tlv tlv;
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_msg_signaling_base {
+	struct ptp_port_identity targetPortIdentity;
+} __packed;
+
+struct ptp_msg_signaling {
+	struct ptp_msg_signaling_base b;
+	union {
+		struct ptp_request_unicast_tlv request[1];
+		struct ptp_grant_unicast_tlv grant[1];
+		struct ptp_cancel_unicast_tlv cancel[1];
+	} tlv;
+} __packed;
+
+#define M_NULL_MANAGEMENT				0x0000
+#define M_CLOCK_DESCRIPTION				0x0001
+#define M_DEFAULT_DATA_SET				0x2000
+#define M_CURRENT_DATA_SET				0x2001
+#define M_PARENT_DATA_SET				0x2002
+#define M_PORT_DATA_SET					0x2004
+#define M_PRIORITY1					0x2005
+#define M_PRIORITY2					0x2006
+#define M_DOMAIN					0x2007
+#define M_SLAVE_ONLY					0x2008
+#define M_VERSION_NUMBER				0x200C
+#define M_ENABLE_PORT					0x200D
+#define M_DISABLE_PORT					0x200E
+#define M_TIME						0x200F
+#define M_UNICAST_NEGOTIATION_ENABLE			0x2014
+#define M_PATH_TRACE_LIST				0x2015
+#define M_PATH_TRACE_ENABLE				0x2016
+#define M_GRANDMASTER_CLUSTER_TABLE			0x2017
+#define M_UNICAST_MASTER_TABLE				0x2018
+#define M_UNICAST_MASTER_MAX_TABLE_SIZE			0x2019
+#define M_ACCEPTABLE_MASTER_TABLE			0x201A
+#define M_ACCEPTABLE_MASTER_TABLE_ENABLED		0x201B
+#define M_ACCEPTABLE_MASTER_MAX_TABLE_SIZE		0x201C
+#define M_ALTERNATE_MASTER				0x201D
+#define M_ALTERNATE_TIME_OFFSET_ENABLE			0x201E
+#define M_ALTERNATE_TIME_OFFSET_NAME			0x201F
+#define M_ALTERNATE_TIME_OFFSET_MAX_KEY			0x2020
+#define M_ALTERNATE_TIME_OFFSET_PROPERTIES		0x2021
+
+struct ptp_management_unicast_negotiation {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_management_unicast_master_table {
+	u8 logQueryInterval;
+	u16 tableSize;
+	struct ptp_port_address unicastMasterTable[1];
+} __packed;
+
+struct ptp_management_unicast_master_max_table_size {
+	u16 maxTableSize;
+} __packed;
+
+struct ptp_management_alternate_time_offset {
+	u8 keyField;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+} __packed;
+
+struct ptp_management_alternate_time_offset_name {
+	u8 keyField;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_management_alternate_time_offset_max_key {
+	u8 keyField;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_alternate_time_offset_properties {
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_tlv {
+	struct ptp_tlv tlv;
+	u16 managementId;
+	u8 dataField[1];
+} __packed;
+
+#define M_RESPONSE_TOO_BIG		0x0001
+#define M_NO_SUCH_ID			0x0002
+#define M_WRONG_LENGTH			0x0003
+#define M_WRONG_VALUE			0x0004
+#define M_NOT_SETABLE			0x0005
+#define M_NOT_SUPPORTED			0x0006
+#define M_GENERAL_ERROR			0xFFFE
+
+struct ptp_management_error_tlv {
+	struct ptp_tlv tlv;
+	u16 managementErrorId;
+	u16 managementId;
+	u32 reserved1;
+	u8 data[1];
+} __packed;
+
+#define MANAGEMENT_GET			0
+#define MANAGEMENT_SET			1
+#define MANAGEMENT_RESPONSE		2
+#define MANAGEMENT_COMMAND		3
+#define MANAGEMENT_ACKNOWLEDGE		4
+
+struct ptp_msg_management_base {
+	struct ptp_port_identity targetPortIdentity;
+	u8 startingBoundaryHops;
+	u8 boundaryHops;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:4;
+	u8 actionField:4;
+#else
+	u8 actionField:4;
+	u8 reserved1:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_msg_management {
+	struct ptp_msg_management_base b;
+	union {
+		struct ptp_management_tlv normal[1];
+		struct ptp_management_error_tlv error[1];
+	} tlv;
+} __packed;
+
+struct ptp_msg_announce {
+	struct ptp_timestamp originTimestamp;
+	s16 currentUtcOffset;
+	u8 reserved;
+	u8 grandmasterPriority1;
+	struct ptp_clock_quality grandmasterClockQuality;
+	u8 grandmasterPriority2;
+	struct ptp_clock_identity grandmasterIdentity;
+	u16 stepsRemoved;
+	u8 timeSource;
+} __packed;
+
+union ptp_msg_data {
+	struct ptp_msg_sync sync;
+	struct ptp_msg_follow_up follow_up;
+	struct ptp_msg_delay_resp delay_resp;
+	struct ptp_msg_pdelay_req pdelay_req;
+	struct ptp_msg_pdelay_resp pdelay_resp;
+	struct ptp_msg_pdelay_resp_follow_up pdelay_resp_follow_up;
+	struct ptp_msg_signaling signaling;
+	struct ptp_msg_management management;
+	struct ptp_msg_announce announce;
+	u8 data[8];
+} __packed;
+
+struct ptp_msg {
+	struct ptp_msg_hdr hdr;
+	union ptp_msg_data data;
+} __packed;
+
+
+struct ptp_id {
+	u16 seq;
+	struct ptp_clock_identity clock;
+	u8 mac[2];
+	u8 msg;
+	u8 port;
+};
+
+struct ptp_cfg_options {
+	u8 master:1;
+	u8 two_step:1;
+	u8 p2p:1;
+	u8 as:1;
+	u8 domain_check:1;
+	u8 udp_csum:1;
+	u8 unicast:1;
+	u8 alternate:1;
+	u8 delay_assoc:1;
+	u8 pdelay_assoc:1;
+	u8 sync_assoc:1;
+	u8 drop_sync:1;
+	u8 priority:1;
+	u8 reserved:3;
+	u8 master_set:1;
+	u8 two_step_set:1;
+	u8 p2p_set:1;
+	u8 as_set:1;
+	u8 domain_check_set:1;
+	u8 udp_csum_set:1;
+	u8 unicast_set:1;
+	u8 alternate_set:1;
+	u8 delay_assoc_set:1;
+	u8 pdelay_assoc_set:1;
+	u8 sync_assoc_set:1;
+	u8 drop_sync_set:1;
+	u8 priority_set:1;
+	u8 reserved_set:2;
+	u8 domain_set:1;
+	u8 domain;
+	u8 reserved3;
+	u32 access_delay;
+} __packed;
+
+#define PTP_CMD_RESP			0x01
+#define PTP_CMD_GET_MSG			0x00
+#define PTP_CMD_GET_OUTPUT		0xE0
+#define PTP_CMD_GET_EVENT		0xF0
+
+#define PTP_CMD_INTR_OPER		0x01
+#define PTP_CMD_SILENT_OPER		0x02
+#define PTP_CMD_ON_TIME			0x04
+#define PTP_CMD_REL_TIME		0x08
+#define PTP_CMD_CLK_OPT			0x10
+#define PTP_CMD_CASCADE_RESET_OPER	0x40
+#define PTP_CMD_CANCEL_OPER		0x80
+
+struct ptp_tsi_info {
+	u8 cmd;
+	u8 unit;
+	u8 event;
+	u8 num;
+	u32 edge;
+	struct ptp_utime t[0];
+} __packed;
+
+struct ptp_tsi_options {
+	u8 tsi;
+	u8 gpi;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[3];
+	u32 timeout;
+} __packed;
+
+struct ptp_tso_options {
+	u8 tso;
+	u8 gpo;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[1];
+	u16 cnt;
+	u32 pulse;
+	u32 cycle;
+	u32 sec;
+	u32 nsec;
+	u32 iterate;
+} __packed;
+
+struct ptp_clk_options {
+	u32 sec;
+	u32 nsec;
+	int drift;
+	u32 interval;
+} __packed;
+
+struct ptp_ts_options {
+	u32 timestamp;
+	u32 sec;
+	u32 nsec;
+	u8 msg;
+	u8 port;
+	u16 seqid;
+	u8 mac[2];
+} __packed;
+
+struct ptp_delay_values {
+	u16 rx_latency;
+	u16 tx_latency;
+	short asym_delay;
+	u16 reserved;
+} __packed;
+
+struct ptp_msg_options {
+	struct ptp_port_identity id;
+	u16 seqid;
+	u8 domain;
+	u8 msg;
+	u8 reserved[2];
+	u32 port;
+	struct ptp_ts ts;
+} __packed;
+
+struct ptp_udp_msg {
+	u16 len;
+	u8 data[0];
+} __packed;
+
+#ifdef __KERNEL__
+#define NANOSEC_IN_SEC			1000000000
+
+/* Host port can be any one of the ports. */
+#define MAX_PTP_PORT			SWITCH_PORT_NUM
+
+#define MAX_TSM_UDP_LEN			100
+#define MAX_TSM_UDP_CNT			(1 << 6)
+
+struct ptp_ltime {
+	s64 sec;
+	s64 nsec;
+};
+
+struct ptp_hw_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int sim_2step;
+	int update;
+	int sending;
+};
+
+struct ptp_dev_info {
+	void *ptp;
+	unsigned int minor;
+	u8 *write_buf;
+	u8 *read_buf;
+	size_t read_max;
+	size_t read_len;
+	size_t write_len;
+	struct semaphore sem;
+	struct mutex lock;
+	wait_queue_head_t wait_udp;
+	struct ptp_dev_info *next;
+};
+
+struct ptp_tx_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int missed;
+	unsigned long req_time;
+	unsigned long resp_time;
+	struct {
+		u8 buf[MAX_TSM_UDP_LEN];
+		int len;
+	} data;
+	struct ptp_dev_info *dev;
+	struct sk_buff *skb;
+	struct ptp_msg *msg;
+	struct ptp_msg_hdr hdr;
+};
+
+struct ptp_event {
+	int max;
+	int num;
+	int event;
+	int first;
+	int last;
+	u32 edge;
+	struct ptp_utime t[MAX_TIMESTAMP_EVENT_UNIT];
+	u32 timeout;
+	unsigned long expired;
+};
+
+struct ptp_output {
+	struct ptp_utime trig;
+	struct ptp_utime start;
+	struct ptp_utime stop;
+	struct ksz_ptp_time gap;
+	u32 iterate;
+	u32 len;
+	int gpo;
+	int level;
+};
+
+#define CLOCK_ENTRIES		2
+
+struct ptp_irig_info {
+	u32 pulse[100];
+	int index;
+	u8 tso[8];
+	int cur_tso;
+	int max_tso;
+	struct ptp_utime t;
+};
+
+struct ptp_msg_info {
+	struct ptp_msg_options data;
+	u32 sec;
+	struct ptp_msg_info *next;
+};
+
+struct ptp_info;
+
+struct ptp_work {
+	struct work_struct work;
+	struct completion done;
+	struct ptp_info *ptp;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	int wait;
+	union {
+		struct ptp_cfg_options cfg;
+		struct ptp_tsi_info tsi;
+		struct ptp_tsi_options tsi_opt;
+		struct ptp_tso_options tso_opt;
+		struct ptp_clk_options clk_opt;
+		struct ptp_ts_options ts_opt;
+		struct ptp_delay_values delay;
+		u8 data[8];
+	} param;
+	struct ptp_dev_info *dev_info;
+};
+
+#define PTP_WORK_NUM			(1 << 4)
+#define PTP_WORK_LAST			(PTP_WORK_NUM - 1)
+
+struct ptp_access {
+	struct mutex lock;
+	int index;
+	struct ptp_work works[PTP_WORK_NUM];
+};
+
+struct ptp_reg_ops {
+	void (*lock)(struct ptp_info *ptp);
+	void (*unlock)(struct ptp_info *ptp);
+
+	void (*get_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*set_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*adjust_time)(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+		int adj_hack);
+	void (*adjust_sync_time)(struct ptp_info *ptp, int diff, u32 interval,
+		u32 duration);
+
+	void (*rx_off)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_reset)(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr);
+	void (*rx_restart)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_event)(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+		int intr);
+	void (*rx_cascade_event)(struct ptp_info *ptp, u8 first, u8 total,
+		u8 gpi, u8 event, int intr);
+	void (*read_event)(struct ptp_info *ptp, u8 tsi);
+
+	void (*tx_off)(struct ptp_info *ptp, u8 tso);
+	void (*tx_event)(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+		u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+		int intr, int now, int opt);
+	void (*pps_event)(struct ptp_info *ptp, u8 gpo, u32 sec);
+	void (*ptp_10MHz)(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec);
+	int (*tx_cascade)(struct ptp_info *ptp, u8 first, u8 total,
+		u16 repeat, u32 sec, u32 nsec, int intr);
+
+	void (*start)(struct ptp_info *ptp, int init);
+};
+
+struct ptp_ops {
+	void (*acquire)(struct ptp_info *ptp);
+	void (*release)(struct ptp_info *ptp);
+
+	void (*init)(struct ptp_info *ptp, u8 *mac_addr);
+	void (*exit)(struct ptp_info *ptp);
+	int (*stop)(struct ptp_info *ptp, int hw_access);
+	void (*set_identity)(struct ptp_info *ptp, u8 *addr);
+	struct ptp_msg *(*check_msg)(u8 *data, u16 **udp_check_ptr);
+	int (*update_msg)(u8 *data, u32 port, u32 overrides);
+	void (*get_rx_tstamp)(void *ptr, struct sk_buff *skb);
+	void (*get_tx_tstamp)(struct ptp_info *ptp, struct sk_buff *skb);
+	int (*hwtstamp_ioctl)(struct ptp_info *ptp, struct ifreq *ifr,
+			      u16 ports);
+	int (*ixxat_ioctl)(struct ptp_info *ptp, unsigned int cmd,
+		struct ifreq *ifr);
+	int (*dev_req)(struct ptp_info *ptp, int start, char *arg,
+		struct ptp_dev_info *info);
+	void (*proc_intr)(struct ptp_info *ptp);
+#ifdef ETHTOOL_GET_TS_INFO
+	int (*get_ts_info)(struct ptp_info *ptp, struct net_device *dev,
+		struct ethtool_ts_info *info);
+#endif
+
+	ssize_t (*sysfs_read)(struct ptp_info *ptp, int proc_num, ssize_t len,
+		char *buf);
+	void (*sysfs_write)(struct ptp_info *ptp, int proc_num, int num,
+		const char *buf);
+
+	int (*drop_pkt)(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+		int *tag, int *ptp_tag, int *forward);
+
+	void (*get_rx_info)(struct ptp_info *ptp, u8 *data, u8 port,
+		u32 timestamp);
+	void (*set_tx_info)(struct ptp_info *ptp, u8 *data, void *tag);
+};
+
+#define DEFAULT_GPS_GPI			1
+#define DEFAULT_GPS_TSI			1
+
+#define DEFAULT_PPS_TSI			1
+
+#if 1
+#define DEFAULT_MHZ_GPO			1
+#define DEFAULT_PPS_GPO			0
+#else
+#define DEFAULT_MHZ_GPO			0
+#define DEFAULT_PPS_GPO			1
+#endif
+
+/* TSO 1 is reserved if 10 MHz clock is used. */
+#define DEFAULT_MHZ_TSO			0
+#define DEFAULT_PPS_TSO			2
+
+/* Switch features and bug fixes. */
+#define PTP_ADJ_HACK			(1 << 0)
+#define PTP_ADJ_SEC			(1 << 1)
+#define PTP_PDELAY_HACK			(1 << 2)
+
+/* Software overrides. */
+
+#define PTP_PORT_FORWARD		(1 << 0)
+#define PTP_PORT_TX_FORWARD		(1 << 1)
+
+#define PTP_CHECK_PATH_DELAY		(1 << 7)
+#define PTP_VERIFY_TIMESTAMP		(1 << 8)
+#define PTP_ZERO_RESERVED_FIELD		(1 << 9)
+#define PTP_UPDATE_PDELAY_RESP_PORT	(1 << 10)
+#define PTP_CHECK_SYS_TIME		(1 << 16)
+#define PTP_CHECK_SYNC_TIME		(1 << 24)
+#define PTP_TEST_TX_INFO		(1 << 28)
+#define PTP_USE_DEFAULT_PORT		(1 << 29)
+#define PTP_KEEP_DST_PORT		(1 << 30)
+#define PTP_UPDATE_DST_PORT		(1 << 31)
+
+struct ptp_info {
+	struct mutex lock;
+	struct ptp_access hw_access;
+	struct {
+		u8 buf[MAX_TSM_UDP_LEN];
+		int len;
+	} udp[MAX_TSM_UDP_CNT];
+
+	/* current system time. */
+	struct ptp_utime cur_time;
+	struct ptp_utime gps_time;
+	struct ksz_ptp_time time_diff;
+	u32 sec_hi;
+	u32 sec_lo;
+	struct delayed_work check_pps;
+	struct delayed_work update_sec;
+	unsigned long update_sec_jiffies;
+
+	u32 adjust;
+	int drift;
+	int drift_set;
+
+	int adjust_offset;
+	int offset_changed;
+	s64 adjust_sec;
+	s64 sec_changed;
+
+	struct ptp_utime time_set;
+
+	u32 adj_delay;
+	u32 get_delay;
+	u32 set_delay;
+	int pps_offset;
+	struct ptp_dev_info *gps_dev;
+	unsigned long gps_req_time;
+	unsigned long gps_resp_time;
+	u8 gps_gpi;
+	u8 gps_tsi;
+	u16 gps_seqid;
+	u8 pps_tsi;
+	u8 pps_tso;
+	u8 pps_gpo;
+	u8 mhz_tso;
+	u8 mhz_gpo;
+	u8 version;
+	u8 ports;
+	u8 started;
+
+	/* hardware register values. */
+	u16 rx_latency[MAX_PTP_PORT][3];
+	u16 tx_latency[MAX_PTP_PORT][3];
+	short asym_delay[MAX_PTP_PORT][3];
+	u32 peer_delay[MAX_PTP_PORT];
+
+	spinlock_t rx_msg_lock;
+	spinlock_t tx_msg_lock;
+	struct ptp_msg_info rx_msg_info[MANAGEMENT_MSG + 1];
+	struct ptp_msg_info tx_msg_info[MANAGEMENT_MSG + 1];
+#if 0
+	struct ptp_msg_options rx_msg_chk[MANAGEMENT_MSG + 1];
+#endif
+	struct ptp_msg *rx_msg;
+	struct ptp_msg *tx_msg;
+	int tx_msg_cnt;
+	int tx_msg_parsed;
+	u32 tx_ports;
+	int cap;
+	int forward;
+	int op_mode;
+	int op_state;
+
+	/* used to remember tx timestamp to differentiate between pdelay_req
+	 * and pdelay_resp.
+	 */
+	u32 xdelay_ts[MAX_PTP_PORT];
+	u32 pdresp_ts[MAX_PTP_PORT];
+
+	/* tx timestamp */
+	struct ptp_hw_ts hw_sync[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_dreq[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_resp[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_sync[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_dreq[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_resp[MAX_PTP_PORT];
+	int linked[MAX_PTP_PORT];
+
+	int state;
+	u16 def_mode;
+	u16 def_cfg;
+	u16 mode;
+	u16 cfg;
+	u16 domain;
+	u16 vid;
+	int ptp_synt;
+	u16 trig_intr;
+	u16 ts_intr;
+	u16 tx_intr;
+
+	int tsi_intr;
+	int tsi_used;
+	int tsi_sys;
+	int tso_intr;
+	int tso_used;
+	int tso_sys;
+	int ts_status;
+	int cascade;
+	int cascade_rx;
+	int cascade_tx;
+	struct {
+		int first;
+		int total;
+		int tso;
+	} cascade_gpo[MAX_GPIO];
+	struct ptp_clock_identity clockIdentity;
+	struct ptp_clock_identity masterIdentity;
+	struct ptp_event events[MAX_TIMESTAMP_UNIT];
+	struct ptp_output outputs[MAX_TRIG_UNIT + 1];
+	int udp_head;
+	int udp_tail;
+	int dev_major;
+	struct ptp_dev_info *dev[2];
+	struct ptp_dev_info *tsi_dev[MAX_TIMESTAMP_UNIT];
+	struct ptp_dev_info *tso_dev[MAX_TRIG_UNIT];
+	char dev_name[2][20];
+	wait_queue_head_t wait_ts[MAX_PTP_PORT];
+	wait_queue_head_t wait_intr;
+	unsigned long delay_ticks;
+	int rx_en;
+	int tx_en;
+	u16 rx_en_ports;
+	u16 tx_en_ports;
+	int utc_offset;
+
+	u32 clk_divider;
+	u32 (*get_clk_cnt)(void);
+	u32 last_clk_cnt;
+	u64 total_clk_cnt;
+	u32 first_sec;
+	u32 intr_sec;
+	unsigned long last_jiffies;
+	u64 total_jiffies;
+	union ktime first_ktime;
+	int first_drift;
+	struct ptp_ts last_rx_ts;
+	struct ptp_ts last_tx_ts;
+
+	uint features;
+	uint overrides;
+
+	struct work_struct adj_clk;
+	struct work_struct set_latency;
+
+	const struct ptp_ops *ops;
+	const struct ptp_reg_ops *reg;
+	void (*test_access_time)(struct ptp_info *ptp);
+
+	struct workqueue_struct *access;
+
+	struct device *parent;
+#ifdef CONFIG_PTP_1588_CLOCK
+	void *clock_info;
+	u32 clock_events;
+#endif
+};
+
+struct ksz_ptp_sysfs {
+	struct ksz_dev_attr *ksz_clock_attrs[CLOCK_ENTRIES];
+	struct attribute **clock_attrs[CLOCK_ENTRIES];
+};
+#endif
+
+enum {
+	DEV_IOC_UNIT_UNAVAILABLE = DEV_IOC_LAST,
+	DEV_IOC_UNIT_USED,
+	DEV_IOC_UNIT_ERROR,
+};
+
+enum {
+	DEV_INFO_MSG = DEV_INFO_LAST,
+	DEV_INFO_RESET,
+};
+
+enum {
+	DEV_PTP_CFG,
+	DEV_PTP_TEVT,
+	DEV_PTP_TOUT,
+	DEV_PTP_CLK,
+	DEV_PTP_CASCADE,
+	DEV_PTP_DELAY,
+	DEV_PTP_REG,
+	DEV_PTP_IDENTITY,
+	DEV_PTP_PEER_DELAY,
+	DEV_PTP_UTC_OFFSET,
+	DEV_PTP_TIMESTAMP,
+	DEV_PTP_MSG,
+	DEV_PTP_PORT_CFG,
+};
+
+#ifndef TSM_CMD_CLOCK_SET
+#define TSM_CMD_RESP			0x04
+#define TSM_CMD_GET_TIME_RESP		0x08
+
+#define TSM_CMD_CLOCK_SET		0x10
+#define TSM_CMD_CLOCK_CORRECT		0x20
+#define TSM_CMD_DB_SET			0x30
+#define TSM_CMD_DB_GET			0x40
+#define TSM_CMD_STAT_CLEAR		0x50
+#define TSM_CMD_STAT_GET		0x60
+#define TSM_CMD_CNF_SET			0x70
+#define TSM_CMD_CNF_GET			0x80
+#define TSM_CMD_GPIO_SET		0x90
+#define TSM_CMD_GPIO_GET		0xA0
+#define	TSM_CMD_SET_SECONDS		0xB0
+/* todo */
+#define TSM_CMD_GET_GPS_TS		0xE0
+
+/* used for accessing reserved DB entry for a given port for SYNC or DELAY_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV1		0xB0
+/* used for accessing reserved DB entry for a given port for P2P PATH_DEL_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV2		0xC0
+/* used for getting time from TSM, no look-up of a DB entry with an ingress or
+ * egress time stamp
+ */
+#define TSM_CMD_DB_GET_TIME		0xD0
+#define TSM_CMD_DB_SET_TIME		0xF0
+#endif
+
+struct tsm_cfg {
+	u8 cmd;
+	u8 port;
+	u8 enable;
+	u8 gmp;
+	u32 ingress_delay;
+	u16 egress_delay;
+} __packed;
+
+struct tsm_clock_set {
+	u8 cmd;
+	u32 timestamp;
+	u32 nsec;
+	u32 sec;
+	u8 reserved[5];
+} __packed;
+
+struct tsm_clock_correct {
+	u8 cmd;
+	u8 add;
+	u32 sec;
+	u32 nsec;
+	u32 drift;
+	u32 offset;
+} __packed;
+
+struct tsm_db {
+	u8 cmd;
+	u8 index;
+	u16 seqid;
+	u8 mac[2];
+	u32 cur_sec;
+	u32 cur_nsec;
+	u32 timestamp;
+} __packed;
+
+struct tsm_get_gps {
+	u8 cmd;
+	u8 reserved[7];
+	u16 seqid;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+struct tsm_get_time {
+	u8 cmd;
+	u16 seqid;
+	u8 msg;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+#define PTP_CAN_RX_TIMESTAMP		BIT(0)
+#define PTP_KNOW_ABOUT_LATENCY		BIT(1)
+#define PTP_HAVE_MULT_DEVICES		BIT(2)
+#define PTP_HAVE_MULT_PORTS		BIT(3)
+#define PTP_KNOW_ABOUT_MULT_PORTS	BIT(4)
+#define PTP_USE_RESERVED_FIELDS		BIT(5)
+#define PTP_SEPARATE_PATHS		BIT(6)
+
+#define PTP_PORT_ENABLED		BIT(0)
+#define PTP_PORT_ASCAPABLE		BIT(1)
+
+#ifdef __KERNEL__
+struct ptp_attributes {
+	int features;
+	int overrides;
+	int vid;
+};
+#endif
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_iba.c b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_iba.c
new file mode 100644
index 0000000..7e23217
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_iba.c
@@ -0,0 +1,1363 @@
+/**
+ * Microchip PTP common code in IBA format
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static u32 ptp_unit_index(struct ptp_info *ptp, int shift, u8 unit)
+{
+	u32 index;
+#ifndef USE_OLD_PTP_UNIT_INDEX
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	index = sw->cached.ptp_unit_index;
+	index &= ~(PTP_UNIT_M << shift);
+	index |= (u32) unit << shift;
+	sw->cached.ptp_unit_index = index;
+#else
+	index = unit;
+#endif
+	return index;
+}  /* ptp_unit_index */
+
+static void *get_time_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = *data;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_CLK_CTRL);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_RTC_SEC);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_RTC_NANOSEC);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_RTC_SUB_NANOSEC__2);
+	return info->fptr;
+}  /* get_time_pre */
+
+static int get_time_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	struct ptp_utime *t = obj;
+	int i = 0;
+	u16 subnsec = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_PTP_RTC_SEC:
+				t->sec = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_RTC_NANOSEC:
+				t->nsec = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_RTC_SUB_NANOSEC__2:
+				subnsec = (u16) iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			}
+		}
+		i++;
+	}
+
+	add_nsec(t, subnsec * 8);
+	return i;
+}  /* get_time_post */
+
+static void get_ptp_time_iba(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+
+	data[0] = sw->cached.ptp_clk_ctrl;
+	data[0] |= PTP_READ_TIME;
+	iba_req(info, data, NULL, t, get_time_pre, get_time_post);
+}  /* get_ptp_time_iba */
+
+static void *set_time_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ptp_utime *t = obj;
+
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_RTC_SUB_NANOSEC__2);
+	info->data[0] = t->nsec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RTC_NANOSEC);
+	info->data[0] = t->sec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RTC_SEC);
+	info->data[0] = data[0];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_CLK_CTRL);
+	return info->fptr;
+}  /* set_time_pre */
+
+static void set_ptp_time_iba(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+
+	data[0] = sw->cached.ptp_clk_ctrl;
+	data[0] |= PTP_LOAD_TIME;
+	iba_req(info, data, NULL, t, set_time_pre, NULL);
+}  /* set_ptp_time_iba */
+
+static void *adjust_time_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u32 nsec = data[4];
+	u32 val = nsec;
+
+	info->data[0] = data[3];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RTC_SEC);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		info->data[0] = nsec;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_PTP_RTC_NANOSEC);
+		info->data[0] = data[2];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_PTP_CLK_CTRL);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (data[5]) {
+		info->data[0] = data[5];
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_PTP_CLK_CTRL);
+	}
+	return info->fptr;
+}  /* adjust_time_pre */
+
+static void adjust_ptp_time_iba(struct ptp_info *ptp, int add, u32 sec,
+	u32 nsec, int adj_hack)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[6];
+	u16 ctrl;
+	u16 adj = 0;
+
+	data[2] = sw->cached.ptp_clk_ctrl;
+	ctrl = data[2];
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_ADJ;
+	data[2] = ctrl;
+	data[3] = sec;
+	data[4] = nsec;
+	data[5] = 0;
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		data[5] = adj;
+	iba_req(info, data, NULL, NULL, adjust_time_pre, NULL);
+}  /* adjust_ptp_time_iba */
+
+static void *adjust_sync_time_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+
+	info->data[0] = data[0];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_RATE_DURATION);
+	info->data[0] = data[1];
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_SUBNANOSEC_RATE);
+	return info->fptr;
+}
+
+static void adjust_sync_time_iba(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[2];
+	u32 adjust;
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE;
+	data[0] = duration;
+	data[1] = adjust;
+	iba_req(info, data, NULL, NULL, adjust_sync_time_pre, NULL);
+}  /* adjust_sync_time_iba */
+
+static void *ptp_unit_index_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int shift = data[0];
+	u8 unit = (u8) data[1];
+	struct ptp_info *ptp = obj;
+
+	info->data[0] = ptp_unit_index(ptp, shift, unit);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_UNIT_INDEX__4);
+	return info->fptr;
+}  /* ptp_unit_index_pre */
+
+static void *rx_reset_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_ENABLE | TS_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_reset_pre */
+
+static void ptp_rx_reset_iba(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	u32 buf[8];
+	void *func[3];
+	void *data_in[2];
+	u32 *data = buf;
+	int i = 0;
+
+	if (!ctrl_ptr) {
+		data_in[i] = data;
+		data[0] = PTP_TSI_INDEX_S;
+		data[1] = tsi;
+		data += 2;
+		func[i++] = ptp_unit_index_pre;
+	}
+
+	data_in[i] = data;
+	func[i++] = rx_reset_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_reset_iba */
+
+static void *rx_off_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_INT_ENABLE | TS_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_off_pre */
+
+static void ptp_rx_off_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u16 tsi_bit = (1 << tsi);
+	u32 ts_intr = 0;
+	int rc;
+	u32 buf[13];
+	void *func[6];
+	void *data_in[5];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		ts_intr = tsi_bit;
+	}
+
+	data_in[i] = data;
+	func[i++] = rx_off_pre;
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		data_in[i] = data;
+		func[i++] = rx_reset_pre;
+
+		data_in[i] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_TS_CTRL_STAT__4;
+		data[2] = 0;
+		data += 3;
+		func[i++] = iba_w_pre;
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+	if (ts_intr) {
+		data_in[i] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_PTP_INT_STATUS__4;
+		data[2] = ts_intr;
+		data += 3;
+		func[i++] = iba_w_pre;
+	}
+
+	data_out = data;
+	data = iba_prepare_data(REG_PTP_CTRL_STAT__4, data);
+	data = iba_prepare_data(-1, data);
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+}  /* ptp_rx_off_iba */
+
+static inline void rx_intr_iba(struct ptp_info *ptp, u16 tsi_bit, u32 *ctrl)
+{
+	ptp->ts_intr |= tsi_bit;
+	*ctrl |= TS_INT_ENABLE;
+}  /* rx_intr_iba */
+
+static void *rx_on_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tsi = (u8) data[0];
+	int intr = data[1];
+	struct ptp_info *ptp = obj;
+	u32 ctrl = 0;
+
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+
+	ctrl |= TS_ENABLE;
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_on_pre */
+
+static void *rx_restart_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TS_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_restart_pre */
+
+static void ptp_rx_restart_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	func[i++] = rx_restart_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_restart_iba */
+
+static void *rx_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tsi = data[0];
+	u8 gpi = data[1];
+	u8 event = data[2];
+	struct ptp_info *ptp = obj;
+	u32 ctrl;
+
+	info->data[0] = ptp_unit_index(ptp, PTP_TSI_INDEX_S, tsi);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_UNIT_INDEX__4);
+
+	/* Config pattern. */
+	ctrl = ts_event_gpi(gpi, event);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TS_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_event_pre */
+
+static void ptp_rx_event_iba(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = tsi;
+	data[1] = gpi;
+	data[2] = event;
+	data += 3;
+	func[i++] = rx_event_pre;
+
+	data_in[i] = data;
+	data[0] = tsi;
+	data[1] = intr;
+	data += 2;
+	func[i++] = rx_on_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_event_iba */
+
+static void *rx_cascade_event_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u8 first = data[0];
+	u8 total = data[1];
+	u8 gpi = data[2];
+	u8 event = data[3];
+	int intr = data[4];
+	struct ptp_info *ptp = obj;
+	int last;
+	int tsi;
+	u32 ctrl;
+	u32 tail;
+	int i;
+	int prev;
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		info->data[0] = ptp_unit_index(ptp, PTP_TSI_INDEX_S, tsi);
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_PTP_UNIT_INDEX__4);
+
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = ts_event_gpi(gpi, event);
+		ctrl |= ts_cascade(prev);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		info->data[0] = ctrl;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TS_CTRL_STAT__4);
+
+		/* Enable timestamp interrupt. */
+		if (intr) {
+			ctrl = 0;
+			ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+			info->data[0] = 0;
+			info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+				IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+			info->data[0] = ctrl;
+			info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1,
+				IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		}
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	info->data[0] = ptp_unit_index(ptp, PTP_TSI_INDEX_S, first);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_UNIT_INDEX__4);
+
+	ctrl = ts_event_gpi(gpi, event);
+	ctrl |= ts_cascade(last);
+	ptp->cascade_rx |= (1 << first);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TS_CTRL_STAT__4);
+	return info->fptr;
+}  /* rx_cascade_event_pre */
+
+static void ptp_rx_cascade_event_iba(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = first;
+	data[1] = total;
+	data[2] = gpi;
+	data[3] = event;
+	data[4] = intr;
+	data += 5;
+	func[i++] = rx_cascade_event_pre;
+
+	data_in[i] = data;
+	data[0] = first;
+	data[1] = intr;
+	data += 2;
+	func[i++] = rx_on_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_cascade_event_iba */
+
+static u32 ptp_get_event_cnt_iba(struct ptp_info *ptp, u8 tsi, void *ptr)
+{
+	struct ksz_iba_info *info = ptr;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	data[0] = IBA_CMD_32;
+	data[1] = REG_TS_CTRL_STAT__4;
+	data += 3;
+	func[i++] = iba_r_pre;
+
+	data_out = data;
+	data = iba_prepare_data(REG_TS_CTRL_STAT__4, data);
+	data = iba_prepare_data(-1, data);
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+	return data_out[1];
+}  /* ptp_get_event_cnt_iba */
+
+static void ptp_get_events_iba(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr)
+{
+	struct ksz_iba_info *info = ptr;
+
+	iba_burst(info, reg_ns, len, buf, 0,
+		iba_get_pre, iba_get_post_le);
+}  /* ptp_get_events_iba */
+
+static void ptp_read_event_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+
+	ptp_read_event_func(ptp, tsi, info, ptp_get_event_cnt_iba,
+		ptp_get_events_iba);
+}  /* ptp_read_event_iba */
+
+static void *tx_reset_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_ENABLE | TRIG_RESET;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* tx_reset_pre */
+
+static void *tx_off_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* tx_off_pre */
+
+static void *tx_init_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	info->data[0] = TRIG_CASCADE_ENABLE | TRIG_CASCADE_TAIL;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	info->data[0] = trig_cascade(TRIG_CASCADE_UPS_M);
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	return info->fptr;
+}  /* tx_init_pre */
+
+static void ptp_tx_off_iba(struct ptp_info *ptp, u8 tso)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u16 tso_bit = (1 << tso);
+	int rc;
+	u32 buf[8 + 8];
+	void *func[4];
+	void *data_in[3];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	func[i++] = tx_off_pre;
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		data_in[i] = data;
+		func[i++] = tx_reset_pre;
+
+		ptp->cascade_gpo[ptp->outputs[tso].gpo].tso &= ~tso_bit;
+		ptp->cascade_tx &= ~tso_bit;
+	} else {
+		data_in[i] = data;
+		func[i++] = tx_init_pre;
+	}
+
+	data_out = data;
+	data = iba_prepare_data(-1, data);
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+}  /* ptp_tx_off_iba */
+
+static void *tx_on_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	info->data[0] = TRIG_ENABLE;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+		REG_PTP_CTRL_STAT__4);
+	return info->fptr;
+}  /* tx_on_pre */
+
+static void *tx_trigger_time_iba(struct ksz_iba_info *info, u8 tso, u32 sec,
+	u32 nsec)
+{
+	info->data[0] = sec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_TARGET_SEC);
+	info->data[0] = nsec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_TARGET_NANOSEC);
+	return info->fptr;
+}  /* tx_trigger_time_iba */
+
+static void *tx_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 gpo = data[4];
+	u8 event = data[5];
+	u32 pulse = data[6];
+	u32 cycle = data[7];
+	u16 cnt = data[8];
+	u32 sec = data[9];
+	u32 nsec = data[10];
+	u32 iterate = data[11];
+	int intr = data[12];
+	int now = data[13];
+	int opt = data[14];
+	struct ptp_info *ptp = obj;
+	u32 ctrl;
+	u32 pattern = 0;
+	u16 tso_bit = (1 << tso);
+	struct ptp_output *cur = &ptp->outputs[tso];
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	if (intr)
+		ctrl |= TRIG_NOTIFY;
+	if (now)
+		ctrl |= TRIG_NOW;
+	if (opt)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+
+	/* Config pulse width. */
+	if (TRIG_REG_OUTPUT == event) {
+		pattern = pulse & TRIG_BIT_PATTERN_M;
+		cur->level = 0;
+		if (cnt) {
+			u32 reg;
+
+			reg = cnt - 1;
+			reg %= 16;
+			while (reg) {
+				pulse >>= 1;
+				reg--;
+			}
+			if (pulse & 1)
+				cur->level = 1;
+		}
+		pulse = 0;
+	} else if (event >= TRIG_NEG_PULSE) {
+		if (0 == pulse)
+			pulse = 1;
+		else if (pulse > TRIG_PULSE_WIDTH_M)
+			pulse = TRIG_PULSE_WIDTH_M;
+		info->data[0] = pulse;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_24,
+			REG_TRIG_PULSE_WIDTH__4 + 1);
+	}
+
+	/* Config cycle width. */
+	if (event >= TRIG_NEG_PERIOD) {
+		u32 data = cnt;
+		int min_cycle = pulse * PULSE_NSEC + MIN_CYCLE_NSEC;
+
+		if (cycle < min_cycle)
+			cycle = min_cycle;
+		info->data[0] = cycle;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TRIG_CYCLE_WIDTH);
+
+		/* Config trigger count. */
+		data <<= TRIG_CYCLE_CNT_S;
+		pattern |= data;
+		info->data[0] = pattern;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TRIG_CYCLE_CNT);
+	}
+
+	cur->len = 0;
+	if (event >= TRIG_NEG_PERIOD) {
+		if (cnt)
+			cur->len += cycle * cnt;
+		else
+			cur->len += 0xF0000000;
+	} else if (event >= TRIG_NEG_PULSE)
+		cur->len += pulse * PULSE_NSEC;
+	else
+		cur->len += MIN_CYCLE_NSEC;
+
+	cur->start.sec = sec;
+	cur->start.nsec = nsec;
+	cur->iterate = iterate;
+	cur->trig = cur->start;
+	cur->stop = cur->start;
+	add_nsec(&cur->stop, cur->len);
+	cur->gpo = gpo;
+
+	switch (event) {
+	case TRIG_POS_EDGE:
+	case TRIG_NEG_PULSE:
+	case TRIG_NEG_PERIOD:
+		cur->level = 1;
+		break;
+	case TRIG_REG_OUTPUT:
+		break;
+	default:
+		cur->level = 0;
+		break;
+	}
+
+	if (ptp->cascade)
+		return info->fptr;
+
+	/*
+	 * Need to reset after completion.  Otherwise, this output pattern
+	 * does not behave consistently in cascade mode.
+	 */
+	if (TRIG_NEG_EDGE == event)
+		ptp->cascade_tx |= tso_bit;
+
+	ptp->cascade_gpo[gpo].total = 0;
+	if (cur->level)
+		ptp->cascade_gpo[gpo].tso |= tso_bit;
+	else
+		ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+
+	/* Config trigger time. */
+	tx_trigger_time_iba(info, tso, sec, nsec);
+
+	return info->fptr;
+}  /* tx_event_pre */
+
+static void ptp_tx_event_iba(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+	int intr, int now, int opt)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	struct ptp_output *cur = &ptp->outputs[tso];
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	/* Hardware immediately keeps level high on new GPIO if not reset. */
+	if (cur->level && gpo != cur->gpo) {
+		data_in[i] = data;
+		func[i++] = tx_reset_pre;
+
+		ptp->cascade_gpo[cur->gpo].tso &= ~(1 << tso);
+	}
+
+	data_in[i] = data;
+	data[3] = tso;
+	data[4] = gpo;
+	data[5] = event;
+	data[6] = pulse;
+	data[7] = cycle;
+	data[8] = cnt;
+	data[9] = sec;
+	data[10] = nsec;
+	data[11] = iterate;
+	data[12] = intr;
+	data[13] = now;
+	data[14] = opt;
+	data += 15;
+	func[i++] = tx_event_pre;
+
+	if (!ptp->cascade) {
+		data_in[i] = data;
+		func[i++] = tx_on_pre;
+	}
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_tx_event_iba */
+
+static void *pps_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 gpo = data[3];
+	u32 sec = data[4];
+	struct ptp_info *ptp = obj;
+	u32 pattern;
+	u32 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 tso = ptp->pps_tso;
+	u8 event = TRIG_POS_PERIOD;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	info->data[0] = pulse;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_24,
+		REG_TRIG_PULSE_WIDTH__4 + 1);
+
+	/* Config cycle width. */
+	info->data[0] = cycle;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_WIDTH);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	info->data[0] = pattern;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_CNT);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	tx_trigger_time_iba(info, tso, sec, nsec);
+
+	return info->fptr;
+}  /* pps_event_pre */
+
+static void ptp_pps_event_iba(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u8 tso = ptp->pps_tso;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	int i = 0;
+
+	ptp_tx_off_iba(ptp, tso);
+
+	data_in[i] = data;
+	data[3] = gpo;
+	data[4] = sec;
+	data += 5;
+	func[i++] = pps_event_pre;
+
+	data_in[i] = data;
+	func[i++] = tx_on_pre;
+
+	func[i] = NULL;
+	assert_buf(__func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_pps_event */
+
+static void *ptp_10MHz_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 gpo = data[4];
+	u32 sec = data[5];
+	u32 nsec = data[6];
+	u32 pattern;
+	u32 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	if (1 == tso)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	info->data[0] = pulse;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_PULSE_WIDTH__4 + 0);
+
+	/* Config cycle width. */
+	info->data[0] = cycle;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_WIDTH);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	info->data[0] = pattern;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CYCLE_CNT);
+
+	tx_trigger_time_iba(info, tso, sec, nsec);
+
+	return info->fptr;
+}  /* ptp_10MHz_pre */
+
+static void ptp_10MHz_iba(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int n;
+	u32 nsec;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	u32 *data_out;
+	int k = 0;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+
+	for (n = 0; n < 2; n++) {
+		ptp_tx_off_iba(ptp, tso);
+
+		k = 0;
+		data = buf;
+
+		data_in[k] = data;
+		data[3] = tso;
+		data[4] = gpo;
+		data[5] = sec;
+		data[6] = nsec;
+		data += 7;
+		func[k++] = ptp_10MHz_pre;
+
+		data_in[k] = data;
+		func[k++] = tx_on_pre;
+
+		data_out = data;
+		data = iba_prepare_data(REG_PTP_CTRL_STAT__4, data);
+		data = iba_prepare_data(-1, data);
+
+		func[k] = NULL;
+		assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+		rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz_iba */
+
+static void *tx_cascade_cycle_iba(struct ksz_iba_info *info, u8 tso, u32 nsec)
+{
+	info->data[0] = nsec;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_ITERATE_TIME);
+	return info->fptr;
+}  /* tx_cascade_cycle_iba */
+
+static void *tx_cascade_on_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 first = data[4];
+	u8 last = data[5];
+	u16 repeat = data[6];
+	struct ptp_output *cur = obj;
+	u32 ctrl;
+
+	tx_trigger_time_iba(info, tso, cur->trig.sec, cur->trig.nsec);
+	tx_cascade_cycle_iba(info, tso, cur->iterate);
+
+	ctrl = data[2];
+	ctrl |= TRIG_CASCADE_ENABLE;
+	ctrl &= ~trig_cascade(TRIG_CASCADE_UPS_M);
+	if (tso == first)
+		ctrl |= trig_cascade(last);
+	else
+		ctrl |= trig_cascade(tso - 1);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		ctrl |= repeat - 1;
+	}
+	info->data[0] = ctrl;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_CTRL__4);
+	return info->fptr;
+}  /* tx_cascade_on_pre */
+
+static int ptp_tx_cascade_iba(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	int n;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur = NULL;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	u32 *data_out;
+	int k = 0;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	if (check_cascade(ptp, first, total, &repeat, sec, nsec)) {
+		dbg_msg("cascade repeat timing is not right\n");
+		return 1;
+	}
+	tso = last;
+	for (n = 0; n < total; n++, tso--) {
+		cur = &ptp->outputs[tso];
+	data = buf;
+	k = 0;
+	data_in[k] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[k++] = ptp_unit_index_pre;
+
+	data_in[k] = data;
+	data[0] = IBA_CMD_32;
+	data[1] = REG_TRIG_CTRL__4;
+	data += 3;
+	func[k++] = iba_r_pre;
+
+	data_out = data;
+	data = iba_prepare_data(REG_TRIG_CTRL__4, data);
+	data = iba_prepare_data(-1, data);
+
+		func[k] = NULL;
+		assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+		rc = iba_reqs(info, data_in, data_out, ptp, func, iba_r_post);
+
+		data = buf;
+		k = 0;
+
+		data_in[k] = data;
+		data[2] = data_out[1];
+		data[3] = tso;
+		data[4] = first;
+		data[5] = last;
+		data[6] = repeat;
+		data += 7;
+		func[k++] = tx_cascade_on_pre;
+
+		func[k] = NULL;
+		assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+		if (tso != first)
+			rc = iba_reqs(info, data_in, NULL, cur, func, NULL);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	/* Do not reset last unit to keep level high. */
+	if (ptp->outputs[last].level) {
+		ptp->cascade_tx &= ~(1 << last);
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso |= (1 << last);
+	} else
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso &= ~(1 << last);
+
+	data_in[k] = data;
+	func[k++] = tx_on_pre;
+
+	func[k] = NULL;
+	assert_buf(__func__, k, sizeof(func), buf, data, sizeof(buf));
+	rc = iba_reqs(info, data_in, NULL, cur, func, NULL);
+	return 0;
+}  /* ptp_tx_cascade_iba */
+
+static void *start_pre_1(struct ksz_iba_info *info, void *in, void *obj)
+{
+	info->data[0] = 0;
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_MSG_CONF1);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_MSG_CONF2);
+	info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_16,
+		REG_PTP_DOMAIN_VERSION);
+	return info->fptr;
+}  /* start_pre_1 */
+
+static int start_post_1(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_PTP_MSG_CONF1:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_MSG_CONF2:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_DOMAIN_VERSION:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* start_post_1 */
+
+static void *start_pre_2(struct ksz_iba_info *info, void *in, void *obj)
+{
+	struct ptp_info *ptp = obj;
+
+	info->data[0] = ptp->mode;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_MSG_CONF1);
+	info->data[0] = ptp->cfg;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_MSG_CONF2);
+	info->data[0] = 0xffffffff;
+	info->fptr = iba_cmd_data(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_PTP_INT_STATUS__4);
+	return info->fptr;
+}  /* start_pre_2 */
+
+static void ptp_start_iba(struct ptp_info *ptp, int init)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+	u16 ctrl;
+	struct timespec ts;
+	struct ptp_utime t;
+
+	if (!ptp->version) {
+		ptp_hw_enable(ptp);
+		ptp_check(ptp);
+		if (ptp->test_access_time)
+			ptp->test_access_time(ptp);
+		ptp_init_hw(ptp);
+	} else
+	if (init && (sw->features & NEW_CAP))
+		ptp_hw_enable(ptp);
+	ptp->ops->acquire(ptp);
+	iba_req(info, data, data, NULL, start_pre_1, start_post_1);
+	ctrl = data[0];
+	if (ctrl == ptp->mode) {
+		ptp->cfg = data[1];
+		ptp->domain = data[2] & PTP_DOMAIN_M;
+		if (!init) {
+			ptp->ops->release(ptp);
+			return;
+		}
+	} else if (!init)
+		ptp->mode = ctrl;
+	if (ptp->mode != ptp->def_mode) {
+		dbg_msg("mode changed: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->ptp_synt = false;
+	}
+	dbg_msg("ptp_start: %04x %04x\n",
+		ptp->mode, ptp->cfg);
+	iba_req(info, data, NULL, ptp, start_pre_2, NULL);
+	ptp->tx_intr = PTP_PORT_XDELAY_REQ_INT;
+	ptp_tx_intr_enable(ptp);
+	ptp->ops->release(ptp);
+
+	ts = ktime_to_timespec(ktime_get_real());
+	t.sec = ts.tv_sec;
+	t.nsec = ts.tv_nsec;
+	ptp->ops->acquire(ptp);
+	set_ptp_time_iba(ptp, &t);
+	ptp->cur_time = t;
+	ptp->ops->release(ptp);
+
+	prepare_pps(ptp);
+	ptp->started = true;
+}  /* ptp_start_iba */
+
+
+static struct ptp_reg_ops ptp_iba_ops = {
+	.get_time		= get_ptp_time_iba,
+	.set_time		= set_ptp_time_iba,
+	.adjust_time		= adjust_ptp_time_iba,
+	.adjust_sync_time	= adjust_sync_time_iba,
+
+	.rx_off			= ptp_rx_off_iba,
+	.rx_reset		= ptp_rx_reset_iba,
+	.rx_restart		= ptp_rx_restart_iba,
+	.rx_event		= ptp_rx_event_iba,
+	.rx_cascade_event	= ptp_rx_cascade_event_iba,
+	.read_event		= ptp_read_event_iba,
+
+	.tx_off			= ptp_tx_off_iba,
+	.tx_event		= ptp_tx_event_iba,
+	.pps_event		= ptp_pps_event_iba,
+	.ptp_10MHz		= ptp_10MHz_iba,
+	.tx_cascade		= ptp_tx_cascade_iba,
+
+	.start			= ptp_start_iba,
+};
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_sysfs.c b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_sysfs.c
new file mode 100644
index 0000000..4ba6fef
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_ptp_sysfs.c
@@ -0,0 +1,126 @@
+/**
+ * Microchip PTP common sysfs code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2013 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static ssize_t ptp_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ptp_info *ptp;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	ptp = &sw->ptp_hw;
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = ptp->ops->sysfs_read(ptp, proc_num, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t ptp_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ptp_info *ptp;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	ptp = &sw->ptp_hw;
+	proc_num = offset / sizeof(int);
+	ret = count;
+	ptp->ops->acquire(ptp);
+	ptp->ops->sysfs_write(ptp, proc_num, num, buf);
+	ptp->ops->release(ptp);
+	up(proc_sem);
+	return ret;
+}
+
+#define PTP_ATTR(_name, _mode, _show, _store) \
+struct device_attribute ptp_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define PTP_RD_ENTRY(name)						\
+static ssize_t show_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return ptp_show(d, attr, buf,					\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static PTP_ATTR(name, S_IRUGO, show_ptp_##name, NULL)
+
+/* generate a write-able attribute */
+#define PTP_WR_ENTRY(name)						\
+static ssize_t show_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return ptp_show(d, attr, buf,					\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static ssize_t store_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return ptp_store(d, attr, buf, count,				\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static PTP_ATTR(name, S_IRUGO | S_IWUSR, show_ptp_##name, store_ptp_##name)
+
+PTP_WR_ENTRY(features);
+PTP_WR_ENTRY(overrides);
+PTP_WR_ENTRY(vid);
+
+static struct attribute *ptp_attrs[] = {
+	&ptp_attr_features.attr,
+	&ptp_attr_overrides.attr,
+	&ptp_attr_vid.attr,
+	NULL
+};
+
+static struct attribute_group ptp_group = {
+	.name  = "ptpfs",
+	.attrs  = ptp_attrs,
+};
+
+static void exit_ptp_sysfs(struct ksz_ptp_sysfs *info, struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, &ptp_group);
+}
+
+static int init_ptp_sysfs(struct ksz_ptp_sysfs *info, struct device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->kobj, &ptp_group);
+	if (err)
+		return err;
+	return err;
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_req.c b/drivers/net/ethernet/micrel/ksz9897/ksz_req.c
new file mode 100644
index 0000000..ad69bed
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_req.c
@@ -0,0 +1,95 @@
+/**
+ * Microchip driver request common code
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_req.h"
+
+
+#define PARAM_DATA_SIZE			80
+
+static void get_user_data(int *kernel, int *user, void *info)
+{
+	if (info)
+		__get_user(*kernel, user);
+	else
+		*kernel = *user;
+}  /* get_user_data */
+
+static void put_user_data(int *kernel, int *user, void *info)
+{
+	if (info)
+		__put_user(*kernel, user);
+	else
+		*user = *kernel;
+}  /* put_user_data */
+
+static int read_user_data(void *kernel, void *user, size_t size, void *info)
+{
+	if (info) {
+		if (!access_ok(VERIFY_READ, user, size) ||
+		    copy_from_user(kernel, user, size))
+			return -EFAULT;
+	} else
+		memcpy(kernel, user, size);
+	return 0;
+}  /* read_user_data */
+
+static int write_user_data(void *kernel, void *user, size_t size, void *info)
+{
+	if (info) {
+		if (!access_ok(VERIFY_WRITE, user, size) ||
+		    copy_to_user(user, kernel, size))
+			return -EFAULT;
+	} else
+		memcpy(user, kernel, size);
+	return 0;
+}  /* write_user_data */
+
+static int _chk_ioctl_size(int len, int size, int additional, int *req_size,
+	int *result, void *param, u8 *data, void *info)
+{
+	if (len < size) {
+		printk(KERN_INFO "wrong size: %d %d\n", len, size);
+		*req_size = size + additional;
+		*result = DEV_IOC_INVALID_LEN;
+		return -1;
+	}
+	if (size >= PARAM_DATA_SIZE) {
+		printk(KERN_INFO "large size: %d\n", size);
+		*result = -EFAULT;
+		return -1;
+	}
+	if (data) {
+		int err = read_user_data(data, param, size, info);
+
+		if (err) {
+			*result = -EFAULT;
+			return -1;
+		}
+	}
+	return 0;
+}  /* _chk_ioctl_size */
+
+static int chk_ioctl_size(int len, int size, int additional, int *req_size,
+	int *result, void *param, u8 *data)
+{
+	return _chk_ioctl_size(len, size, additional, req_size, result, param,
+		data, data);
+	return 0;
+}  /* chk_ioctl_size */
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_req.h b/drivers/net/ethernet/micrel/ksz9897/ksz_req.h
new file mode 100644
index 0000000..ca141fe
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_req.h
@@ -0,0 +1,110 @@
+/**
+ * Microchip driver request common header
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_REQ_H
+#define KSZ_REQ_H
+
+enum {
+	DEV_IOC_OK,
+	DEV_IOC_INVALID_SIZE,
+	DEV_IOC_INVALID_CMD,
+	DEV_IOC_INVALID_LEN,
+
+	DEV_IOC_LAST
+};
+
+enum {
+	DEV_CMD_INFO,
+	DEV_CMD_GET,
+	DEV_CMD_PUT,
+
+	DEV_CMD_LAST
+};
+
+enum {
+	DEV_INFO_INIT,
+	DEV_INFO_EXIT,
+	DEV_INFO_QUIT,
+	DEV_INFO_NOTIFY,
+	DEV_INFO_PORT,
+
+	DEV_INFO_LAST
+};
+
+struct ksz_request {
+	int size;
+	int cmd;
+	int subcmd;
+	int output;
+	int result;
+	union {
+		u8 data[1];
+		int num[1];
+	} param;
+};
+
+/* Some compilers in different OS cannot have zero number in array. */
+#define SIZEOF_ksz_request	(sizeof(struct ksz_request) - sizeof(int))
+
+/* Not used in the driver. */
+
+#ifndef MAX_REQUEST_SIZE
+#define MAX_REQUEST_SIZE	20
+#endif
+
+struct ksz_request_actual {
+	int size;
+	int cmd;
+	int subcmd;
+	int output;
+	int result;
+	union {
+		u8 data[MAX_REQUEST_SIZE];
+		int num[MAX_REQUEST_SIZE / sizeof(int)];
+	} param;
+};
+
+#define DEV_IOC_MAGIC			0x92
+
+#define DEV_IOC_MAX			1
+
+
+struct ksz_read_msg {
+	u16 len;
+	u8 data[0];
+} __packed;
+
+
+enum {
+	DEV_MOD_BASE,
+	DEV_MOD_PTP,
+	DEV_MOD_MRP,
+	DEV_MOD_DLR,
+	DEV_MOD_HSR,
+};
+
+struct ksz_resp_msg {
+	u16 module;
+	u16 cmd;
+	union {
+		u32 data[1];
+	} resp;
+} __packed;
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_spi_net.h b/drivers/net/ethernet/micrel/ksz9897/ksz_spi_net.h
new file mode 100644
index 0000000..ada7acd
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_spi_net.h
@@ -0,0 +1,187 @@
+/**
+ * Microchip SPI switch common header
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SPI_NET_H
+#define KSZ_SPI_NET_H
+
+
+#if defined(_LINUX_I2C_H)
+/**
+ * struct i2c_hw_priv - I2C device private data structure
+ * @i2cdev:		Adapter device information.
+ * @rxd:		Buffer for receiving I2C data.
+ * @txd:		Buffer for transmitting I2C data.
+ */
+struct i2c_hw_priv {
+	struct i2c_client *i2cdev;
+
+	u8 rxd[8];
+	u8 txd[32];
+};
+#endif
+
+#if defined(__LINUX_SPI_H)
+/**
+ * struct spi_hw_priv - SPI device private data structure
+ * @spidev:		Adapter device information.
+ * @spi_msg1:		Used for SPI transfer with one message.
+ * @spi_msg2:		Used for SPI transfer with two messages.
+ * @spi_xfer1:		Used for SPI transfer with one message.
+ * @spi_xfer2:		Used for SPI transfer with two messages.
+ * @rx_1msg:		Flag to receive SPI data with single message.
+ * @rxd:		Buffer for receiving SPI data.
+ * @txd:		Buffer for transmitting SPI data.
+ */
+struct spi_hw_priv {
+	struct spi_device *spidev;
+	struct spi_message spi_msg1;
+	struct spi_message spi_msg2;
+	struct spi_transfer spi_xfer1;
+	struct spi_transfer spi_xfer2[2];
+	int rx_1msg;
+
+	u8 rxd[128];
+	u8 txd[128];
+};
+#endif
+
+/**
+ * struct sw_priv - Switch device private data structure
+ * @hw_dev:		Pointer to hardware access device structure.
+ * @dev:		Pointer to Linux base device of hardware device.
+ * @intr_mode:		Indicate which interrupt mode to use.
+ * @irq:		A copy of the hardware device interrupt.
+ * @sysfs:		Sysfs structure.
+ * @proc_sem:		Semaphore for sysfs accessing.
+ * @hwlock:
+ * @lock:
+ * @link_read:		Work queue for detecting link.
+ * @mib_read:		Work queue for reading MIB counters.
+ * @stp_monitor:	Work queue for STP monitoring.
+ * @mib_timer_info:	Timer information for reading MIB counters.
+ * @monitor_timer_info:	Timer information for monitoring.
+ * @counter:		MIB counter data.
+ * @ports:		Virtual switch ports.
+ * @debug_root:
+ * @debug_file:
+ * @irq_gpio:		GPIO pin used for interrupt.
+ * @gpio_val:		GPIO value during interrupt.
+ * @phy_id:		Point to active PHY.
+ * @intr_working:	Working interrupt indications.
+ * @intr_mask:
+ * @pdev:		Point to platform device.
+ * @bus:		Point to MDIO bus.
+ * @bus_irqs:
+ * @name:
+ * @phydev:		Point to active PHY device.
+ * @sw:			Virtual switch structure.
+ */
+struct sw_priv {
+	void *hw_dev;
+	struct device *dev;
+	int intr_mode;
+	int irq;
+
+	struct ksz_sw_sysfs sysfs;
+#ifdef CONFIG_1588_PTP
+	struct ksz_ptp_sysfs ptp_sysfs;
+#endif
+	struct semaphore proc_sem;
+
+	struct mutex hwlock;
+	struct mutex lock;
+
+	struct work_struct irq_work;
+	struct delayed_work link_read;
+	struct work_struct mib_read;
+	struct delayed_work stp_monitor;
+	struct ksz_timer_info mib_timer_info;
+	struct ksz_timer_info monitor_timer_info;
+	struct ksz_counter_info counter[TOTAL_PORT_NUM];
+	struct ksz_port ports[TOTAL_PORT_NUM + 1];
+
+	struct dentry *debug_root;
+	struct dentry *debug_file;
+
+	int irq_gpio;
+	int gpio_val;
+	int phy_id;
+	int intr_working;
+	uint intr_mask;
+
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	int bus_irqs[PHY_MAX_ADDR];
+	char name[40];
+	struct phy_device *phydev;
+
+	/* Switch structure size can be variable. */
+	struct ksz_sw sw;
+};
+
+/**
+ * struct dev_priv - Network device private data structure
+ * @adapter:		Adapter device information.
+ * @dev:
+ * @parent:
+ * @port:
+ * @monitor_timer_info:	Timer information for monitoring.
+ * @stats:		Network statistics.
+ * @phydev:		The PHY device associated with the device.
+ * @phy_pause:		Workqueue to pause the PHY state machine.
+ * @id:			Device ID.
+ * @mii_if:		MII interface information.
+ * @advertising:	Temporary variable to store advertised settings.
+ * @msg_enable:		The message flags controlling driver output.
+ * @media_state:	The connection status of the device.
+ * @multicast:		The all multicast state of the device.
+ * @promiscuous:	The promiscuous state of the device.
+ */
+struct dev_priv {
+	void *adapter;
+	struct net_device *dev;
+	void *parent;
+	struct ksz_port port;
+	struct ksz_timer_info monitor_timer_info;
+	struct net_device_stats stats;
+
+	struct phy_device dummy_phy;
+	struct phy_device *phydev;
+	struct work_struct phy_pause;
+
+	int id;
+
+	struct mii_if_info mii_if;
+	u32 advertising;
+
+	u32 msg_enable;
+	int media_state;
+	int multicast;
+	int promiscuous;
+	u8 phy_addr;
+	u8 state;
+	u8 multi_list_size;
+
+#ifdef MAX_MULTICAST_LIST
+	u8 multi_list[MAX_MULTICAST_LIST][ETH_ALEN];
+#endif
+};
+
+#endif
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_stp.c b/drivers/net/ethernet/micrel/ksz9897/ksz_stp.c
new file mode 100644
index 0000000..9b5d228
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_stp.c
@@ -0,0 +1,5058 @@
+/**
+ * Microchip RSTP code
+ *
+ * Copyright (c) 2016-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 0
+#define DBG_STP_STATE
+#ifdef DBG_STP_STATE
+#if 1
+#endif
+#if 0
+#define DBG_STP_STATE_RX
+#define DBG_STP_STATE_PROTO
+#define DBG_STP_STATE_INFO
+#define DBG_STP_STATE_ROLE_TR
+#define DBG_STP_STATE_TX
+#define DBG_STP_STATE_TR
+#define DBG_STP_STATE_TOPOLOGY
+#endif
+#endif
+#endif
+
+#if 0
+#define DBG_STP_PORT_BLOCK
+#endif
+#if 0
+#define DBG_STP_PORT_FLUSH
+#endif
+#if 0
+#define DBG_STP_ROLE
+#endif
+#if 0
+#define DBG_STP_RX
+#endif
+#if 0
+#define DBG_STP_TX
+#endif
+
+
+#ifndef FALSE
+#define FALSE	0
+#define TRUE	1
+#endif
+
+
+#define BPDU_TYPE_CONFIG	0
+#define BPDU_TYPE_CONFIG_RSTP	2
+#define BPDU_TYPE_TCN		0x80
+
+
+#define TOPOLOGY_CHANGE		(1 << 0)
+#define PROPOSAL		(1 << 1)
+#define PORT_ROLE_S		2
+#define PORT_ROLE_ALTERNATE	1
+#define PORT_ROLE_ROOT		2
+#define PORT_ROLE_DESIGNATED	3
+#define LEARNING		(1 << 4)
+#define FORWARDING		(1 << 5)
+#define AGREEMENT		(1 << 6)
+#define TOPOLOGY_CHANGE_ACK	(1 << 7)
+
+
+static u16 get_bpdu_time(u16 time)
+{
+	int val;
+
+	val = ntohs(time);
+
+	/* Round up to whole second. */
+	val += 255;
+	return (u16)(val / 256);
+}  /* get_bpdu_time */
+
+static void set_bpdu_time(u16 *dst, u16 src)
+{
+	if (src < 256) {
+		src *= 256;
+		*dst = htons(src);
+	} else
+		*dst = 0xffff;
+}  /* set_bpdu_time */
+
+static void set_bpdu_times(struct bpdu *bpdu, struct stp_times *t)
+{
+	set_bpdu_time(&bpdu->message_age, t->message_age);
+	set_bpdu_time(&bpdu->max_age, t->max_age);
+	set_bpdu_time(&bpdu->hello_time, t->hello_time);
+	set_bpdu_time(&bpdu->forward_delay, t->forward_delay);
+}  /* set_bpdu_times */
+
+static void prep_bpdu(struct bpdu *bpdu, struct stp_prio *p,
+	struct stp_times *t)
+{
+	bpdu->protocol = 0;
+	bpdu->flags = 0;
+	memcpy(&bpdu->root, p, sizeof(struct stp_prio));
+	set_bpdu_times(bpdu, t);
+	bpdu->version_1_length = 0;
+}  /* prep_bpdu */
+
+static void prep_stp(struct bpdu *bpdu, struct stp_prio *p,
+	struct stp_times *t)
+{
+	prep_bpdu(bpdu, p, t);
+	bpdu->version = 0;
+	bpdu->type = BPDU_TYPE_CONFIG;
+}  /* prep_stp */
+
+static void prep_rstp(struct bpdu *bpdu, struct stp_prio *p,
+	struct stp_times *t)
+{
+	prep_bpdu(bpdu, p, t);
+	bpdu->version = 2;
+	bpdu->type = BPDU_TYPE_CONFIG_RSTP;
+}  /* prep_rstp */
+
+#if defined(DBG_STP_RX) || defined(DBG_STP_TX)
+static void disp_bpdu(struct bpdu *bpdu)
+{
+	u8 role;
+
+	if (BPDU_TYPE_TCN != bpdu->type) {
+		dbg_msg("%04x=%02x%02x%02x%02x%02x%02x "
+			"%04x=%02x%02x%02x%02x%02x%02x:"
+			"%02x%02x %u\n",
+			ntohs(bpdu->root.prio),
+			bpdu->root.addr[0],
+			bpdu->root.addr[1],
+			bpdu->root.addr[2],
+			bpdu->root.addr[3],
+			bpdu->root.addr[4],
+			bpdu->root.addr[5],
+			ntohs(bpdu->bridge_id.prio),
+			bpdu->bridge_id.addr[0],
+			bpdu->bridge_id.addr[1],
+			bpdu->bridge_id.addr[2],
+			bpdu->bridge_id.addr[3],
+			bpdu->bridge_id.addr[4],
+			bpdu->bridge_id.addr[5],
+			bpdu->port_id.prio, bpdu->port_id.num,
+			ntohl(bpdu->root_path_cost));
+		dbg_msg("%u %u %u %u  ",
+			htons(bpdu->message_age) / 256,
+			htons(bpdu->max_age) / 256,
+			htons(bpdu->hello_time) / 256,
+			htons(bpdu->forward_delay) / 256);
+		dbg_msg("%02X:", bpdu->flags);
+		if (bpdu->flags & TOPOLOGY_CHANGE_ACK)
+			dbg_msg("K");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & AGREEMENT)
+			dbg_msg("A");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & FORWARDING)
+			dbg_msg("F");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & LEARNING)
+			dbg_msg("L");
+		else
+			dbg_msg("-");
+		role = bpdu->flags >> PORT_ROLE_S;
+		role &= PORT_ROLE_DESIGNATED;
+		switch (role) {
+		case PORT_ROLE_ALTERNATE:
+			dbg_msg("N");
+			break;
+		case PORT_ROLE_ROOT:
+			dbg_msg("R");
+			break;
+		case PORT_ROLE_DESIGNATED:
+			dbg_msg("D");
+			break;
+		default:
+			if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+				dbg_msg("?");
+			else
+				dbg_msg("-");
+		}
+		if (bpdu->flags & PROPOSAL)
+			dbg_msg("P");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & TOPOLOGY_CHANGE)
+			dbg_msg("T");
+		else
+			dbg_msg("-");
+	}
+	dbg_msg("  %04x %u %02x",
+		htons(bpdu->protocol),
+		bpdu->version, bpdu->type);
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+		dbg_msg(" %u", bpdu->version_1_length);
+	dbg_msg("\n");
+}
+#endif
+
+static struct bpdu *chk_bpdu(u8 *data, u16 *size)
+{
+	struct llc *llc = (struct llc *) &data[12];
+	u16 len = ntohs(llc->len);
+
+	if (len < 1500) {
+		if (0x42 == llc->dsap &&
+		    0x42 == llc->ssap &&
+		    0x03 == llc->ctrl)
+		if (size)
+			*size = len - 3;
+		return (struct bpdu *)(llc + 1);
+	}
+	return NULL;
+}  /* chk_bpdu */
+
+
+#define STP_TIMER_TICK		200
+#define STP_TIMER_SCALE		1000
+
+#define to_stp_timer(x)		((x) * STP_TIMER_SCALE)
+#define from_stp_timer(x)	((x) / STP_TIMER_SCALE)
+#define NEQ(x, y)		(abs((y) - (x)) >= STP_TIMER_SCALE)
+
+
+#define BridgeIdentifier	(p->br->vars.br_id_)
+#define BridgePriority		(p->br->vars.bridgePrio_)
+#define BridgeTimes		(p->br->vars.bridgeTimes_)
+#define rootPortId		(p->br->vars.rootPortId_)
+#define rootPriority		(p->br->vars.rootPrio_)
+#define rootTimes		(p->br->vars.rootTimes_)
+
+#define timeSinceTC		(p->br->vars.TC_sec_)
+#define cntTC			(p->br->vars.TC_cnt_)
+#define isTC			(p->br->vars.TC_set_)
+
+/* MigrateTime is only used internally for timer. */
+#define MigrateTime		(p->br->vars.MigrateTime_)
+#define TxHoldCount		(p->br->vars.TxHoldCount_)
+#define ForceProtocolVersion	(p->br->vars.ForceProtocolVersion_)
+
+
+#define edgeDelayWhile		(p->vars.timers[0])
+#define fdWhile			(p->vars.timers[1])
+#define helloWhen		(p->vars.timers[2])
+#define mdelayWhile		(p->vars.timers[3])
+#define rbWhile			(p->vars.timers[4])
+#define rcvdInfoWhile		(p->vars.timers[5])
+#define rrWhile			(p->vars.timers[6])
+#define tcWhile			(p->vars.timers[7])
+
+/* For MRP. */
+#define tcDetected		(p->vars.timers[8])
+
+#define tcPropWhile		(p->vars.timers[9])
+
+#define AdminEdgePort		(p->vars.admin_var.AdminEdgePort_)
+#define AdminPortPathCost	(p->vars.admin_var.adminPortPathCost_)
+#define AutoEdgePort		(p->vars.admin_var.AutoEdgePort_)
+#define adminPointToPointMAC	(p->vars.admin_var.adminPointToPointMAC_)
+#define operPointToPointMAC	(p->vars.admin_var.operPointToPointMAC_)
+
+#define ageingTime		(p->vars.stp_var.ageingTime_)
+#define agree			(p->vars.stp_var.agree_)
+#define agreed			(p->vars.stp_var.agreed_)
+#define designatedPriority	(p->vars.stp_var.desgPrio_)
+#define designatedTimes		(p->vars.stp_var.desgTimes_)
+#define disputed		(p->vars.stp_var.disputed_)
+#define fdbFlush		(p->vars.stp_var.fdbFlush_)
+#define forward			(p->vars.stp_var.forward_)
+#define forwarding		(p->vars.stp_var.forwarding_)
+#define infoIs			(p->vars.stp_var.infoIs_)
+#define learn			(p->vars.stp_var.learn_)
+#define learning		(p->vars.stp_var.learning_)
+#define mcheck			(p->vars.stp_var.mcheck_)
+#define msgPriority		(p->vars.stp_var.msgPrio_)
+#define msgTimes		(p->vars.stp_var.msgTimes_)
+#define newInfo			(p->vars.stp_var.newInfo_)
+#define operEdge		(p->vars.stp_var.operEdge_)
+#define portEnabled		(p->vars.stp_var.portEnabled_)
+#define portId			(p->vars.stp_var.portId_)
+#define PortPathCost		(p->vars.stp_var.PortPathCost_)
+#define portPriority		(p->vars.stp_var.portPrio_)
+#define portTimes		(p->vars.stp_var.portTimes_)
+#define proposed		(p->vars.stp_var.proposed_)
+#define proposing		(p->vars.stp_var.proposing_)
+#define rcvdBPDU		(p->vars.stp_var.rcvdBPDU_)
+#define rcvdInfo		(p->vars.stp_var.rcvdInfo_)
+#define rcvdMsg			(p->vars.stp_var.rcvdMsg_)
+#define rcvdRSTP		(p->vars.stp_var.rcvdRSTP_)
+#define rcvdSTP			(p->vars.stp_var.rcvdSTP_)
+#define rcvdTc			(p->vars.stp_var.rcvdTc_)
+#define rcvdTcAck		(p->vars.stp_var.rcvdTcAck_)
+#define rcvdTcn			(p->vars.stp_var.rcvdTcn_)
+#define reRoot			(p->vars.stp_var.reRoot_)
+#define reselect		(p->vars.stp_var.reselect_)
+#define role			(p->vars.stp_var.role_)
+#define selected		(p->vars.stp_var.selected_)
+#define selectedRole		(p->vars.stp_var.selectedRole_)
+#define sendRSTP		(p->vars.stp_var.sendRSTP_)
+#define sync			(p->vars.stp_var.sync_)
+#define synced			(p->vars.stp_var.synced_)
+#define tcAck			(p->vars.stp_var.tcAck_)
+#define tcProp			(p->vars.stp_var.tcProp_)
+#define tick			(p->vars.stp_var.tick_)
+#define txCount			(p->vars.stp_var.txCount_)
+#define updtInfo		(p->vars.stp_var.updtInfo_)
+
+#define bpduVersion		(p->vars.bpduVersion_)
+#define bpduType		(p->vars.bpduType_)
+#define bpduFlags		(p->vars.bpduFlags_)
+#define bpduRole		(p->vars.bpduRole_)
+#define bpduPriority		(p->vars.bpduPrio_)
+#define bpduTimes		(p->vars.bpduTimes_)
+
+#define DesignatedPort		(ROLE_DESIGNATED == role)
+#define RootPort		(ROLE_ROOT == role)
+#define AlternatePort		(ROLE_ALTERNATE == role)
+#define BackupPort		(ROLE_BACKUP == role)
+#define DisabledPort		(ROLE_DISABLED == role)
+
+#define ForwardPort		\
+	(DesignatedPort || RootPort)
+
+#define BridgeFwdDelay		(BridgeTimes.forward_delay)
+#define BridgeHelloTime		(BridgeTimes.hello_time)
+#define BridgeMaxAge		(BridgeTimes.max_age)
+
+#define AdminEdge		AdminEdgePort
+#define AutoEdge		AutoEdgePort
+
+#define EdgeDelay()		(operPointToPointMAC ? MigrateTime : MaxAge)
+#define forwardDelay()		(sendRSTP ? HelloTime : FwdDelay)
+
+/* These times are in timer unit. */
+#define FwdDelay		to_stp_timer(designatedTimes.forward_delay)
+#define HelloTime		to_stp_timer(designatedTimes.hello_time)
+#define MaxAge			to_stp_timer(designatedTimes.max_age)
+
+#define rstpVersion		(ForceProtocolVersion >= 2)
+#define stpVersion		(ForceProtocolVersion < 2)
+
+
+/* Shortcuts for some common qualifications. */
+#define canChange		(selected && !updtInfo)
+#define canSend			\
+	(newInfo && (txCount < TxHoldCount) && (helloWhen != 0))
+
+/*
+ * agree means the port accepts incoming Designated Port, and AGREEMNT flag is
+ * set.
+ * There is a case that after becoming Designated Port agree is never reset.
+ *
+ * agreed means the proposing Designated Port receives AGREEMENT and so can
+ * stop proposing and can enable learning and forwarding immediately.
+ * Not operPointToPointMAC causes that not to happen and the Designated Port
+ * keeps proposing, but half-duplex connection will cause operPointToPoint to
+ * be FALSE.  In old time half-duplex was associated with hub and what is
+ * called Shared Media.  The sent BPDU may be dropped because of collisions,
+ * but that is not what happens here.
+ * agreed is used to qualify synced after betterorsameInfo call for Designated
+ * Port.
+ *
+ * proposing means the Designated Port is asking approval before opening the
+ * port, and PROPOSAL flag is set.
+ * It is set when the Designated Port is not forwarding.  It is reset when
+ * AGREEMENT flag is received, or when infoIs is set to Mine.
+ * Does it mean in !operPointToPointMAC the PROPOSAL flag will be dropped when
+ * the Designated Port information is changed?!
+ * There is a case that after becoming Root Port proposing is not reset.  It
+ * should not be set in DESIGNATED_PROPOSE state.
+ *
+ * proposed means receiving PROPOSAL from Desginated Port.  It will be reset
+ * either agree is TRUE or not.  The only difference is setSyncTree will be
+ * called when agree is FALSE.
+ *
+ * sync is set in the setSyncTree call.  It is reset when synced is set.
+ * It is reset for Root Port but not Alternate Port.  It is required to be
+ * FALSE for Designated Port to move to learning state.  If it is TRUE then
+ * Designated Port moves to discarding state.
+ *
+ * synced is used in the allSynced call.  That call is only used to get into
+ * _AGREED states in which proposed is reset and agree is set and a BPDU will
+ * be sent.
+ *
+ * tcAck is used by Designated Port to set TOPOLOGY_CHANGE_ACK flag in response
+ * of TOPOLOGY_CHANGE flag.
+ *
+ * rcvdTc is not accepted from inferior designated port.
+ *
+ * rcvdTcAck is used by Root Port to stop sending topology change immediately.
+ *
+ * newInfo is set to send out BPDU.  It is checked periodically so that
+ * Designated Port can always send but Root Port only sends when there is a
+ * topology change.
+ * It is set when agree is set the first time.  However, Alternate Port does
+ * not always send after becoming one.
+ * There is a case a BPDU is sent during initializion when the port role is not
+ * defined yet.  It should be handled in the _DISABLE_PORT state.
+ * HelloTime is zero when Port Transmit state machine starts.  Because of that
+ * PERIODIC is called instead.  Is that the original intention?
+ * However, newInfo is still not cleared because of the OR operation.
+ *
+ * reselect means port information is being changed so selected will not be set
+ * to TRUE.  Typically selected is set to FALSE while reselect is set so that
+ * role selection is done.
+ *
+ * selected means the role selection is completed so states can be changed.  It
+ * is set to FALSE when infoIs is changed.
+ *
+ * updtInfo is set when the port is becoming Designated Port or its parameters
+ * are changed.  It is reset when the Designated Port sets infoIs to Mine.  It
+ * is explicitly set to FALSE for other port roles.
+ * !updtInfo is required in many state changes.  Basically it means the
+ * Designated Port needs to set infoIs to Mine before moving to other states.
+ * There is a case that a Designated Port changing to Root Port will set
+ * proposing, even though it will not be a Designated Port.
+ *
+ * infoIs is initialized in Port Information state machine, so it should be run
+ * before Port Role Selection state machine.
+ *
+ * selectedRole is initialized by updtRoleDisabledTree in INIT_BRIDGE, so
+ * Port Role Selection state machine should be run before Port Role Transitions
+ * state machine.
+ *
+ * There is a problem that selectedRole is changed to DesignatedPort but
+ * DISABLE_PORT changes role like selectedRole is DisabledPort and is stuck in
+ * DISABLED_PORT state.
+ *
+ * MaxAge is initialized in updtRolesTree, so INIT_PORT should be called after
+ * that.
+ */
+
+
+#define CMP(first, second)	memcmp(&first, &second, sizeof(first))
+#define COPY(first, second)	memcpy(&first, &second, sizeof(first))
+#define ZERO(first)		memset(&first, 0, sizeof(first))
+
+
+#define COPY_PRIO(vector, p, id)	\
+	memcpy(&(vector.prio), &p, sizeof(p));	\
+	memcpy(&(vector.port_id), &id, sizeof(id));
+
+
+static int superiorPriority(struct stp_prio *first, struct stp_prio *second)
+{
+	int prio;
+
+	prio = memcmp(first, second, sizeof(struct stp_prio));
+	if (prio > 0 &&
+	    !memcmp(first->bridge_id.addr, second->bridge_id.addr, ETH_ALEN) &&
+	    first->port_id.num == second->port_id.num)
+		prio = -256;
+#ifdef DBG_STP_ROLE
+if (prio < -127)
+dbg_msg("  %s same br/port\n", __func__);
+#endif
+	return (prio < 0);
+}
+
+static void dbgPriority(struct stp_prio *prio, struct _port_id *id)
+{
+	int j;
+	u8 *data;
+
+	data = (u8 *) prio;
+	for (j = 0; j < sizeof(struct stp_prio); j++) {
+		if (8 == j || 12 == j)
+			dbg_msg(" ");
+		dbg_msg("%02x", data[j]);
+	}
+	if (id) {
+		dbg_msg(" ");
+		data = (u8 *) id;
+		for (j = 0; j < sizeof(struct _port_id); j++)
+			dbg_msg("%02x", data[j]);
+	}
+	dbg_msg("\n");
+}
+
+static int betterSamePriority(struct stp_prio *first, struct stp_prio *second)
+{
+	int prio;
+
+	prio = memcmp(first, second, sizeof(struct stp_prio));
+	return (prio <= 0);
+}
+
+static int betterVector(struct stp_vector *first, struct stp_vector *second)
+{
+	int prio;
+
+	prio = memcmp(&first->prio, &second->prio, sizeof(struct stp_prio));
+	if (!prio) {
+#ifdef DBG_STP_ROLE
+dbg_msg(" same prio\n");
+		dbgPriority(&first->prio, &first->port_id);
+		dbgPriority(&second->prio, &second->port_id);
+#endif
+		prio = memcmp(&first->port_id, &second->port_id,
+			sizeof(struct _port_id));
+	}
+	return (prio < 0);
+}
+
+
+#define FOREACH_P_IN_T(statements...)					\
+{									\
+	int c;								\
+	for (c = 0; c < p->br->port_cnt; c++) {				\
+		p = &p->br->ports[c];					\
+		statements						\
+	}								\
+}
+
+
+#if defined(DBG_STP_STATE) || defined(DBG)
+static void dbg_stp(struct ksz_stp_port *p, const char *msg, int rx)
+{
+	if (rx && !p->dbg_rx)
+		return;
+	dbg_msg(" %d=", p->port_index);
+	switch (role) {
+	case ROLE_DISABLED:
+		dbg_msg("Z");
+		break;
+	case ROLE_DESIGNATED:
+		dbg_msg("D");
+		break;
+	case ROLE_ROOT:
+		dbg_msg("R");
+		break;
+	case ROLE_ALTERNATE:
+		dbg_msg("A");
+		break;
+	case ROLE_BACKUP:
+		dbg_msg("B");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	switch (selectedRole) {
+	case ROLE_DISABLED:
+		dbg_msg("z");
+		break;
+	case ROLE_DESIGNATED:
+		dbg_msg("d");
+		break;
+	case ROLE_ROOT:
+		dbg_msg("r");
+		break;
+	case ROLE_ALTERNATE:
+		dbg_msg("a");
+		break;
+	case ROLE_BACKUP:
+		dbg_msg("b");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	dbg_msg(" ");
+	switch (infoIs) {
+	case INFO_TYPE_DISABLED:
+		dbg_msg("Z");
+		break;
+	case INFO_TYPE_MINE:
+		dbg_msg("M");
+		break;
+	case INFO_TYPE_AGED:
+		dbg_msg("A");
+		break;
+	case INFO_TYPE_RECEIVED:
+		dbg_msg("R");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	dbg_msg(" ");
+	dbg_msg("%c", agree ? 'A' : '.');
+	dbg_msg("%c", agreed ? 'a' : '.');
+	dbg_msg("%c", proposing ? 'P' : '.');
+	dbg_msg("%c", proposed ? 'p' : '.');
+	dbg_msg("%c", sync ? 'S' : '.');
+	dbg_msg("%c", synced ? 's' : '.');
+	dbg_msg("%c", learn ? 'L' : '.');
+	dbg_msg("%c", learning ? 'l' : '.');
+	dbg_msg("%c", forward ? 'F' : '.');
+	dbg_msg("%c", forwarding ? 'f' : '.');
+	dbg_msg("%c", disputed ? 'D' : '.');
+	dbg_msg("%c", operEdge ? 'E' : '.');
+	dbg_msg("%c", reRoot ? 'R' : '.');
+	dbg_msg("%c", tcProp ? 'T' : '.');
+	dbg_msg("%c", rcvdTc ? 'C' : '.');
+	dbg_msg("  ");
+	for (rx = 0; rx < NUM_OF_PORT_STATE_MACHINES; rx++)
+		dbg_msg("%2d", p->states[rx]);
+	dbg_msg("  %s\n", msg);
+}  /* dbg_stp */
+
+static void d_stp_states(struct ksz_stp_bridge *br)
+{
+	int i;
+	struct ksz_stp_port *p;
+
+	dbg_msg("\n");
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		dbg_stp(p, "", false);
+	}
+}
+#endif
+
+
+#define PATH_COST		20000000
+
+static uint computePathCost(int speed)
+{
+	if (speed > 0)
+		return speed < PATH_COST ? PATH_COST / speed : 1;
+	else
+		return PATH_COST * 10;
+}  /* computePathCost */
+
+static int checkP2P(struct ksz_stp_port *p)
+{
+	u8 p2p;
+
+	if (ADMIN_P2P_AUTO == adminPointToPointMAC)
+		p2p = !!p->duplex;
+	else
+		p2p = (ADMIN_P2P_FORCE_TRUE == adminPointToPointMAC);
+	if (p2p != operPointToPointMAC) {
+		operPointToPointMAC = p2p;
+		return TRUE;
+	}
+	return FALSE;
+}  /* checkP2P */
+
+static int checkPathCost(struct ksz_stp_port *p)
+{
+	uint pathCost;
+
+	if (!AdminPortPathCost)
+		pathCost = computePathCost(p->speed);
+	else
+		pathCost = AdminPortPathCost;
+	if (pathCost != PortPathCost) {
+		PortPathCost = pathCost;
+		reselect = TRUE;
+		selected = FALSE;
+		return TRUE;
+	}
+	return FALSE;
+}  /* checkPathCost */
+
+static int checkParameters(int hello_time, int max_age, int fwd_delay)
+{
+	if (max_age < 6 || max_age > 40)
+		return FALSE;
+	if (fwd_delay < 4 || fwd_delay > 30)
+		return FALSE;
+	if (2 * (fwd_delay - 1) < max_age)
+		return FALSE;
+	if (max_age < 2 * (hello_time + 1))
+		return FALSE;
+	return TRUE;
+}  /* checkParameters */
+
+static void sw_cfg_forwarding(struct ksz_sw *sw, uint port, bool open)
+{
+	struct ksz_sw_info *info = sw->info;
+	u8 member = info->member[0];
+
+	if (open)
+		member |= (1 << port);
+	else
+		member &= ~(1 << port);
+	if (member != info->member[0]) {
+#if 0
+		int cnt = 0;
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+		for (port = 0; port < sw->mib_port_cnt; port++) {
+			if (skip_host_port(sw, port))
+				continue;
+#else
+		for (port = 0; port < SWITCH_PORT_NUM; port++) {
+#endif
+			if (member & (1 << port))
+				cnt++;
+		}
+#endif
+
+		info->member[0] = member;
+		bridge_change(sw);
+#ifdef CONFIG_KSZ_MRP
+		if (open && (sw->features & MRP_SUPPORT)) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			mrp_open_port(mrp, port);
+		}
+#endif
+	}
+}  /* sw_cfg_forwarding */
+
+static void doFlush_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_port *q;
+	struct ksz_stp_dbg_times *x;
+	struct ksz_stp_dbg_times *y;
+#endif
+
+	if (operEdge) {
+dbg_msg(" no flush\n");
+		fdbFlush = FALSE;
+		return;
+	}
+	sw->ops->acquire(sw);
+	sw_flush_dyn_mac_table(sw, i);
+	sw->ops->release(sw);
+	fdbFlush = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+	q = p;
+	y = &q->dbg_times[0];
+	for (i = 0; i < p->br->port_cnt; i++) {
+		p = &p->br->ports[i];
+		if (q == p)
+			continue;
+		x = &p->dbg_times[0];
+		if (y->lastPriority.port_id.num) {
+			int cmp = memcmp(&y->lastPriority.bridge_id,
+				&x->downPriority.bridge_id,
+				sizeof(struct _bridge_id));
+			if (!cmp && x->learn_jiffies) {
+dbg_msg(" %ld [%ld] %02x%02x ", jiffies - y->alt_jiffies,
+jiffies - x->learn_jiffies, y->lastPriority.port_id.prio, y->lastPriority.port_id.num);
+				x->learn_jiffies = 0;
+				y->lastPriority.port_id.num = 0;
+				y->learn_jiffies = 0;
+				y->alt_jiffies = 0;
+			}
+		}
+	}
+	p = q;
+	switch (y->role_ & ~0x80) {
+	case PORT_ROLE_ROOT:
+		dbg_msg("  R");
+		break;
+	case PORT_ROLE_ALTERNATE:
+		dbg_msg("  A");
+		break;
+	case PORT_ROLE_DESIGNATED:
+		if (RootPort)
+			dbg_msg("  r");
+		else if (AlternatePort)
+			dbg_msg("  a");
+		else
+			dbg_msg("  d");
+		break;
+	default:
+		if (DisabledPort)
+			dbg_msg("  Z");
+		else if (DesignatedPort)
+			dbg_msg("  D");
+		break;
+	}
+dbg_msg("  F:%d\n", q->port_index);
+#endif
+}  /* doFlush_ */
+
+#define doFlush()			doFlush_(p)
+
+static void stp_chk_flush(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_port *q;
+	struct ksz_stp_bridge *br = p->br;
+	uint i;
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+	struct ksz_stp_dbg_times *y;
+
+	if (bpduRole == x->role_)
+		return;
+	x->role_ = bpduRole;
+	if (rcvdTc || !learning) {
+		int flush = 0;
+
+		for (i = 0; i < br->port_cnt; i++) {
+			q = &br->ports[i];
+			if (q == p)
+				continue;
+			y = &q->dbg_times[0];
+			if (y->downPriority.port_id.num) {
+				int cmp = memcmp(&y->downPriority.bridge_id,
+					&msgPriority.bridge_id,
+					sizeof(struct _bridge_id));
+				if (!cmp) {
+					if (learning)
+						x->learn_jiffies = jiffies;
+					flush = q->port_index + 1;
+					y->role_ = ROLE_ALT_BACKUP | 0x80;
+					break;
+				}
+			}
+		}
+dbg_msg("  %s %d=%d\n", __func__, p->port_index, flush);
+	}
+	if (bpduRole == ROLE_ALT_BACKUP)
+		x->alt_jiffies = jiffies;
+	if (bpduRole == ROLE_ROOT) {
+		COPY(x->downPriority, msgPriority);
+		COPY(x->lastPriority, msgPriority);
+	}
+#endif
+}
+
+static int allSynced_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+	int ret = TRUE;
+	int all_others = FALSE;
+	int skip_root = FALSE;
+
+	if (DesignatedPort)
+		all_others = TRUE;
+	else if (!DisabledPort)
+		skip_root = TRUE;
+
+	/* Check only when role is not in transition. */
+	FOREACH_P_IN_T(
+		if (!(selected && (role == selectedRole) && !updtInfo)) {
+#if 0
+dbg_msg(" allSync: %d= %d %d %d %d\n", i, selected, role, selectedRole, synced);
+#endif
+			ret = FALSE;
+			break;
+		} else {
+			if ((all_others && p == q) ||
+			    (skip_root && RootPort))
+				continue;
+			if (!synced) {
+				ret = FALSE;
+				break;
+			}
+		}
+	)
+	return ret;
+}
+
+#define allSynced()			allSynced_(p)
+
+static int reRooted_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+	int ret = TRUE;
+
+	FOREACH_P_IN_T(
+		if (p == q)
+			continue;
+		if (0 != rrWhile) {
+			ret = FALSE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define reRooted()			reRooted_(p)
+
+static int betterorsameInfo_(struct ksz_stp_port *p, u8 newInfoIs)
+{
+	int ret = FALSE;
+
+	if (INFO_TYPE_RECEIVED == newInfoIs && INFO_TYPE_RECEIVED == infoIs) {
+		ret = betterSamePriority(&msgPriority, &portPriority);
+#ifdef DBG_STP_ROLE
+dbg_msg("  recd: %d\n", ret);
+#endif
+	} else if (INFO_TYPE_MINE == newInfoIs && INFO_TYPE_MINE == infoIs) {
+		ret = betterSamePriority(&designatedPriority, &portPriority);
+#ifdef DBG_STP_ROLE
+dbg_msg("  mine: %d\n", ret);
+#endif
+#if 0
+	} else if (INFO_TYPE_MINE == newInfoIs && INFO_TYPE_AGED == infoIs) {
+		ret = betterSamePriority(&designatedPriority, &portPriority);
+dbgPriority(&designatedPriority, NULL);
+dbgPriority(&portPriority, NULL);
+#endif
+	}
+#ifdef DBG_STP_ROLE
+dbg_msg("%s %d\n", __func__, ret);
+#endif
+	return ret;
+}
+
+#define betterorsameInfo(i)		betterorsameInfo_(p, i)
+
+static void clearReselectTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		reselect = FALSE;
+	)
+}
+
+#define clearReselectTree()		clearReselectTree_(br)
+
+static void disableForwarding_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	if (!learning) {
+#ifdef DBG_STP_PORT_BLOCK
+		struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+		x->block_jiffies = jiffies;
+#endif
+		if (portEnabled)
+			port_set_stp_state(sw, i, STP_STATE_BLOCKED);
+		sw_cfg_forwarding(sw, i, false);
+	} else {
+		port_cfg_rx(sw, i, false);
+		port_cfg_tx(sw, i, false);
+	}
+	sw->ops->release(sw);
+}
+
+#define disableForwarding()		disableForwarding_(p)
+
+static void disableLearning_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_dis_learn(sw, i, true);
+	sw->ops->release(sw);
+}
+
+#define disableLearning()		disableLearning_(p)
+
+static void enableForwarding_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+#ifdef DBG_STP_PORT_BLOCK
+	do {
+		struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+		if (x->block_jiffies)
+dbg_msg("  B:%d=%ld\n", p->port_index, jiffies - x->block_jiffies);
+		x->block_jiffies = 0;
+	} while (0);
+#endif
+	if (learning) {
+		port_set_stp_state(sw, i, STP_STATE_FORWARDING);
+		sw_cfg_forwarding(sw, i, true);
+	} else {
+		port_cfg_rx(sw, i, true);
+		port_cfg_tx(sw, i, true);
+	}
+	sw->ops->release(sw);
+}
+
+#define enableForwarding()		enableForwarding_(p)
+
+static void enableLearning_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_dis_learn(sw, i, false);
+	sw->ops->release(sw);
+#ifdef DBG_STP_PORT_BLOCK
+	p->dbg_times[0].learn_jiffies = jiffies;
+#endif
+}
+
+#define enableLearning()		enableLearning_(p)
+
+static void newTcDetected_(struct ksz_stp_port *p)
+{
+	if (tcDetected)
+		return;
+	if (sendRSTP) {
+		tcDetected = to_stp_timer(portTimes.hello_time + 1);
+	} else {
+		tcDetected = rootTimes.max_age + rootTimes.forward_delay;
+		tcDetected = to_stp_timer(tcDetected);
+	}
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->tc_detected(sw, p->port_index);
+	} while (0);
+}
+
+#define newTcDetected()			newTcDetected_(p)
+
+static void newTcWhile_(struct ksz_stp_port *p)
+{
+	if (tcWhile)
+		return;
+	if (sendRSTP) {
+		tcWhile = HelloTime + to_stp_timer(1);
+		newInfo = TRUE;
+	} else {
+		tcWhile = rootTimes.max_age + rootTimes.forward_delay;
+		tcWhile = to_stp_timer(tcWhile);
+	}
+	if (!isTC) {
+		timeSinceTC = 0;
+		cntTC++;
+	}
+	isTC |= (1 << p->port_index);
+}
+
+#define newTcWhile()			newTcWhile_(p)
+
+static u8 rcvInfo_(struct ksz_stp_port *p)
+{
+	int prio;
+	int time;
+
+	if (BPDU_TYPE_TCN == bpduType) {
+		rcvdTcn = TRUE;
+		return INFO_OTHER;
+	}
+
+	if (BPDU_TYPE_CONFIG == bpduType)
+		bpduRole = ROLE_DESIGNATED;
+
+	COPY(msgPriority, bpduPriority);
+	COPY(msgTimes, bpduTimes);
+
+	prio = CMP(msgPriority, portPriority);
+	time = CMP(msgTimes, portTimes);
+	if (bpduRole == ROLE_DESIGNATED) {
+		if (superiorPriority(&msgPriority, &portPriority))
+			return INFO_SUPERIOR_DESIGNATED;
+
+		if (0 == prio && time != 0)
+			return INFO_SUPERIOR_DESIGNATED;
+
+		if (0 == prio && 0 == time)
+			return INFO_REPEATED_DESIGNATED;
+
+		if (prio > 0)
+			return INFO_INFERIOR_DESIGNATED;
+	}
+
+	if ((bpduRole == ROLE_ROOT || bpduRole == ROLE_ALT_BACKUP) &&
+	    prio >= 0)
+		return INFO_INFERIOR_ROOT_ALT;
+
+	return INFO_OTHER;
+}
+
+#define rcvInfo()			rcvInfo_(p)
+
+static void recordAgreement_(struct ksz_stp_port *p)
+{
+	/*
+	 * Not operPointToPointMAC will keep proposing and root port sending
+	 * agreement forever.
+	 */
+	if (rstpVersion && operPointToPointMAC &&
+	    (bpduFlags & AGREEMENT)) {
+		agreed = TRUE;
+		proposing = FALSE;
+	} else {
+		agreed = FALSE;
+	}
+}
+
+#define recordAgreement()		recordAgreement_(p)
+
+static void recordDispute_(struct ksz_stp_port *p)
+{
+	if ((bpduFlags & LEARNING)) {
+		disputed = TRUE;
+		agreed = FALSE;
+	}
+}
+
+#define recordDispute()			recordDispute_(p)
+
+static void recordPriority_(struct ksz_stp_port *p)
+{
+	COPY(portPriority, msgPriority);
+}
+
+#define recordPriority()		recordPriority_(p)
+
+static void recordProposal_(struct ksz_stp_port *p)
+{
+	if (bpduRole == ROLE_DESIGNATED && (bpduFlags & PROPOSAL))
+		proposed = TRUE;
+}
+
+#define recordProposal()		recordProposal_(p)
+
+#define MIN_COMPAT_HELLO_TIME		1
+
+static void recordTimes_(struct ksz_stp_port *p)
+{
+	if (checkParameters(2, msgTimes.max_age, msgTimes.forward_delay)) {
+		COPY(portTimes, msgTimes);
+
+		/* portTimes.hello_time is used to determine rcvdInfoWhile. */
+		if (portTimes.hello_time < MIN_COMPAT_HELLO_TIME)
+			portTimes.hello_time = MIN_COMPAT_HELLO_TIME;
+	} else
+		portTimes.message_age = msgTimes.message_age;
+}
+
+#define recordTimes()			recordTimes_(p)
+
+static void setReRootTree_(struct ksz_stp_port *p)
+{
+	FOREACH_P_IN_T(
+		reRoot = TRUE;
+	)
+}
+
+#define setReRootTree()			setReRootTree_(p)
+
+static void setSelectedTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		if (reselect)
+			return;
+	)
+	FOREACH_P_IN_T(
+		selected = TRUE;
+	)
+}
+
+#define setSelectedTree()		setSelectedTree_(br)
+
+static void setSyncTree_(struct ksz_stp_port *p)
+{
+	FOREACH_P_IN_T(
+		sync = TRUE;
+	)
+}
+
+#define setSyncTree()			setSyncTree_(p)
+
+static void setTcFlags_(struct ksz_stp_port *p)
+{
+	if (bpduFlags & TOPOLOGY_CHANGE)
+		rcvdTc = TRUE;
+	if (bpduFlags & TOPOLOGY_CHANGE_ACK)
+		rcvdTcAck = TRUE;
+}
+
+#define setTcFlags()			setTcFlags_(p)
+
+static void setTcPropTree_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+
+#if 1
+	if (sendRSTP) {
+		if (tcPropWhile)
+			return;
+		tcPropWhile = HelloTime + to_stp_timer(1);
+	}
+#endif
+	FOREACH_P_IN_T(
+		if (p == q)
+			continue;
+		tcProp = TRUE;
+	)
+}
+
+#define setTcPropTree()			setTcPropTree_(p)
+
+static int stp_xmit(struct ksz_stp_info *stp, u8 port)
+{
+	int rc;
+	struct sk_buff *skb;
+	u8 *frame = stp->tx_frame;
+	struct ksz_sw *sw = stp->sw_dev;
+	int len = stp->len;
+	uint ports;
+	const struct net_device_ops *ops = stp->dev->netdev_ops;
+	struct llc *llc = (struct llc *) &frame[12];
+	struct ksz_port_info *info = &sw->port_info[port];
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(stp->dev) || !netif_carrier_ok(stp->dev))
+		return 0;
+
+	ports = (1 << port);
+	ports |= 0x80000000;
+
+	len += 3;
+	llc->len = htons(len);
+	len += 14;
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, stp->tx_frame, len);
+	memcpy(&skb->data[6], info->mac_addr, ETH_ALEN);
+
+	skb_put(skb, len);
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	skb->protocol = htons(STP_TAG_TYPE);
+	skb->dev = stp->dev;
+	do {
+		struct ksz_sw *sw = stp->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(stp->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* stp_xmit */
+
+static void txConfig_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+
+	prep_stp(bpdu, &designatedPriority, &designatedTimes);
+	bpdu->flags = 0;
+	if (tcWhile != 0)
+		bpdu->flags |= TOPOLOGY_CHANGE;
+	if (tcAck)
+		bpdu->flags |= TOPOLOGY_CHANGE_ACK;
+
+	stp->len = sizeof(struct bpdu) - 1;
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txConfig()			txConfig_(p)
+
+static void txRstp_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+	u8 r = role;
+
+if (!portEnabled)
+dbg_msg(" ?! %s %d %d\n", __func__, p->port_index, r);
+	prep_rstp(bpdu, &designatedPriority, &designatedTimes);
+	if (AlternatePort || BackupPort)
+		r = PORT_ROLE_ALTERNATE;
+if (r != PORT_ROLE_ALTERNATE && r != PORT_ROLE_ROOT && r != PORT_ROLE_DESIGNATED)
+dbg_msg("  invalid role %d\n", r);
+	r &= PORT_ROLE_DESIGNATED;
+	r <<= PORT_ROLE_S;
+	bpdu->flags = r;
+	if (tcWhile != 0)
+		bpdu->flags |= TOPOLOGY_CHANGE;
+	if (agree)
+		bpdu->flags |= AGREEMENT;
+	if (proposing)
+		bpdu->flags |= PROPOSAL;
+	if (learning)
+		bpdu->flags |= LEARNING;
+	if (forwarding)
+		bpdu->flags |= FORWARDING;
+	if (tcAck)
+		bpdu->flags |= TOPOLOGY_CHANGE_ACK;
+	stp->len = sizeof(struct bpdu);
+
+#ifdef DBG_STP_TX
+	do {
+		int cmp = memcmp(&p->tx_bpdu0, bpdu, stp->len);
+
+		if (cmp) {
+			p->dbg_tx++;
+			memcpy(&p->tx_bpdu0, bpdu, stp->len);
+		}
+		if (p->dbg_tx) {
+			dbg_msg("<T> %d: ", p->port_index);
+			disp_bpdu(bpdu);
+			p->dbg_tx--;
+		}
+	} while (0);
+#endif
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txRstp()			txRstp_(p)
+
+static void txTcn_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+
+	bpdu->protocol = 0;
+	bpdu->version = 0;
+	bpdu->type = BPDU_TYPE_TCN;
+
+	stp->len = 4;
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txTcn()				txTcn_(p)
+
+static void updtBPDUVersion_(struct ksz_stp_port *p)
+{
+	if ((0 == bpduVersion || 1 == bpduVersion) &&
+	    (BPDU_TYPE_TCN == bpduType || BPDU_TYPE_CONFIG == bpduType))
+		rcvdSTP = TRUE;
+	if (BPDU_TYPE_CONFIG_RSTP == bpduType)
+		rcvdRSTP = TRUE;
+}
+
+#define updtBPDUVersion()		updtBPDUVersion_(p)
+
+static void updtRcvdInfoWhile_(struct ksz_stp_port *p)
+{
+	/*
+	 * Definition is not clear!
+	 * It is mentioned several times that Message Age should be less than
+	 * Max Age in BPDU to be accepted.
+	 */
+	if (portTimes.message_age + 1 <= portTimes.max_age)
+		rcvdInfoWhile = to_stp_timer(3 * portTimes.hello_time);
+	else
+		rcvdInfoWhile = 0;
+}
+
+#define updtRcvdInfoWhile()		updtRcvdInfoWhile_(p)
+
+static void updtRoleDisabledTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		selectedRole = ROLE_DISABLED;
+	)
+}
+
+#define updtRoleDisabledTree()		updtRoleDisabledTree_(br)
+
+static u32 add_path_cost(u32 x, u32 y)
+{
+	u32 z;
+
+	z = ntohl(x) + y;
+	return htonl(z);
+}
+
+static void updtRolesTree_(struct ksz_stp_bridge *br)
+{
+	int better;
+	uint i;
+	int id;
+	int prio;
+	int time;
+	struct ksz_stp_port *p = &br->ports[0];
+	struct stp_vector root_path;
+	struct stp_vector best_path;
+	struct stp_prio *root_Priority = NULL;
+	struct _port_id root_PortId;
+	struct ksz_stp_port *q = NULL;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+#endif
+
+	COPY(root_PortId, BridgePriority.port_id);
+
+	/* Find out the best path from all ports. */
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+
+		/* Check only after receiving new BPDU. */
+		if (infoIs != INFO_TYPE_RECEIVED)
+			continue;
+
+		/* Not from self. */
+		prio = CMP(portPriority.bridge_id.addr, BridgeIdentifier.addr);
+		if (!prio)
+			continue;
+
+		/* Ports may receive same BPDU when coming through a hub. */
+		COPY_PRIO(root_path, portPriority, portId);
+		root_path.prio.root_path_cost = add_path_cost(
+			root_path.prio.root_path_cost, PortPathCost);
+
+		better = FALSE;
+		if (!root_Priority)
+			better = TRUE;
+		else if (betterVector(&root_path, &best_path))
+			better = TRUE;
+		if (better) {
+			COPY(best_path, root_path);
+			root_Priority = &portPriority;
+			COPY(root_PortId, portId);
+			q = p;
+		}
+	}
+#ifdef DBG_STP_ROLE
+if (root_Priority) {
+dbg_msg(" best \n");
+dbgPriority(&best_path.prio, &best_path.port_id);
+dbg_msg(" root prio\n");
+dbgPriority(root_Priority, &root_PortId);
+}
+#endif
+
+	/* Compare with the bridge. */
+	better = FALSE;
+	if (!root_Priority)
+		better = TRUE;
+	else if (betterVector(&BridgePriority, &best_path))
+		better = TRUE;
+	if (better) {
+		COPY(best_path, BridgePriority);
+		root_Priority = NULL;
+		COPY(root_PortId, BridgePriority.port_id);
+	}
+
+	COPY(rootPriority, best_path.prio);
+	COPY(rootPortId, best_path.port_id);
+
+	if (root_Priority) {
+		u8 num = root_PortId.num - 1;
+
+		p = &br->ports[num];
+		COPY(rootTimes, portTimes);
+		rootTimes.message_age += 1;
+	} else {
+		COPY(rootTimes, BridgeTimes);
+	}
+
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+
+#ifdef DBG_STP_PORT_FLUSH
+		x = &p->dbg_times[0];
+#endif
+
+		COPY(designatedPriority, rootPriority);
+		COPY(designatedPriority.bridge_id, BridgeIdentifier);
+		COPY(designatedPriority.port_id, portId);
+
+		COPY(designatedTimes, rootTimes);
+		designatedTimes.hello_time = BridgeTimes.hello_time;
+
+		switch (infoIs) {
+		case INFO_TYPE_DISABLED:
+			selectedRole = ROLE_DISABLED;
+#ifdef DBG_STP_PORT_FLUSH
+			x->role_ = ROLE_DISABLED;
+			x->downPriority.port_id.num = 0;
+			x->learn_jiffies = 0;
+#endif
+			break;
+		case INFO_TYPE_AGED:
+			updtInfo = TRUE;
+			selectedRole = ROLE_DESIGNATED;
+#ifdef DBG_STP_PORT_FLUSH
+			if (ROLE_DISABLED == x->role_)
+				x->role_ = ROLE_DESIGNATED;
+#endif
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 2;
+#endif
+			break;
+		case INFO_TYPE_MINE:
+			selectedRole = ROLE_DESIGNATED;
+			prio = CMP(designatedPriority, portPriority);
+			time = CMP(designatedTimes, portTimes);
+			if (prio || time)
+				updtInfo = TRUE;
+#ifdef DBG_STP_PORT_FLUSH
+			if (ROLE_DISABLED == x->role_)
+				x->role_ = ROLE_DESIGNATED;
+#endif
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 1;
+#endif
+			break;
+		case INFO_TYPE_RECEIVED:
+			if (root_Priority == &portPriority) {
+				selectedRole = ROLE_ROOT;
+				updtInfo = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+				x->role_ = ROLE_DESIGNATED;
+#endif
+			} else {
+				prio = CMP(designatedPriority, portPriority);
+				if (prio >= 0) {
+					id = CMP(portPriority.bridge_id.addr,
+						BridgeIdentifier.addr);
+					if (id)
+						selectedRole = ROLE_ALTERNATE;
+					else
+						selectedRole = ROLE_BACKUP;
+					updtInfo = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+					x->role_ = ROLE_DESIGNATED;
+#endif
+				} else {
+					selectedRole = ROLE_DESIGNATED;
+					updtInfo = TRUE;
+				}
+			}
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 3;
+#endif
+			break;
+		default:
+dbg_msg("unknown\n");
+		}
+#ifdef DBG_STP_ROLE
+if (!p->off)
+dbg_msg("  %s %d %d\n", __func__, p->port_index, selectedRole);
+#endif
+	}
+}
+
+#define updtRolesTree()			updtRolesTree_(br)
+
+
+enum {
+	STP_BEGIN,
+	STP_LAST
+};
+
+enum {
+	STP_PortTimers_ONE_SECOND = STP_LAST,
+	STP_PortTimers_TICK,
+};
+
+enum {
+	STP_PortReceive_DISCARD = STP_LAST,
+	STP_PortReceive_RECEIVE,
+};
+
+enum {
+	STP_PortProtoMigr_CHECKING_RSTP = STP_LAST,
+	STP_PortProtoMigr_SELECTING_STP,
+	STP_PortProtoMigr_SENSING,
+};
+
+enum {
+	STP_BridgeDetection_EDGE = STP_LAST,
+	STP_BridgeDetection_NOT_EDGE,
+};
+
+enum {
+	STP_PortTransmit_INIT = STP_LAST,
+	STP_PortTransmit_PERIODIC,
+	STP_PortTransmit_CONFIG,
+	STP_PortTransmit_TCN,
+	STP_PortTransmit_RSTP,
+	STP_PortTransmit_IDLE,
+};
+
+enum {
+	STP_PortInfo_DISABLED = STP_LAST,
+	STP_PortInfo_AGED,
+	STP_PortInfo_UPDATE,
+	STP_PortInfo_SUPERIOR_DESIGNATED,
+	STP_PortInfo_REPEATED_DESIGNATED,
+	STP_PortInfo_INFERIOR_DESIGNATED,
+	STP_PortInfo_NOT_DESIGNATED,
+	STP_PortInfo_OTHER,
+	STP_PortInfo_CURRENT,
+	STP_PortInfo_RECEIVE,
+};
+
+enum {
+	STP_PortRoleSel_INIT_BRIDGE = STP_LAST,
+	STP_PortRoleSel_ROLE_SELECTION,
+};
+
+enum {
+	STP_PortRoleTrans_INIT_PORT = STP_LAST,
+	STP_PortRoleTrans_DISABLE_PORT,
+	STP_PortRoleTrans_DISABLED_PORT,
+	STP_PortRoleTrans_ROOT_PROPOSED,
+	STP_PortRoleTrans_ROOT_AGREED,
+	STP_PortRoleTrans_ROOT_SYNCED,
+	STP_PortRoleTrans_REROOT,
+	STP_PortRoleTrans_REROOTED,
+	STP_PortRoleTrans_ROOT_LEARN,
+	STP_PortRoleTrans_ROOT_FORWARD,
+	STP_PortRoleTrans_ROOT_PORT,
+	STP_PortRoleTrans_DESIGNATED_PROPOSE,
+	STP_PortRoleTrans_DESIGNATED_SYNCED,
+	STP_PortRoleTrans_DESIGNATED_RETIRED,
+	STP_PortRoleTrans_DESIGNATED_DISCARD,
+	STP_PortRoleTrans_DESIGNATED_LEARN,
+	STP_PortRoleTrans_DESIGNATED_FORWARD,
+	STP_PortRoleTrans_DESIGNATED_PORT,
+	STP_PortRoleTrans_BLOCK_PORT,
+	STP_PortRoleTrans_ALTERNATE_PROPOSED,
+	STP_PortRoleTrans_ALTERNATE_AGREED,
+	STP_PortRoleTrans_BACKUP_PORT,
+	STP_PortRoleTrans_ALTERNATE_PORT,
+};
+
+enum {
+	STP_PortStateTrans_DISCARDING = STP_LAST,
+	STP_PortStateTrans_LEARNING,
+	STP_PortStateTrans_FORWARDING,
+};
+
+enum {
+	STP_TopologyChange_INACTIVE = STP_LAST,
+	STP_TopologyChange_LEARNING,
+	STP_TopologyChange_DETECTED,
+	STP_TopologyChange_ACKNOWLEDGED,
+	STP_TopologyChange_PROPAGATING,
+	STP_TopologyChange_NOTIFIED_TC,
+	STP_TopologyChange_NOTIFIED_TCN,
+	STP_TopologyChange_ACTIVE,
+};
+
+enum {
+	STP_PortTimers,
+	STP_PortReceive,
+	STP_PortProtoMigr,
+	STP_BridgeDetection,
+	STP_PortInfo,
+	STP_PortRoleTrans,
+	STP_PortStateTrans,
+	STP_TopologyChange,
+	STP_PortTransmit,
+
+	STP_PortRoleSel,
+};
+
+#ifdef DBG_STP_STATE
+static char *PortTimers_names[] = {
+	"ONE_SECOND",
+	"TICK",
+};
+
+static char *PortReceive_names[] = {
+	"DISCARD,"
+	"RECEIVE",
+};
+
+static char *PortProtoMigr_names[] = {
+	"CHECKING_RSTP",
+	"SELECTING_STP",
+	"SENSING",
+};
+
+static char *BridgeDetection_names[] = {
+	"EDGE",
+	"NOT_EDGE",
+};
+
+static char *PortTransmit_names[] = {
+	"INIT",
+	"PERIODIC",
+	"CONFIG",
+	"TCN",
+	"RSTP",
+	"IDLE",
+};
+
+static char *PortInfo_names[] = {
+	"DISABLED",
+	"AGED",
+	"UPDATE",
+	"SUPERIOR_DESIGNATED",
+	"REPEATED_DESIGNATED",
+	"INFERIOR_DESIGNATED",
+	"NOT_DESIGNATED",
+	"OTHER",
+	"CURRENT",
+	"RECEIVE",
+};
+
+static char *PortRoleSel_names[] = {
+	"INIT_BRIDGE",
+	"ROLE_SELECTION",
+};
+
+static char *PortRoleTrans_names[] = {
+	"INIT_PORT",
+	"DISABLE_PORT",
+	"DISABLED_PORT",
+	"ROOT_PROPOSED",
+	"ROOT_AGREED",
+	"ROOT_SYNCED",
+	"REROOT",
+	"REROOTED",
+	"ROOT_LEARN",
+	"ROOT_FORWARD",
+	"ROOT_PORT",
+	"DESIGNATED_PROPOSE",
+	"DESIGNATED_SYNCED",
+	"DESIGNATED_RETIRED",
+	"DESIGNATED_DISCARD",
+	"DESIGNATED_LEARN",
+	"DESIGNATED_FORWARD",
+	"DESIGNATED_PORT",
+	"BLOCK_PORT",
+	"ALTERNATE_PROPOSED",
+	"ALTERNATE_AGREED",
+	"BACKUP_PORT",
+	"ALTERNATE_PORT",
+};
+
+static char *PortStateTrans_names[] = {
+	"DISCARDING",
+	"LEARNING",
+	"FORWARDING",
+};
+
+static char *TopologyChange_names[] = {
+	"INACTIVE",
+	"LEARNING",
+	"DETECTED",
+	"ACKNOWLEDGED",
+	"PROPAGATING",
+	"NOTIFIED_TC",
+	"NOTIFIED_TCN",
+	"ACTIVE",
+};
+
+static char **stp_state_names[] = {
+	PortTimers_names,
+	PortReceive_names,
+	PortProtoMigr_names,
+	BridgeDetection_names,
+	PortInfo_names,
+	PortRoleTrans_names,
+	PortStateTrans_names,
+	TopologyChange_names,
+	PortTransmit_names,
+
+	PortRoleSel_names,
+};
+#endif
+
+
+struct ksz_stp_state {
+	int index;
+	int change;
+	int new_state;
+};
+
+static int stp_proc_state(struct ksz_stp_port *p, struct ksz_stp_state *state,
+	void (*state_init)(struct ksz_stp_port *p),
+	void (*state_next)(struct ksz_stp_port *p, struct ksz_stp_state *state))
+{
+	if (state->new_state) {
+		state->new_state = 0;
+		state_init(p);
+	}
+	state_next(p, state);
+	return 0;
+}  /* stp_proc_state */
+
+static void stp_change_state(struct ksz_stp_state *state, int cond, int new)
+{
+	if (!cond)
+		return;
+	if (state->new_state) {
+#ifdef DBG_STP_STATE
+		if (state->change) {
+			char **names = stp_state_names[state->index];
+			char *last;
+			char *next;
+
+			if (state->new_state != STP_BEGIN)
+				last = names[state->new_state - STP_LAST];
+			else
+				last = "BEGIN";
+			if (new != STP_BEGIN)
+				next = names[new - STP_LAST];
+			else
+				next = "BEGIN";
+dbg_msg("  %s %d %s %d %s\n", __func__, state->new_state, last, new, next);
+		}
+#endif
+		return;
+	}
+	state->new_state = new;
+}  /* stp_change_state */
+
+
+static void stp_one_sec_init(struct ksz_stp_port *p)
+{
+	tick = FALSE;
+}  /* stp_one_sec_init */
+
+static void stp_one_sec_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state, (tick), STP_PortTimers_TICK);
+}  /* stp_one_sec_next */
+
+#define dec_timer(x)	if (x) x -= STP_TIMER_TICK
+#define dec(x)		if (x) x--
+
+static void stp_tick_init(struct ksz_stp_port *p)
+{
+	int i;
+
+	for (i = 0; i < NUM_OF_PORT_TIMERS; i++)
+		dec_timer(p->vars.timers[i]);
+	if (!tcWhile)
+		isTC &= ~(1 << p->port_index);
+	if (!p->port_index && !(helloWhen % STP_TIMER_SCALE))
+		timeSinceTC++;
+	if (!(helloWhen % STP_TIMER_SCALE))
+		dec(txCount);
+}  /* stp_tick_init */
+
+static void stp_tick_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state, TRUE, STP_PortTimers_ONE_SECOND);
+}  /* stp_tick_next */
+
+static int PortTimers(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortTimers;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortTimers_ONE_SECOND;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortTimers_ONE_SECOND:
+			if (stp_proc_state(p, &state_info,
+			    stp_one_sec_init, stp_one_sec_next))
+				goto done;
+			break;
+		case STP_PortTimers_TICK:
+			if (stp_proc_state(p, &state_info,
+			    stp_tick_init, stp_tick_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortTimers */
+
+static void stp_rx_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_RX
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdBPDU = rcvdRSTP = rcvdSTP = FALSE;
+	rcvdMsg = FALSE;
+	edgeDelayWhile = MigrateTime;
+}  /* stp_rx_discard_init */
+
+static void stp_rx_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdBPDU && portEnabled),
+		STP_PortReceive_RECEIVE);
+}  /* stp_rx_discard_next */
+
+static void stp_rx_receive_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_RX
+	dbg_stp(p, __func__, true);
+#endif
+	updtBPDUVersion();
+	operEdge = rcvdBPDU = FALSE;
+	rcvdMsg = TRUE;
+	edgeDelayWhile = MigrateTime;
+}  /* stp_rx_receive_init */
+
+static void stp_rx_receive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdBPDU && portEnabled && !rcvdMsg),
+		STP_PortReceive_RECEIVE);
+}  /* stp_rx_receive_next */
+
+static void stp_rx_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Reset rcvdBPDU and edgeDelayWhile if !portEnabled. */
+	stp_change_state(state,
+		((rcvdBPDU || NEQ(edgeDelayWhile, MigrateTime)) &&
+		!portEnabled),
+		STP_PortReceive_DISCARD);
+}  /* stp_rx_next */
+
+static int PortReceive(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortReceive;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortReceive_DISCARD;
+	} else
+		stp_rx_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortReceive_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_rx_discard_init, stp_rx_discard_next))
+				goto done;
+			break;
+		case STP_PortReceive_RECEIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_rx_receive_init, stp_rx_receive_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortReceive */
+
+static void stp_proto_check_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_PROTO
+	dbg_stp(p, __func__, false);
+#endif
+	mcheck = FALSE;
+	sendRSTP = rstpVersion;
+	mdelayWhile = MigrateTime;
+}  /* stp_proto_check_init */
+
+static void stp_proto_check_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(0 == mdelayWhile),
+		STP_PortProtoMigr_SENSING);
+
+	/* Reset mdelayWhile if !portEnabled */
+	stp_change_state(state,
+		(NEQ(mdelayWhile, MigrateTime) && !portEnabled),
+		STP_PortProtoMigr_CHECKING_RSTP);
+}  /* stp_proto_check_next */
+
+static void stp_proto_select_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	sendRSTP = FALSE;
+	mdelayWhile = MigrateTime;
+}  /* stp_proto_select_init */
+
+static void stp_proto_select_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((0 == mdelayWhile) || !portEnabled || mcheck),
+		STP_PortProtoMigr_SENSING);
+}  /* stp_proto_select_next */
+
+static void stp_proto_sense_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdRSTP = rcvdSTP = FALSE;
+}  /* stp_proto_sense_init */
+
+static void stp_proto_sense_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!portEnabled || mcheck || (rstpVersion && !sendRSTP &&
+		rcvdRSTP)),
+		STP_PortProtoMigr_CHECKING_RSTP);
+	stp_change_state(state,
+		(sendRSTP && rcvdSTP),
+		STP_PortProtoMigr_SELECTING_STP);
+}  /* stp_proto_sense_next */
+
+static int PortProtocolMigration(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortProtoMigr;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortProtoMigr_CHECKING_RSTP;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortProtoMigr_CHECKING_RSTP:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_check_init, stp_proto_check_next))
+				goto done;
+			break;
+		case STP_PortProtoMigr_SELECTING_STP:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_select_init, stp_proto_select_next))
+				goto done;
+			break;
+		case STP_PortProtoMigr_SENSING:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_sense_init, stp_proto_sense_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortProtocolMigration */
+
+static void stp_br_det_edge_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = TRUE;
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+if (x->block_jiffies)
+dbg_msg(" b: %ld %s\n", jiffies - x->block_jiffies, __func__);
+} while (0);
+}  /* stp_br_det_edge_init */
+
+static void stp_br_det_edge_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!portEnabled && !AdminEdge) || !operEdge),
+		STP_BridgeDetection_NOT_EDGE);
+}  /* stp_br_det_edge_next */
+
+static void stp_br_det_not_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_BR_DET
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = FALSE;
+}  /* stp_br_det_not_init */
+
+static void stp_br_det_not_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!portEnabled && AdminEdge) ||
+		((0 == edgeDelayWhile) && AutoEdge && sendRSTP && proposing)),
+		STP_BridgeDetection_EDGE);
+}  /* stp_br_det_not_next */
+
+static int BridgeDetection(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_BridgeDetection;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = AdminEdge ?
+			STP_BridgeDetection_EDGE :
+			STP_BridgeDetection_NOT_EDGE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_BridgeDetection_EDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_edge_init, stp_br_det_edge_next))
+				goto done;
+			break;
+		case STP_BridgeDetection_NOT_EDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_not_init, stp_br_det_not_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* BridgeDetection */
+
+static void stp_tx_init_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+#if 0
+#if 0
+	/* Send RSTP after initialization when the port is disabled? */
+	if (!sendRSTP)
+#endif
+	newInfo = TRUE;
+#endif
+	txCount = 0;
+}  /* stp_tx_init_init */
+
+static void stp_tx_init_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_init_next */
+
+static void stp_tx_periodic_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX_0
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = newInfo || (DesignatedPort || (RootPort && (tcWhile != 0)));
+}  /* stp_tx_periodic_init */
+
+static void stp_tx_periodic_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_periodic_next */
+
+static void stp_tx_config_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txConfig();
+	txCount++;
+	tcAck = FALSE;
+}  /* stp_tx_config_init */
+
+static void stp_tx_config_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_config_next */
+
+static void stp_tx_tcn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txTcn();
+	txCount++;
+}  /* stp_tx_tcn_init */
+
+static void stp_tx_tcn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_tcn_next */
+
+static void stp_tx_rstp_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txRstp();
+	txCount++;
+	tcAck = FALSE;
+}  /* stp_tx_rstp_init */
+
+static void stp_tx_rstp_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_rstp_next */
+
+static void stp_tx_idle_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX_0
+	dbg_stp(p, __func__, false);
+#endif
+	helloWhen = HelloTime;
+}  /* stp_tx_idle_init */
+
+static void stp_tx_idle_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((0 == helloWhen) && canChange),
+		STP_PortTransmit_PERIODIC);
+	stp_change_state(state,
+		(sendRSTP && canChange && canSend),
+		STP_PortTransmit_RSTP);
+	stp_change_state(state,
+		((!sendRSTP && RootPort) && canChange && canSend),
+		STP_PortTransmit_TCN);
+	stp_change_state(state,
+		((!sendRSTP && DesignatedPort) && canChange && canSend),
+		STP_PortTransmit_CONFIG);
+}  /* stp_tx_idle_next */
+
+static int PortTransmit(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortTransmit;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortTransmit_INIT;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortTransmit_INIT:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_init_init, stp_tx_init_next))
+				goto done;
+			break;
+		case STP_PortTransmit_PERIODIC:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_periodic_init, stp_tx_periodic_next))
+				goto done;
+			break;
+		case STP_PortTransmit_CONFIG:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_config_init, stp_tx_config_next))
+				goto done;
+			break;
+		case STP_PortTransmit_TCN:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_tcn_init, stp_tx_tcn_next))
+				goto done;
+			break;
+		case STP_PortTransmit_RSTP:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_rstp_init, stp_tx_rstp_next))
+				goto done;
+			break;
+		case STP_PortTransmit_IDLE:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_idle_init, stp_tx_idle_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortTransmit */
+
+static void stp_info_disable_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdMsg = FALSE;
+	proposing = proposed = agree = agreed = FALSE;
+	rcvdInfoWhile = 0;
+
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_DISABLED;
+	reselect = TRUE;
+	selected = FALSE;
+}  /* stp_info_disable_init */
+
+static void stp_info_disable_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(portEnabled),
+		STP_PortInfo_AGED);
+	stp_change_state(state,
+		(rcvdMsg),
+		STP_PortInfo_DISABLED);
+}  /* stp_info_disable_next */
+
+static void stp_info_age_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_AGED;
+	reselect = TRUE;
+	selected = FALSE;
+}  /* stp_info_age_init */
+
+static void stp_info_age_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(selected && updtInfo),
+		STP_PortInfo_UPDATE);
+}  /* stp_info_age_next */
+
+static void stp_info_update_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposing = proposed = FALSE;
+	agreed = agreed && betterorsameInfo(INFO_TYPE_MINE);
+
+#if 1
+	/*
+	 * When switching back to Root Bridge because of timeout the port will
+	 * have inferior priority, resetting both agreed and synced.  The port
+	 * is still in forwarding state though.  When the port becomes
+	 * forwarding agreed is always set.  Now that agreed is reset synced
+	 * will not be set, making allSynced to always fail, which prevents
+	 * the Root Port from sending an Agreement, thus failing RSTP.op.5.3.
+	 */
+	if (forward)
+		agreed = sendRSTP;
+#endif
+	synced = synced && agreed;
+
+#if 0
+	/* agree is never turned off if priority is changed. */
+	agree = FALSE;
+#endif
+#ifdef DBG_STP_STATE_INFO
+dbg_msg("  %s %d %d\n", __func__, agreed, synced);
+#endif
+	COPY(portPriority, designatedPriority);
+	COPY(portTimes, designatedTimes);
+	updtInfo = FALSE;
+	infoIs = INFO_TYPE_MINE;
+	newInfo = TRUE;
+}  /* stp_info_update_init */
+
+static void stp_info_update_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_update_next */
+
+static void stp_info_superior_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	agreed = proposing = FALSE;
+	recordProposal();
+	setTcFlags();
+	agree = agree && betterorsameInfo(INFO_TYPE_RECEIVED);
+	recordPriority();
+	recordTimes();
+
+	/* Keep from aged out. */
+	updtRcvdInfoWhile();
+
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_RECEIVED;
+	reselect = TRUE;
+	selected = FALSE;
+
+	rcvdMsg = FALSE;
+}  /* stp_info_superior_init */
+
+static void stp_info_superior_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_superior_next */
+
+static void stp_info_repeat_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, true);
+#endif
+	recordProposal();
+	setTcFlags();
+
+	/* Keep from aged out. */
+	updtRcvdInfoWhile();
+	rcvdMsg = FALSE;
+}  /* stp_info_repeat_init */
+
+static void stp_info_repeat_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_repeat_next */
+
+static void stp_info_inferior_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	recordDispute();
+
+#if 1
+	/*
+	 * Priority is not recorded, so in updtRolesTree this port is still
+	 * selected as Root Port!
+	 * This can happen in test tool as the bridge id and port id is changed
+	 * from the port!
+	 */
+	if (p->br->hack_5_2 && INFO_TYPE_RECEIVED == infoIs &&
+	    memcmp(msgPriority.bridge_id.addr, portPriority.bridge_id.addr,
+	    ETH_ALEN)) {
+dbg_msg(" bridge id changed!\n");
+		recordPriority();
+		recordTimes();
+	}
+#endif
+
+	/* Will age out if keep receiving inferior messages. */
+	rcvdMsg = FALSE;
+}  /* stp_info_inferior_init */
+
+static void stp_info_inferior_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_inferior_next */
+
+static void stp_info_not_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, false);
+#endif
+	recordAgreement();
+#if 1
+	/*
+	 * This port is not yet designated but receives an Agreement, most
+	 * likely when running the test tool.  Prepare the portPriority so that
+	 * betterorsameInfo can return true.
+	 */
+	if (agreed && role != ROLE_DESIGNATED && role != ROLE_BACKUP) {
+dbgPriority(&rootPriority, &rootPortId);
+		COPY(portPriority, rootPriority);
+		COPY(portPriority.port_id, portId);
+		COPY(portTimes, rootTimes);
+	}
+#endif
+	setTcFlags();
+	rcvdMsg = FALSE;
+
+	if (role != ROLE_BACKUP)
+		stp_chk_flush(p);
+}  /* stp_info_not_init */
+
+static void stp_info_not_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_not_next */
+
+static void stp_info_other_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdMsg = FALSE;
+}  /* stp_info_other_init */
+
+static void stp_info_other_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_other_next */
+
+static void stp_info_cur_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, true);
+#endif
+}  /* stp_info_cur_init */
+
+static void stp_info_cur_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(selected && updtInfo),
+		STP_PortInfo_UPDATE);
+	stp_change_state(state,
+		((infoIs == INFO_TYPE_RECEIVED) && (0 == rcvdInfoWhile) &&
+		!updtInfo && !rcvdMsg),
+		STP_PortInfo_AGED);
+	stp_change_state(state,
+		(rcvdMsg && !updtInfo),
+		STP_PortInfo_RECEIVE);
+}  /* stp_info_cur_next */
+
+static void stp_info_receive_init(struct ksz_stp_port *p)
+{
+	rcvdInfo = rcvInfo();
+#ifdef DBG_STP_RX
+if (p->dbg_rx)
+dbg_msg("  %s:%u=%d\n", __func__, p->port_index, rcvdInfo);
+#endif
+}  /* stp_info_receive_init */
+
+static void stp_info_receive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdInfo == INFO_SUPERIOR_DESIGNATED),
+		STP_PortInfo_SUPERIOR_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_REPEATED_DESIGNATED),
+		STP_PortInfo_REPEATED_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_INFERIOR_DESIGNATED),
+		STP_PortInfo_INFERIOR_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_INFERIOR_ROOT_ALT),
+		STP_PortInfo_NOT_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_OTHER),
+		STP_PortInfo_OTHER);
+}  /* stp_info_receive_next */
+
+static void stp_info_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Notify port is disabled. */
+	stp_change_state(state,
+		(!portEnabled && (infoIs != INFO_TYPE_DISABLED)),
+		STP_PortInfo_DISABLED);
+}  /* stp_info_next */
+
+static int PortInformation(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortInfo;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortInfo_DISABLED;
+	} else
+		stp_info_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortInfo_DISABLED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_disable_init, stp_info_disable_next))
+				goto done;
+			break;
+		case STP_PortInfo_AGED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_age_init, stp_info_age_next))
+				goto done;
+			break;
+		case STP_PortInfo_UPDATE:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_update_init, stp_info_update_next))
+				goto done;
+			break;
+		case STP_PortInfo_SUPERIOR_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_superior_init, stp_info_superior_next))
+				goto done;
+			break;
+		case STP_PortInfo_REPEATED_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_repeat_init, stp_info_repeat_next))
+				goto done;
+			break;
+		case STP_PortInfo_INFERIOR_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_inferior_init, stp_info_inferior_next))
+				goto done;
+			break;
+		case STP_PortInfo_NOT_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_not_init, stp_info_not_next))
+				goto done;
+			break;
+		case STP_PortInfo_OTHER:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_other_init, stp_info_other_next))
+				goto done;
+			break;
+		case STP_PortInfo_CURRENT:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_cur_init, stp_info_cur_next))
+				goto done;
+			break;
+		case STP_PortInfo_RECEIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_receive_init, stp_info_receive_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortInformation */
+
+static void stp_role_sel_br_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+#ifdef DBG_STP_STATE_ROLE_SEL
+dbg_msg("  %s\n", __func__);
+#endif
+	updtRoleDisabledTree();
+}  /* stp_role_sel_br_init */
+
+static void stp_role_sel_br_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleSel_ROLE_SELECTION);
+}  /* stp_role_sel_br_next */
+
+static void stp_role_sel_role_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+#ifdef DBG_STP_STATE_ROLE_SEL
+dbg_msg("  %s\n", __func__);
+#endif
+	clearReselectTree();
+	updtRolesTree();
+	setSelectedTree();
+}  /* stp_role_sel_role_init */
+
+static void stp_role_sel_role_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	FOREACH_P_IN_T(
+		if (reselect) {
+			state->new_state = STP_PortRoleSel_ROLE_SELECTION;
+			break;
+		}
+	)
+}  /* stp_role_sel_role_next */
+
+static int PortRoleSelection(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_state state_info;
+	struct ksz_stp_port *p = &br->ports[0];
+	u8 *state = &br->states[br->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortRoleSel;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortRoleSel_INIT_BRIDGE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortRoleSel_INIT_BRIDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_sel_br_init, stp_role_sel_br_next))
+				goto done;
+			break;
+		case STP_PortRoleSel_ROLE_SELECTION:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_sel_role_init, stp_role_sel_role_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortRoleSelection */
+
+static void stp_role_tr_init_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_DISABLED;
+	learn = forward = FALSE;
+	synced = FALSE;
+	sync = reRoot = TRUE;
+	rrWhile = FwdDelay;
+	fdWhile = MaxAge;
+	rbWhile = 0;
+}  /* stp_role_tr_init_init */
+
+static void stp_role_tr_init_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int cond = canChange;
+
+#if 1
+	cond = TRUE;
+#endif
+	stp_change_state(state,
+		cond, STP_PortRoleTrans_DISABLE_PORT);
+}  /* stp_role_tr_init_next */
+
+static void stp_role_tr_dis_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_DISABLED;
+	learn = forward = FALSE;
+
+#if 1
+	/* Port is disabled.  Do not send. */
+#ifdef DBG_STP_STATE
+if (newInfo)
+dbg_msg("  %s clear newInfo\n", __func__);
+#endif
+	newInfo = FALSE;
+#endif
+}  /* stp_role_tr_dis_init */
+
+static void stp_role_tr_dis_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!learning && !forwarding) && canChange),
+		STP_PortRoleTrans_DISABLED_PORT);
+}  /* stp_role_tr_dis_next */
+
+static void stp_role_tr_disd_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = MaxAge;
+#ifdef DBG_STP_STATE
+if (!synced)
+dbg_msg(" %s:%u\n", __func__, p->port_index);
+#endif
+	synced = TRUE;
+	rrWhile = 0;
+	sync = reRoot = FALSE;
+}  /* stp_role_tr_disd_init */
+
+static void stp_role_tr_disd_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Reset if !updtInfo. */
+	stp_change_state(state,
+		((NEQ(fdWhile, MaxAge) || sync || reRoot || !synced) &&
+		canChange),
+		STP_PortRoleTrans_DISABLED_PORT);
+}  /* stp_role_tr_disd_next */
+
+static void stp_role_tr_reroot_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setReRootTree();
+}  /* stp_role_tr_reroot_init */
+
+static void stp_role_tr_reroot_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_reroot_next */
+
+static void stp_role_tr_rerooted_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_rerooted_init */
+
+static void stp_role_tr_rerooted_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_rerooted_next */
+
+static void stp_role_tr_root_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_root_proposed_init */
+
+static void stp_role_tr_root_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_proposed_next */
+
+static void stp_role_tr_root_agreed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = sync = FALSE;
+	agree = TRUE;
+	newInfo = TRUE;
+}  /* stp_role_tr_root_agreed_init */
+
+static void stp_role_tr_root_agreed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_agreed_next */
+
+static void stp_role_tr_root_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_root_s_init */
+
+static void stp_role_tr_root_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_s_next */
+
+static void stp_role_tr_root_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = forwardDelay();
+	learn = TRUE;
+}  /* stp_role_tr_root_l_init */
+
+static void stp_role_tr_root_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_l_next */
+
+static void stp_role_tr_root_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = 0;
+	forward = TRUE;
+}  /* stp_role_tr_root_f_init */
+
+static void stp_role_tr_root_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_f_next */
+
+static void stp_role_tr_root_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_ROOT)
+		dbg_stp(p, __func__, false);
+#endif
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		if (DesignatedPort)
+			sw->ops->from_designated(sw, p->port_index, false);
+		else if (!RootPort)
+			sw->ops->from_backup(sw, p->port_index);
+	} while (0);
+	role = ROLE_ROOT;
+	rrWhile = FwdDelay;
+#ifdef CONFIG_KSZ_MRP
+	if (mrp_10_1_8a_hack)
+		rrWhile = to_stp_timer(4);
+#endif
+}  /* stp_role_tr_root_p_init */
+
+static void stp_role_tr_root_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int delay = FwdDelay;
+
+#ifdef CONFIG_KSZ_MRP
+	if (mrp_10_1_8a_hack)
+		delay = to_stp_timer(4);
+#endif
+
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_ROOT_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_ROOT_AGREED);
+	stp_change_state(state,
+		(((agreed && !synced) || (sync && synced)) && canChange),
+		STP_PortRoleTrans_ROOT_SYNCED);
+	stp_change_state(state,
+		((!forward && !reRoot) && canChange),
+		STP_PortRoleTrans_REROOT);
+	stp_change_state(state,
+		((reRoot && forward) && canChange),
+		STP_PortRoleTrans_REROOTED);
+	stp_change_state(state,
+		((((0 == fdWhile) || ((reRooted() && (0 == rbWhile)) &&
+		rstpVersion)) && !learn) && canChange),
+		STP_PortRoleTrans_ROOT_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || ((reRooted() && (0 == rbWhile)) &&
+		rstpVersion)) && learn && !forward) && canChange),
+		STP_PortRoleTrans_ROOT_FORWARD);
+	stp_change_state(state,
+		(NEQ(rrWhile, delay) && canChange),
+		STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_p_next */
+
+static void stp_role_tr_desg_r_init(struct ksz_stp_port *p)
+{
+	int delay = EdgeDelay();
+
+	if (AutoEdge && p->br->hack_4_1)
+		delay = 0;
+
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposing = TRUE;
+	edgeDelayWhile = delay;
+	newInfo = TRUE;
+}  /* stp_role_tr_desg_r_init */
+
+static void stp_role_tr_desg_r_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_r_next */
+
+static void stp_role_tr_desg_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rrWhile = 0;
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_desg_s_init */
+
+static void stp_role_tr_desg_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_s_next */
+
+static void stp_role_tr_desg_retired_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_desg_retired_init */
+
+static void stp_role_tr_desg_retired_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_retired_next */
+
+static void stp_role_tr_desg_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = forward = disputed = FALSE;
+	fdWhile = forwardDelay();
+}  /* stp_role_tr_desg_discard_init */
+
+static void stp_role_tr_desg_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_discard_next */
+
+static void stp_role_tr_desg_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = TRUE;
+	fdWhile = forwardDelay();
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+if (x->block_jiffies)
+dbg_msg(" b: %ld %d %s\n", jiffies - x->block_jiffies, fdWhile, __func__);
+} while (0);
+}  /* stp_role_tr_desg_l_init */
+
+static void stp_role_tr_desg_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_l_next */
+
+static void stp_role_tr_desg_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	forward = TRUE;
+	fdWhile = 0;
+	agreed = sendRSTP;
+
+#if 1
+	/*
+	 * proposing will be reset when infoIs is changed and betterorsameInfo
+	 * is called, even though the forwarding state is not changed.  Why
+	 * not just reset here?
+	 */
+	if (proposing)
+		proposing = FALSE;
+#endif
+#ifdef DBG_STP_STATE_
+	do {
+		char buf[40];
+
+		sprintf(buf, " %s", __func__);
+	dbg_stp(p, buf, false);
+	} while (0);
+#endif
+}  /* stp_role_tr_desg_f_init */
+
+static void stp_role_tr_desg_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_f_next */
+
+static void stp_role_tr_desg_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_DESIGNATED)
+		dbg_stp(p, __func__, false);
+#endif
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+if (x->block_jiffies && role != ROLE_DESIGNATED)
+dbg_msg(" b: %ld %d %s\n", jiffies - x->block_jiffies, fdWhile, __func__);
+} while (0);
+	if (RootPort || AlternatePort) {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->to_designated(sw, p->port_index);
+	}
+	if (BackupPort) {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->from_backup(sw, p->port_index);
+	}
+	role = ROLE_DESIGNATED;
+}  /* stp_role_tr_desg_p_init */
+
+static void stp_role_tr_desg_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!forward && !agreed && !proposing && !operEdge) &&
+		canChange),
+		STP_PortRoleTrans_DESIGNATED_PROPOSE);
+	stp_change_state(state,
+		(((!learning && !forwarding && !synced) ||
+		(agreed && !synced) || (operEdge && !synced) ||
+		(sync && synced)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_SYNCED);
+	stp_change_state(state,
+		((reRoot && (0 == rrWhile)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_RETIRED);
+	stp_change_state(state,
+		((((sync && !synced) || (reRoot && (rrWhile != 0)) || disputed)
+		&& !operEdge && (learn || forward)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_DISCARD);
+	stp_change_state(state,
+		((((0 == fdWhile) || agreed || operEdge) &&
+		((0 == rrWhile) || !reRoot) && !sync && !learn) && canChange),
+		STP_PortRoleTrans_DESIGNATED_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || agreed || operEdge) &&
+		((0 == rrWhile) || !reRoot) && !sync && (learn && !forward)) &&
+		canChange),
+		STP_PortRoleTrans_DESIGNATED_FORWARD);
+}  /* stp_role_tr_desg_p_next */
+
+static void stp_role_tr_block_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		if (DesignatedPort && selectedRole == ROLE_ALTERNATE)
+			sw->ops->from_designated(sw, p->port_index, true);
+		else
+			sw->ops->to_backup(sw, p->port_index);
+	} while (0);
+	role = selectedRole;
+	learn = forward = FALSE;
+}  /* stp_role_tr_block_init */
+
+static void stp_role_tr_block_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!learning && !forwarding) && canChange),
+		STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_block_next */
+
+static void stp_role_tr_backup_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, true);
+#endif
+	rbWhile = 2 * HelloTime;
+}  /* stp_role_tr_backup_init */
+
+static void stp_role_tr_backup_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_backup_next */
+
+static void stp_role_tr_alt_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_alt_proposed_init */
+
+static void stp_role_tr_alt_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_alt_proposed_next */
+
+static void stp_role_tr_alt_a_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = FALSE;
+	agree = TRUE;
+	newInfo = TRUE;
+}  /* stp_role_tr_alt_a_init */
+
+static void stp_role_tr_alt_a_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_alt_a_next */
+
+static void stp_role_tr_alt_p_init(struct ksz_stp_port *p)
+{
+	int delay = forwardDelay();
+
+	if (p->br->hack_4_2)
+		delay = FwdDelay;
+
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, true);
+#endif
+	fdWhile = delay;
+	synced = TRUE;
+	rrWhile = 0;
+	sync = reRoot = FALSE;
+}  /* stp_role_tr_alt_p_init */
+
+static void stp_role_tr_alt_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int delay = forwardDelay();
+
+	if (p->br->hack_4_2)
+		delay = FwdDelay;
+
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_ALTERNATE_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_ALTERNATE_AGREED);
+	state->change = 0;
+	stp_change_state(state,
+		((NEQ(rbWhile, 2 * HelloTime) && BackupPort) && canChange),
+		STP_PortRoleTrans_BACKUP_PORT);
+	stp_change_state(state,
+		((NEQ(fdWhile, delay) || sync || reRoot || !synced) &&
+		canChange),
+		STP_PortRoleTrans_ALTERNATE_PORT);
+	state->change = 1;
+}  /* stp_role_tr_alt_p_next */
+
+static void stp_role_tr_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(((selectedRole == ROLE_DISABLED) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_DISABLE_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_ROOT) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_ROOT_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_DESIGNATED) && (role != selectedRole))
+		&& canChange),
+		STP_PortRoleTrans_DESIGNATED_PORT);
+	stp_change_state(state,
+		((((selectedRole == ROLE_ALTERNATE) ||
+		(selectedRole == ROLE_BACKUP)) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_BLOCK_PORT);
+}  /* stp_role_tr_next */
+
+static int PortRoleTransitions(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortRoleTrans;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortRoleTrans_INIT_PORT;
+	} else
+		stp_role_tr_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortRoleTrans_INIT_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_init_init, stp_role_tr_init_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DISABLE_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_dis_init, stp_role_tr_dis_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DISABLED_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_disd_init, stp_role_tr_disd_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_proposed_init,
+			    stp_role_tr_root_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_agreed_init,
+			    stp_role_tr_root_agreed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_s_init, stp_role_tr_root_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_REROOT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_reroot_init, stp_role_tr_reroot_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_REROOTED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_rerooted_init,
+			    stp_role_tr_rerooted_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_l_init, stp_role_tr_root_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_f_init, stp_role_tr_root_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_p_init, stp_role_tr_root_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_PROPOSE:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_r_init, stp_role_tr_desg_r_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_s_init, stp_role_tr_desg_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_RETIRED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_retired_init,
+			    stp_role_tr_desg_retired_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_discard_init,
+			    stp_role_tr_desg_discard_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_l_init, stp_role_tr_desg_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_f_init, stp_role_tr_desg_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_p_init, stp_role_tr_desg_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_BLOCK_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_block_init, stp_role_tr_block_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_proposed_init,
+			    stp_role_tr_alt_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_a_init, stp_role_tr_alt_a_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_BACKUP_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_backup_init, stp_role_tr_backup_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_p_init, stp_role_tr_alt_p_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortRoleTransitions */
+
+static void stp_discarding_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	disableLearning();
+	learning = FALSE;
+	disableForwarding();
+	forwarding = FALSE;
+}  /* stp_discarding_init */
+
+static void stp_discarding_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(learn),
+		STP_PortStateTrans_LEARNING);
+}  /* stp_discarding_next */
+
+static void stp_learning_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	enableLearning();
+	learning = TRUE;
+}  /* stp_learning_init */
+
+static void stp_learning_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!learn),
+		STP_PortStateTrans_DISCARDING);
+	stp_change_state(state,
+		(forward),
+		STP_PortStateTrans_FORWARDING);
+}  /* stp_learning_next */
+
+static void stp_forwarding_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	enableForwarding();
+	forwarding = TRUE;
+}  /* stp_forwarding_init */
+
+static void stp_forwarding_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!forward),
+		STP_PortStateTrans_DISCARDING);
+}  /* stp_forwarding_next */
+
+static int PortStateTransition(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortStateTrans;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortStateTrans_DISCARDING;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortStateTrans_DISCARDING:
+			if (stp_proc_state(p, &state_info,
+			    stp_discarding_init, stp_discarding_next))
+				goto done;
+			break;
+		case STP_PortStateTrans_LEARNING:
+			if (stp_proc_state(p, &state_info,
+			    stp_learning_init, stp_learning_next))
+				goto done;
+			break;
+		case STP_PortStateTrans_FORWARDING:
+			if (stp_proc_state(p, &state_info,
+			    stp_forwarding_init, stp_forwarding_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortStateTransition */
+
+static void stp_top_inactive_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdbFlush = TRUE;
+	doFlush();
+
+	/* Stop sending topology change. */
+	tcDetected = 0;
+	tcWhile = 0;
+	tcPropWhile = 0;
+	tcAck = FALSE;
+}  /* stp_top_inactive_init */
+
+static void stp_top_inactive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(learn && !fdbFlush),
+		STP_TopologyChange_LEARNING);
+}  /* stp_top_inactive_next */
+
+static void stp_top_learn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdTc = rcvdTcn = rcvdTcAck = FALSE;
+	tcProp = FALSE;
+}  /* stp_top_learn_init */
+
+static void stp_top_learn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(ForwardPort && forward && !operEdge),
+		STP_TopologyChange_DETECTED);
+	stp_change_state(state,
+		(!(ForwardPort) && !(learn || learning) &&
+		!(rcvdTc || rcvdTcn || rcvdTcAck || tcProp)),
+		STP_TopologyChange_INACTIVE);
+
+	/*
+	 * LEARNING will be called after DETECTED because the other port is
+	 * in forwarding while this port is going forwarding.
+	 */
+	stp_change_state(state,
+		(rcvdTc || rcvdTcn || rcvdTcAck || tcProp),
+		STP_TopologyChange_LEARNING);
+}  /* stp_top_learn_next */
+
+static void stp_top_det_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+	setTcPropTree();
+	newTcDetected();
+	newInfo = TRUE;
+}  /* stp_top_det_init */
+
+static void stp_top_det_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_det_next */
+
+static void stp_top_ack_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	/* Stop sending topology change. */
+	tcWhile = 0;
+	tcPropWhile = 0;
+	rcvdTcAck = FALSE;
+}  /* stp_top_ack_init */
+
+static void stp_top_ack_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_ack_next */
+
+static void stp_top_prop_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+	fdbFlush = TRUE;
+	doFlush();
+	tcProp = FALSE;
+}  /* stp_top_prop_init */
+
+static void stp_top_prop_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_prop_next */
+
+static void stp_top_tc_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdTc = rcvdTcn = FALSE;
+	if (role == ROLE_DESIGNATED)
+		tcAck = TRUE;
+	setTcPropTree();
+}  /* stp_top_tc_init */
+
+static void stp_top_tc_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_tc_next */
+
+static void stp_top_tcn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+}  /* stp_top_tcn_init */
+
+static void stp_top_tcn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_NOTIFIED_TC);
+}  /* stp_top_tcn_next */
+
+static void stp_top_active_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+}  /* stp_top_active_init */
+
+static void stp_top_active_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/*
+	 * rcvd* variables are reset in LEARNING, so it does have higher
+	 * precedence.
+	 */
+	stp_change_state(state,
+		(!(ForwardPort) || operEdge),
+		STP_TopologyChange_LEARNING);
+	stp_change_state(state,
+		(rcvdTcAck),
+		STP_TopologyChange_ACKNOWLEDGED);
+	stp_change_state(state,
+		(tcProp && !operEdge),
+		STP_TopologyChange_PROPAGATING);
+	stp_change_state(state,
+		(rcvdTc),
+		STP_TopologyChange_NOTIFIED_TC);
+	stp_change_state(state,
+		(rcvdTcn),
+		STP_TopologyChange_NOTIFIED_TCN);
+}  /* stp_top_active_next */
+
+static int TopologyChange(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_TopologyChange;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_TopologyChange_INACTIVE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_TopologyChange_INACTIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_inactive_init, stp_top_inactive_next))
+				goto done;
+			break;
+		case STP_TopologyChange_LEARNING:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_learn_init, stp_top_learn_next))
+				goto done;
+			break;
+		case STP_TopologyChange_DETECTED:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_det_init, stp_top_det_next))
+				goto done;
+			break;
+		case STP_TopologyChange_ACKNOWLEDGED:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_ack_init, stp_top_ack_next))
+				goto done;
+			break;
+		case STP_TopologyChange_PROPAGATING:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_prop_init, stp_top_prop_next))
+				goto done;
+			break;
+		case STP_TopologyChange_NOTIFIED_TC:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_tc_init, stp_top_tc_next))
+				goto done;
+			break;
+		case STP_TopologyChange_NOTIFIED_TCN:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_tcn_init, stp_top_tcn_next))
+				goto done;
+			break;
+		case STP_TopologyChange_ACTIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_active_init, stp_top_active_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* TopologyChange */
+
+
+static
+int (*bridge_stms[NUM_OF_BRIDGE_STATE_MACHINES])(struct ksz_stp_bridge *) = {
+	PortRoleSelection,
+};
+
+/* The bridge state machine needs to be run between the port state machines. */
+#define PORT_STATE_MACHINES_BREAK	5
+#define PORT_STATE_MACHINES_TX		8
+
+static
+int (*port_stms[NUM_OF_PORT_STATE_MACHINES])(struct ksz_stp_port *) = {
+	PortTimers,
+	PortReceive,
+	PortProtocolMigration,
+	BridgeDetection,
+	PortInformation,
+
+	PortRoleTransitions,
+	PortStateTransition,
+	TopologyChange,
+	PortTransmit,
+};
+
+
+static void stp_state_machines(struct ksz_stp_bridge *br)
+{
+	int changed;
+	int update;
+	int i;
+	uint p;
+	struct ksz_stp_port *port;
+
+	do {
+		changed = update = 0;
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = 0; i < PORT_STATE_MACHINES_BREAK; i++) {
+				port->state_index = i;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+			}
+		}
+		for (i = 0; i < NUM_OF_BRIDGE_STATE_MACHINES; i++) {
+			br->state_index = i;
+			changed = bridge_stms[i](br);
+			update |= (changed << 0);
+		}
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_BREAK;
+			     i < PORT_STATE_MACHINES_TX; i++) {
+				port->state_index = i;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+			}
+		}
+
+		/*
+		 * Cannot send if all received BPDU are not processed in case
+		 * they are looped back to the bridge.
+		 */
+		if (br->skip_tx)
+			break;
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_TX;
+			     i < NUM_OF_PORT_STATE_MACHINES; i++) {
+				port->state_index = i;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+			}
+		}
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+	} while (update);
+}  /* stp_state_machines */
+
+static void proc_state_machines(struct work_struct *work)
+{
+	struct ksz_stp_info *stp =
+		container_of(work, struct ksz_stp_info, state_machine);
+
+	stp->machine_running = true;
+	mutex_lock(&stp->br.lock);
+	stp_state_machines(&stp->br);
+	mutex_unlock(&stp->br.lock);
+	stp->machine_running = false;
+	if (stp->br.skip_tx)
+		schedule_work(&stp->rx_proc);
+}  /* proc_state_machines */
+
+static void invoke_state_machines(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_info *stp = br->parent;
+
+	if (br->bridgeEnabled && stp->timer_tick)
+		schedule_work(&stp->state_machine);
+}  /* invoke_state_machines */
+
+static void stp_br_init(struct ksz_stp_port *p)
+{
+	BridgeIdentifier.prio = htons(0x8000);
+	BridgePriority.prio.root.prio =
+	BridgePriority.prio.bridge_id.prio =
+		BridgeIdentifier.prio;
+
+	/* MigrateTime is only used internally. */
+	MigrateTime = to_stp_timer(3);
+	BridgeHelloTime = 2;
+	BridgeMaxAge = 20;
+	BridgeFwdDelay = 15;
+	TxHoldCount = 6;
+	ForceProtocolVersion = 2;
+
+	p->br->hack_4_1 = 0;
+	p->br->hack_4_2 = 0;
+	p->br->hack_5_2 = 0;
+}  /* stp_br_init */
+
+static void stp_port_init(struct ksz_stp_port *p)
+{
+	portId.prio = 0x80;
+	adminPointToPointMAC = ADMIN_P2P_AUTO;
+	AdminPortPathCost = 0;
+	AdminEdge = FALSE;
+	AutoEdge = FALSE;
+#if 1
+	AutoEdge = TRUE;
+#endif
+}  /* stp_port_init */
+
+static void stp_state_init(struct ksz_stp_bridge *br)
+{
+	int i;
+	uint port;
+	struct ksz_stp_port *p;
+	struct ksz_stp_info *stp = br->parent;
+
+	stp->timer_tick = 1000;
+	for (i = 0; i < NUM_OF_BRIDGE_STATE_MACHINES; i++) {
+		br->state_index = i;
+		br->states[br->state_index] = STP_BEGIN;
+	}
+	for (port = 0; port < br->port_cnt; port++) {
+		p = &br->ports[port];
+		for (i = 0; i < NUM_OF_PORT_STATE_MACHINES; i++) {
+			p->state_index = i;
+			p->states[p->state_index] = STP_BEGIN;
+		}
+
+		memset(p->vars.timers, 0, sizeof(p->vars.timers));
+		selected = FALSE;
+	}
+	invoke_state_machines(br);
+}  /* stp_state_init */
+
+static int stp_cfg_port(struct ksz_stp_port *p, int link, int speed,
+	int duplex)
+{
+	int change = 0;
+
+	if (link) {
+		if (!portEnabled) {
+			struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+			portEnabled = TRUE;
+if (x->block_jiffies) {
+dbg_msg("%s %d %lu\n", __func__, p->port_index, jiffies - x->block_jiffies);
+x->block_jiffies = jiffies;
+}
+			change = 1;
+		}
+		if (p->speed != speed) {
+			p->speed = speed;
+			if (checkPathCost(p))
+				change = 1;
+		}
+		if (p->duplex != duplex) {
+			p->duplex = duplex;
+			if (checkP2P(p))
+				change = 1;
+		}
+	} else {
+		if (portEnabled) {
+			portEnabled = FALSE;
+			change = 1;
+		}
+	}
+	return change;
+}  /* stp_cfg_port */
+
+static void stp_disable_port(struct ksz_stp_info *stp, uint port)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[port];
+
+	portEnabled = FALSE;
+	p->link = 0;
+}  /* stp_disable_port */
+
+static void stp_enable_port(struct ksz_stp_info *stp, uint port, u8 *state)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[port];
+
+	if (br->bridgeEnabled && !p->off)
+		*state = STP_STATE_DISABLED;
+}  /* stp_enable_port */
+
+#if 0
+static u8 wrong_root[] = {
+	0x10, 0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66
+};
+#endif
+
+static void stp_proc_rx(struct ksz_stp_port *p, struct bpdu *bpdu, u16 len)
+{
+	u16 size;
+#ifdef DBG_STP_RX
+	int cmp;
+	int bpdu_len = len;
+
+	if (bpdu_len > sizeof(struct bpdu))
+		bpdu_len = sizeof(struct bpdu);
+	cmp = memcmp(&p->rx_bpdu0, bpdu, bpdu_len);
+	if (cmp || ((bpdu->flags & (PORT_ROLE_DESIGNATED << PORT_ROLE_S)) !=
+	    (PORT_ROLE_DESIGNATED << PORT_ROLE_S) &&
+	    BPDU_TYPE_CONFIG_RSTP == bpdu->type)) {
+		if (p->dbg_rx < 2)
+			p->dbg_rx = 2;
+	}
+	if (p->dbg_rx)
+		p->dbg_rx--;
+	if (p->dbg_rx) {
+dbg_msg("[R] %d: ", p->port_index);
+		disp_bpdu(bpdu);
+	}
+	memcpy(&p->rx_bpdu0, bpdu, bpdu_len);
+#endif
+
+	/* Reject bad BPDU. */
+	if (bpdu->protocol != 0)
+		return;
+	size = sizeof(struct bpdu);
+	if (BPDU_TYPE_CONFIG == bpdu->type)
+		size--;
+	else if (BPDU_TYPE_TCN == bpdu->type)
+		size = 7;
+	else if (BPDU_TYPE_CONFIG_RSTP != bpdu->type)
+		size = 10000;
+	if (len < size)
+		return;
+	bpduVersion = bpdu->version <= 2 ? bpdu->version : 2;
+	bpduType = bpdu->type;
+	bpduFlags = 0;
+	bpduRole = (bpdu->flags >> PORT_ROLE_S) & PORT_ROLE_DESIGNATED;
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+		bpduFlags = bpdu->flags;
+	else if (BPDU_TYPE_TCN != bpdu->type)
+		bpduFlags = bpdu->flags &
+			(TOPOLOGY_CHANGE_ACK | TOPOLOGY_CHANGE);
+	if (BPDU_TYPE_TCN != bpdu->type) {
+#if 0
+if (!memcmp(&bpdu->root, wrong_root, 8)) {
+dbg_msg(" wrong root\n");
+bpdu->root.prio = 0;
+}
+#endif
+		COPY(bpduPriority, bpdu->root);
+		bpduTimes.message_age = get_bpdu_time(bpdu->message_age);
+		bpduTimes.max_age = get_bpdu_time(bpdu->max_age);
+		bpduTimes.hello_time = get_bpdu_time(bpdu->hello_time);
+		bpduTimes.forward_delay = get_bpdu_time(bpdu->forward_delay);
+		if ((BPDU_TYPE_CONFIG == bpdu->type &&
+		    bpduTimes.message_age >= bpduTimes.max_age) ||
+		    bpduTimes.message_age > bpduTimes.max_age)
+			return;
+	} else {
+		ZERO(bpduPriority);
+		ZERO(bpduTimes);
+	}
+	rcvdBPDU = TRUE;
+}  /* stp_proc_rx */
+
+static void stp_proc_tick(struct ksz_stp_bridge *br)
+{
+	uint i;
+	struct ksz_stp_port *p;
+
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		tick = TRUE;
+if (p->dbg_rx)
+	p->dbg_rx--;
+	}
+	invoke_state_machines(br);
+}  /* stp_proc_tick */
+
+static void proc_rx(struct work_struct *work)
+{
+	struct ksz_stp_info *stp =
+		container_of(work, struct ksz_stp_info, rx_proc);
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p;
+	uint i;
+	bool not_empty;
+	bool last;
+	struct sk_buff *skb;
+
+	if (mutex_is_locked(&stp->br.lock)) {
+		schedule_work(&stp->rx_proc);
+		return;
+	}
+	not_empty = false;
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (rcvdBPDU)
+			continue;
+		last = skb_queue_empty(&p->rxq);
+		not_empty |= !last;
+		if (!last) {
+			skb = skb_dequeue(&p->rxq);
+			last = skb_queue_empty(&p->rxq);
+			if (skb) {
+				uint port;
+				struct bpdu *bpdu;
+				u16 len = 0;
+
+				port = skb->cb[0];
+				bpdu = chk_bpdu(skb->data, &len);
+				if (bpdu)
+					stp_proc_rx(p, bpdu, len);
+				dev_kfree_skb_irq(skb);
+			}
+		} else
+			br->port_rx &= ~(1 << p->port_index);
+	}
+	if (not_empty || br->port_rx) {
+		br->skip_tx = true;
+		invoke_state_machines(br);
+	} else if (!br->port_rx && br->skip_tx) {
+		br->skip_tx = false;
+		invoke_state_machines(br);
+	}
+}  /* proc_rx */
+
+static int stp_rcv(struct ksz_stp_info *stp, struct sk_buff *skb, uint port)
+{
+	struct bpdu *bpdu;
+	u16 len = 0;
+
+	bpdu = chk_bpdu(skb->data, &len);
+	if (bpdu) {
+		struct ksz_stp_bridge *br = &stp->br;
+		struct ksz_stp_port *p = &br->ports[port];
+
+		if (stp->machine_running || rcvdBPDU || br->port_rx) {
+
+			/* Use control buffer to save port information. */
+			skb->cb[0] = (char) port;
+			skb_queue_tail(&p->rxq, skb);
+			br->port_rx |= (1 << port);
+			schedule_work(&stp->rx_proc);
+		} else {
+			stp_proc_rx(p, bpdu, len);
+			dev_kfree_skb_irq(skb);
+
+			invoke_state_machines(br);
+		}
+		return 0;
+	}
+	return 1;
+}  /* stp_rcv */
+
+static void port_timer_monitor(unsigned long ptr)
+{
+	struct ksz_stp_info *stp = (struct ksz_stp_info *) ptr;
+
+	stp_proc_tick(&stp->br);
+
+	ksz_update_timer(&stp->port_timer_info);
+}  /* port_timer_monitor */
+
+static void reselectAll(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		reselect = TRUE;
+		selected = FALSE;
+		p->rx_bpdu0.protocol = 0xff;
+		p->tx_bpdu0.protocol = 0xff;
+	)
+}
+
+static const u8 *stp_br_id(struct ksz_stp_info *stp)
+{
+	return (u8 *) &stp->br.vars.br_id_;
+}
+
+static int stp_change_addr(struct ksz_stp_info *stp, u8 *addr)
+{
+	int diff;
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[0];
+
+	mutex_lock(&br->lock);
+	diff = memcmp(BridgeIdentifier.addr, addr, ETH_ALEN);
+	memcpy(BridgeIdentifier.addr, addr, ETH_ALEN);
+
+	COPY(BridgePriority.prio.root, BridgeIdentifier);
+	COPY(BridgePriority.prio.bridge_id, BridgeIdentifier);
+	mutex_unlock(&br->lock);
+
+	memcpy(&stp->tx_frame[6], addr, ETH_ALEN);
+	if (diff) {
+		reselectAll(br);
+		invoke_state_machines(br);
+	}
+	return diff;
+}  /* stp_change_addr */
+
+static void stp_set_addr(struct ksz_stp_info *stp, u8 *addr)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[0];
+
+	memcpy(BridgeIdentifier.addr, addr, ETH_ALEN);
+}  /* stp_set_addr */
+
+static void stp_link_change(struct ksz_stp_info *stp, int update)
+{
+	uint i;
+	uint j;
+	int duplex;
+	int speed;
+	u8 state;
+	int change = 0;
+	struct ksz_port_info *info;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	if (!br->bridgeEnabled)
+		return;
+	mutex_lock(&br->lock);
+	for (i = 0; i < br->port_cnt; i++) {
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+		if (skip_host_port(sw, i))
+			continue;
+#endif
+		if (!(sw->dev_ports & (1 << i)))
+			continue;
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		j = i;
+		info = &sw->port_info[j];
+		if (p->link != (media_connected == info->state)) {
+			p->link = (media_connected == info->state);
+			if (p->link)
+				state = STP_STATE_BLOCKED;
+			else
+				state = STP_STATE_DISABLED;
+#ifdef CONFIG_KSZ_MRP
+			if (!p->link && (sw->features & MRP_SUPPORT)) {
+				struct mrp_info *mrp = &sw->mrp;
+
+				mrp_close_port(mrp, j);
+			}
+#endif
+			sw->ops->acquire(sw);
+			port_set_stp_state(sw, j, state);
+			sw->ops->release(sw);
+			duplex = (2 == info->duplex);
+			speed = info->tx_rate / TX_RATE_UNIT;
+			change |= stp_cfg_port(p, p->link, speed, duplex);
+		}
+	}
+	mutex_unlock(&br->lock);
+	if (change && update)
+		invoke_state_machines(br);
+}  /* stp_link_change */
+
+static int stp_get_tcDetected(struct ksz_stp_info *stp, int i)
+{
+	struct ksz_stp_port *p;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	p = &br->ports[i];
+	return tcDetected != 0;
+}  /* stp_get_tcDetected */
+
+static void stp_start(struct ksz_stp_info *stp)
+{
+	uint i;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	stp->dev = sw->main_dev;
+	sw->ops->acquire(sw);
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		if (skip_host_port(sw, i))
+			continue;
+#else
+	for (i = 0; i < SWITCH_PORT_NUM; i++) {
+#endif
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		port_set_stp_state(sw, i, STP_STATE_DISABLED);
+	}
+	sw->ops->release(sw);
+	stp->timer_tick = 1000;
+	ksz_start_timer(&stp->port_timer_info, stp->port_timer_info.period);
+	stp_link_change(stp, false);
+	if (!stp_change_addr(stp, sw->info->mac_addr))
+		stp_state_init(&stp->br);
+}  /* stp_start */
+
+static void stp_stop(struct ksz_stp_info *stp, int hw_access)
+{
+	uint i;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	if (hw_access) {
+		sw->ops->acquire(sw);
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+		for (i = 0; i < sw->mib_port_cnt; i++) {
+			if (skip_host_port(sw, i))
+				continue;
+#else
+		for (i = 0; i < SWITCH_PORT_NUM; i++) {
+#endif
+			p = &br->ports[i];
+			if (p->off)
+				continue;
+			port_set_stp_state(sw, i, STP_STATE_FORWARDING);
+		}
+		sw->ops->release(sw);
+	}
+#ifdef CONFIG_KSZ_IBA_ONLY
+	sw->features &= ~(STP_SUPPORT);
+	stp->br.bridgeEnabled = FALSE;
+#endif
+	ksz_stop_timer(&stp->port_timer_info);
+	stp->timer_tick = 0;
+	flush_work(&stp->rx_proc);
+	flush_work(&stp->state_machine);
+
+	p = &br->ports[0];
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		stp_port_init(p);
+	}
+}  /* stp_stop */
+
+static void stp_br_test_setup(struct ksz_stp_bridge *br)
+{
+	uint i;
+	struct ksz_stp_port *p;
+
+	p = &br->ports[0];
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		stp_port_init(p);
+
+		/* Switch back to RSTP if port is in STP. */
+		mcheck = TRUE;
+		AdminPortPathCost = 200000;
+		checkPathCost(p);
+		checkP2P(p);
+		p->rx_bpdu0.protocol = 0xff;
+	}
+}  /* stp_br_test_setup */
+
+#define BR_ID_FMT  "%04x.%02x%02x%02x%02x%02x%02x"
+
+#define BR_ID_ARGS(x)  \
+	ntohs(x.prio), x.addr[0], x.addr[1], x.addr[2], \
+	x.addr[3], x.addr[4], x.addr[5]
+
+#define BOOL_STR(x)  ((x) ? "yes" : "no")
+
+static char *get_admin_p2p_str(int p2p)
+{
+	switch (p2p) {
+	case ADMIN_P2P_FORCE_FALSE:
+		return "no";
+	case ADMIN_P2P_FORCE_TRUE:
+		return "yes";
+	case ADMIN_P2P_AUTO:
+		return "auto";
+	}
+	return "unk";
+}
+
+static char *get_port_state_str(int state)
+{
+	switch (state) {
+	case STP_PortStateTrans_DISCARDING:
+		return "discarding";
+	case STP_PortStateTrans_LEARNING:
+		return "learning";
+	case STP_PortStateTrans_FORWARDING:
+		return "forwarding";
+	}
+	return "unknown";
+}
+
+static char *get_port_role_str(int _role)
+{
+	switch (_role) {
+	case ROLE_ROOT:
+		return "root";
+	case ROLE_DESIGNATED:
+		return "designated";
+	case ROLE_ALTERNATE:
+		return "alternate";
+	case ROLE_BACKUP:
+		return "backup";
+	case ROLE_DISABLED:
+		return "disabled";
+	}
+	return "unknown";
+}
+
+static ssize_t sysfs_stp_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	int chk = 0;
+	int type = SHOW_HELP_NUM;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & STP_SUPPORT))
+		return 0;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[0];
+	switch (proc_num) {
+	case PROC_GET_STP_BR_INFO:
+		len += sprintf(buf + len,
+			" enabled\t\t%4s\n",
+			BOOL_STR(stp->br.bridgeEnabled));
+		len += sprintf(buf + len,
+			" bridge id\t\t" BR_ID_FMT "\n",
+			BR_ID_ARGS(BridgeIdentifier));
+		len += sprintf(buf + len,
+			" designated root\t" BR_ID_FMT "\n",
+			BR_ID_ARGS(rootPriority.root));
+		len += sprintf(buf + len,
+			" root port\t\t%4u",
+			rootPortId.num);
+		len += sprintf(buf + len,
+			"\t\t\tpath cost\t%12u\n",
+			ntohl(rootPriority.root_path_cost));
+		len += sprintf(buf + len,
+			" max age\t\t%4u",
+			rootTimes.max_age);
+		len += sprintf(buf + len,
+			"\t\t\tbridge max age\t\t%4u\n",
+			BridgeMaxAge);
+		len += sprintf(buf + len,
+			" hello time\t\t%4u",
+			rootTimes.hello_time);
+		len += sprintf(buf + len,
+			"\t\t\tbridge hello time\t%4u\n",
+			BridgeHelloTime);
+		len += sprintf(buf + len,
+			" forward delay\t\t%4u",
+			rootTimes.forward_delay);
+		len += sprintf(buf + len,
+			"\t\t\tbridge forward delay\t%4u\n",
+			BridgeFwdDelay);
+		len += sprintf(buf + len,
+			" tx hold count\t\t%4u",
+			TxHoldCount);
+		len += sprintf(buf + len,
+			"\t\t\tprotocol version\t%4u\n",
+			ForceProtocolVersion);
+		len += sprintf(buf + len,
+			" time since topology change\t%9u\n",
+			timeSinceTC);
+		len += sprintf(buf + len,
+			" topology change count\t\t%9u\n",
+			cntTC);
+		len += sprintf(buf + len,
+			" topology change\t\t%9s\n",
+			BOOL_STR(isTC));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_BR_ON:
+		chk = p->br->bridgeEnabled;
+		type = SHOW_HELP_ON_OFF;
+#if defined(DBG_STP_STATE) || defined(DBG)
+		d_stp_states(p->br);
+#endif
+		break;
+	case PROC_SET_STP_BR_PRIO:
+		chk = ntohs(BridgeIdentifier.prio);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_STP_BR_FWD_DELAY:
+		chk = BridgeFwdDelay;
+		break;
+	case PROC_SET_STP_BR_MAX_AGE:
+		chk = BridgeMaxAge;
+		break;
+	case PROC_SET_STP_BR_HELLO_TIME:
+		chk = BridgeHelloTime;
+		break;
+	case PROC_SET_STP_BR_TX_HOLD:
+		chk = TxHoldCount;
+		break;
+	case PROC_SET_STP_VERSION:
+		chk = ForceProtocolVersion;
+		if (sw->verbose) {
+			switch (chk) {
+			case 2:
+				strcpy(note, " (rstp)");
+				break;
+			case 0:
+				strcpy(note, " (stp)");
+				break;
+			default:
+				strcpy(note, " (unknown)");
+			}
+		}
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+	}
+	mutex_unlock(&stp->br.lock);
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_stp_read */
+
+static int sysfs_stp_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p;
+	uint i;
+	int set;
+	int change = 0;
+	int processed = true;
+
+	if (!(sw->features & STP_SUPPORT))
+		return false;
+	mutex_lock(&stp->br.lock);
+	p = &br->ports[0];
+	switch (proc_num) {
+	case PROC_SET_STP_BR_ON:
+		set = !!num;
+		if (set != br->bridgeEnabled) {
+			br->bridgeEnabled = set;
+			if (br->bridgeEnabled) {
+				mutex_unlock(&stp->br.lock);
+				stp_start(stp);
+				mutex_lock(&stp->br.lock);
+			} else
+				stp_stop(stp, true);
+		}
+		break;
+	case PROC_SET_STP_BR_PRIO:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		if ((0 <= num && num <= 0xf000) && !(num & ~0xf000)) {
+			u16 prio = ntohs(BridgeIdentifier.prio);
+
+			if (num != prio) {
+				prio = num;
+				BridgeIdentifier.prio = htons(prio);
+				BridgePriority.prio.root.prio =
+				BridgePriority.prio.bridge_id.prio =
+					BridgeIdentifier.prio;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_FWD_DELAY:
+		if (checkParameters(BridgeHelloTime, BridgeMaxAge, num)) {
+			if (num != BridgeFwdDelay) {
+				BridgeFwdDelay = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_MAX_AGE:
+		if (checkParameters(BridgeHelloTime, num, BridgeFwdDelay)) {
+			if (num != BridgeMaxAge) {
+				BridgeMaxAge = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_HELLO_TIME:
+		if (num == 2) {
+			if (num != BridgeHelloTime) {
+				BridgeHelloTime = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_TX_HOLD:
+		if (1 <= num && num <= 10) {
+			if (num != TxHoldCount) {
+				TxHoldCount = num;
+				for (i = 0; i < br->port_cnt; i++) {
+					p = &br->ports[i];
+					txCount = 0;
+				}
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_VERSION:
+		if (0 == num || 2 == num) {
+			ForceProtocolVersion = num;
+			change = 2;
+		}
+		if (1 == num) {
+			stp_br_test_setup(br);
+			change = 1;
+		}
+		if (41 == num)
+			br->hack_4_1 = 1;
+		if (42 == num)
+			br->hack_4_2 = 1;
+		if (52 == num)
+			br->hack_5_2 = 1;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	mutex_unlock(&stp->br.lock);
+	if (change && br->bridgeEnabled) {
+		reselectAll(br);
+		if (2 == change)
+			stp_state_init(br);
+		else
+			invoke_state_machines(br);
+	}
+	return processed;
+}  /* sysfs_stp_write */
+
+static ssize_t sysfs_stp_port_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	char *state_str;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & STP_SUPPORT))
+		return 0;
+#ifdef USE_FEWER_PORTS
+	port = chk_last_port(sw, port);
+	if (port == sw->HOST_PORT)
+		return 0;
+#endif
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[port];
+	switch (proc_num) {
+	case PROC_GET_STP_INFO:
+		if (p->off)
+			break;
+		state_str = get_port_role_str(role);
+		len += sprintf(buf + len,
+			" enabled\t\t%4s",
+			BOOL_STR(portEnabled));
+		len += sprintf(buf + len,
+			"\t\t\trole\t\t%12s\n",
+			state_str);
+		state_str = get_port_state_str(p->states[6]);
+		len += sprintf(buf + len,
+			" port id\t\t%02x%02x\t\t\tstate\t\t%12s\n",
+		       portId.prio, portId.num, state_str);
+		len += sprintf(buf + len,
+			" path cost\t%12u\t\t\tadmin path cost\t%12u\n",
+			PortPathCost, AdminPortPathCost);
+		len += sprintf(buf + len,
+			" designated root\t" BR_ID_FMT,
+			BR_ID_ARGS(designatedPriority.root));
+		len += sprintf(buf + len,
+			"\tdesignated cost\t%12u\n",
+			ntohl(designatedPriority.root_path_cost));
+		len += sprintf(buf + len,
+			" designated bridge\t" BR_ID_FMT,
+			BR_ID_ARGS(designatedPriority.bridge_id));
+		len += sprintf(buf + len,
+			"\tdesignated port\t\t%02x%02x\n",
+			designatedPriority.port_id.prio,
+			designatedPriority.port_id.num);
+		len += sprintf(buf + len,
+			" admin edge port\t%4s",
+			BOOL_STR(AdminEdge));
+		len += sprintf(buf + len,
+			"\t\t\tauto edge port\t\t%4s\n",
+			BOOL_STR(AutoEdge));
+		len += sprintf(buf + len,
+			" oper edge port\t\t%4s",
+			BOOL_STR(operEdge));
+		len += sprintf(buf + len,
+			"\t\t\ttopology change ack\t%4s\n",
+			BOOL_STR(tcAck));
+		len += sprintf(buf + len,
+			" point to point\t\t%4s",
+			BOOL_STR(operPointToPointMAC));
+		len += sprintf(buf + len,
+			"\t\t\tadmin point to point\t%4s\n",
+			get_admin_p2p_str(adminPointToPointMAC));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_ON:
+		chk = portEnabled;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_PRIO:
+		chk = portId.prio;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_STP_ADMIN_PATH_COST:
+		chk = AdminPortPathCost;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_STP_PATH_COST:
+		chk = PortPathCost;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_STP_ADMIN_EDGE:
+		chk = AdminEdge;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_AUTO_EDGE:
+		chk = AutoEdge;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_MCHECK:
+		chk = mcheck;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_ADMIN_P2P:
+		chk = adminPointToPointMAC;
+		if (sw->verbose) {
+			switch (chk) {
+			case ADMIN_P2P_AUTO:
+				strcpy(note, " (auto)");
+				break;
+			case ADMIN_P2P_FORCE_TRUE:
+				strcpy(note, " (force true)");
+				break;
+			case ADMIN_P2P_FORCE_FALSE:
+				strcpy(note, " (force false)");
+				break;
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	}
+	mutex_unlock(&stp->br.lock);
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_stp_port_read */
+
+static int sysfs_stp_port_write(struct ksz_sw *sw, int proc_num, uint port,
+	int num, const char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	int set;
+	int change = 0;
+	int processed = true;
+
+	if (!(sw->features & STP_SUPPORT))
+		return false;
+#ifdef USE_FEWER_PORTS
+	port = chk_last_port(sw, port);
+	if (port == sw->HOST_PORT)
+		return false;
+#endif
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[port];
+	switch (proc_num) {
+	case PROC_SET_STP_ON:
+		set = !!num;
+		if (set != portEnabled) {
+			portEnabled = set;
+			if (portEnabled && p->br->hack_4_2)
+				mdelayWhile = 0;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_PRIO:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		if ((0 <= num && num <= 0xf0) && !(num & ~0xf0)) {
+			u8 prio = portId.prio;
+
+			if (num != prio) {
+				portId.prio = num;
+				reselect = TRUE;
+				selected = FALSE;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_ADMIN_PATH_COST:
+		if (0 <= num && num <= PATH_COST * 10) {
+			AdminPortPathCost = num;
+			change = checkPathCost(p);
+		}
+		break;
+	case PROC_SET_STP_PATH_COST:
+		if (1 <= num && num <= PATH_COST * 10) {
+			AdminPortPathCost = num;
+			change = checkPathCost(p);
+		}
+		break;
+	case PROC_SET_STP_ADMIN_EDGE:
+		set = !!num;
+		if (set != AdminEdge) {
+			AdminEdge = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_AUTO_EDGE:
+		set = !!num;
+		if (set != AutoEdge) {
+			AutoEdge = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_MCHECK:
+		set = !!num;
+		if (set != mcheck) {
+			mcheck = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_ADMIN_P2P:
+		if (ADMIN_P2P_FORCE_FALSE <= num && num <= ADMIN_P2P_AUTO) {
+			if (num != adminPointToPointMAC) {
+				adminPointToPointMAC = num;
+				change = checkP2P(p);
+			}
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	mutex_unlock(&stp->br.lock);
+	if (change) {
+		p->rx_bpdu0.protocol = 0xff;
+		invoke_state_machines(p->br);
+	}
+	return processed;
+}  /* sysfs_stp_port_write */
+
+static u8 MAC_ADDR_STP[] = { 0x01, 0x80, 0xC2, 0x00, 0x00, 0x00 };
+
+static void prep_stp_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, MAC_ADDR_STP, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_stp_mcast */
+
+static void leave_stp(struct ksz_stp_info *stp)
+{
+	if (stp->dev)
+		dev_mc_del(stp->dev, MAC_ADDR_STP);
+}  /* leave_stp */
+
+static struct stp_ops stp_ops = {
+	.change_addr		= stp_change_addr,
+	.link_change		= stp_link_change,
+
+	.get_tcDetected		= stp_get_tcDetected,
+};
+
+static void ksz_stp_exit(struct ksz_stp_info *stp)
+{
+	flush_work(&stp->state_machine);
+	flush_work(&stp->rx_proc);
+}  /* ksz_stp_exit */
+
+static void ksz_stp_init(struct ksz_stp_info *stp, struct ksz_sw *sw)
+{
+	struct ksz_stp_bridge *br;
+	struct ksz_stp_port *p;
+	uint i;
+	int num;
+	struct llc *llc;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_dbg_times *x;
+#endif
+
+	stp->sw_dev = sw;
+	ksz_init_timer(&stp->port_timer_info, STP_TIMER_TICK * HZ / 1000,
+		port_timer_monitor, stp);
+	INIT_WORK(&stp->state_machine, proc_state_machines);
+	INIT_WORK(&stp->rx_proc, proc_rx);
+	mutex_init(&stp->br.lock);
+	stp->ops = &stp_ops;
+
+	memcpy(stp->tx_frame, MAC_ADDR_STP, ETH_ALEN);
+	llc = (struct llc *) &stp->tx_frame[12];
+	llc->dsap = 0x42;
+	llc->ssap = 0x42;
+	llc->ctrl = 0x03;
+	stp->bpdu = (struct bpdu *)(llc + 1);
+
+	br = &stp->br;
+	br->parent = stp;
+	if (sw->stp)
+		br->bridgeEnabled = TRUE;
+
+	br->port_cnt = sw->port_cnt;
+
+	/* Can turn off ports.  Useful for using one port for telnet. */
+	num = sw->stp;
+	if (1 == num)
+		num = sw->PORT_MASK;
+	num &= ~sw->HOST_MASK;
+	for (i = 0; i < SWITCH_PORT_NUM; i++) {
+		p = &br->ports[i];
+		if (!(num & (1 << i)))
+			p->off = TRUE;
+		skb_queue_head_init(&p->rxq);
+		p->port_index = i;
+		p->br = br;
+		stp_port_init(p);
+	}
+
+	num = 1;
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (p->off) {
+			selectedRole = ROLE_DISABLED;
+			role = ROLE_DISABLED;
+			infoIs = INFO_TYPE_DISABLED;
+			synced = TRUE;
+			continue;
+		}
+		portId.num = num++;
+
+#ifdef DBG_STP_PORT_FLUSH
+		x = &p->dbg_times[0];
+
+		/* No Root Port connected to port yet. */
+		x->role_ = ROLE_DISABLED;
+		x->downPriority.port_id.num = 0;
+#endif
+	}
+
+	p = &br->ports[0];
+	ZERO(BridgePriority);
+	stp_br_init(p);
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	sw->info->vid2fid[0] = sw->info->vid2fid[NUM_OF_VID + 1] = 0;
+	for (i = 1; i < NUM_OF_VID; i++) {
+		sw->info->vid2fid[i] = ((i - 1) % (FID_ENTRIES - 1)) + 1;
+	}
+#endif
+}  /* ksz_stp_init */
+
+
+#undef forward
+#undef forwarding
+#undef learn
+#undef learning
+#undef proposed
+#undef proposing
+#undef reselect
+#undef selected
+#undef sync
+#undef synced
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_stp.h b/drivers/net/ethernet/micrel/ksz9897/ksz_stp.h
new file mode 100644
index 0000000..136bb30
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_stp.h
@@ -0,0 +1,347 @@
+/**
+ * Microchip STP driver header
+ *
+ * Copyright (c) 2016-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_STP_H
+#define KSZ_STP_H
+
+
+#define STP_TAG_TYPE			0x1080
+
+
+struct llc {
+	u16 len;
+	u8 dsap;
+	u8 ssap;
+	u8 ctrl;
+} __packed;
+
+struct _bridge_id {
+	u16 prio;
+	u8 addr[6];
+} __packed;
+
+struct _port_id {
+	u8 prio;
+	u8 num;
+} __packed;
+
+struct bpdu {
+	u16 protocol;
+	u8 version;
+	u8 type;
+	u8 flags;
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+	u8 version_1_length;
+} __packed;
+
+
+struct stp_prio {
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+} __packed;
+
+struct stp_vector {
+	struct stp_prio prio;
+	struct _port_id port_id;
+} __packed;
+
+struct stp_times {
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+} __packed;
+
+
+enum {
+	ADMIN_P2P_FORCE_FALSE,
+	ADMIN_P2P_FORCE_TRUE,
+	ADMIN_P2P_AUTO,
+};
+
+enum {
+	ROLE_UNKNOWN,
+	ROLE_ALT_BACKUP,
+	ROLE_ROOT,
+	ROLE_DESIGNATED,
+	ROLE_ALTERNATE,
+	ROLE_BACKUP,
+	ROLE_DISABLED,
+};
+
+enum {
+	INFO_SUPERIOR_DESIGNATED,
+	INFO_REPEATED_DESIGNATED,
+	INFO_INFERIOR_DESIGNATED,
+	INFO_INFERIOR_ROOT_ALT,
+	INFO_OTHER,
+};
+
+enum {
+	INFO_TYPE_UNKNOWN,
+	INFO_TYPE_MINE,
+	INFO_TYPE_AGED,
+	INFO_TYPE_RECEIVED,
+	INFO_TYPE_DISABLED,
+};
+
+struct stp_br_vars {
+	struct _bridge_id br_id_;
+	struct stp_vector bridgePrio_;
+	struct stp_prio rootPrio_;
+	struct stp_times bridgeTimes_;
+	struct stp_times rootTimes_;
+	struct _port_id rootPortId_;
+	u32 TC_sec_;
+	u32 TC_cnt_;
+	u32 TC_set_;
+
+	uint MigrateTime_;
+	uint TxHoldCount_;
+	u8 ForceProtocolVersion_;
+};
+
+struct ksz_stp_timers {
+	/*
+	 * Edge Delay timer is time remaining before port is identified as an
+	 * operEdgePort.
+	 */
+	uint edgeDelayWhile_;
+	/*
+	 * Forward Delay timer is used to delay Port State transitions until
+	 * other Bridges have received spanning tree information.
+	 */
+	uint fdWhile_;
+	/*
+	 * Hello timer is used to ensure that at least one BPDU is sent by a
+	 * Desginated Port in each HelloTime period.
+	 */
+	uint helloWhen_;
+	/*
+	 * Migration Delay timer is used to allow time for another RSTP Bridge
+	 * to synchronize its migration state before the receipt of a BPDU can
+	 * cause this Port to change the BPDU type it sends.
+	 */
+	uint mdelayWhile_;
+	/*
+	 * Recent Backup timer is maintained at twice HelloTime while the Port
+	 * is a Backup Port.
+	 */
+	uint rbWhile_;
+	/*
+	 * Received Info timer is time remaining before spanning tree
+	 * inforamtion is aged out.
+	 */
+	uint rcvdInfoWhile_;
+	/*
+	 * Recent Root timer.
+	 */
+	uint rrWhile_;
+	/*
+	 * Topology Change timer is used to send TCN Messages.
+	 */
+	uint tcWhile_;
+	uint tcDetected_;
+};
+
+struct stp_admin_vars {
+	u32 AdminEdgePort_:1;
+	u32 AutoEdgePort_:1;
+	u32 operPointToPointMAC_:1;
+
+	u8 adminPointToPointMAC_;
+	uint adminPortPathCost_;
+
+};
+
+struct stp_vars {
+	u32 agree_:1;
+	u32 agreed_:1;
+	u32 disputed_:1;
+	u32 fdbFlush_:1;
+	u32 forward_:1;
+	u32 forwarding_:1;
+	u32 learn_:1;
+	u32 learning_:1;
+	u32 mcheck_:1;
+	u32 newInfo_:1;
+	u32 operEdge_:1;
+	u32 portEnabled_:1;
+	u32 proposed_:1;
+	u32 proposing_:1;
+	u32 rcvdBPDU_:1;
+	u32 rcvdMsg_:1;
+	u32 rcvdRSTP_:1;
+	u32 rcvdSTP_:1;
+	u32 rcvdTc_:1;
+	u32 rcvdTcAck_:1;
+	u32 rcvdTcn_:1;
+	u32 reRoot_:1;
+	u32 reselect_:1;
+	u32 selected_:1;
+	u32 sendRSTP_:1;
+	u32 sync_:1;
+	u32 synced_:1;
+	u32 tcAck_:1;
+	u32 tcProp_:1;
+	u32 tick_:1;
+	u32 updtInfo_:1;
+
+	uint ageingTime_;
+
+	u8 infoIs_;
+	u8 rcvdInfo_;
+	u8 role_;
+	u8 selectedRole_;
+
+	uint txCount_;
+	struct _port_id portId_;
+	uint PortPathCost_;
+
+	struct stp_prio desgPrio_;
+	struct stp_prio msgPrio_;
+	struct stp_prio portPrio_;
+	struct stp_times desgTimes_;
+	struct stp_times msgTimes_;
+	struct stp_times portTimes_;
+};
+
+#define NUM_OF_PORT_TIMERS		(9 + 1)
+
+struct stp_port_vars {
+	uint timers[NUM_OF_PORT_TIMERS];
+	struct stp_admin_vars admin_var;
+	struct stp_vars stp_var;
+
+	u8 bpduVersion_;
+	u8 bpduType_;
+	u8 bpduFlags_;
+	u8 bpduRole_;
+	struct stp_prio bpduPrio_;
+	struct stp_times bpduTimes_;
+};
+
+struct ksz_stp_bridge;
+
+#define NUM_OF_PORT_STATE_MACHINES	9
+
+struct ksz_stp_dbg_times {
+	struct stp_prio downPriority;
+	struct stp_prio lastPriority;
+	unsigned long alt_jiffies;
+	unsigned long learn_jiffies;
+	unsigned long block_jiffies;
+	u8 role_;
+};
+
+struct ksz_stp_port {
+	struct stp_port_vars vars;
+
+	u8 states[NUM_OF_PORT_STATE_MACHINES];
+	u8 state_index;
+	u8 port_index;
+	int off;
+	int link;
+	int duplex;
+	int speed;
+	struct sk_buff_head rxq;
+
+#if 0
+	struct bpdu own_bpdu;
+#endif
+	struct bpdu rx_bpdu0;
+	struct bpdu tx_bpdu0;
+#if 0
+	struct bpdu tx_bpdu1;
+#endif
+	int dbg_rx;
+	int dbg_tx;
+#if 0
+	int dbg_tx_bpdu;
+#endif
+
+	struct ksz_stp_dbg_times dbg_times[1];
+
+	struct ksz_stp_bridge *br;
+};
+
+#define NUM_OF_BRIDGE_STATE_MACHINES	1
+
+struct ksz_stp_bridge {
+	struct stp_br_vars vars;
+
+	u8 bridgeEnabled;
+
+	struct ksz_stp_port ports[SWITCH_PORT_NUM];
+	u8 port_cnt;
+	u8 skip_tx;
+	u16 port_rx;
+
+	u8 states[NUM_OF_BRIDGE_STATE_MACHINES];
+	u8 state_index;
+
+	void *parent;
+	struct mutex lock;
+
+	u32 hack_4_1:1;
+	u32 hack_4_2:1;
+	u32 hack_5_2:1;
+};
+
+
+struct ksz_stp_info;
+
+struct stp_ops {
+	int (*change_addr)(struct ksz_stp_info *stp, u8 *addr);
+	void (*link_change)(struct ksz_stp_info *stp, int update);
+
+	int (*dev_req)(struct ksz_stp_info *stp, char *arg, void *info);
+
+	int (*get_tcDetected)(struct ksz_stp_info *info, int p);
+};
+
+struct ksz_stp_info {
+	struct ksz_stp_bridge br;
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct ksz_timer_info port_timer_info;
+	uint timer_tick;
+	struct work_struct state_machine;
+	bool machine_running;
+	struct work_struct rx_proc;
+
+	u8 tx_frame[60];
+	struct bpdu *bpdu;
+	int len;
+
+	struct sw_dev_info *dev_info;
+	uint notifications;
+
+	const struct stp_ops *ops;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_sw_9897.c b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_9897.c
new file mode 100644
index 0000000..a566135
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_9897.c
@@ -0,0 +1,18336 @@
+/**
+ * Microchip gigabit switch common code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+/* (288 - 24 - 4) / (8 * 3) */
+#define MAX_IBA_MIB_ENTRIES		10
+
+#define READ_MIB_ENTRY_SIZE		2
+
+/* (288 - 24 - 4) / (8 * 6) */
+#define MAX_IBA_MAC_ENTRIES		5
+
+/* (288 - 24 - 4) / (8 * 6) */
+#define MAX_IBA_VLAN_ENTRIES		5
+
+#define READ_VLAN_ENTRY_SIZE		3
+#define WRITE_VLAN_ENTRY_SIZE		4
+
+#define MAX_SYSFS_BUF_SIZE		(4080 - 80)
+
+enum {
+	PROC_SW_INFO,
+	PROC_SW_VERSION,
+
+	PROC_SET_SW_DUPLEX,
+	PROC_SET_SW_SPEED,
+	PROC_SET_SW_FORCE,
+	PROC_SET_SW_FLOW_CTRL,
+
+	PROC_SET_SW_FEATURES,
+	PROC_SET_SW_OVERRIDES,
+	PROC_SET_SW_MIB,
+
+	PROC_SET_SW_REG,
+	PROC_SET_SW_VID,
+
+	PROC_DYNAMIC,
+	PROC_STATIC,
+	PROC_VLAN,
+	PROC_HSR,
+
+	PROC_SET_AGING,
+	PROC_SET_FAST_AGING,
+	PROC_SET_LINK_AGING,
+
+	PROC_SET_BROADCAST_STORM,
+	PROC_SET_MULTICAST_STORM,
+	PROC_SET_TX_RATE_QUEUE_BASED,
+	PROC_SET_DIFFSERV,
+	PROC_SET_802_1P,
+
+	PROC_ENABLE_VLAN,
+	PROC_SET_REPLACE_NULL_VID,
+	PROC_SET_DROP_INVALID_VID,
+	PROC_SET_MAC_ADDR,
+	PROC_SET_MIRROR_MODE,
+
+	PROC_SET_IGMP_SNOOP,
+	PROC_SET_IPV6_MLD_SNOOP,
+	PROC_SET_IPV6_MLD_OPTION,
+
+	PROC_SET_AGGR_BACKOFF,
+	PROC_SET_NO_EXC_DROP,
+
+	PROC_SET_JUMBO_PACKET,
+	PROC_SET_LEGAL_PACKET,
+	PROC_SET_LENGTH_CHECK,
+
+	PROC_SET_BACK_PRESSURE_MODE,
+	PROC_SET_SWITCH_FLOW_CTRL,
+	PROC_SET_SWITCH_HALF_DUPLEX,
+	PROC_SET_SWITCH_10_MBIT,
+
+	PROC_SET_FAIR_FLOW_CTRL,
+	PROC_SET_VLAN_BOUNDARY,
+	PROC_SET_DOUBLE_TAG,
+	PROC_SET_ISP_TAG,
+	PROC_SET_HSR_TAG,
+	PROC_SET_HSR_REDBOX_ID,
+	PROC_SET_HSR_NET_ID,
+	PROC_SET_MTU,
+	PROC_SET_FORWARD_UNKNOWN_UNICAST,
+	PROC_SET_UNKNOWN_UNICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST,
+	PROC_SET_UNKNOWN_MULTICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_VID,
+	PROC_SET_UNKNOWN_VID_PORTS,
+
+	PROC_SET_PASS_PAUSE,
+	PROC_ENABLE_PME,
+	PROC_ENABLE_PME_POLARITY,
+
+	PROC_GET_HOST_PORT,
+	PROC_GET_PORTS,
+	PROC_GET_DEV_START,
+	PROC_GET_VLAN_START,
+	PROC_GET_AVB,
+	PROC_GET_STP,
+	PROC_GET_TWO_DEV,
+	PROC_SET_AUTHEN,
+
+	PROC_SET_STATIC_FID,
+	PROC_SET_STATIC_USE_FID,
+	PROC_SET_STATIC_OVERRIDE,
+	PROC_SET_STATIC_VALID,
+	PROC_SET_STATIC_MSTP,
+	PROC_SET_STATIC_PRIO,
+	PROC_SET_STATIC_SRC,
+	PROC_SET_STATIC_DST,
+	PROC_SET_STATIC_PORTS,
+	PROC_SET_STATIC_MAC_ADDR,
+	PROC_SET_STATIC_TYPE,
+	PROC_SET_STATIC_INDEX,
+	PROC_SET_STATIC_INFO,
+
+	PROC_SET_VLAN_VALID,
+	PROC_SET_VLAN_PORTS,
+	PROC_SET_VLAN_UNTAG,
+	PROC_SET_VLAN_FID,
+	PROC_SET_VLAN_MSTP,
+	PROC_SET_VLAN_PRIO,
+	PROC_SET_VLAN_OPTION,
+	PROC_SET_VLAN_VID,
+	PROC_SET_VLAN_INFO,
+	PROC_SET_VID_2_FID,
+	PROC_SET_FID_2_MSTID,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_BR_INFO,
+	PROC_SET_STP_BR_ON,
+	PROC_SET_STP_BR_PRIO,
+	PROC_SET_STP_BR_FWD_DELAY,
+	PROC_SET_STP_BR_MAX_AGE,
+	PROC_SET_STP_BR_HELLO_TIME,
+	PROC_SET_STP_BR_TX_HOLD,
+	PROC_SET_STP_VERSION,
+#ifdef CONFIG_KSZ_MSTP
+	PROC_SET_STP_BR_MAX_HOPS,
+	PROC_SET_STP_MSTI,
+	PROC_SET_STP_MSTI_VID,
+	PROC_SET_STP_MSTP_CFG,
+	PROC_SET_STP_MSTP_NAME,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	PROC_SET_MRP_SRC_ADDR,
+
+#ifdef CONFIG_KSZ_MSRP
+	PROC_GET_MSRP_INFO,
+	PROC_SET_MSRP_ENABLED,
+	PROC_SET_MSRP_SR_A,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	PROC_SET_HSR_VALID,
+	PROC_SET_HSR_AGE_CNT,
+	PROC_SET_HSR_PATH_ID,
+	PROC_SET_HSR_SRC_MAC_ADDR,
+	PROC_SET_HSR_INDEX,
+	PROC_SET_HSR_INFO,
+	PROC_GET_HSR_STATE,
+#endif
+
+	PROC_SET_NO_COLOR,
+	PROC_SET_COLOR_RED,
+	PROC_SET_COLOR_YELLOW,
+	PROC_SET_COLOR_GREEN,
+
+};
+
+enum {
+	PROC_SET_PORT_MIB,
+
+	PROC_SET_DEF_VID,
+	PROC_SET_MEMBER,
+
+	PROC_ENABLE_BROADCAST_STORM,
+	PROC_ENABLE_DIFFSERV,
+	PROC_ENABLE_802_1P,
+	PROC_ENABLE_VLAN_PRIO,
+	PROC_ENABLE_MAC_PRIO,
+	PROC_ENABLE_ACL_PRIO,
+	PROC_SET_HIGHEST_PRIO,
+	PROC_SET_OR_PRIO,
+
+	PROC_SET_PORT_BASED,
+
+	PROC_SET_DIS_NON_VID,
+	PROC_SET_INGRESS,
+	PROC_SET_DROP_NON_VLAN,
+	PROC_SET_DROP_TAG,
+	PROC_SET_REPLACE_VID,
+	PROC_SET_REPLACE_PRIO,
+	PROC_SET_MAC_BASED_802_1X,
+	PROC_SET_SRC_ADDR_FILTER,
+	PROC_SET_VLAN_LOOKUP_0,
+
+	PROC_SET_MSTP,
+	PROC_SET_RX,
+	PROC_SET_TX,
+	PROC_SET_LEARN,
+	PROC_SET_POWER,
+
+	PROC_ENABLE_PRIO_QUEUE,
+
+	PROC_ENABLE_RX_PRIO_RATE,
+	PROC_ENABLE_TX_PRIO_RATE,
+	PROC_SET_LIMIT,
+	PROC_SET_LIMIT_PORT_BASED,
+	PROC_SET_LIMIT_PACKET_BASED,
+	PROC_SET_LIMIT_FLOW_CTRL,
+	PROC_SET_LIMIT_CNT_IFG,
+	PROC_SET_LIMIT_CNT_PRE,
+	PROC_SET_RX_P0_RATE,
+	PROC_SET_RX_P1_RATE,
+	PROC_SET_RX_P2_RATE,
+	PROC_SET_RX_P3_RATE,
+	PROC_SET_RX_P4_RATE,
+	PROC_SET_RX_P5_RATE,
+	PROC_SET_RX_P6_RATE,
+	PROC_SET_RX_P7_RATE,
+	PROC_SET_TX_Q0_RATE,
+	PROC_SET_TX_Q1_RATE,
+	PROC_SET_TX_Q2_RATE,
+	PROC_SET_TX_Q3_RATE,
+
+	PROC_SET_COLOR_MAP,
+	PROC_SET_TC_MAP,
+
+	PROC_SET_MIRROR_PORT,
+	PROC_SET_MIRROR_RX,
+	PROC_SET_MIRROR_TX,
+
+	PROC_SET_BACK_PRESSURE,
+	PROC_SET_FORCE_FLOW_CTRL,
+	PROC_SET_PASS_ALL,
+	PROC_SET_TAIL_TAG,
+
+	PROC_SET_CUSTOM_VID,
+	PROC_SET_SR_1_VID,
+	PROC_SET_SR_2_VID,
+	PROC_SET_SR_1_TYPE,
+	PROC_SET_SR_2_TYPE,
+	PROC_SET_PME_CTRL,
+	PROC_SET_PME_STATUS,
+
+	PROC_SET_AUTHEN_MODE,
+	PROC_SET_ACL,
+	PROC_SET_ACL_FIRST_RULE,
+	PROC_SET_ACL_RULESET,
+	PROC_SET_ACL_MODE,
+	PROC_SET_ACL_ENABLE,
+	PROC_SET_ACL_SRC,
+	PROC_SET_ACL_EQUAL,
+	PROC_SET_ACL_MAC_ADDR,
+	PROC_SET_ACL_TYPE,
+	PROC_SET_ACL_CNT,
+	PROC_SET_ACL_MSEC,
+	PROC_SET_ACL_INTR_MODE,
+	PROC_SET_ACL_IP_ADDR,
+	PROC_SET_ACL_IP_MASK,
+	PROC_SET_ACL_PROTOCOL,
+	PROC_SET_ACL_SEQNUM,
+	PROC_SET_ACL_PORT_MODE,
+	PROC_SET_ACL_MAX_PORT,
+	PROC_SET_ACL_MIN_PORT,
+	PROC_SET_ACL_TCP_FLAG_ENABLE,
+	PROC_SET_ACL_TCP_FLAG,
+	PROC_SET_ACL_TCP_FLAG_MASK,
+	PROC_SET_ACL_PRIO_MODE,
+	PROC_SET_ACL_PRIO,
+	PROC_SET_ACL_VLAN_PRIO_REPLACE,
+	PROC_SET_ACL_VLAN_PRIO,
+	PROC_SET_ACL_MAP_MODE,
+	PROC_SET_ACL_PORTS,
+	PROC_SET_ACL_INDEX,
+	PROC_SET_ACL_ACTION_INDEX,
+	PROC_SET_ACL_ACTION,
+	PROC_SET_ACL_RULE_INDEX,
+	PROC_SET_ACL_INFO,
+	PROC_GET_ACL_TABLE,
+
+	PROC_SET_P_INDEX,
+	PROC_SET_Q_INDEX,
+
+	PROC_SET_POLICE_PACKET_TYPE,
+	PROC_SET_NON_DSCP_COLOR,
+	PROC_ENABLE_POLICE_DROP_ALL,
+	PROC_ENABLE_PORT_BASED_POLICING,
+	PROC_ENABLE_COLOR_MARK,
+	PROC_ENABLE_COLOR_REMAP,
+	PROC_ENABLE_DROP_SRP,
+	PROC_ENABLE_COLOR_AWARE,
+	PROC_ENABLE_POLICE,
+
+	PROC_SET_Q_CIR,
+	PROC_SET_Q_PIR,
+	PROC_SET_Q_CBS,
+	PROC_SET_Q_PBS,
+
+	PROC_SET_WRED_MAX_THRESHOLD,
+	PROC_SET_WRED_MIN_THRESHOLD,
+	PROC_SET_WRED_MULTIPLIER,
+	PROC_GET_WRED_AVG_SIZE,
+
+	PROC_SET_WRED_Q_MAX_THRESHOLD,
+	PROC_SET_WRED_Q_MIN_THRESHOLD,
+	PROC_SET_WRED_Q_MULTIPLIER,
+	PROC_GET_WRED_Q_AVG_SIZE,
+
+	PROC_SET_WRED_RANDOM_DROP,
+	PROC_SET_WRED_DROP_GYR,
+	PROC_SET_WRED_DROP_YR,
+	PROC_SET_WRED_DROP_R,
+	PROC_SET_WRED_DROP_ALL,
+	PROC_GET_WRED_PMON,
+
+	PROC_SET_QUEUE_SCHEDULING,
+	PROC_SET_QUEUE_SHAPING,
+#ifdef MTI_PREEMPT_ENABLE
+	PROC_SET_PREEMPT,
+#endif
+	PROC_SET_TX_RATIO,
+	PROC_SET_CREDIT_HI_WATER_MARK,
+	PROC_SET_CREDIT_LO_WATER_MARK,
+	PROC_SET_CREDIT_INCREMENT,
+	PROC_SET_SRP,
+
+	PROC_SET_QM_DROP,
+	PROC_SET_QM_BURST_SIZE,
+	PROC_SET_QM_RESV_SPACE,
+	PROC_SET_QM_HI_WATER_MARK,
+	PROC_SET_QM_LO_WATER_MARK,
+	PROC_GET_QM_TX_USED,
+	PROC_GET_QM_TX_AVAIL,
+	PROC_GET_QM_TX_CALCULATED,
+
+	PROC_SET_MMD_ID,
+	PROC_SET_MMD_REG,
+	PROC_SET_MMD_VAL,
+
+	PROC_GET_RX_FLOW_CTRL,
+	PROC_GET_TX_FLOW_CTRL,
+
+	PROC_SET_PORT_DUPLEX,
+	PROC_SET_PORT_SPEED,
+
+	PROC_SET_MAC_OPERATIONAL,
+	PROC_SET_VLAN_RESTRICTED,
+	PROC_SET_VLAN_UNTAGGED,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_INFO,
+	PROC_SET_STP_ON,
+	PROC_SET_STP_PRIO,
+	PROC_SET_STP_ADMIN_PATH_COST,
+	PROC_SET_STP_PATH_COST,
+	PROC_SET_STP_ADMIN_EDGE,
+	PROC_SET_STP_AUTO_EDGE,
+	PROC_SET_STP_MCHECK,
+	PROC_SET_STP_ADMIN_P2P,
+	PROC_SET_STP_AUTO_ISOLATE,
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	PROC_SET_PORT_MMRP_ENABLED,
+	PROC_SET_PORT_MMRP_MAC,
+	PROC_SET_PORT_MMRP_SVC,
+	PROC_SET_PORT_MMRP_REG,
+	PROC_SET_PORT_MVRP_ENABLED,
+	PROC_SET_PORT_MVRP_VID,
+	PROC_SET_PORT_MVRP_REG,
+
+#ifdef CONFIG_KSZ_MSRP
+	PROC_SET_PORT_ASCAPABLE,
+	PROC_SET_PORT_MSRP_ENABLED,
+	PROC_SET_TC_DELTA_BANDWIDTH,
+	PROC_SET_TC_ADMIN_IDLE_SLOPE,
+	PROC_GET_TC_OPER_IDLE_SLOPE,
+	PROC_SET_TC_ALGORITHM,
+	PROC_GET_SR_0_RX_PRIO,
+	PROC_SET_SR_0_TX_PRIO,
+	PROC_GET_SR_0_SRP_DOMAIN_BOUNDARY,
+	PROC_GET_SR_1_RX_PRIO,
+	PROC_SET_SR_1_TX_PRIO,
+	PROC_GET_SR_1_SRP_DOMAIN_BOUNDARY,
+#endif
+#endif
+
+	PROC_SET_LINK_MD,
+	PROC_SET_SQI,
+	PROC_SET_MAC_LOOPBACK,
+	PROC_SET_PHY_LOOPBACK,
+	PROC_SET_REMOTE_LOOPBACK,
+
+};
+
+/* -------------------------------------------------------------------------- */
+
+static inline int chk_last_port(struct ksz_sw *sw, uint p)
+{
+#ifdef USE_FEWER_PORTS
+	if (sw->last_port && p == sw->last_port)
+		p = sw->HOST_PORT;
+#endif
+	return p;
+}  /* chk_last_port */
+
+static inline int skip_host_port(struct ksz_sw *sw, uint p)
+{
+	int skip = false;
+
+#ifdef USE_FEWER_PORTS
+	if (sw->last_port && p == sw->last_port)
+		p = sw->HOST_PORT;
+#endif
+	if (p == sw->HOST_PORT)
+		skip = true;
+	return skip;
+}  /* skip_host_port */
+
+static void sw_acquire(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->lock);
+	sw->reg->lock(sw);
+	mutex_unlock(&sw->lock);
+}  /* sw_acquire */
+
+static void sw_release(struct ksz_sw *sw)
+{
+	sw->reg->unlock(sw);
+}  /* sw_release */
+
+#if defined(CONFIG_KSZ_MRP)
+/**
+ * sw_get_dest_port - Convert virtual port mask to physical port mask
+ * @sw:		The switch instance.
+ * @start:	0-based starting port of the network device.
+ * @num:	Number of ports.
+ * @ports:	virtual port map.
+ *
+ * This function converts virtual ports used inside the network device to
+ * actual ports in the switch.
+ */
+static u32 sw_get_dest_port(struct ksz_sw *sw, int start, int num, u32 ports)
+{
+	int c;
+	int i;
+	int j;
+	u32 dest = 0;
+	u32 mask = ports & ~sw->PORT_MASK;
+
+	for (c = 0, i = start, j = start; c < num; c++, i++, j++) {
+		if (i == sw->HOST_PORT)
+			j++;
+		if (ports & (1 << c))
+			dest |= 1 << j;
+	}
+	if (ports & (1 << c))
+		dest |= sw->HOST_MASK;
+	dest |= mask;
+	return dest;
+}  /* sw_get_dest_port */
+
+/**
+ * sw_get_dev_port - Convert virtual port to physical port
+ * @sw:		The switch instance.
+ * @start:	0-based starting port of the network device.
+ * @num:	Number of ports.
+ * @port:	1-based virtual port of the network device.
+ *
+ * This function converts virtual port used inside the network device to
+ * actual port in the switch.
+ */
+static u32 sw_get_dev_port(struct ksz_sw *sw, int start, int num, uint port)
+{
+	if (0 == port || port > num)
+		return sw->HOST_PORT;
+	if (port <= sw->HOST_PORT || start > sw->HOST_PORT)
+		--port;
+	return start + port;
+}  /* sw_get_dev_port */
+
+/**
+ * sw_get_net_port - Convert physical port to virtual port
+ * @sw:		The switch instance.
+ * @start:	0-based starting port of the network device.
+ * @num:	Number of ports.
+ * @port:	0-based physcial port of the switch.
+ *
+ * This function converts physical port used in the switch to virtual port used
+ * by the network device.
+ */
+static u32 sw_get_net_port(struct ksz_sw *sw, int start, int num, uint port)
+{
+	if (port == sw->HOST_PORT)
+		return num + 1;
+	if (port < sw->HOST_PORT || start > sw->HOST_PORT)
+		++port;
+	return port - start;
+}  /* sw_get_net_port */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Common routines used by both SPI and IBA accesses. */
+
+/**
+ * get_mac_table_info - Get MAC table information
+ * @mac:	Buffer to store the MAC table entry.
+ * @data:	Buffer holding MAC table information.
+ *
+ * This helper routine retrieves information from MAC table entry data.
+ */
+static void get_mac_table_info(struct ksz_mac_table *mac, u32 data[])
+{
+	mac->valid = !!(data[0] & ALU_V_STATIC_VALID);
+	mac->src = !!(data[0] & ALU_V_SRC_FILTER);
+	mac->dst = !!(data[0] & ALU_V_DST_FILTER);
+	mac->prio = (data[0] >> ALU_V_PRIO_AGE_CNT_S) &
+		ALU_V_PRIO_AGE_CNT_M;
+	mac->mstp = data[0] & ALU_V_MSTP_M;
+	mac->override = !!(data[1] & ALU_V_OVERRIDE);
+	mac->use_fid = !!(data[1] & ALU_V_USE_FID);
+	mac->ports = data[1] & ALU_V_PORT_MAP;
+	mac->fid = data[2] >> ALU_V_FID_S;
+	mac->addr[1] = (u8) data[2];
+	mac->addr[0] = (u8)(data[2] >> 8);
+	mac->addr[5] = (u8) data[3];
+	mac->addr[4] = (u8)(data[3] >> 8);
+	mac->addr[3] = (u8)(data[3] >> 16);
+	mac->addr[2] = (u8)(data[3] >> 24);
+}  /* get_mac_table_info */
+
+/**
+ * set_mac_table_info - Set MAC table information
+ * @mac:	The MAC table entry.
+ * @data:	Buffer to hold MAC table information.
+ *
+ * This helper routine puts information to MAC table entry.
+ */
+static void set_mac_table_info(struct ksz_mac_table *mac, u32 data[])
+{
+	data[0] = (u32)(mac->prio & ALU_V_PRIO_AGE_CNT_M) <<
+		ALU_V_PRIO_AGE_CNT_S;
+	data[0] |= mac->mstp & ALU_V_MSTP_M;
+	if (mac->src)
+		data[0] |= ALU_V_SRC_FILTER;
+	if (mac->dst)
+		data[0] |= ALU_V_DST_FILTER;
+	if (mac->valid)
+		data[0] |= ALU_V_STATIC_VALID;
+	data[1] = mac->ports & ALU_V_PORT_MAP;
+	if (mac->override)
+		data[1] |= ALU_V_OVERRIDE;
+	if (mac->use_fid)
+		data[1] |= ALU_V_USE_FID;
+	data[2] = (u32) mac->fid << ALU_V_FID_S;
+	data[2] |= ((u32) mac->addr[0] << 8) | mac->addr[1];
+	data[3] = ((u32) mac->addr[2] << 24) |
+		((u32) mac->addr[3] << 16) |
+		((u32) mac->addr[4] << 8) | mac->addr[5];
+}  /* set_mac_table_info */
+
+/**
+ * wait_for_dyn_mac_table - Wait for dynamic MAC table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for dynamic MAC table to be ready for access.
+ */
+static int wait_for_dyn_mac_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	} while ((ctrl & ALU_START) || ALU_SEARCH == (ctrl & ALU_ACTION));
+	return 0;
+}  /* wait_for_dyn_mac_table */
+
+/**
+ * wait_for_sta_mac_table - Wait for static MAC table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for static MAC table to be ready for access.
+ */
+static int wait_for_sta_mac_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+	} while (ctrl & ALU_STAT_START);
+	return 0;
+}  /* wait_for_sta_mac_table */
+
+/**
+ * get_vlan_table_info - Get VLAN table information
+ * @vlan:	Buffer to store the VLAN table entry.
+ * @data:	Buffer holding VLAN table information.
+ *
+ * This helper routine retrieves information from VLAN table entry data.
+ */
+static void get_vlan_table_info(struct ksz_vlan_table *vlan, u32 data[])
+{
+	vlan->valid = !!(data[0] & VLAN_VALID);
+	vlan->fid = (data[0] & VLAN_FID_M);
+	vlan->mstp = (data[0] >> VLAN_MSTP_S) & VLAN_MSTP_M;
+	vlan->prio = (data[0] >> VLAN_PRIO_S) & VLAN_PRIO_M;
+	vlan->option = !!(data[0] & VLAN_FORWARD_OPTION);
+	vlan->untag = data[1];
+	vlan->ports = data[2];
+}  /* get_vlan_table_info */
+
+/**
+ * set_vlan_table_info - set VLAN table information
+ * @vlan:	The VLAN table entry.
+ * @data:	Buffer to hold VLAN table information.
+ *
+ * This helper routine puts information to VLAN table entry.
+ */
+static void set_vlan_table_info(struct ksz_vlan_table *vlan, u32 data[])
+{
+	data[0] = vlan->fid & VLAN_FID_M;
+	data[0] |= (u32)(vlan->mstp & VLAN_MSTP_M) << VLAN_MSTP_S;
+	data[0] |= (u32)(vlan->prio & VLAN_PRIO_M) << VLAN_PRIO_S;
+	if (vlan->option)
+		data[0] |= VLAN_FORWARD_OPTION;
+	if (vlan->valid)
+		data[0] |= VLAN_VALID;
+	data[1] = vlan->untag;
+	data[2] = vlan->ports;
+}  /* set_vlan_table_info */
+
+/**
+ * wait_for_vlan_table - Wait for VLAN table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for VLAN table to be ready for access.
+ */
+static int wait_for_vlan_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+	} while (ctrl & VLAN_START);
+	return 0;
+}  /* wait_for_vlan_table */
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * get_hsr_table_info - Get HSR table information
+ * @hsr:	Buffer to store the HSR table entry.
+ * @data:	Buffer holding HSR table information.
+ *
+ * This helper routine retrieves information from HSR table entry data.
+ */
+static void get_hsr_table_info(struct ksz_hsr_table *hsr, u32 data[])
+{
+	u8 *byte;
+	u16 *word;
+
+	hsr->valid = !!(data[0] & HSR_V_STATIC_VALID);
+	hsr->age_cnt = (data[0] >> HSR_V_AGE_CNT_S) & HSR_V_AGE_CNT_M;
+	hsr->path_id = (data[0] & HSR_V_PATH_ID_M);
+	data[1] = htonl(data[1]);
+	memcpy(hsr->dst_mac, &data[1], 4);
+	byte = (u8 *) &data[2];
+	data[2] = htonl(data[2]);
+	memcpy(&hsr->dst_mac[4], &byte[0], 2);
+	memcpy(&hsr->src_mac[0], &byte[2], 2);
+	data[3] = htonl(data[3]);
+	memcpy(&hsr->src_mac[2], &data[3], 4);
+	word = (u16 *) &data[4];
+	memcpy(&hsr->start_seq[1], word, 2);
+	word++;
+	memcpy(&hsr->start_seq[0], word, 2);
+	word++;
+	memcpy(&hsr->exp_seq[1], word, 2);
+	word++;
+	memcpy(&hsr->exp_seq[0], word, 2);
+	word++;
+	memcpy(&hsr->seq_cnt[1], word, 2);
+	word++;
+	memcpy(&hsr->seq_cnt[0], word, 2);
+	word++;
+}  /* get_hsr_table_info */
+
+/**
+ * set_hsr_table_info - Set HSR table information
+ * @hsr:	The HSR table entry.
+ * @data:	Buffer to hold HSR table information.
+ *
+ * This helper routine puts information to HSR table entry.
+ */
+static void set_hsr_table_info(struct ksz_hsr_table *hsr, u32 data[])
+{
+	u8 *byte;
+	u16 *word;
+
+	data[0] = (u32)(hsr->age_cnt & HSR_V_AGE_CNT_M) << HSR_V_AGE_CNT_S;
+	data[0] |= hsr->path_id & HSR_V_PATH_ID_M;
+	if (hsr->valid)
+		data[0] |= HSR_V_STATIC_VALID;
+	memcpy(&data[1], hsr->dst_mac, 4);
+	data[1] = htonl(data[1]);
+	byte = (u8 *) &data[2];
+	memcpy(&byte[0], &hsr->dst_mac[4], 2);
+	memcpy(&byte[2], &hsr->src_mac[0], 2);
+	data[2] = htonl(data[2]);
+	memcpy(&data[3], &hsr->src_mac[2], 4);
+	data[3] = htonl(data[3]);
+	word = (u16 *) &data[4];
+	memcpy(word, &hsr->start_seq[1], 2);
+	word++;
+	memcpy(word, &hsr->start_seq[0], 2);
+	word++;
+	memcpy(word, &hsr->exp_seq[1], 2);
+	word++;
+	memcpy(word, &hsr->exp_seq[0], 2);
+	word++;
+	memcpy(word, &hsr->seq_cnt[1], 2);
+	word++;
+	memcpy(word, &hsr->seq_cnt[0], 2);
+	word++;
+}  /* set_hsr_table_info */
+
+/**
+ * wait_for_hsr_table - Wait for HSR table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for HSR table to be ready for access.
+ */
+static int wait_for_hsr_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	} while ((ctrl & HSR_START) || HSR_SEARCH == (ctrl & HSR_ACTION));
+	return 0;
+}  /* wait_for_hsr_table */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_9897.c"
+#endif
+#ifdef CONFIG_KSZ_IBA
+static void sw_set_spi(struct ksz_sw *sw, struct ksz_iba_info *iba);
+
+#include "ksz_iba.c"
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Switch functions */
+
+/**
+ * sw_r_mac_table - read from MAC table
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This helper function reads an entry of the MAC table of the switch.
+ */
+static void sw_r_mac_table(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	sw_r(sw, REG_SW_ALU_VAL_A, data, 4 * 4);
+	data[0] = be32_to_cpu(data[0]);
+	data[1] = be32_to_cpu(data[1]);
+	data[2] = be32_to_cpu(data[2]);
+	data[3] = be32_to_cpu(data[3]);
+	get_mac_table_info(mac, data);
+}  /* sw_r_mac_table */
+
+/**
+ * sw_w_mac_table - write to MAC table
+ * @sw:		The switch instance.
+ * @mac:	The MAC table entry.
+ *
+ * This helper function writes an entry of the MAC table of the switch.
+ */
+static void sw_w_mac_table(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	set_mac_table_info(mac, data);
+	data[0] = cpu_to_be32(data[0]);
+	data[1] = cpu_to_be32(data[1]);
+	data[2] = cpu_to_be32(data[2]);
+	data[3] = cpu_to_be32(data[3]);
+	sw_w(sw, REG_SW_ALU_VAL_A, data, 4 * 4);
+}  /* sw_w_mac_table */
+
+/**
+ * sw_s_dyn_mac_table - prepare dynmaic MAC table for access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ *
+ * This helper function prepares dynmaic MAC table for access.
+ */
+static u32 sw_s_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & ALU_DIRECT_INDEX_M;
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_1, data);
+		ctrl |= ALU_DIRECT;
+	} else {
+		data = (u32) src_fid << ALU_FID_INDEX_S;
+		data |= ((u32) src_addr[0] << 8) | src_addr[1];
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_0, data);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_1, data);
+	}
+	ctrl |= ALU_START;
+	return ctrl;
+}  /* sw_s_dyn_mac_table */
+
+/**
+ * sw_r_dyn_mac_hw - read from dynamic MAC table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This function reads an entry of the dynamic MAC table using default access.
+ */
+static int sw_r_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_dyn_mac_table(sw, addr, src_addr, src_fid);
+	ctrl |= ALU_READ;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	sw->reg->w32(sw, REG_SW_ALU_VAL_B, 0);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	do {
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	} while (ctrl & ALU_START);
+	sw_r_mac_table(sw, mac);
+
+	/* Hash read. */
+	if (!addr && entry)
+		*entry = (sw->reg->r16(sw, REG_SW_LUE_INDEX_0__2) &
+			ENTRY_INDEX_M) + 1;
+	return 1;
+}  /* sw_r_dyn_mac_hw */
+
+/**
+ * sw_r_dyn_mac_table - read from dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This routine reads an entry of the dynamic MAC table of the switch.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_r_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	if (entry)
+		*entry = 0;
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_dyn_mac_table(sw))
+		goto r_dyn_exit;
+	sw->reg->r_dyn_mac_hw(sw, addr, src_addr, src_fid, mac, entry);
+
+r_dyn_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_r_dyn_mac_table */
+
+/**
+ * sw_w_dyn_mac_hw - write to dynamic MAC table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This function writes an entry of the dynamic MAC table using default access.
+ */
+static int sw_w_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_dyn_mac_table(sw, addr, src_addr, src_fid);
+	ctrl |= ALU_WRITE;
+	sw_w_mac_table(sw, mac);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_w_dyn_mac_hw */
+
+/**
+ * sw_w_dyn_mac_table - write to dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the dynamic MAC table of the switch.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_w_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_dyn_mac_table(sw))
+		goto w_dyn_exit;
+
+	/* Dynamic MAC table does not use USE_FID bit. */
+	mac->use_fid = 0;
+	sw->reg->w_dyn_mac_hw(sw, addr, src_addr, src_fid, mac);
+
+w_dyn_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_w_dyn_mac_table */
+
+/**
+ * sw_start_dyn_mac_hw - start dynamic MAC table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function starts dynamic MAC table search using default access.
+ */
+static int sw_start_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = ALU_SEARCH;
+	ctrl |= ALU_START;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	sw->reg->w32(sw, REG_SW_ALU_VAL_B, 0);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_start_dyn_mac_hw */
+
+/**
+ * sw_start_dyn_mac_table - start dynamic MAC table search
+ * @sw:		The switch instance.
+ *
+ * This routine starts dynamic MAC table search.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_start_dyn_mac_table(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_dyn_mac_table(sw))
+		goto s_dyn_exit;
+	sw->reg->start_dyn_mac_hw(sw);
+
+s_dyn_exit:
+	sw->ops->release(sw);
+}  /* sw_start_dyn_mac_table */
+
+/**
+ * sw_g_dyn_mac_hw - retrieve dynamic MAC table result using default access
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves dynamic MAC table result using default access.
+ */
+static int sw_g_dyn_mac_hw(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	sw_r_mac_table(sw, mac);
+	return 1;
+}  /* sw_g_dyn_mac_hw */
+
+/**
+ * sw_g_dyn_mac_table - retrieve dynamic MAC table result
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves an entry of the dynamic MAC table search result.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_g_dyn_mac_table(struct ksz_sw *sw, u16 *addr,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+	int rc = 0;
+
+	sw->ops->acquire(sw);
+	do {
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+/**
+ * THa  2015/04/21
+ * The long delay before the chip select is dropped causes the chip to think
+ * the register is written twice, so the valid bit will not be set when read.
+ */
+#if 0
+	if (!*addr && 1 == (ctrl >> ALU_VALID_CNT_S) && !(ctrl & ALU_VALID))
+		ctrl |= ALU_VALID;
+#endif
+	} while (!(ctrl & ALU_VALID) && (ctrl & ALU_START));
+	if (ctrl & ALU_VALID) {
+		ctrl >>= ALU_VALID_CNT_S;
+		ctrl &= ALU_VALID_CNT_M;
+		rc = !sw->reg->g_dyn_mac_hw(sw, mac);
+		if (ctrl != *addr + 1)
+			*addr = ctrl - 1;
+	} else
+		rc = -1;
+	sw->ops->release(sw);
+	return rc;
+}  /* sw_g_dyn_mac_table */
+
+/**
+ * sw_stop_dyn_mac_hw - stop dynamic MAC table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function stops dynamic MAC table search using default access.
+ *
+ * Return the last MAC table control.
+ */
+static u32 sw_stop_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	return ctrl;
+}  /* sw_stop_dyn_mac_hw */
+
+/**
+ * sw_stop_dyn_mac_table - stop dynamic MAC table search
+ * @sw:		The switch instance.
+ * @addr:	The address of the last table entry.
+ *
+ * This routine stops dynamic MAC table search.
+ */
+static void sw_stop_dyn_mac_table(struct ksz_sw *sw, u16 addr)
+{
+	u32 ctrl;
+
+	sw->ops->acquire(sw);
+	ctrl = sw->reg->stop_dyn_mac_hw(sw);
+	ctrl >>= ALU_VALID_CNT_S;
+	ctrl &= ALU_VALID_CNT_M;
+if (ctrl != addr)
+dbg_msg(" ?stop: %x %x\n", ctrl, addr);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_stop_dyn_mac_table */
+
+/**
+ * sw_d_dyn_mac_table - dump dynamic MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps dynamic MAC table contents.
+ */
+static ssize_t sw_d_dyn_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	struct ksz_mac_table mac;
+	int first_break = true;
+
+	sw_start_dyn_mac_table(sw);
+	i = 0;
+	do {
+		if (sw_g_dyn_mac_table(sw, &i, &mac))
+			break;
+		if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+			first_break = false;
+			len += sprintf(buf + len, "...\n");
+		}
+		if (len < MAX_SYSFS_BUF_SIZE)
+		len += sprintf(buf + len,
+			"%4x: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  t:%u  s:%u  d:%u  o:%u  %02x  [%u]\n",
+			i, mac.addr[0], mac.addr[1], mac.addr[2],
+			mac.addr[3], mac.addr[4], mac.addr[5],
+			mac.ports, mac.mstp, mac.prio, mac.src, mac.dst,
+			mac.override, mac.fid, mac.valid);
+		else {
+		printk(KERN_INFO
+			"%4x: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  t:%u  s:%u  d:%u  o:%u  %02x  [%u]\n",
+			i, mac.addr[0], mac.addr[1], mac.addr[2],
+			mac.addr[3], mac.addr[4], mac.addr[5],
+			mac.ports, mac.mstp, mac.prio, mac.src, mac.dst,
+			mac.override, mac.fid, mac.valid);
+		yield();
+		}
+		i++;
+	} while (1);
+	sw_stop_dyn_mac_table(sw, i);
+	return len;
+}  /* sw_d_dyn_mac_table */
+
+static u8 mcast_reserved_map[RESERVED_MCAST_TABLE_ENTRIES] = {
+	0, 1, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	4, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+};
+
+static u32 get_mac_table_ctrl(u16 addr, int mcast)
+{
+	u32 ctrl;
+
+	ctrl = addr;
+	if (mcast)
+		ctrl &= ALU_RESV_MCAST_INDEX_M;
+	else
+		ctrl &= ALU_STAT_INDEX_M;
+	ctrl <<= ALU_STAT_INDEX_S;
+	if (mcast)
+		ctrl |= ALU_RESV_MCAST_ADDR;
+	ctrl |= ALU_STAT_START;
+	return ctrl;
+}  /* get_mac_table_ctrl */
+
+/**
+ * sw_r_sta_mac_hw - read from static MAC table using default access
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for read operation.
+ * @num:	Number of entries to read.
+ * @mac:	Buffer to hold the MAC table entries.
+ *
+ * This function reads from static MAC table using default access.
+ */
+static int sw_r_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 status;
+	int cnt = 0;
+
+	do {
+		sw->reg->w32(sw, REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		do {
+			status = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+		} while (status & ALU_STAT_START);
+		sw_r_mac_table(sw, mac);
+		++cnt;
+		++ctrl;
+		++mac;
+	} while (cnt < num);
+	return 1;
+}  /* sw_r_sta_mac_hw */
+
+/**
+ * sw_r_sta_mac_table - read from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This routine reads an entry of the static MAC table of the switch.
+ */
+static void sw_r_sta_mac_table(struct ksz_sw *sw, u16 addr, int mcast,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto r_sta_exit;
+	ctrl = get_mac_table_ctrl(addr, mcast);
+	ctrl |= ALU_STAT_READ;
+	sw->reg->r_sta_mac_hw(sw, &ctrl, 1, mac);
+
+r_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	if (mcast) {
+		mac->addr[0] = 0x01;
+		mac->addr[1] = 0x80;
+		mac->addr[2] = 0xC2;
+		mac->addr[3] = 0x00;
+		mac->addr[4] = 0x00;
+		mac->addr[5] = addr;
+		mac->valid = 1;
+	}
+	mac->dirty = 0;
+}  /* sw_r_sta_mac_table */
+
+/**
+ * sw_r_m_sta_mac_table - read many from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The addresses of the table entries.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	Buffer to store the MAC table entries.
+ *
+ * This routine reads several entries of the static MAC table of the switch.
+ */
+static void sw_r_m_sta_mac_table(struct ksz_sw *sw, u16 addr[], int mcast,
+	int num, struct ksz_mac_table *mac)
+{
+	u32 ctrl[MAX_IBA_MAC_ENTRIES];
+	u16 max_addr;
+	int i;
+	int j;
+	int rc;
+	int cnt = MAX_IBA_MAC_ENTRIES;
+
+	if (mcast)
+		max_addr = RESERVED_MCAST_TABLE_ENTRIES;
+	else
+		max_addr = STATIC_MAC_TABLE_ENTRIES;
+	memset(mac, 0, sizeof(struct ksz_mac_table) * num);
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto r_m_sta_exit;
+	for (i = 0; i < num; i += MAX_IBA_MAC_ENTRIES) {
+#if 1
+		if (addr[i] >= max_addr)
+			break;
+#endif
+		if (cnt > max_addr - addr[i])
+			cnt = max_addr - addr[i];
+		if (cnt > num - i)
+			cnt = num - i;
+		for (j = 0; j < cnt; j++) {
+			ctrl[j] = get_mac_table_ctrl(addr[i + j], mcast);
+			ctrl[j] |= ALU_STAT_READ;
+		}
+		rc = sw->reg->r_sta_mac_hw(sw, ctrl, cnt, mac);
+		if (!rc)
+			break;
+		for (j = 0; j < cnt; j++) {
+			if (mcast) {
+				mac->addr[0] = 0x01;
+				mac->addr[1] = 0x80;
+				mac->addr[2] = 0xC2;
+				mac->addr[3] = 0x00;
+				mac->addr[4] = 0x00;
+				mac->addr[5] = addr[i + j];
+				mac->valid = 1;
+			}
+			++mac;
+		}
+	}
+
+r_m_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_r_m_sta_mac_table */
+
+/**
+ * sw_w_sta_mac_hw - write to static MAC table using default access
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for write operation.
+ * @num:	Number of entries to write.
+ * @mac:	MAC table entries.
+ *
+ * This function writes to static MAC table using default access.
+ */
+static int sw_w_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 status;
+	int cnt;
+
+	for (cnt = 0; cnt < num; cnt++, ctrl++, mac++) {
+		sw_w_mac_table(sw, mac);
+		sw->reg->w32(sw, REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		do {
+			status = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+		} while (status & ALU_STAT_START);
+	}
+	return 1;
+}  /* sw_w_sta_mac_hw */
+
+/**
+ * sw_w_sta_mac_table - write to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the static MAC table of the switch.
+ */
+static void sw_w_sta_mac_table(struct ksz_sw *sw, u16 addr, int mcast,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto w_sta_exit;
+	ctrl = get_mac_table_ctrl(addr, mcast);
+	sw->reg->w_sta_mac_hw(sw, &ctrl, 1, mac);
+
+w_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_w_sta_mac_table */
+
+/**
+ * sw_w_m_sta_mac_table - write many to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The addresses of the table entries.
+ * @mcast:	Multicast reserved table indication.
+ * num:		Numbe of entries.
+ * @mac:	The MAC table entries.
+ *
+ * This routine writes several entries of the static MAC table of the switch.
+ */
+static void sw_w_m_sta_mac_table(struct ksz_sw *sw, u16 addr[], int mcast,
+	int num, struct ksz_mac_table *mac)
+{
+	u32 ctrl[MAX_IBA_MAC_ENTRIES];
+	int i;
+	int j;
+	int rc;
+	int cnt = MAX_IBA_MAC_ENTRIES;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto w_m_sta_exit;
+	for (i = 0; i < num; i += MAX_IBA_MAC_ENTRIES) {
+		if (cnt > num - i)
+			cnt = num - i;
+		for (j = 0; j < cnt; j++) {
+			ctrl[j] = get_mac_table_ctrl(addr[i + j], mcast);
+		}
+		rc = sw->reg->w_sta_mac_hw(sw, ctrl, cnt, mac);
+		if (!rc)
+			break;
+		mac += MAX_IBA_MAC_ENTRIES;
+	}
+
+w_m_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_w_m_sta_mac_table */
+
+static int get_mcast_reserved_addr(u8 group)
+{
+	int i;
+
+	for (i = 0; i < RESERVED_MCAST_TABLE_ENTRIES; i++)
+		if (group == mcast_reserved_map[i])
+			return i;
+	return -1;
+}  /* get_mcast_reserved_addr */
+
+/**
+ * sw_d_sta_mac_table - dump static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps static MAC table contents.
+ */
+static ssize_t sw_d_sta_mac_table(struct ksz_sw *sw, char *sbuf, ssize_t slen)
+{
+	int i;
+	int j;
+	int seen;
+	int len;
+	char buf[120];
+	u16 addr[8];
+	struct ksz_mac_table *mac;
+	struct ksz_mac_table table[8];
+	int first_static = true;
+
+	for (j = 0; j < 8; j++)
+		addr[j] = get_mcast_reserved_addr(j);
+	sw_r_m_sta_mac_table(sw, addr, 1, 8, table);
+	for (j = 0; j < 8; j++) {
+		mac = &table[j];
+		if (!mac->valid)
+			continue;
+		len = 0;
+		len += sprintf(buf + len,
+			"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x  <",
+			j,
+			mac->addr[0], mac->addr[1], mac->addr[2],
+			mac->addr[3], mac->addr[4], mac->addr[5],
+			mac->ports, mac->mstp, mac->prio,
+			mac->src, mac->dst,
+			mac->override, mac->use_fid, mac->fid);
+
+		seen = 0;
+		for (i = 0; i < RESERVED_MCAST_TABLE_ENTRIES; i++) {
+			if (j == mcast_reserved_map[i]) {
+				if (!seen) {
+					if (i != mac->addr[5])
+						len += sprintf(buf + len, " ");
+					len += sprintf(buf + len,
+						"%02X", i);
+					seen = i + 1;
+				}
+			} else if (seen) {
+				if (i != seen)
+					len += sprintf(buf + len,
+						"..%02X", i - 1);
+				seen = 0;
+			}
+		}
+		if (seen && i != seen)
+			len += sprintf(buf + len, "..%02X", i - 1);
+		len += sprintf(buf + len, ">");
+		slen += sprintf(sbuf + slen, "%s\n", buf);
+	}
+	i = 0;
+	do {
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++)
+			addr[j] = i + j;
+		sw_r_m_sta_mac_table(sw, addr, 0, MAX_IBA_MAC_ENTRIES,
+			table);
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++) {
+			mac = &table[j];
+			if (!mac->valid)
+				continue;
+			if (first_static) {
+				first_static = false;
+				slen += sprintf(sbuf + slen, "\n");
+			}
+			slen += sprintf(sbuf + slen,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x\n",
+				i + j,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				mac->ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid);
+		}
+		i += MAX_IBA_MAC_ENTRIES;
+	} while (i < STATIC_MAC_TABLE_ENTRIES);
+	return slen;
+}  /* sw_d_sta_mac_table */
+
+static ssize_t sw_d_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	struct ksz_mac_table *mac;
+	struct ksz_alu_table *alu;
+	int i;
+	int first_static = true;
+	int first_break = true;
+
+	i = STATIC_MAC_TABLE_ENTRIES;
+	do {
+		alu = &sw->info->alu_table[i];
+		if (alu->valid) {
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "...\n");
+			}
+			if (first_static) {
+				first_static = false;
+				if (len < MAX_SYSFS_BUF_SIZE)
+					len += sprintf(buf + len, "\n");
+				else
+					printk(KERN_INFO "\n");
+			}
+			mac = &sw->info->mac_table[i];
+			if (len < MAX_SYSFS_BUF_SIZE)
+			len += sprintf(buf + len,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x  "
+				"%02x:%02x\n",
+				i,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				mac->ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid,
+				alu->forward, alu->owner);
+			else
+			printk(KERN_INFO
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x  "
+				"%02x:%02x\n",
+				i,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				mac->ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid,
+				alu->forward, alu->owner);
+		}
+		i++;
+		if (SWITCH_MAC_TABLE_ENTRIES == i)
+			first_static = true;
+	} while (i < MULTI_MAC_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_mac_table */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_r_vlan_hw - read from VLAN table using default access
+ * @sw:		The switch instance.
+ * @data:	Buffer to hold the VLAN data.
+ * @num:	Number of entries to read.
+ *
+ * This function reads from VLAN table using default access.
+ */
+static int sw_r_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	u8 ctrl;
+	int cnt = 0;
+	u16 addr = data[READ_VLAN_ENTRY_SIZE];
+
+	do {
+		sw->reg->w16(sw, REG_SW_VLAN_ENTRY_INDEX__2, addr);
+		ctrl = VLAN_READ;
+		ctrl |= VLAN_START;
+		sw->reg->w8(sw, REG_SW_VLAN_CTRL, ctrl);
+		do {
+			ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+		} while (ctrl & VLAN_START);
+		data[0] = sw->reg->r32(sw, REG_SW_VLAN_ENTRY__4);
+		data[1] = 0;
+		data[2] = 0;
+		if (data[0] & VLAN_VALID) {
+			sw_r(sw, REG_SW_VLAN_ENTRY_UNTAG__4, &data[1], 4 * 2);
+			data[1] = be32_to_cpu(data[1]);
+			data[2] = be32_to_cpu(data[2]);
+		}
+		++cnt;
+		++addr;
+		data += READ_VLAN_ENTRY_SIZE;
+	} while (cnt < num);
+	return 1;
+}  /* sw_r_vlan_hw */
+
+/**
+ * sw_r_vlan_table - read from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vlan:	Buffer to store the VLAN table entry.
+ *
+ * This function reads an entry of the VLAN table of the switch.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_vlan_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[READ_VLAN_ENTRY_SIZE + 1];
+	int rc = -1;
+
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	if (wait_for_vlan_table(sw))
+		goto r_vlan_exit;
+	data[READ_VLAN_ENTRY_SIZE] = addr;
+	sw->reg->r_vlan_hw(sw, data, 1);
+	get_vlan_table_info(vlan, data);
+
+r_vlan_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+	if (vlan->valid)
+		rc = 0;
+	vlan->vid = addr;
+	vlan->dirty = 0;
+	return rc;
+}  /* sw_r_vlan_table */
+
+/**
+ * sw_r_m_vlan_table - read many from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The starting address of the table entries.
+ * @num:	Number of entries.
+ * @vlan:	Buffer to store the VLAN table entries.
+ *
+ * This routine reads several entries of the VLAN table of the switch.
+ */
+static void sw_r_m_vlan_table(struct ksz_sw *sw, u16 addr, int num,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[READ_VLAN_ENTRY_SIZE * MAX_IBA_VLAN_ENTRIES + 1];
+	int i;
+	int j;
+	int rc;
+	int cnt = MAX_IBA_VLAN_ENTRIES;
+
+	memset(vlan, 0, sizeof(struct ksz_vlan_table) * num);
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	if (wait_for_vlan_table(sw))
+		goto r_m_vlan_exit;
+	for (i = 0; i < num; i += MAX_IBA_VLAN_ENTRIES,
+	     addr += MAX_IBA_VLAN_ENTRIES) {
+		if (addr >= 4096)
+			break;
+		if (cnt > 4096 - addr)
+			cnt = 4096 - addr;
+		if (cnt > num - i)
+			cnt = num - i;
+		data[READ_VLAN_ENTRY_SIZE] = addr;
+		rc = sw->reg->r_vlan_hw(sw, data, cnt);
+		if (!rc)
+			break;
+		for (j = 0; j < cnt; j++) {
+			get_vlan_table_info(vlan, &data[j *
+				READ_VLAN_ENTRY_SIZE]);
+			vlan->vid = addr + j;
+			vlan++;
+		}
+	}
+
+r_m_vlan_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+}  /* sw_r_m_vlan_table */
+
+/**
+ * sw_w_vlan_hw - write to VLAN table using default access
+ * @sw:		The switch instance.
+ * @data:	VLAN data to write.
+ * @num:	Number of entries to write.
+ *
+ * This function writes to VLAN table using default access.
+ */
+static int sw_w_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	u8 ctrl;
+	int cnt;
+
+	for (cnt = 0; cnt < num; cnt++) {
+		data[0] = cpu_to_be32(data[0]);
+		data[1] = cpu_to_be32(data[1]);
+		data[2] = cpu_to_be32(data[2]);
+		data[3] = cpu_to_be16(data[3]);
+#if 0
+		sw_w(sw, REG_SW_VLAN_ENTRY__4, data, 4 * 3 + 2);
+#else
+		sw_w(sw, REG_SW_VLAN_ENTRY__4, data, 4 * 3 + 4);
+#endif
+		ctrl = VLAN_WRITE;
+		ctrl |= VLAN_START;
+		sw->reg->w8(sw, REG_SW_VLAN_CTRL, ctrl);
+		do {
+			ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+		} while (ctrl & VLAN_START);
+		data += WRITE_VLAN_ENTRY_SIZE;
+	}
+	return 1;
+}  /* sw_w_vlan_hw */
+
+/**
+ * sw_w_vlan_table - write to VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vlan:	The VLAN table entry.
+ *
+ * This routine writes an entry of the VLAN table of the switch.
+ */
+static void sw_w_vlan_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[4];
+	int bit;
+	int index;
+
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	if (wait_for_vlan_table(sw))
+		goto w_vlan_exit;
+	set_vlan_table_info(vlan, data);
+	data[3] = addr;
+	sw->reg->w_vlan_hw(sw, data, 1);
+	index = addr / VID_IN_DATA;
+	bit = addr % VID_IN_DATA;
+	if (vlan->valid) {
+		int p;
+		struct ksz_port_cfg *cfg;
+
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			cfg = &sw->info->port_cfg[p];
+			if (vlan->untag & (1 << p))
+				cfg->untagged[index] |= (1 << bit);
+			else
+				cfg->untagged[index] &= ~(1 << bit);
+		}
+		sw->info->vid[index] |= (1 << bit);
+	} else {
+		sw->info->vid[index] &= ~(1 << bit);
+	}
+
+w_vlan_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+	vlan->dirty = 0;
+}  /* sw_w_vlan_table */
+
+/**
+ * sw_d_vlan_table - dump VLAN table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps the VLAN table of the switch.
+ */
+static ssize_t sw_d_vlan_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	int j;
+	struct ksz_vlan_table *vlan;
+	struct ksz_vlan_table table[8];
+	int first_break = true;
+
+	i = 0;
+	do {
+		sw_r_m_vlan_table(sw, i, MAX_IBA_VLAN_ENTRIES, table);
+		for (j = 0; j < MAX_IBA_VLAN_ENTRIES; j++) {
+			vlan = &table[j];
+			if (!vlan->valid)
+				continue;
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "...\n");
+			}
+			if (len < MAX_SYSFS_BUF_SIZE)
+			len += sprintf(buf + len,
+				"%3x: 0x%03x  m:%x  p:%x  o:%u  %04x  %04x\n",
+				vlan->vid,
+				vlan->fid, vlan->mstp, vlan->prio,
+				vlan->option, vlan->untag, vlan->ports);
+			else
+			printk(KERN_INFO
+				"%3x: 0x%03x  m:%x  p:%x  o:%u  %04x  %04x\n",
+				vlan->vid,
+				vlan->fid, vlan->mstp, vlan->prio,
+				vlan->option, vlan->untag, vlan->ports);
+		}
+		if (len >= MAX_SYSFS_BUF_SIZE)
+		yield();
+		i += MAX_IBA_VLAN_ENTRIES;
+	} while (i < VLAN_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_vlan_table */
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * sw_s_hsr_table - prepare HSR table for access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @path_id:	The path ID.
+ *
+ * This helper function prepares HSR table for access.
+ */
+static u32 sw_s_hsr_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u8 path_id)
+{
+	u32 ctrl;
+	u32 data;
+	u16 *word;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & HSR_DIRECT_INDEX_M;
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_2, data);
+		ctrl |= HSR_DIRECT;
+	} else {
+		data = 0;
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_0, data);
+		word = (u16 *) &data;
+		word++;
+		memcpy(word, &src_addr[0], 2);
+		data = htonl(data);
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_1, data);
+		memcpy(&data, &src_addr[2], 4);
+		data = htonl(data);
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_2, data);
+		data = path_id & HSR_PATH_INDEX_M;
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_3, data);
+	}
+	ctrl |= HSR_START;
+	return ctrl;
+}  /* sw_s_hsr_table */
+
+static void sw_get_hsr_table(struct ksz_sw *sw, struct ksz_hsr_table *hsr)
+{
+	int i;
+	u32 data[7];
+
+	sw_r(sw, REG_HSR_ALU_VAL_A, data, 7 * 4);
+	for (i = 0; i < 7; i++)
+		data[i] = be32_to_cpu(data[i]);
+	get_hsr_table_info(hsr, data);
+}  /* sw_get_hsr_table */
+
+/**
+ * sw_r_hsr_hw - read from HSR table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function reads an entry of the HSR table using default access.
+ */
+static int sw_r_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_hsr_table(sw, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_READ;
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	do {
+		ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	} while (ctrl & HSR_START);
+	sw_get_hsr_table(sw, hsr);
+	return 1;
+}  /* sw_r_hsr_hw */
+
+/**
+ * sw_r_hsr_table - read from HSR table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This routine reads an entry of the HSR table of the switch.
+ * HSR table is locked to prevent corruption of read data.
+ */
+static void sw_r_hsr_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	mutex_lock(&sw->hsrlock);
+	sw->ops->acquire(sw);
+	if (wait_for_hsr_table(sw))
+		goto r_hsr_exit;
+	sw->reg->r_hsr_hw(sw, addr, hsr);
+
+r_hsr_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->hsrlock);
+	hsr->dirty = 0;
+}  /* sw_r_hsr_table */
+
+/**
+ * sw_w_hsr_hw - write to HSR table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	The HSR table entry.
+ *
+ * This function writes an entry of the HSR table using default access.
+ */
+static int sw_w_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 ctrl;
+	u32 data[7];
+	int i;
+
+	ctrl = sw_s_hsr_table(sw, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_WRITE;
+	set_hsr_table_info(hsr, data);
+	for (i = 0; i < 7; i++)
+		data[i] = cpu_to_be32(data[i]);
+	sw_w(sw, REG_HSR_ALU_VAL_A, data, 7 * 4);
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_w_hsr_hw */
+
+/**
+ * sw_w_hsr_table - write to HSR table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	The HSR table entry.
+ *
+ * This routine writes an entry of the HSR table of the switch.
+ * HSR table is locked to prevent corruption of read data.
+ */
+static void sw_w_hsr_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	mutex_lock(&sw->hsrlock);
+	sw->ops->acquire(sw);
+	if (wait_for_hsr_table(sw))
+		goto w_hsr_exit;
+	sw->reg->w_hsr_hw(sw, addr, hsr);
+
+w_hsr_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->hsrlock);
+	hsr->dirty = 0;
+}  /* sw_w_hsr_table */
+
+/**
+ * sw_start_hsr_hw - start HSR table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function starts HSR table search using default access.
+ */
+static int sw_start_hsr_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = HSR_SEARCH;
+	ctrl |= HSR_START;
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_start_hsr_hw */
+
+/**
+ * sw_start_hsr_table - start HSR table search
+ * @sw:		The switch instance.
+ *
+ * This routine starts HSR table search.
+ * HSR table is locked to prevent corruption of read data.
+ */
+static void sw_start_hsr_table(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->hsrlock);
+	sw->ops->acquire(sw);
+	if (wait_for_hsr_table(sw))
+		goto s_hsr_exit;
+	sw->reg->start_hsr_hw(sw);
+
+s_hsr_exit:
+	sw->ops->release(sw);
+}  /* sw_start_hsr_table */
+
+/**
+ * sw_g_hsr_hw - retrieve HSR table result using default access
+ * @sw:		The switch instance.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function retrieves HSR table result using default access.
+ */
+static int sw_g_hsr_hw(struct ksz_sw *sw, struct ksz_hsr_table *hsr)
+{
+	sw_get_hsr_table(sw, hsr);
+	return 1;
+}  /* sw_g_hsr_hw */
+
+/**
+ * sw_g_hsr_table - retrieve HSR table result
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function retrieves an entry of the HSR table search result.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_g_hsr_table(struct ksz_sw *sw, u16 *addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 ctrl;
+	int rc = 0;
+
+	sw->ops->acquire(sw);
+	do {
+		ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	} while (!(ctrl & HSR_VALID) && (ctrl & HSR_START));
+	if (ctrl & HSR_VALID) {
+		ctrl >>= HSR_VALID_CNT_S;
+		ctrl &= HSR_VALID_CNT_M;
+		rc = !sw->reg->g_hsr_hw(sw, hsr);
+		if (ctrl != *addr + 1)
+			*addr = ctrl - 1;
+	} else
+		rc = -1;
+	sw->ops->release(sw);
+	return rc;
+}  /* sw_g_hsr_table */
+
+/**
+ * sw_stop_hsr_hw - stop HSR table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function stops HSR table search using default access.
+ *
+ * Return the last HSR table control.
+ */
+static u32 sw_stop_hsr_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	return ctrl;
+}  /* sw_stop_hsr_hw */
+
+/**
+ * sw_stop_hsr_table - stop HSR table search
+ * @sw:		The switch instance.
+ * @addr:	The address of the last table entry.
+ *
+ * This routine stops HSR table search.
+ */
+static void sw_stop_hsr_table(struct ksz_sw *sw, u16 addr)
+{
+	u32 ctrl;
+
+	sw->ops->acquire(sw);
+	ctrl = sw->reg->stop_hsr_hw(sw);
+	ctrl >>= HSR_VALID_CNT_S;
+	ctrl &= HSR_VALID_CNT_M;
+if (ctrl != addr)
+dbg_msg(" ?stop: %x %x\n", ctrl, addr);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->hsrlock);
+}  /* sw_stop_hsr_table */
+
+/**
+ * sw_d_hsr_table - dump HSR table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps HSR table contents.
+ */
+static ssize_t sw_d_hsr_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	struct ksz_hsr_table hsr;
+	int first_break = true;
+
+	sw_start_hsr_table(sw);
+	i = 0;
+	do {
+		if (sw_g_hsr_table(sw, &i, &hsr))
+			break;
+		if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+			first_break = false;
+			len += sprintf(buf + len, "...\n");
+		}
+		if (len < MAX_SYSFS_BUF_SIZE)
+		len += sprintf(buf + len,
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]\n",
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		else {
+		printk(KERN_INFO
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]\n",
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		yield();
+		}
+		i++;
+	} while (1);
+	sw_stop_hsr_table(sw, i);
+#if 0
+printk("\n");
+	for (i = 0; i < HSR_INDEX_MAX; i++) {
+		sw_r_hsr_table(sw, i + 1, &hsr);
+		if (hsr.valid || hsr.age_cnt)
+		printk(KERN_INFO
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]\n",
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		yield();
+	}
+#endif
+	return len;
+}  /* sw_d_hsr_table */
+
+/* -------------------------------------------------------------------------- */
+#endif
+
+/*
+ * Port functions
+ */
+
+static void port_phy_w(struct ksz_sw *sw, u32 addr, u16 val)
+{
+	int shift;
+	u32 data;
+
+	shift = addr & 3;
+	shift *= 8;
+	shift = 16 - shift;
+	addr &= ~3;
+	data = sw->reg->r32(sw, addr);
+	data &= ~(0xffff << shift);
+	data |= (val << shift);
+	sw->reg->w32(sw, addr, data);
+}  /* port_phy_w */
+
+/**
+ * port_chk - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk(struct ksz_sw *sw, uint port, uint offset, SW_D bits)
+{
+	u32 addr;
+	SW_D data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk */
+
+/**
+ * port_cfg - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg(struct ksz_sw *sw, uint port, uint offset, SW_D bits,
+	bool set)
+{
+	u32 addr;
+	SW_D data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* port_cfg */
+
+/**
+ * port_chk16 - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk16(struct ksz_sw *sw, uint port, uint offset, u16 bits)
+{
+	u32 addr;
+	u16 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r16(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk32 */
+
+/**
+ * port_cfg16 - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg16(struct ksz_sw *sw, uint port, uint offset, u16 bits,
+	bool set)
+{
+	u32 addr;
+	u16 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r16(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	if (0x120 <= offset && offset <= 0x13F)
+		port_phy_w(sw, addr, data);
+	else
+		sw->reg->w16(sw, addr, data);
+}  /* port_cfg16 */
+
+/**
+ * port_chk32 - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk32(struct ksz_sw *sw, uint port, uint offset, u32 bits)
+{
+	u32 addr;
+	u32 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r32(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk32 */
+
+/**
+ * port_cfg32 - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg32(struct ksz_sw *sw, uint port, uint offset, u32 bits,
+	bool set)
+{
+	u32 addr;
+	u32 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r32(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	sw->reg->w32(sw, addr, data);
+}  /* port_cfg32 */
+
+#if 0
+/**
+ * port_chk_shift - check port bit
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the register.
+ * @shift:	Number of bits to shift.
+ *
+ * This function checks whether the specified port is set in the register or
+ * not.
+ *
+ * Return 0 if the port is not set.
+ */
+static int port_chk_shift(struct ksz_sw *sw, uint port, u32 addr, int shift)
+{
+	SW_D data;
+	SW_D bit = 1 << port;
+
+	data = SW_R(sw, addr);
+	data >>= shift;
+	return (data & bit) == bit;
+}
+
+/**
+ * port_cfg_shift - set port bit
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the register.
+ * @shift:	Number of bits to shift.
+ * @set:	The flag indicating whether the port is to be set or not.
+ *
+ * This routine sets or resets the specified port in the register.
+ */
+static void port_cfg_shift(struct ksz_sw *sw, uint port, u32 addr, int shift,
+	bool set)
+{
+	SW_D data;
+	SW_D bits = 1 << port;
+
+	data = SW_R(sw, addr);
+	bits <<= shift;
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}
+#endif
+
+/**
+ * port_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r8(sw, addr);
+}  /* port_r8 */
+
+/**
+ * port_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w8(sw, addr, data);
+}  /* port_w8 */
+
+/**
+ * port_r16 - read word from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a word from the port register.
+ */
+static void port_r16(struct ksz_sw *sw, uint port, uint offset, u16 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r16(sw, addr);
+}  /* port_r16 */
+
+/**
+ * port_w16 - write word to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a word to the port register.
+ */
+static void port_w16(struct ksz_sw *sw, uint port, uint offset, u16 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	if (0x120 <= offset && offset <= 0x13F)
+		port_phy_w(sw, addr, data);
+	else
+		sw->reg->w16(sw, addr, data);
+}  /* port_w16 */
+
+/**
+ * port_r32 - read dword from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a dword from the port register.
+ */
+static void port_r32(struct ksz_sw *sw, uint port, uint offset, u32 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r32(sw, addr);
+}  /* port_r32 */
+
+/**
+ * port_w32 - write dword to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a dword to the port register.
+ */
+static void port_w32(struct ksz_sw *sw, uint port, uint offset, u32 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w32(sw, addr, data);
+}  /* port_w32 */
+
+static void port_get(struct ksz_sw *sw, uint port, uint offset, void *buf,
+	size_t cnt)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->r(sw, addr, buf, cnt);
+}
+
+#if 0
+static void port_set(struct ksz_sw *sw, uint port, uint offset, void *buf,
+	size_t cnt)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w(sw, addr, buf, cnt);
+}
+#endif
+
+/**
+ * port_r_s - read bits with shift from port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This function reads bits from the port register.
+ */
+static u8 port_r_s(struct ksz_sw *sw, uint p, u32 reg, u8 mask, u8 shift)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}  /* port_r_s */
+
+/**
+ * port_w_s - write bits with shift to port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This routine writes bits to the port register.
+ */
+static void port_w_s(struct ksz_sw *sw, uint p, u32 reg, u8 mask, u8 shift,
+	u8 val)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w8(sw, p, reg, data);
+}  /* port_w_s */
+
+static u32 port_r_s_32(struct ksz_sw *sw, uint p, u32 reg, u32 mask,
+	u32 shift)
+{
+	u32 data;
+
+	port_r32(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}
+
+static void port_w_s_32(struct ksz_sw *sw, uint p, u32 reg, u32 mask,
+	u32 shift, u32 val)
+{
+	u32 data;
+
+	port_r32(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w32(sw, p, reg, data);
+}
+
+/**
+ * sw_chk - check switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the switch register are
+ * set or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int sw_chk(struct ksz_sw *sw, u32 addr, SW_D bits)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* sw_chk */
+
+/**
+ * sw_cfg - set switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This function sets or resets the specified bits of the switch register.
+ */
+static void sw_cfg(struct ksz_sw *sw, u32 addr, SW_D bits, bool set)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* sw_cfg */
+
+static SW_D sw_r_shift(struct ksz_sw *sw, u32 addr, u32 mask,
+	u32 shift)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	data >>= shift;
+	data &= mask;
+	return data;
+}
+
+static void sw_w_shift(struct ksz_sw *sw, u32 addr, u32 mask, u32 shift,
+	SW_D val)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	SW_W(sw, addr, data);
+}
+
+#ifdef PORT_OUT_RATE_ADDR
+/**
+ * port_out_rate_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_out_rate_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}
+
+/**
+ * port_out_rate_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_out_rate_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* ACL */
+
+static inline void port_cfg_acl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_ACL_ENABLE, set);
+}
+
+static inline int port_chk_acl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_ACL_ENABLE);
+}
+
+static inline u8 port_get_authen_mode(struct ksz_sw *sw, uint p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_AUTHEN_MODE, 0);
+}
+
+static void port_set_authen_mode(struct ksz_sw *sw, uint p, u8 mode)
+{
+	port_w_s(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_AUTHEN_MODE, 0, mode);
+}
+
+/**
+ * get_acl_action_info - Get ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL action field information.
+ */
+static void get_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	acl->prio_mode = (data[10] >> ACL_PRIO_MODE_S) & ACL_PRIO_MODE_M;
+	acl->prio = (data[10] >> ACL_PRIO_S) & ACL_PRIO_M;
+	acl->vlan_prio_replace = !!(data[10] & ACL_VLAN_PRIO_REPLACE);
+	acl->vlan_prio = data[11] >> ACL_VLAN_PRIO_S;
+	acl->vlan_prio |= (data[10] & ACL_VLAN_PRIO_HI_M) << 1;
+	acl->map_mode = (data[11] >> ACL_MAP_MODE_S) & ACL_MAP_MODE_M;
+
+	/* byte 12 is not used at all. */
+	acl->ports = data[13];
+}  /* get_acl_action_info */
+
+/**
+ * get_acl_table_info - Get ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL table information.
+ */
+static void get_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+	int cnt = 0;
+
+	acl->first_rule = data[0] & ACL_FIRST_RULE_M;
+	acl->mode = (data[1] >> ACL_MODE_S) & ACL_MODE_M;
+	acl->enable = (data[1] >> ACL_ENABLE_S) & ACL_ENABLE_M;
+	acl->src = !!(data[1] & ACL_SRC);
+	acl->equal = !!(data[1] & ACL_EQUAL);
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			acl->mac[i] = data[2 + i];
+		ptr_16 = (u16 *) &data[8];
+		acl->eth_type = be16_to_cpu(*ptr_16);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			cnt = 1;
+			ptr_16 = (u16 *) &data[10];
+			acl->cnt = (be16_to_cpu(*ptr_16) >> ACL_CNT_S) &
+				ACL_CNT_M;
+			acl->msec =
+				!!(data[ACL_INTR_CNT_START] & ACL_MSEC_UNIT);
+			acl->intr_mode =
+				!!(data[ACL_INTR_CNT_START] & ACL_INTR_MODE);
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_addr[i] = data[j];
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_mask[i] = data[j];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			acl->seqnum = be32_to_cpu(*ptr_32);
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			acl->max_port = be16_to_cpu(*ptr_16);
+			++ptr_16;
+			acl->min_port = be16_to_cpu(*ptr_16);
+			acl->port_mode = (data[6] >> ACL_PORT_MODE_S) &
+				ACL_PORT_MODE_M;
+		}
+		acl->protocol = (data[6] & 1) << 7;
+		acl->protocol |= (data[7] >> 1);
+		acl->tcp_flag_enable = !!(data[7] & ACL_TCP_FLAG_ENABLE);
+		acl->tcp_flag_mask = data[8];
+		acl->tcp_flag = data[9];
+		break;
+	default:
+		break;
+	}
+	if (!cnt)
+		get_acl_action_info(acl, data);
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	acl->ruleset = be16_to_cpu(*ptr_16);
+}  /* get_acl_table_info */
+
+/**
+ * set_acl_action_info - Set ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL action field information.
+ */
+static void set_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[10] = (acl->prio_mode & ACL_PRIO_MODE_M) << ACL_PRIO_MODE_S;
+	data[10] |= (acl->prio & ACL_PRIO_M) << ACL_PRIO_S;
+	if (acl->vlan_prio_replace)
+		data[10] |= ACL_VLAN_PRIO_REPLACE;
+	data[10] |= (acl->vlan_prio >> 1);
+	data[11] = acl->vlan_prio << ACL_VLAN_PRIO_S;
+	data[11] |= (acl->map_mode & ACL_MAP_MODE_M) << ACL_MAP_MODE_S;
+
+	/* byte 12 is not used at all. */
+	data[13] = acl->ports;
+}  /* set_acl_action_info */
+
+/**
+ * set_acl_ruleset_info - Set ACL ruleset field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL ruleset field information.
+ */
+static void set_acl_ruleset_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[0] = acl->first_rule & ACL_FIRST_RULE_M;
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	*ptr_16 = cpu_to_be16(acl->ruleset);
+}  /* set_acl_ruleset_info */
+
+/**
+ * set_acl_table_info - Set ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL table information.
+ */
+static void set_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[1] = (acl->mode & ACL_MODE_M) << ACL_MODE_S;
+	data[1] |= (acl->enable & ACL_ENABLE_M) << ACL_ENABLE_S;
+	if (acl->src)
+		data[1] |= ACL_SRC;
+	if (acl->equal)
+		data[1] |= ACL_EQUAL;
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			data[2 + i] = acl->mac[i];
+		ptr_16 = (u16 *) &data[8];
+		*ptr_16 = cpu_to_be16(acl->eth_type);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			data[ACL_INTR_CNT_START] = 0;
+			ptr_16 = (u16 *) &data[10];
+			*ptr_16 = cpu_to_be16((acl->cnt & ACL_CNT_M) <<
+				ACL_CNT_S);
+			data[12] = 0;
+			if (acl->msec)
+				data[ACL_INTR_CNT_START] |= ACL_MSEC_UNIT;
+			if (acl->intr_mode)
+				data[ACL_INTR_CNT_START] |= ACL_INTR_MODE;
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_addr[i];
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_mask[i];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			*ptr_32 = cpu_to_be32(acl->seqnum);
+			data[6] = 0;
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			*ptr_16 = cpu_to_be16(acl->max_port);
+			++ptr_16;
+			*ptr_16 = cpu_to_be16(acl->min_port);
+			data[6] = (acl->port_mode & ACL_PORT_MODE_M) <<
+				ACL_PORT_MODE_S;
+		}
+		data[6] |= (acl->protocol >> 7);
+		data[7] = (acl->protocol << 1);
+		if (acl->tcp_flag_enable)
+			data[7] |= ACL_TCP_FLAG_ENABLE;
+		data[8] = acl->tcp_flag_mask;
+		data[9] = acl->tcp_flag;
+		break;
+	default:
+		break;
+	}
+}  /* set_acl_table_info */
+
+/**
+ * wait_for_acl_table - Wait for ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This helper function waits for ACL table to be ready for access.
+ */
+static int wait_for_acl_table(struct ksz_sw *sw, uint port)
+{
+	u8 ctrl;
+	int timeout = 100;
+
+	if (!port_chk_acl(sw, port))
+		dbg_msg("acl not on: %d\n", port);
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+	} while (!(ctrl & (PORT_ACL_WRITE_DONE | PORT_ACL_READ_DONE)));
+	return 0;
+}  /* wait_for_acl_table */
+
+/**
+ * sw_r_acl_hw - read from ACL table using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This function reads from ACL table of the port using default access.
+ */
+static int sw_r_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M);
+
+	port_w(sw, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+	} while (!(ctrl & PORT_ACL_READ_DONE));
+	sw_r(sw, PORT_CTRL_ADDR(port, REG_PORT_ACL_0), data, ACL_TABLE_LEN);
+	return 1;
+}  /* sw_r_acl_hw */
+
+/**
+ * sw_r_acl_table - read from ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	Buffer to store the ACL entry.
+ *
+ * This function reads an entry of the ACL table of the port.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_acl_table(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	int rc = -1;
+
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto r_acl_exit;
+	sw->reg->r_acl_hw(sw, port, addr, data);
+	get_acl_table_info(acl, data);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+
+r_acl_exit:
+	sw->ops->release(sw);
+	if (acl->mode)
+		rc = 0;
+	acl->changed = 0;
+	return rc;
+}  /* sw_r_acl_table */
+
+/**
+ * sw_w_acl_hw - write to ACL table using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This function writes to ACL table of the port using default access.
+ */
+static int sw_w_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+
+	sw_w(sw, PORT_CTRL_ADDR(port, REG_PORT_ACL_0), data, ACL_TABLE_LEN);
+	port_w(sw, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+	} while (!(ctrl & PORT_ACL_WRITE_DONE));
+	return 1;
+}  /* sw_w_acl_hw */
+
+/**
+ * sw_w_acl_action - write to ACL action field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the action field of an entry of the ACL table.
+ */
+static void sw_w_acl_action(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_act_exit;
+	set_acl_action_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_ACTION_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(&acl->data[ACL_ACTION_START], &data[ACL_ACTION_START],
+		ACL_ACTION_LEN);
+
+w_acl_act_exit:
+	sw->ops->release(sw);
+	acl->action_changed = 0;
+}  /* sw_w_acl_action */
+
+/**
+ * sw_w_acl_ruleset - write to ACL ruleset field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the ruleset field of an entry of the ACL table.
+ */
+static void sw_w_acl_ruleset(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_ruleset_exit;
+	set_acl_ruleset_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_RULESET_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+
+	/* First rule */
+	acl->data[0] = data[0];
+	memcpy(&acl->data[ACL_RULESET_START], &data[ACL_RULESET_START],
+		ACL_RULESET_LEN);
+
+w_acl_ruleset_exit:
+	sw->ops->release(sw);
+	acl->ruleset_changed = 0;
+}  /* sw_w_acl_ruleset */
+
+/**
+ * sw_w_acl_rule - write to ACL matching and process fields
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the matching and process fields of an entry of the
+ * ACL table of the port.
+ */
+static void sw_w_acl_rule(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	u16 byte_enable = ACL_MATCH_ENABLE;
+	int len = ACL_ACTION_START;
+
+	if (ACL_MODE_LAYER_2 == acl->mode &&
+	    ACL_ENABLE_2_COUNT == acl->enable) {
+		byte_enable |= ACL_ACTION_ENABLE;
+		len += ACL_ACTION_LEN;
+	}
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_rule_exit;
+	set_acl_table_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, byte_enable);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(acl->data, data, len);
+
+w_acl_rule_exit:
+	sw->ops->release(sw);
+	acl->changed = 0;
+}  /* sw_w_acl_rule */
+
+/**
+ * sw_w_acl_table - write to ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes an entry of the ACL table of the port.
+ */
+static void sw_w_acl_table(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	acl->data[0] = 0xff;
+	memset(data, 0, sizeof(data));
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_exit;
+	if (!(ACL_MODE_LAYER_2 == acl->mode &&
+	    ACL_ENABLE_2_COUNT == acl->enable))
+		set_acl_action_info(acl, data);
+	set_acl_table_info(acl, data);
+	set_acl_ruleset_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_BYTE_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+
+w_acl_exit:
+	sw->ops->release(sw);
+}  /* sw_w_acl_table */
+
+/**
+ * acl_action_info - format ACL action field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL action field information.
+ */
+static int acl_action_info(struct ksz_acl_table *acl, u16 index, char *buf,
+	int len)
+{
+	char prio = 'p';
+	char vlan = 'v';
+
+	if (acl->prio_mode != ACL_PRIO_MODE_DISABLE)
+		prio = 'P';
+	if (acl->vlan_prio_replace)
+		vlan = 'V';
+	len += sprintf(buf + len,
+		"%x: %c:%u=%u %c:%u=%u %u=%04x [%u]\n",
+		index,
+		prio, acl->prio_mode, acl->prio,
+		vlan, acl->vlan_prio_replace, acl->vlan_prio,
+		acl->map_mode, acl->ports,
+		acl->action_changed ? 8 : 1);
+	return len;
+}  /* acl_action_info */
+
+/**
+ * acl_ruleset_info - format ACL ruleset field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL ruleset field information.
+ */
+static int acl_ruleset_info(struct ksz_acl_table *acl, u16 index, char *buf,
+	int len)
+{
+	len += sprintf(buf + len,
+		"%x: %x:%04x [%u]\n",
+		index,
+		acl->first_rule, acl->ruleset,
+		acl->ruleset_changed ? 8 : 1);
+	return len;
+}  /* acl_ruleset_info */
+
+/**
+ * acl_info - format ACL matching and process field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL matching and process field information.
+ */
+static int acl_info(struct ksz_acl_table *acl, u16 index, char *buf, int len)
+{
+	char enable = 'e';
+	char equal = 'q';
+	char src = 's';
+	char cnt = 'c';
+	char protocol = 'x';
+	char flag = 'f';
+	char seqnum = 's';
+	char msec[4];
+
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		enable = 'E';
+		*msec = 0;
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+			cnt = 'C';
+			if (acl->intr_mode)
+				*msec = 0;
+			else if (acl->msec)
+				strcpy(msec, "ms ");
+			else
+				strcpy(msec, "us ");
+		} else {
+			equal = 'Q';
+			if (ACL_ENABLE_2_TYPE != acl->enable)
+				src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %02X:%02X:%02X:%02X:%02X:%02X-%04x "
+			"%c:%u.%u %s"
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5],
+			acl->eth_type,
+			cnt, acl->intr_mode, acl->cnt, msec,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_3:
+		if (ACL_ENABLE_3_IP == acl->enable ||
+		    ACL_ENABLE_3_SRC_DST_COMP == acl->enable)
+			enable = 'E';
+		if (ACL_ENABLE_3_IP == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %u.%u.%u.%u:%u.%u.%u.%u "
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3],
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3],
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_4:
+		enable = 'E';
+		if (ACL_ENABLE_4_PROTOCOL == acl->enable) {
+			protocol = 'X';
+			equal = 'Q';
+		} else if (ACL_ENABLE_4_TCP_SEQN_COMP == acl->enable) {
+			seqnum = 'S';
+			equal = 'Q';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else if (ACL_ENABLE_4_TCP_PORT_COMP == acl->enable) {
+			src = 'S';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else
+			src = 'S';
+		len += sprintf(buf + len,
+			"%x: %u=%4x-%4x 0%c%x %c:%08x %c:%u=%x:%x "
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			acl->port_mode, acl->min_port, acl->max_port,
+			protocol, acl->protocol, seqnum, acl->seqnum,
+			flag, acl->tcp_flag_enable,
+			acl->tcp_flag, acl->tcp_flag_mask,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	default:
+		len += sprintf(buf + len,
+			"%x: "
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	}
+	return len;
+}  /* acl_info */
+
+/**
+ * sw_d_acl_table - dump ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine dumps ACL table of the port.
+ */
+static ssize_t sw_d_acl_table(struct ksz_sw *sw, uint port, char *buf,
+	ssize_t len)
+{
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+	int acl_on;
+	int min = 0;
+
+	sw->ops->acquire(sw);
+	acl_on = port_chk_acl(sw, port);
+	if (!acl_on) {
+		printk(KERN_INFO "ACL not on for port %d\n", port);
+		port_cfg_acl(sw, port, true);
+	}
+	sw->ops->release(sw);
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		acl->action_selected = false;
+		sw_r_acl_table(sw, port, i, acl);
+	}
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->mode)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "rules:\n");
+		len = acl_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (min)
+		len += sprintf(buf + len, "\n");
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->ruleset)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "rulesets:\n");
+		cfg->acl_info[acl->first_rule].action_selected = true;
+		len = acl_ruleset_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (min)
+		len += sprintf(buf + len, "\n");
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (ACL_PRIO_MODE_DISABLE == acl->prio_mode &&
+		    ACL_MAP_MODE_DISABLE == acl->map_mode &&
+		    !acl->ports &&
+		    !acl->vlan_prio_replace && !acl->action_selected)
+			continue;
+		if (ACL_MODE_LAYER_2 == acl->mode &&
+		    ACL_ENABLE_2_COUNT == acl->enable)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "actions:\n");
+		len = acl_action_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (!acl_on) {
+		sw->ops->acquire(sw);
+		port_cfg_acl(sw, port, false);
+		sw->ops->release(sw);
+	}
+	return len;
+}  /* sw_d_acl_table */
+
+static void sw_reset_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	uint port;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		cfg = &sw->info->port_cfg[port];
+		memset(cfg->acl_info, 0, sizeof(struct ksz_acl_table) *
+			ACL_TABLE_ENTRIES);
+		cfg->acl_index = cfg->acl_act_index = cfg->acl_rule_index = 0;
+	}
+}  /* sw_reset_acl */
+
+static void sw_reset_acl_hw(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	int acl_on;
+	uint port;
+
+	sw_reset_acl(sw);
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		acl_on = port_chk_acl(sw, port);
+		if (!acl_on)
+			port_cfg_acl(sw, port, true);
+		sw->ops->release(sw);
+		cfg = &sw->info->port_cfg[port];
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			acl->mode = 0;
+			acl->ruleset = 0;
+			sw_w_acl_table(sw, port, i, acl);
+		}
+		sw->ops->acquire(sw);
+		if (!acl_on)
+			port_cfg_acl(sw, port, false);
+	}
+}  /* sw_reset_acl_hw */
+
+static void sw_init_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	uint port;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		port_cfg_acl(sw, port, 1);
+#if 0
+		port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, 0xffff);
+/*
+ * THa  2015/11/29
+ * The DLR beacon drop 2-entry rule occasionally leaks the beacon when running
+ * in Gigabit and heavy traffic unless the port mapping action in first entry
+ * is set in AND or REPLACE with empty ports.
+ */
+		wait_for_acl_table(sw, port);
+		sw->reg->w_acl_hw(sw, port, 0, data);
+#endif
+		cfg = &sw->info->port_cfg[port];
+		sw->ops->release(sw);
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			sw_r_acl_table(sw, port, i, acl);
+		}
+		sw->ops->acquire(sw);
+	}
+}  /* sw_init_acl */
+
+/* -------------------------------------------------------------------------- */
+
+/* 36-bit counts. */
+#define MIB_RX_HI_PRIO			0x00
+#define MIB_RX_UNDERSIZE		0x01
+#define MIB_RX_FRAGMENT			0x02
+#define MIB_RX_OVERSIZE			0x03
+#define MIB_RX_JABBER			0x04
+#define MIB_RX_SYMBOL_ERR		0x05
+#define MIB_RX_CRC_ERR			0x06
+#define MIB_RX_ALIGNMENT_ERR		0x07
+#define MIB_RX_CTRL_8808		0x08
+#define MIB_RX_PAUSE			0x09
+#define MIB_RX_BROADCAST		0x0A
+#define MIB_RX_MULTICAST		0x0B
+#define MIB_RX_UNICAST			0x0C
+#define MIB_RX_OCTET_64			0x0D
+#define MIB_RX_OCTET_65_127		0x0E
+#define MIB_RX_OCTET_128_255		0x0F
+#define MIB_RX_OCTET_256_511		0x10
+#define MIB_RX_OCTET_512_1023		0x11
+#define MIB_RX_OCTET_1024_1522		0x12
+#define MIB_RX_OCTET_1523_2000		0x13
+#define MIB_RX_OCTET_2001		0x14
+#define MIB_TX_HI_PRIO			0x15
+#define MIB_TX_LATE_COLLISION		0x16
+#define MIB_TX_PAUSE			0x17
+#define MIB_TX_BROADCAST		0x18
+#define MIB_TX_MULTICAST		0x19
+#define MIB_TX_UNICAST			0x1A
+#define MIB_TX_DEFERRED			0x1B
+#define MIB_TX_TOTAL_COLLISION		0x1C
+#define MIB_TX_EXCESS_COLLISION		0x1D
+#define MIB_TX_SINGLE_COLLISION		0x1E
+#define MIB_TX_MULTI_COLLISION		0x1F
+
+#define MIB_RX_TOTAL			0x20
+#define MIB_TX_TOTAL			0x21
+
+#define MIB_RX_DROPS			0x22
+#define MIB_TX_DROPS			0x23
+
+/* Actual locations. */
+#define MIB_9897_RX_BYTE_CNT		0x80
+#define MIB_9897_TX_BYTE_CNT		0x81
+
+#define MIB_9897_RX_DROPPED_PACKET	0x82
+#define MIB_9897_TX_DROPPED_PACKET	0x83
+
+static struct {
+	char string[20];
+} mib_names[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx_hi        " },
+	{ "rx_undersize" },
+	{ "rx_fragments" },
+	{ "rx_oversize" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_err" },
+	{ "rx_crc_err" },
+	{ "rx_align_err" },
+	{ "rx_mac_ctrl" },
+	{ "rx_pause" },
+	{ "rx_bcast" },
+	{ "rx_mcast" },
+	{ "rx_ucast" },
+	{ "rx_64_or_less" },
+	{ "rx_65_127" },
+	{ "rx_128_255" },
+	{ "rx_256_511" },
+	{ "rx_512_1023" },
+	{ "rx_1024_1522" },
+	{ "rx_1523_2000" },
+	{ "rx_2001     " },
+
+	{ "tx_hi        " },
+	{ "tx_late_col" },
+	{ "tx_pause" },
+	{ "tx_bcast" },
+	{ "tx_mcast" },
+	{ "tx_ucast" },
+	{ "tx_deferred" },
+	{ "tx_total_col" },
+	{ "tx_exc_col" },
+	{ "tx_single_col" },
+	{ "tx_mult_col" },
+
+	{ "rx_total" },
+	{ "tx_total" },
+
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+/*
+ * Some counters do not need to be read too often because they are less likely
+ * to increase much.
+ */
+static u8 mib_read_max[SWITCH_COUNTER_NUM] = {
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	1,
+	1,
+	1,
+	1,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+
+	1,
+	4,
+	1,
+	1,
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+};
+
+static u8 mib_start[TOTAL_SWITCH_COUNTER_NUM];
+static u8 mib_index[TOTAL_SWITCH_COUNTER_NUM];
+
+static u8 sw_fill_mib_index(struct ksz_sw *sw, u8 index, u8 interval)
+{
+	int i;
+
+	for (i = 0; i < SWITCH_COUNTER_NUM; i++) {
+		if (interval == mib_read_max[i])
+			mib_start[index++] = i;
+	}
+	return index;
+}  /* sw_fill_mib_index */
+
+static void sw_setup_mib(struct ksz_sw *sw)
+{
+	int i;
+	int j;
+	u8 index = 0;
+
+	for (i = 0; i < 3; i++) {
+		j = 2 - i;
+		sw->mib_interval_start[j] = index;
+		index = sw_fill_mib_index(sw, index, 1 << j);
+	}
+	memcpy(mib_index, mib_start, SWITCH_COUNTER_NUM);
+	j = MIB_9897_RX_BYTE_CNT;
+	for (i = 0; i < 4; i++, j++, index++) {
+		mib_start[index] = SWITCH_COUNTER_NUM + i;
+		mib_index[index] = j;
+	}
+}  /* sw_setup_mib */
+
+static void get_mib_cnt_info(u64 *cnt, u32 data[])
+{
+	u64 num;
+
+	if (data[0] & MIB_COUNTER_OVERFLOW) {
+		num = 1;
+		num <<= 32 + 4;
+		*cnt += num;
+	}
+	num = (data[0] & MIB_COUNTER_DATA_HI_M);
+	num <<= 32;
+	num |= data[1];
+	*cnt += num;
+}  /* get_mib_cnt_info */
+
+static int dbg_mib;
+
+/**
+ * sw_r_mib_cnt_hw - read MIB counters using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The addresses of the counters.
+ * @num:	Number of entries to read.
+ * @data:	Buffer to store the counters.
+ *
+ * This function reads MIB counters of the port using default access.
+ */
+static int sw_r_mib_cnt_hw(struct ksz_sw *sw, uint port, u32 addr[], int num,
+	u32 data[])
+{
+	int i;
+	u32 ctrl_addr;
+	u32 freeze = sw->info->port_cfg[port].freeze ?
+		MIB_COUNTER_FLUSH_FREEZE : 0;
+
+	for (i = 0; i < num; i++) {
+		ctrl_addr = (addr[i] & MIB_COUNTER_INDEX_M);
+		ctrl_addr <<= MIB_COUNTER_INDEX_S;
+		ctrl_addr |= MIB_COUNTER_READ;
+		ctrl_addr |= freeze;
+
+#if 1
+		/*
+		 * First KSZ956X revision chip has a bug that writing 32-bit
+		 * data to the register does not trigger the read operation.
+		 */
+		port_w16(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl_addr >> 16);
+#else
+		port_w32(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl_addr);
+#endif
+		/*
+		 * Need to check the valid bit, but SPI access is slow enough
+		 * to have that bit always set when reading.
+		 */
+		do {
+			port_get(sw, port, REG_PORT_MIB_CTRL_STAT__4, data, 8);
+
+			data[0] = be32_to_cpu(data[0]);
+			if (!(data[0] & MIB_COUNTER_VALID) && dbg_mib++ < 5)
+				dbg_msg(" !valid: %08x\n", data[0]);
+			data[1] = be32_to_cpu(data[1]);
+		} while (!(data[0] & MIB_COUNTER_VALID));
+		data += READ_MIB_ENTRY_SIZE;
+	}
+	return 1;
+}  /* sw_r_mib_cnt_hw */
+
+/**
+ * port_r_cnt - read MIB counters periodically
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to read the counters of the port periodically to avoid
+ * counter overflow.  The hardware should be acquired first before calling this
+ * routine.
+ *
+ * Return non-zero when not all counters not read.
+ */
+static int port_r_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = &sw->port_mib[port];
+	u32 index[MAX_IBA_MIB_ENTRIES];
+	u32 data[MAX_IBA_MIB_ENTRIES * READ_MIB_ENTRY_SIZE];
+	u8 start;
+	int cnt;
+	int i;
+	int rc;
+
+	/* First read in this interval. */
+	if (!mib->cnt_ptr) {
+		u8 interval;
+
+		++mib->interval;
+		switch (mib->interval) {
+		case 2:
+			interval = 1;
+			break;
+		case 4:
+			interval = 2;
+			mib->interval = 0;
+			break;
+		default:
+			interval = 0;
+		}
+
+		/* Determine the starting index in this interval. */
+		mib->cnt_ptr = sw->mib_interval_start[interval];
+	}
+	while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM) {
+		cnt = MAX_IBA_MIB_ENTRIES;
+		if (cnt > TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr)
+			cnt = TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr;
+		for (i = 0; i < cnt; i++)
+			index[i] = mib_index[mib->cnt_ptr + i];
+		sw->ops->acquire(sw);
+		rc = sw->reg->r_mib_cnt_hw(sw, port, index, cnt, data);
+		sw->ops->release(sw);
+		if (!rc)
+			return mib->cnt_ptr;
+		for (i = 0; i < cnt; i++, mib->cnt_ptr++) {
+			start = mib_start[mib->cnt_ptr];
+			get_mib_cnt_info(&mib->counter[start],
+				&data[i * READ_MIB_ENTRY_SIZE]);
+		}
+		if (exit_mib_read(sw))
+			return mib->cnt_ptr;
+	}
+	mib->cnt_ptr = 0;
+	return 0;
+}  /* port_r_cnt */
+
+static void port_freeze_mib(struct ksz_sw *sw, uint port, bool freeze)
+{
+	u32 ctrl = freeze ? MIB_COUNTER_FLUSH_FREEZE : 0;
+
+	sw->info->port_cfg[port].freeze = !!freeze;
+	port_w32(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl);
+}  /* port_freeze_mib */
+
+static void sw_freeze_mib(struct ksz_sw *sw, bool freeze)
+{
+	sw_cfg(sw, REG_SW_MAC_CTRL_6, SW_MIB_COUNTER_FREEZE, freeze);
+}  /* sw_freeze_mib */
+
+/**
+ * port_init_cnt - initialize MIB counter values
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to initialize all counters to zero if the hardware
+ * cannot do it after reset.
+ */
+static inline void port_init_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = &sw->port_mib[port];
+	u32 index[MAX_IBA_MIB_ENTRIES];
+	u32 data[MAX_IBA_MIB_ENTRIES * READ_MIB_ENTRY_SIZE];
+	int cnt;
+	int i;
+	int rc;
+
+	sw->ops->acquire(sw);
+	mib->cnt_ptr = 0;
+	mib->interval = 0;
+	do {
+		cnt = MAX_IBA_MIB_ENTRIES;
+		if (cnt > TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr)
+			cnt = TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr;
+		for (i = 0; i < cnt; i++, mib->cnt_ptr++)
+			index[i] = mib_index[mib->cnt_ptr];
+		rc = sw->reg->r_mib_cnt_hw(sw, port, index, cnt, data);
+		if (!rc)
+			break;
+	} while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM);
+	memset((void *) mib->counter, 0, sizeof(u64) *
+		TOTAL_SWITCH_COUNTER_NUM);
+	mib->cnt_ptr = 0;
+	mib->rate[0].last = mib->rate[1].last = 0;
+	mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+	mib->rate[0].peak = mib->rate[1].peak = 0;
+	sw->ops->release(sw);
+}  /* port_init_cnt */
+
+/* -------------------------------------------------------------------------- */
+
+/* Bandwidth */
+
+static inline void port_cfg_broad_storm(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM, set);
+}
+
+static inline int port_chk_broad_storm(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM);
+}
+
+/* Driver set switch broadcast storm protection at 10% rate. */
+#define BROADCAST_STORM_PROTECTION_RATE	10
+
+/* 148,800 frames * 67 ms / 100 */
+#define BROADCAST_STORM_VALUE		9969
+
+/**
+ * sw_cfg_broad_storm - configure broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ */
+static void sw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	u16 data;
+	u32 value = ((u32) BROADCAST_STORM_VALUE * (u32) percent / 100);
+
+	if (value > BROADCAST_STORM_RATE)
+		value = BROADCAST_STORM_RATE;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	data &= ~BROADCAST_STORM_RATE;
+	data |= value;
+	sw->reg->w16(sw, S_REPLACE_VID_CTRL, data);
+}  /* sw_cfg_broad_storm */
+
+/**
+ * sw_get_board_storm - get broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Buffer to store the broadcast storm threshold percentage.
+ *
+ * This routine retrieves the broadcast storm threshold of the switch.
+ */
+static void sw_get_broad_storm(struct ksz_sw *sw, u8 *percent)
+{
+	int num;
+	u16 data;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	num = (data & BROADCAST_STORM_RATE);
+	num = (num * 100 + BROADCAST_STORM_VALUE / 2) / BROADCAST_STORM_VALUE;
+	*percent = (u8) num;
+}  /* sw_get_broad_storm */
+
+/**
+ * sw_dis_broad_storm - disable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the broadcast storm limit function of the switch.
+ */
+static void sw_dis_broad_storm(struct ksz_sw *sw, uint port)
+{
+	port_cfg_broad_storm(sw, port, 0);
+}  /* sw_dis_broad_storm */
+
+/**
+ * sw_ena_broad_storm - enable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the broadcast storm limit function of the switch.
+ */
+static void sw_ena_broad_storm(struct ksz_sw *sw, uint port)
+{
+	sw_cfg_broad_storm(sw, sw->info->broad_per);
+	port_cfg_broad_storm(sw, port, 1);
+}  /* sw_ena_broad_storm */
+
+/**
+ * sw_init_broad_storm - initialize broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the broadcast storm limit function of the switch.
+ */
+static void sw_init_broad_storm(struct ksz_sw *sw)
+{
+	u8 percent;
+
+	sw_get_broad_storm(sw, &percent);
+	sw->info->broad_per = percent;
+}  /* sw_init_broad_storm */
+
+/**
+ * hw_cfg_broad_storm - configure broadcast storm
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	if (percent > 100)
+		percent = 100;
+
+	sw_cfg_broad_storm(sw, percent);
+	sw_init_broad_storm(sw);
+}  /* hw_cfg_broad_storm */
+
+/**
+ * sw_setup_broad_storm - setup broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine setup the broadcast storm limit function of the switch.
+ */
+static void sw_setup_broad_storm(struct ksz_sw *sw)
+{
+	uint port;
+
+	/* Enable switch broadcast storm protection at 10% percent rate. */
+	hw_cfg_broad_storm(sw, BROADCAST_STORM_PROTECTION_RATE);
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		sw_ena_broad_storm(sw, port);
+	}
+	sw_cfg(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE, 1);
+}  /* sw_setup_broad_storm */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Limit */
+
+/**
+ * hw_cfg_rate_limit - configure port rate limit modes
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mask:	The mask value.
+ * @shift:	The shift position.
+ * @mode:	The rate limit mode.
+ *
+ * This helper routine configures the rate limit modes of the port.
+ */
+static void hw_cfg_rate_limit(struct ksz_sw *sw, uint port, u8 mask, u8 shift,
+	u8 mode)
+{
+	u8 data;
+	u8 saved;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	saved = data;
+	data &= ~(mask << shift);
+	data |= mode << shift;
+	if (data != saved)
+		port_w8(sw, port, P_RATE_LIMIT_CTRL, data);
+	sw->info->port_cfg[port].rate_limit = data;
+}  /* hw_cfg_rate_limit */
+
+static void hw_cfg_in_port_based(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_PORT_BASED_S, set != 0);
+}
+
+static void hw_cfg_in_flow_ctrl(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_FLOW_CTRL_S, set != 0);
+}
+
+/**
+ * hw_cfg_cnt_ifg - configure port rate limit count IFG control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count IFG control of the port.
+ */
+static void hw_cfg_cnt_ifg(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_IFG_S, set != 0);
+}  /* hw_cfg_cnt_ifg */
+
+/**
+ * hw_cfg_cnt_pre - configure port rate limit count preamble control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count preamble control of the port.
+ */
+static void hw_cfg_cnt_pre(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_PREAMBLE_S, set != 0);
+}  /* hw_cfg_cnt_pre */
+
+/**
+ * hw_cfg_rx_limit - configure port rate limit mode
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mode:	The rate limit mode.
+ *
+ * This routine configures the rate limit mode of the port.
+ */
+static void hw_cfg_rx_limit(struct ksz_sw *sw, uint port, u8 mode)
+{
+	if (mode > PORT_IN_LIMIT_MODE_M)
+		return;
+
+	hw_cfg_rate_limit(sw, port, PORT_IN_LIMIT_MODE_M,
+		PORT_IN_LIMIT_MODE_S, mode);
+}  /* hw_cfg_rx_limit */
+
+/**
+ * hw_get_rate_limit - get port rate limit control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine retrieves the rate limit of the port.
+ */
+static void hw_get_rate_limit(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	sw->info->port_cfg[port].rate_limit = data;
+	sw->info->port_cfg[port].packet_based = (data >>
+		PORT_RATE_PACKET_BASED_S) & 1;
+}  /* hw_get_rate_limit */
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_rate_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1000 * val;
+	else
+		i = 64 * (val - 100);
+	return i;
+}
+
+static int get_rate_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1000) {
+		i = (rate + 500) / 1000;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate)
+		i = 0;
+	else {
+		i = (rate + 32) / 64;
+		if (0 == i)
+			i = 1;
+		else if (i > 15)
+			i = 15;
+		i += 100;
+	}
+	return i;
+}
+
+static uint get_packet_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1920 * val;
+	else if (101 == val)
+		i = 64;
+	else
+		i = 128 * (val - 101);
+	return i;
+}
+
+static int get_packet_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1920) {
+		i = (rate + 960) / 1920;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate)
+		i = 0;
+	else if (rate <= 64)
+		i = 101;
+	else {
+		i = (rate + 64) / 128;
+		if (0 == i)
+			i = 1;
+		else if (i > 14)
+			i = 14;
+		i += 101;
+	}
+	return i;
+}
+
+/**
+ * port_cfg_rate - configure port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to set the value.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port.
+ */
+static void port_cfg_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint rate, bool packet)
+{
+	u8 factor;
+
+	if (sw->port_info[port].tx_rate / TX_RATE_UNIT == 1000)
+		rate /= 10;
+	if (packet)
+		factor = (u8) get_packet_to_val(rate);
+	else
+		factor = (u8) get_rate_to_val(rate);
+
+	port_w8(sw, port, offset + prio, factor);
+}  /* port_cfg_rate */
+
+/**
+ * port_get_rate - get port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to get the value.
+ * @rate:	Buffer to store the data rate in number of Kbps (or Kpps).
+ * @packet:	Packet indication.
+ *
+ * This helper routine retrieves the priority rate of the port.
+ */
+static void port_get_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint *rate, bool packet)
+{
+	u8 data;
+
+	port_r8(sw, port, offset + prio, &data);
+	if (packet)
+		*rate = get_packet_from_val(data);
+	else
+		*rate = get_rate_from_val(data);
+	if (sw->port_info[port].tx_rate / TX_RATE_UNIT == 1000)
+		*rate *= 10;
+}  /* port_get_rate */
+
+/**
+ * hw_cfg_prio_rate - configure port priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ * @offset:	The receive or transmit rate offset.
+ * @result:	Buffer to store the data rate in number of Kbps (or Kpps).
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port and retrieves
+ * the actual rate number.
+ */
+static void hw_cfg_prio_rate(struct ksz_sw *sw, uint port, uint prio, uint rate,
+	uint offset, uint *result, bool packet)
+{
+	port_cfg_rate(sw, port, prio, offset, rate, packet);
+	port_get_rate(sw, port, prio, offset, result, packet);
+}  /* hw_cfg_prio_rate */
+
+/*
+ * THa  2016/02/24
+ * The receive rate limit does not take effect until the last priority is also
+ * written!  It can be turned off without writing the last priority.  Setting
+ * it turns on rate limiting but the hardware seems to use the last value.
+ */
+static void hw_set_rx_prio(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+
+	port_r8(sw, port, REG_PORT_IN_RATE_0 + RX_PRIO_QUEUES - 1, &data);
+	port_w8(sw, port, REG_PORT_IN_RATE_0 + RX_PRIO_QUEUES - 1, data);
+}  /* hw_set_rx_prio */
+
+/**
+ * hw_cfg_rx_prio_rate - configure port receive priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ *
+ * This routine configures the receive priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_rx_prio_rate(struct ksz_sw *sw, uint port, uint prio,
+	uint rate)
+{
+	uint *result;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	if (cfg->packet_based)
+		result = &cfg->rx_packet[prio];
+	else
+		result = &cfg->rx_rate[prio];
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		REG_PORT_IN_RATE_0,
+		result, cfg->packet_based);
+}  /* hw_cfg_rx_prio_rate */
+
+/**
+ * hw_cfg_tx_prio_rate - configure port transmit priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ *
+ * This routine configures the transmit priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tx_prio_rate(struct ksz_sw *sw, uint port, uint prio,
+	uint rate)
+{
+	uint *result;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	if (cfg->packet_based)
+		result = &cfg->tx_packet[prio];
+	else
+		result = &cfg->tx_rate[prio];
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		REG_PORT_OUT_RATE_0,
+		result, cfg->packet_based);
+}  /* hw_cfg_tx_prio_rate */
+
+/**
+ * sw_chk_rx_prio_rate - check switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the rx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 in_rate0;
+	u32 in_rate1;
+
+	rate_addr = PORT_CTRL_ADDR(port, REG_PORT_IN_RATE_0);
+	in_rate0 = sw->reg->r32(sw, rate_addr);
+	in_rate1 = sw->reg->r32(sw, rate_addr + 4);
+	return (in_rate0 | in_rate1) != 0;
+}  /* sw_chk_rx_prio_rate */
+
+/**
+ * sw_chk_tx_prio_rate - check switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the tx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 out_rate;
+
+	rate_addr = PORT_CTRL_ADDR(port, REG_PORT_OUT_RATE_0);
+	if (sw_chk(sw, REG_SW_MAC_CTRL_5, SW_OUT_RATE_LIMIT_QUEUE_BASED))
+		out_rate = sw->reg->r32(sw, rate_addr);
+
+	/* Only need to check first priority as the others do not matter. */
+	else
+		out_rate = sw->reg->r8(sw, rate_addr);
+	return (out_rate) != 0;
+}  /* sw_chk_tx_prio_rate */
+
+/**
+ * sw_dis_rx_prio_rate - disable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the rx priority rate function of the switch.
+ */
+static void sw_dis_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	rate_addr = PORT_CTRL_ADDR(port, REG_PORT_IN_RATE_0);
+	sw->reg->w32(sw, rate_addr, 0);
+	sw->reg->w32(sw, rate_addr + 4, 0);
+}  /* sw_dis_rx_prio_rate */
+
+/**
+ * sw_dis_tx_prio_rate - disable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the tx priority rate function of the switch.
+ */
+static void sw_dis_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	rate_addr = PORT_CTRL_ADDR(port, REG_PORT_OUT_RATE_0);
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_tx_prio_rate */
+
+/**
+ * sw_ena_rx_prio_rate - enable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the rx priority rate function of the switch.
+ */
+static void sw_ena_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+	u32 *rate;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	if (cfg->packet_based)
+		rate = cfg->rx_packet;
+	else
+		rate = cfg->rx_rate;
+/*
+ * THa  2016/02/24
+ * The receive rate limit does not take effect until the last priority is also
+ * written!  It can be turned off without writing the last priority.  Setting
+ * it turns on rate limiting but the hardware seems to use the last value.
+ */
+	for (prio = 0; prio < RX_PRIO_QUEUES; prio++, rate++)
+		hw_cfg_rx_prio_rate(sw, port, prio, *rate);
+}  /* sw_ena_rx_prio_rate */
+
+/**
+ * sw_ena_tx_prio_rate - enable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the tx priority rate function of the switch.
+ */
+static void sw_ena_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+	u32 *rate;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	if (cfg->packet_based)
+		rate = cfg->tx_packet;
+	else
+		rate = cfg->tx_rate;
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rate++)
+		hw_cfg_tx_prio_rate(sw, port, prio, *rate);
+}  /* sw_ena_tx_prio_rate */
+
+static void hw_cfg_rate_packet_based(struct ksz_sw *sw, uint port, bool set)
+{
+	int prio;
+	u32 *rx_rate;
+	u32 *tx_rate;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+
+	cfg->packet_based = set;
+	hw_cfg_rate_limit(sw, port, 1, PORT_RATE_PACKET_BASED_S, set != 0);
+	if (cfg->packet_based) {
+		rx_rate = cfg->rx_packet;
+		tx_rate = cfg->tx_packet;
+	} else {
+		rx_rate = cfg->rx_rate;
+		tx_rate = cfg->tx_rate;
+	}
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rx_rate++, tx_rate++) {
+
+		/* Rate limiting is not enabled. */
+		if (!cfg->rx_rate[prio] && !cfg->rx_packet[prio])
+			continue;
+		hw_cfg_rx_prio_rate(sw, port, prio, *rx_rate);
+
+		/* Rate limiting is not enabled. */
+		if (!cfg->tx_rate[prio] && !cfg->tx_packet[prio])
+			continue;
+		hw_cfg_tx_prio_rate(sw, port, prio, *tx_rate);
+	}
+	for (; prio < RX_PRIO_QUEUES; prio++, rx_rate++) {
+
+/* See issue above about configuring rx priority rates. */
+#if 0
+		/* Rate limiting is not enabled. */
+		if (!cfg->rx_rate[prio] && !cfg->rx_packet[prio])
+			continue;
+#endif
+		hw_cfg_rx_prio_rate(sw, port, prio, *rx_rate);
+	}
+}  /* hw_cfg_rate_packet_based */
+
+/**
+ * sw_init_prio_rate - initialize switch prioirty rate
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the priority rate function of the switch.
+ */
+static void sw_init_prio_rate(struct ksz_sw *sw)
+{
+	uint offset;
+	uint port;
+	uint prio;
+	struct ksz_port_cfg *cfg;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		cfg = &sw->info->port_cfg[port];
+		hw_get_rate_limit(sw, port);
+		for (prio = 0; prio < RX_PRIO_QUEUES; prio++) {
+			offset = REG_PORT_IN_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&cfg->rx_rate[prio], false);
+			port_get_rate(sw, port, prio, offset,
+				&cfg->rx_packet[prio], true);
+		}
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			offset = REG_PORT_OUT_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&cfg->tx_rate[prio], false);
+			port_get_rate(sw, port, prio, offset,
+				&cfg->tx_rate[prio], true);
+		}
+	}
+}  /* sw_init_prio_rate */
+
+/* -------------------------------------------------------------------------- */
+
+/* Communication */
+
+static inline void port_cfg_back_pressure(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MAC_CTRL_1, PORT_BACK_PRESSURE, set);
+}
+
+static inline void port_cfg_force_flow_ctrl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0,
+		PORT_FORCE_TX_FLOW_CTRL | PORT_FORCE_RX_FLOW_CTRL, set);
+}
+
+static inline void port_cfg_mac_loopback(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0, PORT_MAC_LOOPBACK, set);
+}
+
+static void port_cfg_phy_loopback(struct ksz_sw *sw, uint p, bool set)
+{
+	u16 data;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[p];
+
+	port_r16(sw, p, REG_PORT_PHY_CTRL, &data);
+	if (set) {
+		if (!(data & PORT_PHY_LOOPBACK)) {
+			cfg->phy_ctrl = data;
+			data &= ~PORT_AUTO_NEG_ENABLE;
+			data |= PORT_PHY_LOOPBACK;
+		}
+	} else {
+		if (data & PORT_PHY_LOOPBACK)
+			data = cfg->phy_ctrl;
+	}
+	port_w16(sw, p, REG_PORT_PHY_CTRL, data);
+}
+
+static inline void port_cfg_remote_loopback(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg16(sw, p,
+		REG_PORT_PHY_REMOTE_LB_LED, PORT_REMOTE_LOOPBACK, set);
+}
+
+static inline void port_cfg_tail_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0, PORT_TAIL_TAG_ENABLE, set);
+}
+
+static inline int port_chk_back_pressure(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MAC_CTRL_1, PORT_BACK_PRESSURE);
+}
+
+static inline int port_chk_force_flow_ctrl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0,
+		PORT_FORCE_TX_FLOW_CTRL | PORT_FORCE_RX_FLOW_CTRL);
+}
+
+static inline int port_chk_mac_loopback(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0, PORT_MAC_LOOPBACK);
+}
+
+static inline int port_chk_phy_loopback(struct ksz_sw *sw, uint p)
+{
+	return port_chk16(sw, p,
+		REG_PORT_PHY_CTRL, PORT_PHY_LOOPBACK);
+}
+
+static inline int port_chk_remote_loopback(struct ksz_sw *sw, uint p)
+{
+	return port_chk16(sw, p,
+		REG_PORT_PHY_REMOTE_LB_LED, PORT_REMOTE_LOOPBACK);
+}
+
+static inline int port_chk_tail_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0, PORT_TAIL_TAG_ENABLE);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Spanning Tree */
+
+static inline void port_cfg_mstp(struct ksz_sw *sw, uint p, u8 mstp)
+{
+	if (sw->info->port_cfg[p].mstp != mstp) {
+		port_w(sw, p, REG_PORT_LUE_MSTP_INDEX, mstp);
+		sw->info->port_cfg[p].mstp = mstp;
+	}
+}
+
+static inline void port_cfg_dis_learn(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE, set);
+}
+
+static inline void port_cfg_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE, set);
+	if (set)
+		sw->rx_ports[sw->info->port_cfg[p].mstp] |= (1 << p);
+	else
+		sw->rx_ports[sw->info->port_cfg[p].mstp] &= ~(1 << p);
+}
+
+static inline void port_cfg_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE, set);
+	if (set)
+		sw->tx_ports[sw->info->port_cfg[p].mstp] |= (1 << p);
+	else
+		sw->tx_ports[sw->info->port_cfg[p].mstp] &= ~(1 << p);
+}
+
+static inline u8 port_chk_mstp(struct ksz_sw *sw, uint p)
+{
+	SW_D mstp;
+
+	port_r(sw, p, REG_PORT_LUE_MSTP_INDEX, &mstp);
+	return mstp;
+}
+
+static inline int port_chk_dis_learn(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE);
+}
+
+static inline int port_chk_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE);
+}
+
+static inline int port_chk_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE);
+}
+
+static void port_cfg_rx_special(struct ksz_sw *sw, uint p, bool set)
+{
+	int hsr = false;
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		hsr = true;
+#endif
+	if (!hsr)
+		port_cfg_rx(sw, p, set);
+#ifdef CONFIG_KSZ_HSR
+	else {
+		if (set)
+			port_cfg_rx(sw, p, set);
+		do {
+			int n;
+			u16 mask = 0;
+
+			for (n = 0; n < sw->eth_cnt; n++) {
+				if (sw->eth_maps[n].proto & HSR_HW) {
+					mask = sw->eth_maps[n].mask;
+					break;
+				}
+			}
+			if (mask & (1 << p)) {
+				u32 val;
+
+				val = sw->reg->r32(sw, REG_HSR_PORT_MAP__4);
+				if (set)
+					val |= mask & ~(1 << p);
+				else
+					val &= (1 << p);
+				sw->reg->w32(sw, REG_HSR_PORT_MAP__4, val);
+			}
+		} while (0);
+		if (!set)
+			port_cfg_rx(sw, p, set);
+	}
+#endif
+}  /* port_cfg_rx_special */
+
+static void port_cfg_power(struct ksz_sw *sw, uint p, bool set)
+{
+	u16 ctrl;
+	u8 intr;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[p];
+
+	if (set) {
+		u32 data;
+
+		port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+		if (!(ctrl & PORT_POWER_DOWN))
+			return;
+		ctrl = cfg->phy_ctrl;
+		port_w16(sw, p, REG_PORT_PHY_CTRL, cfg->phy_ctrl);
+		port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &ctrl);
+		if (ctrl != cfg->phy_adv);
+			port_w16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION,
+				cfg->phy_adv);
+		if (sw->features & GIGABIT_SUPPORT) {
+			port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+			if (ctrl != cfg->phy_adv_g);
+				port_w16(sw, p, REG_PORT_PHY_1000_CTRL,
+					cfg->phy_adv_g);
+		}
+		port_r32(sw, p, REG_PORT_PHY_INT_ENABLE & ~3, &data);
+		data &= 0xffff00ff;
+		data |= cfg->phy_intr << 8;
+		port_w32(sw, p, REG_PORT_PHY_INT_ENABLE & ~3, data);
+		ctrl = cfg->phy_ctrl;
+		if (ctrl & PORT_AUTO_NEG_ENABLE)
+			ctrl |= PORT_AUTO_NEG_RESTART;
+		port_w16(sw, p, REG_PORT_PHY_CTRL, ctrl);
+	} else {
+		port_r8(sw, p, REG_PORT_PHY_INT_ENABLE, &intr);
+		cfg->phy_intr = intr;
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &ctrl);
+		cfg->phy_adv = ctrl;
+		if (sw->features & GIGABIT_SUPPORT) {
+			port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+			cfg->phy_adv_g = ctrl;
+		}
+#if 0
+		port_r16(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, &ctrl);
+		ctrl |= PORT_REG_CLK_SPEED_25_MHZ;
+		port_w16(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, ctrl);
+#endif
+		port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+		ctrl &= ~PORT_POWER_DOWN;
+		cfg->phy_ctrl = ctrl;
+		port_w16(sw, p, REG_PORT_PHY_CTRL, ctrl | PORT_POWER_DOWN);
+	}
+}  /* port_cfg_power */
+
+static int port_chk_power(struct ksz_sw *sw, uint p)
+{
+	u16 ctrl;
+
+	port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+	return !(ctrl & PORT_POWER_DOWN);
+}  /* port_chk_power */
+
+static inline void sw_cfg_fast_aging(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING, set);
+}
+
+static void sw_flush_dyn_mac_table(struct ksz_sw *sw, uint port)
+{
+	int cnt;
+	int first;
+	int index;
+	int learn_disable[TOTAL_PORT_NUM];
+	SW_D data;
+
+	data = SW_R(sw, REG_SW_LUE_CTRL_2);
+	data &= ~(SW_FLUSH_OPTION_M << SW_FLUSH_OPTION_S);
+	data |= (SW_FLUSH_OPTION_DYN_MAC << SW_FLUSH_OPTION_S);
+	SW_W(sw, REG_SW_LUE_CTRL_2, data);
+	if (port < sw->mib_port_cnt) {
+		first = port;
+		cnt = port + 1;
+		for (index = first; index < cnt; index++) {
+			learn_disable[index] = port_chk_dis_learn(sw, index);
+			if (!learn_disable[index])
+				port_cfg_dis_learn(sw, index, 1);
+		}
+		sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_DYN_MAC_TABLE, 1);
+		for (index = first; index < cnt; index++) {
+			if (!learn_disable[index])
+				port_cfg_dis_learn(sw, index, 0);
+		}
+	} else {
+		first = 0;
+		cnt = sw->mib_port_cnt;
+		sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_STP_TABLE, 1);
+	}
+}  /* sw_flush_dyn_mac_table */
+
+/* -------------------------------------------------------------------------- */
+
+/* VLAN */
+
+static inline void port_cfg_drop_non_vlan(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_NON_VLAN, set);
+}
+
+/**
+ * port_cfg_drop_tag - configure 802.1q tagged packet drop
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static inline void port_cfg_drop_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_TAG, set);
+}
+
+static inline int port_chk_drop_non_vlan(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_NON_VLAN);
+}
+
+static inline int port_chk_drop_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_TAG);
+}
+
+/**
+ * port_cfg_dis_non_vid - configure discard non VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Discard Non VID packets of the switch port.
+ * If enabled, the device will discard packets whose VLAN id does not match
+ * ingress port-based default VLAN id.
+ */
+static inline void port_cfg_dis_non_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_LUE_CTRL, PORT_DISCARD_NON_VID, set);
+}  /* port_cfg_dis_non_vid */
+
+/**
+ * port_cfg_in_filter - configure ingress VLAN filtering
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Ingress VLAN filtering of the switch port.
+ * If enabled, the device will discard packets whose VLAN id membership	in the
+ * VLAN table receive ports does not include the ingress port that received
+ * this packet.
+ */
+static inline void port_cfg_in_filter(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_LUE_CTRL, PORT_INGRESS_FILTER, set);
+}  /* port_cfg_in_filter */
+
+static inline int port_chk_dis_non_vid(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_LUE_CTRL, PORT_DISCARD_NON_VID);
+}
+
+static inline int port_chk_in_filter(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_LUE_CTRL, PORT_INGRESS_FILTER);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Mirroring */
+
+static inline void port_cfg_mirror_sniffer(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER, set);
+}
+
+static inline void port_cfg_mirror_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX, set);
+}
+
+static inline void port_cfg_mirror_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX, set);
+}
+
+static inline void sw_cfg_mirror_rx_tx(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX, set);
+}
+
+static inline int port_chk_mirror_sniffer(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER);
+}
+
+static inline int port_chk_mirror_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX);
+}
+
+static inline int port_chk_mirror_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX);
+}
+
+static inline int sw_chk_mirror_rx_tx(struct ksz_sw *sw)
+{
+	return sw_chk(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX);
+}
+
+static void sw_setup_mirror(struct ksz_sw *sw)
+{
+	uint port;
+
+	/*
+	 * The mirror sniffer port requires it to be in the port membership
+	 * of the receive and transmit ports.
+	 * For example, port 3 is the mirror port of traffic between ports 1
+	 * and 2.  Port 3 needs only to turn sniffer on; its port membership
+	 * can be 0.  Ordinarily the port membership of ports 1 and 2 is 3 for
+	 * just commnunicating with eath other.  It has to be set to 7 to pass
+	 * the frames to port 3.  Only one of the ports needs to turn on
+	 * receive and transmit mirroring.
+	 * The mirror receive and transmit mode requires at least two ports to
+	 * turn on receive and transmit mirroring.
+	 */
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		port_cfg_mirror_sniffer(sw, port, 0);
+		port_cfg_mirror_rx(sw, port, 0);
+		port_cfg_mirror_tx(sw, port, 0);
+	}
+	sw_cfg_mirror_rx_tx(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Priority */
+
+static inline void port_cfg_diffserv(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_802_1p(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_vlan_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_VLAN_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_mac_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_MAC_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_acl_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_ACL_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_highest_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_HIGHEST_PRIO, set);
+}
+
+static inline void port_cfg_or_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_OR_PRIO, set);
+}
+
+static inline void port_cfg_replace_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_USER_PRIO_CEILING, set);
+}
+
+static inline void port_set_prio_queue(struct ksz_sw *sw, uint p, uint queue)
+{
+	port_w_s(sw, p,
+		REG_PORT_CTRL_0, PORT_QUEUE_SPLIT_ENABLE, 0, queue);
+}
+
+static inline int port_chk_diffserv(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_PRIO_ENABLE);
+}
+
+static inline int port_chk_802_1p(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_PRIO_ENABLE);
+}
+
+static inline int port_chk_vlan_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_VLAN_PRIO_ENABLE);
+}
+
+static inline int port_chk_mac_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_MAC_PRIO_ENABLE);
+}
+
+static inline int port_chk_acl_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_ACL_PRIO_ENABLE);
+}
+
+static inline int port_chk_highest_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_HIGHEST_PRIO);
+}
+
+static inline int port_chk_or_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_OR_PRIO);
+}
+
+static inline int port_chk_replace_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_USER_PRIO_CEILING);
+}
+
+static inline int port_get_prio_queue(struct ksz_sw *sw, uint p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_CTRL_0, PORT_QUEUE_SPLIT_ENABLE, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Policing */
+
+static void port_cfg_index(struct ksz_sw *sw, uint port, uint p, uint q)
+{
+	u32 data;
+
+	data = (p & MRI_INDEX_P_M) << MRI_INDEX_P_S;
+	data |= (q & MRI_INDEX_Q_M) << MRI_INDEX_Q_S;
+	port_w32(sw, port, REG_PORT_MRI_INDEX__4, data);
+}
+
+static inline void port_cfg_police(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_ENABLE, set);
+}
+
+static inline void port_cfg_color_aware(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_COLOR_NOT_AWARE, !set);
+}
+
+static inline void port_cfg_drop_srp(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_SRP, set);
+}
+
+static inline void port_cfg_color_mark(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_MARK_ENABLE, set);
+}
+
+static inline void port_cfg_color_remap(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_REMAP_ENABLE, set);
+}
+
+static inline void port_cfg_port_based_policing(struct ksz_sw *sw, uint p,
+	bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, PORT_BASED_POLICING, set);
+}
+
+static inline void port_cfg_police_drop_all(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_ALL, set);
+}
+
+static inline void port_set_police_packet_type(struct ksz_sw *sw, uint p,
+	u32 type)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_PACKET_TYPE_M,
+			POLICE_PACKET_TYPE_S, type);
+}
+
+static inline void port_set_non_dscp_color(struct ksz_sw *sw, uint p, u32 color)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, SW_COLOR_M,
+			NON_DSCP_COLOR_S, color);
+}
+
+static inline int port_chk_police(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_ENABLE);
+}
+
+static inline int port_chk_color_aware(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_COLOR_NOT_AWARE);
+}
+
+static inline int port_chk_drop_srp(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_SRP);
+}
+
+static inline int port_chk_color_mark(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_MARK_ENABLE);
+}
+
+static inline int port_chk_color_remap(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_REMAP_ENABLE);
+}
+
+static inline int port_chk_port_based_policing(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, PORT_BASED_POLICING);
+}
+
+static inline int port_chk_police_drop_all(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_ALL);
+}
+
+static inline u32 port_get_police_packet_type(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_PACKET_TYPE_M,
+			POLICE_PACKET_TYPE_S);
+}
+
+static inline u32 port_get_non_dscp_color(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, SW_COLOR_M,
+			NON_DSCP_COLOR_S);
+}
+
+static inline u16 port_get_cir(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_CIR_S);
+}
+
+static inline u16 port_get_pir(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_PIR_S);
+}
+
+static inline void port_set_cir(struct ksz_sw *sw, uint p, u16 rate)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_CIR_S, rate);
+}
+
+static inline void port_set_pir(struct ksz_sw *sw, uint p, u16 rate)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_PIR_S, rate);
+}
+
+static inline u16 port_get_cbs(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_CBS_S);
+}
+
+static inline u16 port_get_pbs(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_PBS_S);
+}
+
+static inline void port_set_cbs(struct ksz_sw *sw, uint p, u16 size)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_CBS_S, size);
+}
+
+static inline void port_set_pbs(struct ksz_sw *sw, uint p, u16 size)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_PBS_S, size);
+}
+
+static inline u16 port_get_wred_max(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_min(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_multiplier(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S);
+}
+
+static inline u16 port_get_wred_avg_size(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_AVG_QUEUE_SIZE_S);
+}
+
+static inline void port_set_wred_max(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_min(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_multiplier(struct ksz_sw *sw, uint p, u16 val)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S, val);
+}
+
+static inline u16 port_get_wred_q_max(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_q_min(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_q_multiplier(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S);
+}
+
+static inline u16 port_get_wred_q_avg_size(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_AVG_QUEUE_SIZE_S);
+}
+
+static inline void port_set_wred_q_max(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_q_min(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_q_multiplier(struct ksz_sw *sw, uint p, u16 val)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S, val);
+}
+
+static inline void port_cfg_wred_random_drop(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_RANDOM_DROP_ENABLE, set);
+}
+
+static inline void port_cfg_wred_drop_gyr(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_GYR_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_yr(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_YR_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_r(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_R_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_all(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_ALL, set);
+}
+
+static inline int port_chk_wred_random_drop(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_RANDOM_DROP_ENABLE);
+}
+
+static inline int port_chk_wred_drop_gyr(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_GYR_DISABLE);
+}
+
+static inline int port_chk_wred_drop_yr(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_YR_DISABLE);
+}
+
+static inline int port_chk_wred_drop_r(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_R_DISABLE);
+}
+
+static inline int port_chk_wred_drop_all(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_ALL);
+}
+
+static u32 port_get_wred_pmon(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_PMON_M, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Control */
+
+#ifdef MTI_PREEMPT_ENABLE
+static inline void port_cfg_preempt(struct ksz_sw *sw, uint p, uint q, bool set)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_cfg(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_PREEMPT_ENABLE, set);
+}
+
+static inline int port_chk_preempt(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_chk(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_PREEMPT_ENABLE);
+}
+#endif
+
+static inline u8 port_get_schedule_mode(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_r_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SCHEDULE_MODE_M,
+			MTI_SCHEDULE_MODE_S);
+}
+
+static inline u8 port_get_shaping(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_r_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SHAPING_M,
+			MTI_SHAPING_S);
+}
+
+static inline u8 port_get_tx_ratio(struct ksz_sw *sw, uint p, uint q)
+{
+	u8 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r(sw, p, REG_PORT_MTI_QUEUE_CTRL_1, &data);
+	return data;
+}
+
+/**
+ * port_set_schedule_mode - configure port rate control
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @q:		The priority queue.
+ * @mode:	The schedule mode to specify strict priority or WRR.
+ *
+ * This routine configures the priority queue rate control of the port.
+ */
+static inline void port_set_schedule_mode(struct ksz_sw *sw, uint p, uint q,
+	u8 mode)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SCHEDULE_MODE_M,
+			MTI_SCHEDULE_MODE_S, mode);
+}  /* port_set_schedule_mode */
+
+static inline void port_set_shaping(struct ksz_sw *sw, uint p, uint q,
+	u8 shaping)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SHAPING_M,
+			MTI_SHAPING_S, shaping);
+}
+
+/**
+ * port_set_tx_ratio - configure port rate ratio
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @q:		The priority queue.
+ * @ratio:	The rate ratio.
+ *
+ * This routine configures the priority queue rate ratio of the port.
+ */
+static inline void port_set_tx_ratio(struct ksz_sw *sw, uint p, uint q, u8 ratio)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w(sw, p, REG_PORT_MTI_QUEUE_CTRL_1, ratio & MTI_TX_RATIO_M);
+}  /* port_set_tx_ratio */
+
+static u16 port_get_hi_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_2__2, &data);
+	return data;
+}  /* port_get_hi_water_mark */
+
+static u16 port_get_lo_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_3__2, &data);
+	return data;
+}  /* port_get_lo_water_mark */
+
+static u16 port_get_increment(struct ksz_sw *sw, uint p, uint q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_4__2, &data);
+	return data;
+}  /* port_get_increment */
+
+static u8 port_get_srp(struct ksz_sw *sw, uint p)
+{
+	u8 data;
+
+	port_r(sw, p, REG_PORT_CTRL_1, &data);
+	return data & PORT_SRP_ENABLE;
+}  /* port_get_srp */
+
+static void port_set_hi_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_2__2, val);
+}  /* port_set_hi_water_mark */
+
+static void port_set_lo_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_3__2, val);
+}  /* port_set_lo_water_mark */
+
+static void port_set_increment(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_4__2, val);
+}  /* port_set_increment */
+
+static void port_set_srp(struct ksz_sw *sw, uint p, u8 srp)
+{
+	port_w(sw, p, REG_PORT_CTRL_1, srp & PORT_SRP_ENABLE);
+}  /* port_set_srp */
+
+/* -------------------------------------------------------------------------- */
+
+/* Queue Management */
+
+static inline u8 port_get_qm_drop(struct ksz_sw *sw, uint p)
+{
+	return (u8) port_r_s_32(sw, p, REG_PORT_QM_CTRL__4,
+		PORT_QM_DROP_PRIO_M, 0);
+}
+
+static u8 port_get_qm_burst_size(struct ksz_sw *sw, uint p)
+{
+	return (u8) port_r_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		3, PORT_QM_BURST_SIZE_S);
+}
+
+static u16 port_get_qm_resv_space(struct ksz_sw *sw, uint p)
+{
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PORT_QM_MIN_RESV_SPACE_M, 0);
+}
+
+static u16 port_get_qm_hi_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_HI_WATER_MARK_S);
+}
+
+static u16 port_get_qm_lo_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_LO_WATER_MARK_S);
+}
+
+static u16 port_get_qm_tx_used(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_0__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_USED_S);
+}
+
+static u16 port_get_qm_tx_avail(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_1__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_AVAIL_S);
+}
+
+static u16 port_get_qm_tx_calculated(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_1__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_CALCULATED_S);
+}
+
+static inline void port_set_qm_drop(struct ksz_sw *sw, uint p, u8 drop)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_CTRL__4,
+		PORT_QM_DROP_PRIO_M, 0, drop);
+}
+
+static inline void port_set_qm_burst_size(struct ksz_sw *sw, uint p, u8 burst)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		3, PORT_QM_BURST_SIZE_S, burst);
+}
+
+static inline void port_set_qm_resv_space(struct ksz_sw *sw, uint p, u16 space)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PORT_QM_MIN_RESV_SPACE_M, 0, space);
+}
+
+static void port_set_qm_hi_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	port_w_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_HI_WATER_MARK_S, val);
+}
+
+static void port_set_qm_lo_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	port_w_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_LO_WATER_MARK_S, val);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_tos_prio - program switch TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the TOS priority into the switch registers.
+ */
+static void sw_set_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	SW_W(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE, prio);
+}  /* sw_set_tos_prio */
+
+/**
+ * sw_dis_diffserv - disable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the DiffServ priority function of the switch.
+ */
+static void sw_dis_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 0);
+}  /* sw_dis_diffserv */
+
+/**
+ * sw_ena_diffserv - enable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the DiffServ priority function of the switch.
+ */
+static void sw_ena_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 1);
+}  /* sw_ena_diffserv */
+
+/**
+ * hw_cfg_tos_prio - configure TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 7.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+	SW_D regmask = KS_PRIO_M;
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	if (prio >= 0x10)
+		regmask = (KS_PRIO_M << KS_PRIO_S) | KS_PRIO_M;
+	shift = (tos & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tos /= KS_PRIO_IN_REG;
+
+	sw->info->diffserv[tos] &= ~mask;
+	sw->info->diffserv[tos] |= prio;
+
+	sw_set_tos_prio(sw, tos, sw->info->diffserv[tos]);
+}  /* hw_cfg_tos_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_802_1p_prio - program switch 802.1p priority
+ * @sw:		The switch instance.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the 802.1p priority into the switch register.
+ */
+static void sw_set_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	SW_W(sw, S_802_1P_PRIO_CTRL + tag / SW_SIZE, prio);
+}  /* sw_set_802_1p_prio */
+
+/**
+ * sw_dis_802_1p - disable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the 802.1p priority function of the switch.
+ */
+static void sw_dis_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 0);
+}  /* sw_dis_802_1p */
+
+/**
+ * sw_ena_802_1p - enable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the 802.1p priority function of the switch.
+ */
+static void sw_ena_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 1);
+}  /* sw_ena_802_1p */
+
+/**
+ * hw_cfg_802_1p_prio - configure 802.1p priority
+ * @sw:		The switch instance.
+ * @tag:	The 802.1p tag priority value, ranging from 0 to 7.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the 802.1p priority in the hardware.
+ * 802.1p Tag priority value 0 ~ 7 is mapped to Priority Queue Number 0 ~ 7.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+	SW_D regmask = KS_PRIO_M;
+
+	if (tag >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	if (prio >= 0x10)
+		regmask = (KS_PRIO_M << KS_PRIO_S) | KS_PRIO_M;
+	shift = (tag & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tag /= KS_PRIO_IN_REG;
+
+	sw->info->p_802_1p[tag] &= ~mask;
+	sw->info->p_802_1p[tag] |= prio;
+
+	sw_set_802_1p_prio(sw, tag, sw->info->p_802_1p[tag]);
+}  /* hw_cfg_802_1p_prio */
+
+/**
+ * sw_cfg_replace_null_vid - enable switch null VID replacement
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the VID to be replaced with port default VID if it is
+ * empty.
+ */
+static void sw_cfg_replace_null_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p, REG_PORT_MTI_QUEUE_CTRL_0__4, MTI_PVID_REPLACE, set);
+}  /* sw_cfg_replace_null_vid */
+
+/**
+ * sw_cfg_replace_prio - enable switch 802.1p priority re-mapping
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the 802.1p priority re-mapping function of the switch.
+ * That allows 802.1p priority field to be replaced with the port's default
+ * tag's priority value if the ingress packet's 802.1p priority has a higher
+ * priority than port's default tag's priority.
+ */
+static void sw_cfg_replace_prio(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_replace_prio(sw, port, set);
+}  /* sw_cfg_replace_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_cfg_port_based - configure switch port based priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority to set.
+ *
+ * This routine configures the port based priority of the switch.
+ */
+static void sw_cfg_port_based(struct ksz_sw *sw, uint port, u8 prio)
+{
+	SW_D data;
+
+	if (prio > PORT_BASED_PRIO_M)
+		prio = PORT_BASED_PRIO_M;
+
+	port_r(sw, port, REG_PORT_MRI_MAC_CTRL, &data);
+	data &= ~(PORT_BASED_PRIO_M << PORT_BASED_PRIO_S);
+	data |= prio << PORT_BASED_PRIO_S;
+	port_w(sw, port, REG_PORT_MRI_MAC_CTRL, data);
+
+	sw->info->port_cfg[port].port_prio = prio;
+}  /* sw_cfg_port_based */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_multi_queue - enable transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @queue:	The queue register setting.
+ *
+ * This routine enables the transmit multiple queues selection of the switch
+ * port.  The port transmit queue is split into two or four priority queues.
+ */
+static void sw_set_multi_queue(struct ksz_sw *sw, uint port, uint queue)
+{
+	port_set_prio_queue(sw, port, queue);
+
+	/* Default is port based for egress rate limit. */
+	if (queue)
+		sw_cfg(sw, REG_SW_MAC_CTRL_5, SW_OUT_RATE_LIMIT_QUEUE_BASED,
+			1);
+}  /* sw_set_multi_queue */
+
+/**
+ * sw_init_prio - initialize switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the switch QoS priority functions.
+ */
+static void sw_init_prio(struct ksz_sw *sw)
+{
+	uint port;
+	int tos;
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+
+	for (tos = 0; tos < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->p_802_1p[tos] =
+			SW_R(sw, S_802_1P_PRIO_CTRL + tos * SW_SIZE);
+
+	for (tos = 0; tos < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->diffserv[tos] =
+			SW_R(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE);
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		port_r(sw, port, REG_PORT_MRI_MAC_CTRL, &data);
+		data &= PORT_BASED_PRIO_M;
+		sw->info->port_cfg[port].port_prio = data;
+		port_cfg = &sw->info->port_cfg[port];
+		for (tos = 0; tos < PRIO_802_1P_ENTRIES / 8; tos++)
+			port_r32(sw, port, REG_PORT_MRI_TC_MAP__4 - tos * 4,
+				&port_cfg->tc_map[tos]);
+
+		for (tos = 0; tos < DIFFSERV_ENTRIES / 16; tos++)
+			port_r32(sw, port, REG_PORT_POLICE_COLOR_3__4 -
+				tos * 4, &port_cfg->color_map[tos]);
+	}
+}  /* sw_init_prio */
+
+static void port_set_color_map(struct ksz_sw *sw, uint port, u8 tos, u32 prio)
+{
+	port_w32(sw, port, REG_PORT_POLICE_COLOR_3__4 - tos * 4, prio);
+}  /* port_set_color_map */
+
+static void port_set_tc_map(struct ksz_sw *sw, uint port, u8 tos, u32 prio)
+{
+	port_w32(sw, port, REG_PORT_MRI_TC_MAP__4 + tos * 4, prio);
+}  /* port_set_color_map */
+
+/**
+ * port_cfg_color_map - configure port police color map
+ * @sw:		The switch instance.
+ * @tos:	ToS value.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the color map in the hardware.
+ */
+static void port_cfg_color_map(struct ksz_sw *sw, uint port, u8 tos, u32 prio)
+{
+	int shift;
+	u32 data = prio;
+	u32 mask = POLICE_COLOR_MAP_M;
+	struct ksz_port_cfg *port_cfg = &sw->info->port_cfg[port];
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10000)
+		mask = 0xffffffff;
+	else if (prio >= 0x100)
+		mask = 0xffff;
+	else if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > POLICE_COLOR_MAP_M)
+		mask = 0xf;
+	shift = (tos & (16 - 1)) * POLICE_COLOR_MAP_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tos /= 16;
+
+	port_cfg->color_map[tos] &= ~mask;
+	port_cfg->color_map[tos] |= prio;
+
+	port_set_color_map(sw, port, tos, port_cfg->color_map[tos]);
+}  /* port_cfg_color_map */
+
+/**
+ * port_cfg_tc_map - configure port traffic class map
+ * @sw:		The switch instance.
+ * @tc:		Traffic class.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the traffic class mapping in the hardware.
+ */
+static void port_cfg_tc_map(struct ksz_sw *sw, uint port, u8 tc, u32 prio)
+{
+	int shift;
+	u32 data = prio;
+	u32 mask = PORT_TC_MAP_M;
+	u32 regmask = PORT_TC_MAP_M;
+	struct ksz_port_cfg *port_cfg = &sw->info->port_cfg[port];
+
+	if (tc >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10000)
+		mask = 0xffffffff;
+	else if (prio >= 0x100)
+		mask = 0xffff;
+	else if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > PORT_TC_MAP_M)
+		mask = 0xf;
+	if (prio >= 0x10000)
+		regmask = 0x33333333;
+	else if (prio >= 0x100)
+		regmask = 0x3333;
+	else if (prio >= 0x10)
+		regmask = 0x33;
+	shift = (tc & (8 - 1)) * PORT_TC_MAP_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tc /= 8;
+
+	port_cfg->tc_map[tc] &= ~mask;
+	port_cfg->tc_map[tc] |= prio;
+
+	port_set_tc_map(sw, port, tc, port_cfg->tc_map[tc]);
+}  /* port_cfg_tc_map */
+
+/**
+ * sw_setup_prio - setup switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine setup the switch QoS priority functions.
+ */
+static void sw_setup_prio(struct ksz_sw *sw)
+{
+	uint port;
+	uint q;
+	struct ksz_port_cfg *cfg;
+
+	/* All QoS functions disabled. */
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		cfg = &sw->info->port_cfg[port];
+
+		sw_set_multi_queue(sw, port, 2);
+		sw_dis_diffserv(sw, port);
+		sw_cfg_replace_prio(sw, port, 0);
+		sw_cfg_replace_null_vid(sw, port, 0);
+		sw_cfg_port_based(sw, port, cfg->port_prio);
+
+		sw_ena_802_1p(sw, port);
+		for (q = 0; q < PRIO_QUEUES; q++)
+			port_set_tx_ratio(sw, port, q, 1 << q);
+		if (sw->features & AVB_SUPPORT) {
+			cfg->tc_map[0] = 0x11113200;
+			port_set_tc_map(sw, port, 0, cfg->tc_map[0]);
+		}
+	}
+}  /* sw_setup_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_cfg_def_vid - configure port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void port_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	port_w16(sw, port, REG_PORT_DEFAULT_VID, vid);
+}  /* port_cfg_def_vid */
+
+/**
+ * port_get_def_vid - get port default VID.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	Buffer to store the VID.
+ *
+ * This routine retrieves the default VID of the port.
+ */
+static void port_get_def_vid(struct ksz_sw *sw, uint port, u16 *vid)
+{
+	port_r16(sw, port, REG_PORT_DEFAULT_VID, vid);
+}  /* port_get_def_vid */
+
+/**
+ * sw_cfg_def_vid - configure switch port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void sw_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	sw->info->port_cfg[port].vid = vid;
+	port_cfg_def_vid(sw, port, vid);
+}  /* sw_cfg_def_vid */
+
+/**
+ * sw_cfg_port_base_vlan - configure port-based VLAN membership
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @member:	The port-based VLAN membership.
+ *
+ * This routine configures the port-based VLAN membership of the port.
+ */
+static void sw_cfg_port_base_vlan(struct ksz_sw *sw, uint port, u16 member)
+{
+/*
+ * THa  2014/10/08
+ * In KSZ9566 the bit for the last host port is 0x40 instead of 0x20.
+ */
+	if (6 == sw->port_cnt && (member & 0x20))
+		member |= 0x40;
+	port_w32(sw, port, REG_PORT_VLAN_MEMBERSHIP__4, member);
+
+	sw->info->port_cfg[port].member = member;
+}  /* sw_cfg_port_base_vlan */
+
+static void sw_cfg_default_vlan(struct ksz_sw *sw, int reset)
+{
+	struct ksz_vlan_table vlan;
+
+	if (sw->overrides & VLAN_SET)
+		return;
+	sw->ops->release(sw);
+	vlan.vid = 1;
+	sw_r_vlan_table(sw, vlan.vid, &vlan);
+	vlan.ports = sw->PORT_MASK;
+	if (reset)
+		vlan.untag = 0;
+	else
+		vlan.untag = vlan.ports;
+	vlan.valid = vlan.ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+	if (!reset) {
+		vlan.vid = 0;
+		vlan.ports = sw->PORT_MASK;
+		vlan.untag = 0;
+		vlan.valid = 1;
+		sw_w_vlan_table(sw, vlan.vid, &vlan);
+	}
+	sw->ops->acquire(sw);
+}  /* sw_cfg_default_vlan */
+
+/**
+ * sw_dis_vlan - disable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine disables the VLAN function of the switch.
+ */
+static void sw_dis_vlan(struct ksz_sw *sw)
+{
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE, 0);
+	sw_cfg_default_vlan(sw, true);
+}  /* sw_dis_vlan */
+
+/**
+ * sw_ena_vlan - enable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine enables the VLAN function of the switch.
+ */
+static void sw_ena_vlan(struct ksz_sw *sw)
+{
+	int p;
+	u32 val;
+
+	/* Hardware may remove priority tag with VID 0. */
+	for (p = 0; p < sw->mib_port_cnt; p++)
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+			true);
+	sw_cfg_default_vlan(sw, false);
+
+	/* Enable 802.1q VLAN mode. */
+	val = sw->reg->r32(sw, REG_SW_QM_CTRL__4);
+	val |= UNICAST_VLAN_BOUNDARY;
+	sw->reg->w32(sw, REG_SW_QM_CTRL__4, val);
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE, 1);
+}  /* sw_ena_vlan */
+
+/**
+ * sw_init_vlan - initialize switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the VLAN function of the switch.
+ */
+static void sw_init_vlan(struct ksz_sw *sw)
+{
+	uint port;
+	u32 data;
+	struct ksz_sw_info *info = sw->info;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		port_get_def_vid(sw, port, &info->port_cfg[port].vid);
+		port_r32(sw, port, REG_PORT_VLAN_MEMBERSHIP__4, &data);
+		info->port_cfg[port].member = (u16) data;
+		info->port_cfg[port].vid_member = (u16) data;
+	}
+}  /* sw_init_vlan */
+
+static void inc_mac_addr(u8 *dst, u8 *src, u8 inc)
+{
+	memcpy(dst, src, ETH_ALEN);
+	dst[5] += inc;
+	if (dst[5] < src[5])
+		dst[4]++;
+	if (dst[4] < src[4])
+		dst[3]++;
+}  /* inc_mac_addr */
+
+/**
+ * sw_get_addr - get the switch MAC address.
+ * @sw:		The switch instance.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the switch.
+ */
+static inline void sw_get_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	sw->reg->r(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_get_addr */
+
+/**
+ * sw_set_addr - configure switch MAC address
+ * @sw:		The switch instance.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the switch.
+ */
+static void sw_set_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	int i;
+	int n;
+	int p;
+	struct ksz_port_info *info;
+
+	for (i = 0, p = 0; i < sw->mib_port_cnt; i++) {
+		info = &sw->port_info[i];
+		if (i == sw->HOST_PORT) {
+			n = 0;
+		} else {
+			++p;
+			n = p;
+		}
+#if 1
+		inc_mac_addr(info->mac_addr, mac_addr, n);
+#else
+		inc_mac_addr(info->mac_addr, mac_addr, 0);
+#endif
+	}
+	sw->reg->w(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+#ifdef CONFIG_KSZ_IBA
+	prepare_iba(&sw->info->iba, mac_addr, sw->info->iba.src);
+#endif
+}  /* sw_set_addr */
+
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+/**
+ * sw_init_phy_addr - initialize switch PHY address
+ * @sw:		The switch instance.
+ *
+ * This function initializes the PHY address of the switch.
+ */
+static void sw_init_phy_addr(struct ksz_sw *sw)
+{
+	u8 addr;
+
+	addr = sw->reg->r8(sw, REG_SWITCH_CTRL_13);
+	addr >>= SWITCH_PORT_PHY_ADDR_SHIFT;
+	addr &= SWITCH_PORT_PHY_ADDR_MASK;
+	sw->info->phy_addr = addr;
+}
+
+/**
+ * sw_set_phy_addr - configure switch PHY address
+ * @sw:		The switch instance.
+ * @addr:	The PHY address.
+ *
+ * This function configures the PHY address of the switch.
+ */
+static void sw_set_phy_addr(struct ksz_sw *sw, u8 addr)
+{
+	sw->info->phy_addr = addr;
+	addr &= SWITCH_PORT_PHY_ADDR_MASK;
+	addr <<= SWITCH_PORT_PHY_ADDR_SHIFT;
+	sw->reg->w8(sw, REG_SWITCH_CTRL_13, addr);
+}
+#endif
+
+static void sw_setup_reserved_multicast(struct ksz_sw *sw)
+{
+	struct ksz_mac_table table[8];
+	u16 addr[8];
+	u32 ctrl[8];
+	int i;
+
+	memset(table, 0, sizeof(struct ksz_mac_table) * 8);
+	for (i = 0; i < 8; i++)
+		addr[i] = get_mcast_reserved_addr(i);
+
+	sw_r_m_sta_mac_table(sw, addr, 1, 8, table);
+	if (table[6].ports != sw->HOST_MASK || (sw->features & MRP_SUPPORT)) {
+		for (i = 0; i < 8; i++)
+			ctrl[i] = get_mac_table_ctrl(addr[i], true);
+		table[0].override = true;
+		table[0].ports = sw->HOST_MASK;
+		table[1].ports = 0;
+		table[2].override = true;
+		table[2].ports = sw->HOST_MASK;
+		table[3].ports = sw->PORT_MASK;
+		if (sw->features & MRP_SUPPORT) {
+#ifdef MRP_PASSTHRU
+			table[4].ports = sw->PORT_MASK;
+			table[5].ports = sw->PORT_MASK;
+#else
+			table[4].override = true;
+			table[4].ports = sw->HOST_MASK;
+			table[5].override = true;
+			table[5].ports = sw->HOST_MASK;
+#endif
+		} else {
+			table[4].ports = sw->PORT_MASK;
+			table[5].ports = sw->PORT_MASK;
+		}
+#ifdef MRP_PASSTHRU
+		table[6].ports = sw->PORT_MASK;
+#else
+		table[6].override = true;
+		table[6].ports = sw->HOST_MASK;
+#endif
+		table[7].ports = sw->PORT_MASK & ~sw->HOST_MASK;
+		sw_w_m_sta_mac_table(sw, addr, true, 8, table);
+	}
+}  /* sw_setup_reserved_multicast */
+
+static int sw_get_gbit(struct ksz_sw *sw, u8 data)
+{
+	int gbit;
+
+	if (sw->features & NEW_XMII)
+		gbit = !(data & PORT_MII_NOT_1GBIT);
+	else
+		gbit = data & PORT_MII_1000MBIT_S1;
+	return gbit;
+}  /* sw_get_gbit */
+
+static void sw_set_gbit(struct ksz_sw *sw, int gbit, u8 *data)
+{
+	if (sw->features & NEW_XMII) {
+		if (gbit)
+			*data &= ~PORT_MII_NOT_1GBIT;
+		else
+			*data |= PORT_MII_NOT_1GBIT;
+	} else {
+		if (gbit)
+			*data |= PORT_MII_1000MBIT_S1;
+		else
+			*data &= ~PORT_MII_1000MBIT_S1;
+	}
+}  /* sw_set_gbit */
+
+static int sw_get_xmii(struct ksz_sw *sw, u8 data)
+{
+	int mode;
+
+	if (sw->features & NEW_XMII) {
+		switch (data & PORT_MII_SEL_M) {
+		case PORT_MII_SEL:
+			mode = 0;
+			break;
+		case PORT_RMII_SEL:
+			mode = 1;
+			break;
+		case PORT_GMII_SEL:
+			mode = 2;
+			break;
+		default:
+			mode = 3;
+		}
+	} else {
+		switch (data & PORT_MII_SEL_M) {
+		case PORT_MII_SEL_S1:
+			mode = 0;
+			break;
+		case PORT_RMII_SEL_S1:
+			mode = 1;
+			break;
+		case PORT_GMII_SEL_S1:
+			mode = 2;
+			break;
+		default:
+			mode = 3;
+		}
+	}
+	return mode;
+}  /* sw_get_xmii */
+
+static void sw_set_xmii(struct ksz_sw *sw, int mode, u8 *data)
+{
+	u8 xmii;
+
+	if (sw->features & NEW_XMII) {
+		switch (mode) {
+		case 0:
+			xmii = PORT_MII_SEL;
+			break;
+		case 1:
+			xmii = PORT_RMII_SEL;
+			break;
+		case 2:
+			xmii = PORT_GMII_SEL;
+			break;
+		default:
+			xmii = PORT_RGMII_SEL;
+		}
+	} else {
+		switch (mode) {
+		case 0:
+			xmii = PORT_MII_SEL_S1;
+			break;
+		case 1:
+			xmii = PORT_RMII_SEL_S1;
+			break;
+		case 2:
+			xmii = PORT_GMII_SEL_S1;
+			break;
+		default:
+			xmii = PORT_RGMII_SEL_S1;
+		}
+	}
+	*data &= ~PORT_MII_SEL_M;
+	*data |= xmii;
+}  /* sw_set_xmii */
+
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.c"
+#endif
+
+#define STP_ENTRY			(MULTI_MAC_TABLE_ENTRIES - 2)
+#define DEV_0_ADDR_ENTRY		0
+#define DEV_1_ADDR_ENTRY		1
+#define BRIDGE_ADDR_ENTRY		2
+
+/**
+ * sw_set_global_ctrl - set switch global control
+ * @sw:		The switch instance.
+ *
+ * This routine sets the global control of the switch function.
+ */
+static void sw_set_global_ctrl(struct ksz_sw *sw)
+{
+	SW_D data;
+	int setup_xmii = sw->HOST_PORT >= sw->phy_port_cnt;
+
+#ifdef CONFIG_KSZ_IBA
+	if (2 <= sw->info->iba.use_iba)
+		setup_xmii = 0;
+#endif
+	if (setup_xmii) {
+		struct phy_device *phydev = sw->phydev;
+		struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+		int gbit;
+		int mode;
+
+		/* Allow slower speed to be used for testing purpose. */
+#ifdef USE_10_MBIT_MODE
+		phydev->speed = SPEED_10;
+		phydev->dev_flags |= 1;
+#endif
+#ifdef USE_HALF_DUPLEX
+		phydev->duplex = DUPLEX_HALF;
+		phydev->dev_flags |= 1;
+#endif
+#if defined(USE_MII_PHY) || defined(USE_RGMII_PHY)
+		phydev->dev_flags |= 2;
+#endif
+		if (sw->net_ops->get_priv_port) {
+			struct ksz_port *sw_port =
+				sw->net_ops->get_priv_port(sw->netdev[0]);
+
+			if (sw_port->flow_ctrl == PHY_NO_FLOW_CTRL) {
+				if (sw->features & IS_9893) {
+					port_cfg_force_flow_ctrl(sw,
+						sw->HOST_PORT, 0);
+				} else {
+					port_r(sw, sw->HOST_PORT,
+						REG_PORT_XMII_CTRL_0, &data);
+					data &= ~(PORT_MII_TX_FLOW_CTRL |
+						  PORT_MII_RX_FLOW_CTRL);
+					port_w(sw, sw->HOST_PORT,
+						REG_PORT_XMII_CTRL_0, data);
+				}
+			}
+		}
+		if ((sw->features & NO_GLOBAL_RESET) ||
+		    (phydev->dev_flags & 1)) {
+			port_r(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_0, &data);
+			data |= PORT_MII_100MBIT;
+			data |= PORT_MII_FULL_DUPLEX;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_10 == phydev->speed)
+					data &= ~PORT_MII_100MBIT;
+				if (DUPLEX_HALF == phydev->duplex)
+					data &= ~PORT_MII_FULL_DUPLEX;
+			}
+			if ((data & PORT_MII_100MBIT) &&
+			    phydev->speed < SPEED_100)
+				phydev->speed = SPEED_100;
+			if ((data & PORT_MII_FULL_DUPLEX) &&
+			    phydev->duplex < DUPLEX_FULL)
+				phydev->duplex = DUPLEX_FULL;
+			port_w(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_0, data);
+		}
+
+		port_r(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_1, &data);
+		data &= ~PORT_MII_MAC_MODE;
+		if (phydev->dev_flags & 2)
+			data |= PORT_MII_MAC_MODE;
+		switch (phydev->interface) {
+		case PHY_INTERFACE_MODE_MII:
+			sw_set_gbit(sw, false, &data);
+			mode = 0;
+			if (phydev->speed > SPEED_100)
+				phydev->speed = SPEED_100;
+			break;
+		case PHY_INTERFACE_MODE_RMII:
+			sw_set_gbit(sw, false, &data);
+			mode = 1;
+			if (phydev->speed > SPEED_100)
+				phydev->speed = SPEED_100;
+			break;
+		case PHY_INTERFACE_MODE_GMII:
+			sw_set_gbit(sw, true, &data);
+			mode = 2;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_1000 != phydev->speed)
+					sw_set_gbit(sw, false, &data);
+			}
+			gbit = sw_get_gbit(sw, data);
+			if (gbit && phydev->speed < SPEED_1000)
+				phydev->speed = SPEED_1000;
+			break;
+		case PHY_INTERFACE_MODE_SGMII:
+			mode = 3;
+			break;
+		default:
+			data &= ~PORT_RGMII_ID_IG_ENABLE;
+			data &= ~PORT_RGMII_ID_EG_ENABLE;
+			if (PHY_INTERFACE_MODE_RGMII_ID == phydev->interface ||
+			    PHY_INTERFACE_MODE_RGMII_RXID == phydev->interface)
+				data |= PORT_RGMII_ID_IG_ENABLE;
+			if (PHY_INTERFACE_MODE_RGMII_ID == phydev->interface ||
+			    PHY_INTERFACE_MODE_RGMII_TXID == phydev->interface)
+				data |= PORT_RGMII_ID_EG_ENABLE;
+			sw_set_gbit(sw, true, &data);
+			mode = 3;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_1000 != phydev->speed)
+					sw_set_gbit(sw, false, &data);
+			}
+			gbit = sw_get_gbit(sw, data);
+			if (gbit && phydev->speed < SPEED_1000)
+				phydev->speed = SPEED_1000;
+			break;
+		}
+		info->tx_rate = phydev->speed * TX_RATE_UNIT;
+		info->duplex = phydev->duplex + 1;
+#ifdef USE_RGMII_PHY
+		data |= PORT_RGMII_ID_EG_ENABLE;
+		mode = 3;
+#endif
+		sw_set_xmii(sw, mode, &data);
+		port_w(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_1, data);
+		sw->cached.xmii[sw->HOST_PORT - sw->phy_port_cnt] &= ~0xff;
+		sw->cached.xmii[sw->HOST_PORT - sw->phy_port_cnt] |= data;
+	}
+
+	data = SW_R(sw, REG_SW_MAC_CTRL_0);
+
+	/* Enable aggressive back off algorithm in half duplex mode. */
+	data |= SW_AGGR_BACKOFF;
+
+/*
+ * THa  2016/10/17
+ * If no excessive collision drop is enabled the default backoff algorithm
+ * may cause both linked device to stop passing traffic completely.
+ */
+	data |= SW_NEW_BACKOFF;
+
+/*
+ * THa  2016/10/17
+ * Recommended to turn on this mode to pass UNH tests.
+ */
+	data |= SW_PAUSE_UNH_MODE;
+	SW_W(sw, REG_SW_MAC_CTRL_0, data);
+
+	data = SW_R(sw, REG_SW_MAC_CTRL_1);
+
+	/* Enable no excessive collision drop. */
+	data |= NO_EXC_COLLISION_DROP;
+	SW_W(sw, REG_SW_MAC_CTRL_1, data);
+
+	data = SW_R(sw, S_LINK_AGING_CTRL);
+
+	data |= SW_AGING_ENABLE;
+	if (sw->overrides & FAST_AGING)
+		data |= SW_FAST_AGING;
+	else
+		data &= ~SW_FAST_AGING;
+
+	/* Enable automatic fast aging when link changed detected. */
+	data |= SW_LINK_AUTO_AGING;
+
+#if 1
+/*
+ * THa  2014/10/08
+ * The host port also gets filtered if lookup is used!
+ */
+	if (sw->features & NEW_CAP)
+		data |= SW_SRC_ADDR_FILTER;
+#endif
+	SW_W(sw, S_LINK_AGING_CTRL, data);
+
+#if 0
+	data = SW_R(sw, REG_SW_QM_CTRL);
+
+	/* Make sure unicast VLAN boundary is set as default. */
+	if (sw->dev_count > 1)
+		data |= UNICAST_VLAN_BOUNDARY;
+	SW_W(sw, REG_SW_QM_CTRL, data);
+#endif
+
+#ifdef IMX6_KSZ9567
+	/* SPI access becomes more stable. */
+	data = SW_R(sw, REG_SW_GLOBAL_OUTPUT_CTRL__1);
+	data |= SW_REFCLKO_IS_125MHZ;
+	SW_W(sw, REG_SW_GLOBAL_OUTPUT_CTRL__1, data);
+
+	data = SW_R(sw, REG_SW_IO_STRENGTH__1);
+	data &= ~(SW_DRIVE_STRENGTH_M << SW_HI_SPEED_DRIVE_STRENGTH_S);
+	data |= SW_DRIVE_STRENGTH_16MA << SW_HI_SPEED_DRIVE_STRENGTH_S;
+	SW_W(sw, REG_SW_IO_STRENGTH__1, data);
+#endif
+}  /* sw_set_global_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_set_stp_state - configure port spanning tree state
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @state:	The spanning tree state.
+ *
+ * This routine configures the spanning tree state of the port.
+ */
+static void port_set_stp_state(struct ksz_sw *sw, uint port, int state)
+{
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+	int member = -1;
+
+#if 0
+dbg_msg("%s %d %d\n", __func__, port, state);
+#endif
+	port_cfg = &sw->info->port_cfg[port];
+	port_r(sw, port, P_STP_CTRL, &data);
+	switch (state) {
+	case STP_STATE_DISABLED:
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT)
+			member = 0;
+		break;
+	case STP_STATE_LISTENING:
+/*
+ * No need to turn on transmit because of port direct mode.
+ * Turning on receive is required if static MAC table is not setup.
+ */
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT &&
+		    STP_STATE_DISABLED == port_cfg->stp_state[port_cfg->mstp])
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_LEARNING:
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_FORWARDING:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data &= ~PORT_LEARN_DISABLE;
+
+		/*
+		 * Actual port membership setting is done in another RSTP
+		 * processing routine.
+		 */
+		if (sw->features & STP_SUPPORT)
+			break;
+		if (((sw->features & (SW_VLAN_DEV | USE_FEWER_PORTS)) ||
+		    sw->dev_offset) && port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_BLOCKED:
+/*
+ * Need to setup static MAC table with override to keep receiving BPDU
+ * messages.  See sw_setup_stp routine.
+ */
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT &&
+		    STP_STATE_DISABLED == port_cfg->stp_state[port_cfg->mstp])
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_SIMPLE:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	}
+	port_w(sw, port, P_STP_CTRL, data);
+	port_cfg->stp_state[port_cfg->mstp] = state;
+	if (data & PORT_RX_ENABLE)
+		sw->rx_ports[port_cfg->mstp] |= (1 << port);
+	else
+		sw->rx_ports[port_cfg->mstp] &= ~(1 << port);
+	if (data & PORT_TX_ENABLE)
+		sw->tx_ports[port_cfg->mstp] |= (1 << port);
+	else
+		sw->tx_ports[port_cfg->mstp] &= ~(1 << port);
+
+	/* Port membership may share register with STP state. */
+	if (member >= 0)
+		sw_cfg_port_base_vlan(sw, port, (u8) member);
+}  /* port_set_stp_state */
+
+static void port_open(struct ksz_sw *sw, uint p, bool open)
+{
+	int acl_on;
+	uint q;
+	u8 acl_rule;
+	u8 map_mode;
+	u8 member;
+	u16 ruleset;
+	struct ksz_acl_table *acl;
+
+dbg_msg("%s %d %d\n", __func__, p, open);
+	acl_rule = 1;
+	if (open) {
+		sw->on_ports |= (1 << p);
+		map_mode = 0;
+		ruleset = 0;
+	} else {
+		sw->on_ports &= ~(1 << p);
+		map_mode = ACL_MAP_MODE_REPLACE;
+		ruleset = (1 << acl_rule);
+	}
+	acl_on = port_chk_acl(sw, p);
+	if (!acl_on)
+		port_cfg_acl(sw, p, true);
+	sw->ops->release(sw);
+	acl = &sw->info->port_cfg[p].acl_info[acl_rule];
+	acl->map_mode = map_mode;
+	acl->ruleset = ruleset;
+	sw_w_acl_table(sw, p, acl_rule, acl);
+	++acl_rule;
+	acl = &sw->info->port_cfg[p].acl_info[acl_rule];
+	if (ruleset)
+		ruleset = (1 << acl_rule);
+	acl->ruleset = ruleset;
+	sw_w_acl_table(sw, p, acl_rule, acl);
+	sw->ops->acquire(sw);
+	if (!acl_on)
+		port_cfg_acl(sw, p, false);
+	for (q = 0; q < sw->mib_port_cnt; q++) {
+		if (q == sw->HOST_PORT)
+			continue;
+		if (sw->on_ports & (1 << q))
+			member = (u8)(sw->HOST_MASK | sw->on_ports);
+		else
+			member = sw->HOST_MASK;
+		if (sw->info->port_cfg[q].member != member)
+			sw_cfg_port_base_vlan(sw, q, member);
+	}
+}  /* port_open */
+
+static void sw_setup_acl(struct ksz_sw *sw)
+{
+	struct ksz_acl_table *acl;
+	int acl_on;
+	uint port;
+	u8 first_rule;
+	u8 acl_rule;
+
+	if (!(sw->overrides & USE_802_1X_AUTH))
+		return;
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (port == sw->HOST_PORT)
+			continue;
+		port_set_authen_mode(sw, port, PORT_AUTHEN_BLOCK);
+		sw_cfg_port_base_vlan(sw, port, sw->HOST_MASK);
+
+		acl_on = port_chk_acl(sw, port);
+		if (!acl_on)
+			port_cfg_acl(sw, port, true);
+
+		first_rule = 0;
+		acl_rule = 0;
+		acl = &sw->info->port_cfg[port].acl_info[acl_rule];
+		acl->mac[0] = 0x01;
+		acl->mac[1] = 0x80;
+		acl->mac[2] = 0xC2;
+		acl->mac[3] = 0x00;
+		acl->mac[4] = 0x00;
+		acl->mac[5] = 0x03;
+		acl->eth_type = ETH_P_PAE;
+		acl->src = 0;
+		acl->equal = 1;
+		acl->enable = ACL_ENABLE_2_BOTH;
+		acl->mode = ACL_MODE_LAYER_2;
+		acl->ruleset = (1 << acl_rule);
+		acl->first_rule = first_rule;
+		acl->map_mode = 0;
+		acl->ports = 0;
+		sw->ops->release(sw);
+		sw_w_acl_table(sw, port, acl_rule, acl);
+		sw->ops->acquire(sw);
+
+		first_rule = 1;
+		acl_rule++;
+		acl++;
+		acl->src = 0;
+		acl->min_port = 1645;
+		acl->max_port = 1812;
+		acl->port_mode = ACL_PORT_MODE_EITHER;
+		acl->enable = ACL_ENABLE_4_UDP_PORT_COMP;
+		acl->mode = ACL_MODE_LAYER_4;
+		acl->ruleset = (1 << acl_rule);
+		acl->first_rule = first_rule;
+		acl->map_mode = ACL_MAP_MODE_REPLACE;
+		acl->ports = sw->HOST_MASK;
+		sw->ops->release(sw);
+		sw_w_acl_table(sw, port, acl_rule, acl);
+		sw->ops->acquire(sw);
+		acl_rule++;
+		acl++;
+		acl->eth_type = ETH_P_ARP;
+		acl->equal = 1;
+		acl->enable = ACL_ENABLE_2_TYPE;
+		acl->mode = ACL_MODE_LAYER_2;
+		acl->ruleset = (1 << acl_rule);
+		acl->first_rule = first_rule;
+		sw->ops->release(sw);
+		sw_w_acl_table(sw, port, acl_rule, acl);
+		sw->ops->acquire(sw);
+		if (!acl_on)
+			port_cfg_acl(sw, port, false);
+	}
+}  /* sw_setup_acl */
+
+/**
+ * sw_clr_sta_mac_table - clear static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine clears the static MAC table.
+ */
+static void sw_clr_sta_mac_table(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	data = SW_R(sw, REG_SW_LUE_CTRL_2);
+	data &= ~(SW_FLUSH_OPTION_M << SW_FLUSH_OPTION_S);
+	data |= (SW_FLUSH_OPTION_STA_MAC << SW_FLUSH_OPTION_S);
+	SW_W(sw, REG_SW_LUE_CTRL_2, data);
+	sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_STP_TABLE, 1);
+}  /* sw_clr_sta_mac_table */
+
+#ifdef CONFIG_KSZ_STP
+/**
+ * sw_setup_stp - setup switch spanning tree support
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the spanning tree support of the switch.
+ */
+static void sw_setup_stp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct ksz_sw_info *info = sw->info;
+
+	entry = &info->mac_table[STP_ENTRY];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x80;
+	entry->addr[2] = 0xC2;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x00;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 1;
+	entry->valid = 1;
+	alu = &info->alu_table[STP_ENTRY];
+	alu->forward = FWD_STP_DEV | FWD_HOST | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+	sw->ops->release(sw);
+	sw_w_sta_mac_table(sw, alu->index, alu->type, entry);
+	sw->ops->acquire(sw);
+}  /* sw_setup_stp */
+#endif
+
+static void sw_set_mcast_table(struct ksz_sw *sw, int i, const u8 *addr)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+
+	entry = &sw->info->mac_table[i];
+	alu = &sw->info->alu_table[i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->fid = 0;
+	entry->ports = sw->PORT_MASK;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[i];
+	alu->forward = FWD_MAIN_DEV | FWD_MCAST | FWD_KNOWN;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+	sw_w_dyn_mac_table(sw, 0, entry->addr, entry->fid, entry);
+}  /* sw_set_mcast_table */
+
+#ifdef CONFIG_1588_PTP
+static void sw_setup_ptp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	u8 forward;
+	struct ksz_sw_info *info = sw->info;
+
+	i = info->multi_sys;
+	forward = FWD_MAIN_DEV;
+	forward |= FWD_VLAN_DEV;
+
+#if 0
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x01;
+	entry->addr[5] = 0x81;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x6B;
+	entry->ports = sw->PORT_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x01;
+	entry->addr[5] = 0x81;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x6B;
+	entry->ports = sw->PORT_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x1B;
+	entry->addr[2] = 0x19;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x00;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+#endif
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x80;
+	entry->addr[2] = 0xC2;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x0E;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0xE;
+	alu->type = 1;
+
+	info->multi_sys = i;
+}
+#endif
+
+static const u8 ipv6_neigh_mcast[] = {
+	0x33, 0x33, 0xFF,
+};
+
+static const u8 known_mcast_addr[][ETH_ALEN] = {
+	/* IGMP V2 */
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0x01 },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x01 },
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0x02 },
+
+	/* ICMPv6 */
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x02 },
+
+	/* IGMP V3 */
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0x16 },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x16 },
+
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0xFB },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0xFB },
+
+	/* Link-Local Multicast Name Resolution */
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0xFC },
+	{ 0x33, 0x33, 0x00, 0x01, 0x00, 0x03 },
+
+	/* Simple Service Discovery Protocol */
+	{ 0x01, 0x00, 0x5E, 0x7F, 0xFF, 0xFA },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x0C },
+
+	{ 0x33, 0x33, 0x00, 0x01, 0x00, 0x02 },
+};
+
+static void sw_setup_multi(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int n;
+	u8 forward;
+	u8 addr[ETH_ALEN];
+	struct ksz_sw_info *info = sw->info;
+
+	sw->ops->release(sw);
+	i = info->multi_sys;
+	forward = FWD_HOST;
+
+	addr[0] = 0x01;
+	addr[1] = 0x80;
+	addr[2] = 0xC2;
+	addr[3] = 0x00;
+	addr[4] = 0x00;
+	addr[5] = 0x00;
+
+	entry = &info->mac_table[--i];
+	memset(entry->addr, 0xFF, ETH_ALEN);
+	entry->ports = sw->PORT_MASK;
+	entry->fid = 0;
+	alu = &info->alu_table[i];
+	alu->forward = FWD_MAIN_DEV | FWD_MCAST | FWD_KNOWN;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+#if 0
+	if (sw->features & DLR_HW) {
+		entry->ports = sw->HOST_MASK;
+		alu->forward |= FWD_HOST;
+	}
+#endif
+
+	i = STP_ENTRY;
+
+#if 1
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x01;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+#endif
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x02;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x03;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x04;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x05;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x06;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x07;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x08;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x09;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0A;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0B;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0C;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0D;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0E;
+	entry->ports = sw->HOST_MASK;
+#ifdef CONFIG_1588_PTP
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+#else
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+#endif
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0F;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x20;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x21;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	forward = FWD_MAIN_DEV;
+	forward |= FWD_MCAST | FWD_KNOWN;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x10;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 1;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	for (n = 0; n < 13; n++) {
+		--i;
+		sw_set_mcast_table(sw, i, known_mcast_addr[n]);
+	}
+
+	sw->ops->acquire(sw);
+	info->multi_sys = i;
+}  /* sw_setup_multi */
+
+#ifdef CONFIG_KSZ_STP
+static void bridge_change(struct ksz_sw *sw)
+{
+	int i;
+	uint port;
+	u8 m;
+	u8 member;
+	struct ksz_sw_info *info = sw->info;
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (skip_host_port(sw, port))
+			continue;
+		member = 0;
+		for (i = 0; i < NUM_OF_MSTI; i++) {
+			if (STP_STATE_FORWARDING ==
+			    info->port_cfg[port].stp_state[i])
+				m = sw->HOST_MASK | info->member[i];
+			else if (STP_STATE_DISABLED ==
+				 info->port_cfg[port].stp_state[i])
+				m = 0;
+			else
+				m = sw->HOST_MASK | (1 << port);
+			member |= m;
+		}
+		if (member != info->port_cfg[port].member)
+			sw_cfg_port_base_vlan(sw, port, member);
+	}
+}  /* bridge_change */
+#endif
+
+static int sw_match_multi(struct ksz_sw *sw, struct ksz_port *priv, u8 *addr)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int owner;
+	uint port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	if (priv->port_cnt != sw->port_cnt)
+		port = priv->first_port + 1;
+	owner = 1 << port;
+
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner) &&
+		    !memcmp(addr, entry->addr, ETH_ALEN))
+			return false;
+	}
+	for (i = info->multi_sys; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && !memcmp(addr, entry->addr, ETH_ALEN))
+			return false;
+	}
+	return true;
+}  /* sw_match_multi */
+
+static void sw_set_multi(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct netdev_hw_addr *ha;
+	int i;
+	int found;
+	int owner;
+	uint port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	if (priv->port_cnt != sw->port_cnt)
+		port = priv->first_port + 1;
+	owner = 1 << port;
+
+	/* Remove old multicast entries. */
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			/* Remove device ownership. */
+			alu->owner &= ~owner;
+			if (!port)
+				alu->forward &= ~FWD_MAIN_DEV;
+			else if (alu->owner <= 1)
+				alu->forward &= ~FWD_STP_DEV;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->ports = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+	netdev_for_each_mc_addr(ha, dev) {
+		if (!(*ha->addr & 1))
+			continue;
+		if (info->multi_net == info->multi_sys)
+			break;
+		found = 0;
+		for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+			entry = &info->mac_table[i];
+			alu = &info->alu_table[i];
+			if (alu->valid &&
+			    !memcmp(entry->addr, ha->addr, ETH_ALEN)) {
+				found = i + 1;
+				break;
+			}
+			if (!alu->valid && !found &&
+			    i >= SWITCH_MAC_TABLE_ENTRIES &&
+			    i < info->multi_net)
+				found = i + 1;
+		}
+		if (!found) {
+			info->multi_net++;
+			found = info->multi_net;
+		}
+		found--;
+		if (found >= SWITCH_MAC_TABLE_ENTRIES &&
+		    found < info->multi_net) {
+			entry = &info->mac_table[found];
+			alu = &info->alu_table[found];
+			if (port)
+				alu->forward |= FWD_STP_DEV;
+			else
+				alu->forward |= FWD_MAIN_DEV;
+			alu->owner |= owner;
+			alu->valid = 1;
+			memcpy(entry->addr, ha->addr, ETH_ALEN);
+			entry->ports = sw->PORT_MASK;
+			entry->valid = 1;
+		}
+	}
+}  /* sw_set_multi */
+
+static void sw_reset_multi(struct ksz_sw *sw, struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int owner;
+	uint port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	if (priv->port_cnt != sw->port_cnt)
+		port = priv->first_port + 1;
+	owner = 1 << port;
+
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			alu->owner &= ~owner;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+}  /* sw_reset_multi */
+
+static void sw_reset_setup(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw_info *info = sw->info;
+
+	for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid) {
+			alu->owner = 0;
+			alu->valid = 0;
+			entry->valid = 0;
+		}
+	}
+	for (i = 0; i < sw->mib_port_cnt; i++)
+		sw->info->port_cfg[i].mstp = 0;
+}  /* sw_reset_setup */
+
+#define MAX_SW_LEN			1500
+
+static void sw_setup_msg(struct sw_dev_info *info, void *data, int len,
+	void (*func)(void *data, void *param), void *param)
+{
+	struct ksz_sw *sw = info->sw;
+	int in_intr = in_interrupt();
+
+	if (len > MAX_SW_LEN)
+		len = MAX_SW_LEN;
+	if (!in_intr)
+		mutex_lock(&info->lock);
+	memcpy(sw->msg_buf, data, len);
+	if (func)
+		func(sw->msg_buf, param);
+	len += 2;
+	if (info->read_len + len <= info->read_max) {
+		u16 *msg_len = (u16 *) &info->read_buf[info->read_len];
+
+		*msg_len = len;
+		msg_len++;
+		memcpy(msg_len, sw->msg_buf, len - 2);
+		info->read_len += len;
+	}
+	if (!in_intr)
+		mutex_unlock(&info->lock);
+	wake_up_interruptible(&info->wait_msg);
+}  /* sw_setup_msg */
+
+#ifdef CONFIG_KSZ_MSTP
+#include "ksz_mstp.c"
+#elif defined(CONFIG_KSZ_STP)
+#include "ksz_stp.c"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.c"
+#endif
+#ifdef CONFIG_KSZ_HSR
+#include "ksz_hsr.c"
+#endif
+
+/*
+ * Link detection routines
+ */
+
+static inline void dbp_link(struct ksz_port *port, struct ksz_sw *sw,
+	int change)
+{
+	struct ksz_port_info *info;
+	int i;
+	int p;
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+		info = &sw->port_info[p];
+
+		/* Port does not have PHY. */
+		if (!info->phy)
+			continue;
+
+		if (media_connected == info->state) {
+			if (change & (1 << i)) {
+				printk(KERN_INFO "link %d-%d: %d, %d\n",
+					sw->id, i + port->first_port,
+					info->tx_rate / TX_RATE_UNIT,
+					info->duplex);
+			}
+		} else {
+			if (change & (1 << i))
+				printk(KERN_INFO "link %d-%d disconnected\n",
+					sw->id, i + port->first_port);
+		}
+	}
+}
+
+static u16 port_advertised_flow_ctrl(struct ksz_port *port, u16 ctrl)
+{
+	ctrl &= ~PORT_AUTO_NEG_PAUSE;
+	switch (port->flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		ctrl |= PORT_AUTO_NEG_SYM_PAUSE;
+		break;
+	case PHY_TX_ONLY:
+		ctrl |= PORT_AUTO_NEG_ASYM_PAUSE;
+		break;
+	case PHY_RX_ONLY:
+		ctrl |= PORT_AUTO_NEG_PAUSE;
+		break;
+	default:
+		break;
+	}
+	return ctrl;
+}  /* port_advertised_flow_ctrl */
+
+static u8 sw_determine_flow_ctrl(struct ksz_sw *sw, struct ksz_port *port,
+	u16 local, u16 remote)
+{
+	int rx;
+	int tx;
+	u8 flow = 0;
+
+	if (sw->overrides & PAUSE_FLOW_CTRL)
+		return flow;
+
+	rx = tx = 0;
+	if (port->force_link)
+		rx = tx = 1;
+	if (remote & PORT_REMOTE_SYM_PAUSE) {
+		if (local & PORT_AUTO_NEG_SYM_PAUSE)
+			rx = tx = 1;
+		else if ((remote & PORT_AUTO_NEG_ASYM_PAUSE) &&
+			 (local & PORT_AUTO_NEG_PAUSE) ==
+			 PORT_AUTO_NEG_ASYM_PAUSE)
+			tx = 1;
+	} else if (remote & PORT_AUTO_NEG_ASYM_PAUSE) {
+		if ((local & PORT_AUTO_NEG_PAUSE) == PORT_AUTO_NEG_PAUSE)
+			rx = 1;
+	}
+	if (rx)
+		flow |= 0x01;
+	if (tx)
+		flow |= 0x02;
+#ifdef DBG_LINK
+	printk(KERN_INFO "pause: %d, %d; %04x %04x\n",
+		rx, tx, local, remote);
+#endif
+	return flow;
+}  /* sw_determine_flow_ctrl */
+
+#if 0
+static void port_reset_phy(struct ksz_sw *sw, uint p)
+{
+	u32 ctrl;
+	u32 data;
+
+	port_r32(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, &data);
+	data |= PORT_REG_CLK_SPEED_25_MHZ;
+	port_w32(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, data);
+	port_r32(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_3, &data);
+	ctrl = data;
+	data |= PORT_RESET;
+	port_w32(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_3, data);
+	delay_micro(100);
+	port_r32(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_3, &data);
+	if (data != ctrl)
+dbg_msg("%s %08x %08x\n", __func__, data, ctrl);
+#if 0
+	if (data != ctrl)
+		port_w32(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_3, ctrl);
+#endif
+}  /* port_reset_phy */
+#endif
+
+static void port_sgmii_r(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	u32 data;
+
+	data = devid & PORT_SGMII_DEVICE_ID_M;
+	data <<= PORT_SGMII_DEVICE_ID_S;
+	data |= reg;
+	if (len > 1)
+		data |= PORT_SGMII_AUTO_INCR;
+	port_w32(sw, port, REG_PORT_SGMII_ADDR__4, data);
+	while (len) {
+		port_r32(sw, port, REG_PORT_SGMII_DATA__4, &data);
+		*buf++ = (u16) data;
+		len--;
+	}
+}  /* port_sgmii_r */
+
+static void port_sgmii_w(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	u32 data;
+
+	data = devid & PORT_SGMII_DEVICE_ID_M;
+	data <<= PORT_SGMII_DEVICE_ID_S;
+	data |= reg;
+	if (len > 1)
+		data |= PORT_SGMII_AUTO_INCR;
+	port_w32(sw, port, REG_PORT_SGMII_ADDR__4, data);
+	while (len) {
+		data = *buf++;
+		port_w32(sw, port, REG_PORT_SGMII_DATA__4, data);
+		len--;
+	}
+}  /* port_sgmii_w */
+
+static u16 port_sgmii_phy_r(struct ksz_sw *sw, uint port, u16 reg)
+{
+	u16 buf;
+
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+	buf = reg;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_ADDR, &buf, 1);
+	buf = 0;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	buf |= SR_MII_PHY_START_BUSY;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+	port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_DATA, &buf, 1);
+	return buf;
+}  /* port_sgmii_phy_r */
+
+static void port_sgmii_phy_w(struct ksz_sw *sw, uint port, u16 reg, u16 val)
+{
+	u16 buf;
+
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+	buf = reg;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_ADDR, &buf, 1);
+	buf = val;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_DATA, &buf, 1);
+	buf = SR_MII_PHY_WRITE;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	buf |= SR_MII_PHY_START_BUSY;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+}  /* port_sgmii_phy_w */
+
+static int port_sgmii_detect(struct ksz_sw *sw, uint p)
+{
+	u16 buf[6];
+	int ret = 0;
+
+	port_sgmii_phy_w(sw, p, SR_MII_PHY_JTAG_CHIP_ID_LO, 0x1234);
+	buf[0] = port_sgmii_phy_r(sw, p, SR_MII_PHY_JTAG_CHIP_ID_LO);
+	buf[1] = port_sgmii_phy_r(sw, p, SR_MII_PHY_JTAG_CHIP_ID_HI);
+dbg_msg("jtag: %04x %04x\n", buf[1], buf[0]);
+#if 0
+	if (SR_MII_JTAG_CHIP_ID_LO == buf[0]);
+		ret = 1;
+#endif
+	port_sgmii_r(sw, p, SR_MII, 0, buf, 6);
+dbg_msg("%04x %04x %04x %04x %04x %04x\n",
+buf[0], buf[1], buf[2], buf[3], buf[4], buf[5]);
+	if (0x7996 == buf[2])
+		ret = 1;
+	else {
+		port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_CTRL, buf, 1);
+		if ((buf[0] & ~SR_MII_AUTO_NEG_COMPLETE_INTR) ==
+		    (SR_MII_PCS_SGMII << SR_MII_PCS_MODE_S))
+			ret = 1;
+	}
+	return ret;
+}  /* port_sgmii_detect */
+
+static void port_sgmii_setup(struct ksz_sw *sw, uint p, bool master,
+	int speed, int duplex)
+{
+	u16 cfg;
+	u16 ctrl;
+	u16 adv;
+
+	/* SGMII registers are not changed by reset. */
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_CTRL, &cfg, 1);
+dbg_msg("  cfg: %04x\n", cfg);
+	if (cfg & SR_MII_AUTO_NEG_COMPLETE_INTR)
+		return;
+	cfg = SR_MII_PCS_SGMII << SR_MII_PCS_MODE_S;
+	if (master) {
+		cfg |= SR_MII_TX_CFG_PHY_MASTER;
+		cfg |= SR_MII_SGMII_LINK_UP;
+	}
+	cfg |= SR_MII_AUTO_NEG_COMPLETE_INTR;
+dbg_msg("CFG: %04x\n", cfg);
+	port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_CTRL, &cfg, 1);
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	if (master) {
+		switch (speed) {
+		case 1:
+			ctrl |= SR_MII_SPEED_100MBIT;
+			break;
+		case 2:
+			ctrl |= SR_MII_SPEED_1000MBIT;
+			break;
+		}
+	}
+	if (!(ctrl & SR_MII_AUTO_NEG_ENABLE)) {
+		ctrl |= SR_MII_AUTO_NEG_ENABLE;
+dbg_msg("CTRL: %04x\n", ctrl);
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+
+	/* Need to write to advertise register to send correct signal. */
+	/* Default value is 0x0020. */
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION, &adv, 1);
+dbg_msg("  adv: %04x\n", adv);
+	adv = SR_MII_AUTO_NEG_ASYM_PAUSE_RX << SR_MII_AUTO_NEG_PAUSE_S;
+	if (duplex)
+		adv |= SR_MII_AUTO_NEG_FULL_DUPLEX;
+	else
+		adv |= SR_MII_AUTO_NEG_HALF_DUPLEX;
+dbg_msg("ADV: %04x\n", adv);
+	port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION, &adv, 1);
+}  /* port_sgmii_setup */
+
+static int port_get_sgmii_speed(struct ksz_port *port, uint p)
+{
+	u16 data;
+	u16 speed;
+	u8 link;
+	int change = 0;
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info = &sw->port_info[p];
+	struct ksz_port_state *state = &sw->port_state[p];
+
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_STATUS, &data, 1);
+	link = data & ~SR_MII_AUTO_NEG_COMPLETE_INTR;
+	if (info->link == link)
+		return change;
+
+	info->link = link;
+
+	/* Need to update control register with same link setting. */
+	if (data & SR_MII_STAT_LINK_UP) {
+		u16 ctrl;
+
+		ctrl = SR_MII_AUTO_NEG_ENABLE;
+		speed = (data >> SR_MII_STAT_S) & SR_MII_STAT_M;
+		if (SR_MII_STAT_1000_MBPS == speed)
+			ctrl |= SR_MII_SPEED_1000MBIT;
+		else if (SR_MII_STAT_100_MBPS == speed)
+			ctrl |= SR_MII_SPEED_100MBIT;
+		if (data & SR_MII_STAT_FULL_DUPLEX)
+			ctrl |= SR_MII_FULL_DUPLEX;
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+do {
+	u16 buf[6];
+
+	port_sgmii_r(sw, p, SR_MII, 0, buf, 6);
+dbg_msg("%04x %04x %04x %04x %04x %04x\n",
+buf[0], buf[1], buf[2], buf[3], buf[4], buf[5]);
+} while (0);
+	if (data & SR_MII_STAT_LINK_UP) {
+		speed = (data >> SR_MII_STAT_S) & SR_MII_STAT_M;
+		info->tx_rate = 10 * TX_RATE_UNIT;
+		if (SR_MII_STAT_1000_MBPS == speed)
+			info->tx_rate = 1000 * TX_RATE_UNIT;
+		else if (SR_MII_STAT_100_MBPS == speed)
+			info->tx_rate = 100 * TX_RATE_UNIT;
+
+		info->duplex = 1;
+		if (data & SR_MII_STAT_FULL_DUPLEX)
+			info->duplex = 2;
+
+		if (media_connected != info->state) {
+			SW_D flow_ctrl;
+
+			port_r(sw, p, REG_PORT_STATUS_0, &flow_ctrl);
+#ifdef DBG_LINK
+			printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR"\n",
+				flow_ctrl & (PORT_RX_FLOW_CTRL |
+				PORT_TX_FLOW_CTRL));
+#endif
+			info->flow_ctrl = 3;
+#if 0
+			info->flow_ctrl = sw_determine_flow_ctrl(sw,
+				port, local, remote);
+#endif
+			if (flow_ctrl & PORT_RX_FLOW_CTRL)
+				info->flow_ctrl |= 0x10;
+			if (flow_ctrl & PORT_TX_FLOW_CTRL)
+				info->flow_ctrl |= 0x20;
+			change = 1 << p;
+		}
+		info->state = media_connected;
+		state->tx_rate = info->tx_rate;
+	} else {
+		if (media_disconnected != info->state) {
+			change = 1 << p;
+
+			/* Indicate the link just goes down. */
+			state->link_down = 1;
+
+			/* For 802.1X Authentication. */
+			if ((sw->overrides & USE_802_1X_AUTH) &&
+			    (sw->on_ports & (1 << p)))
+				port_open(sw, p, false);
+		}
+		info->state = media_disconnected;
+	}
+	info->report = true;
+	return change;
+}  /* port_get_sgmii_speed */
+
+static void port_set_sgmii_speed(struct ksz_port *port, uint p)
+{
+	u16 ctrl;
+	u16 cfg;
+	u16 adv;
+	struct ksz_sw *sw = port->sw;
+
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION, &adv, 1);
+	cfg = adv;
+	switch (port->flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		adv = SR_MII_AUTO_NEG_SYM_PAUSE;
+		break;
+	case PHY_TX_ONLY:
+		adv = SR_MII_AUTO_NEG_ASYM_PAUSE_TX;
+		break;
+	case PHY_RX_ONLY:
+		adv = SR_MII_AUTO_NEG_ASYM_PAUSE_RX;
+		break;
+	default:
+		adv = 0;
+	}
+	adv <<= SR_MII_AUTO_NEG_PAUSE_S;
+	adv |= SR_MII_AUTO_NEG_FULL_DUPLEX;
+	adv |= SR_MII_AUTO_NEG_HALF_DUPLEX;
+	if (port->duplex) {
+		if (1 == port->duplex)
+			adv &= ~SR_MII_AUTO_NEG_FULL_DUPLEX;
+		else if (2 == port->duplex)
+			adv &= ~SR_MII_AUTO_NEG_HALF_DUPLEX;
+	}
+	if (adv != cfg) {
+dbg_msg("ADV: %04x\n", adv);
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION,
+			&adv, 1);
+		port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+		ctrl |= SR_MII_AUTO_NEG_RESTART;
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+}  /* port_set_sgmii_speed */
+
+/**
+ * port_get_link_speed - get current link status
+ * @port:	The port instance.
+ *
+ * This routine reads PHY registers to determine the current link status of the
+ * switch ports.
+ */
+static int port_get_link_speed(struct ksz_port *port)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *linked = NULL;
+	struct ksz_port_state *state;
+	struct ksz_sw *sw = port->sw;
+	u16 data;
+	u16 link;
+	u16 dbg_link;
+	u16 status;
+	u32 local;
+	u32 remote;
+	int i;
+	int p;
+	int change = 0;
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+		info = &sw->port_info[p];
+
+		/* Port does not have PHY. */
+		if (!info->phy) {
+
+			/* By itself. */
+			if (1 == port->port_cnt &&
+			    (sw->phy_intr & (1 << p)))
+				schedule_work(&port->link_update);
+			continue;
+		}
+
+		if (!(sw->phy_intr & (1 << p))) {
+			if (!linked && p != sw->HOST_PORT &&
+			    info->state == media_connected)
+				linked = info;
+			continue;
+		}
+		sw->phy_intr &= ~(1 << p);
+
+		state = &sw->port_state[p];
+
+		if (PHY_INTERFACE_MODE_SGMII == info->interface) {
+			int update = port_get_sgmii_speed(port, p);
+
+			if (update)
+				change |= (1 << p);
+
+			/* Remember the first linked port. */
+			if (!linked && p != sw->HOST_PORT &&
+			    media_connected == info->state)
+				linked = info;
+			goto get_cont3;
+		}
+
+#ifdef NO_PHY_READ
+		if (sw) {
+			link = PORT_LINK_STATUS;
+			dbg_link = link;
+			status = PORT_STAT_SPEED_100MBIT |
+				PORT_STAT_FULL_DUPLEX;
+			local = 0x10001000;
+			remote = 0x10001000;
+			goto get_cont1;
+		}
+#endif
+
+		if (sw->features & GIGABIT_SUPPORT)
+			port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &data);
+		else
+			data = 0;
+		local = data;
+		local <<= 16;
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &data);
+		local |= data;
+		port_r16(sw, p, REG_PORT_PHY_1000_STATUS, &data);
+		remote = data;
+		remote &= PORT_PHY_1000_STATIC_STATUS;
+		remote <<= 16;
+		port_r16(sw, p, REG_PORT_PHY_REMOTE_CAPABILITY, &data);
+		remote |= data;
+		port_r16(sw, p, P_LINK_STATUS, &link);
+
+		/* Read second time in case the status is not latched. */
+		if (!(link & PORT_LINK_STATUS))
+			port_r16(sw, p, P_LINK_STATUS, &link);
+
+		/* SPI has problem reading the link status from the PHY. */
+		dbg_link = link;
+		port_r16(sw, p, P_SPEED_STATUS, &status);
+		if (!(sw->features & NEW_CAP) &&
+		    ((link & PORT_AUTO_NEG_ACKNOWLEDGE) && (status &
+		    (PORT_STAT_SPEED_1000MBIT |
+		     PORT_STAT_SPEED_100MBIT |
+		     PORT_STAT_SPEED_10MBIT))))
+			link |= PORT_LINK_STATUS;
+
+#ifdef NO_PHY_READ
+get_cont1:
+#endif
+		/*
+		 * The partner capability register is updated but the
+		 * auto-negotiation is not completed yet.
+		 */
+		link &= (PORT_AUTO_NEG_ACKNOWLEDGE | PORT_LINK_STATUS);
+		status &= (PORT_STAT_SPEED_1000MBIT |
+			PORT_STAT_SPEED_100MBIT |
+			PORT_STAT_SPEED_10MBIT |
+			PORT_STAT_FULL_DUPLEX);
+		link |= (status << 8);
+
+		if (link & PORT_LINK_STATUS) {
+
+			/* Remember the first linked port. */
+			if (!linked && p != sw->HOST_PORT)
+				linked = info;
+		}
+
+		/* No change to status. */
+		if (local == info->advertised && link == info->link)
+			continue;
+
+		if (!(dbg_link & PORT_LINK_STATUS) &&
+		    dbg_link & PORT_AUTO_NEG_ACKNOWLEDGE)
+dbg_msg(" link? %d=%04x\n", p, dbg_link);
+#ifdef DBG_LINK
+		printk(KERN_INFO
+			"%d=advertised: %08X-%08X; partner: %08X-%08X"
+			"; link: %04X-%04X\n", p,
+			local, info->advertised, remote, info->partner,
+			link, info->link);
+#endif
+		if (link & PORT_LINK_STATUS) {
+			info->tx_rate = 10 * TX_RATE_UNIT;
+			if (status & PORT_STAT_SPEED_100MBIT)
+				info->tx_rate = 100 * TX_RATE_UNIT;
+			else if (status & PORT_STAT_SPEED_1000MBIT)
+				info->tx_rate = 1000 * TX_RATE_UNIT;
+
+			info->duplex = 1;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				info->duplex = 2;
+
+			if (media_connected != info->state) {
+				SW_D flow_ctrl;
+
+				port_r(sw, p, REG_PORT_STATUS_0, &flow_ctrl);
+#ifdef DBG_LINK
+				printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR"\n",
+					flow_ctrl & (PORT_RX_FLOW_CTRL |
+					PORT_TX_FLOW_CTRL));
+#endif
+				info->flow_ctrl = sw_determine_flow_ctrl(sw,
+					port, local, remote);
+				if (flow_ctrl & PORT_RX_FLOW_CTRL)
+					info->flow_ctrl |= 0x10;
+				if (flow_ctrl & PORT_TX_FLOW_CTRL)
+					info->flow_ctrl |= 0x20;
+				if (sw->info)
+					port_cfg_back_pressure(sw, p,
+						(1 == info->duplex));
+				change |= 1 << p;
+			} else if (link != info->link)
+				change |= 1 << p;
+			info->state = media_connected;
+			state->tx_rate = info->tx_rate;
+		} else {
+			if (media_disconnected != info->state) {
+				change |= 1 << p;
+
+				/* Indicate the link just goes down. */
+				state->link_down = 1;
+
+				/* For 802.1X Authentication. */
+				if ((sw->overrides & USE_802_1X_AUTH) &&
+				    (sw->on_ports & (1 << p)))
+					port_open(sw, p, false);
+			}
+			info->state = media_disconnected;
+		}
+		info->report = true;
+		info->advertised = local;
+		info->partner = remote;
+		info->link = link;
+
+get_cont3:
+		if (media_disconnected == info->state)
+			sw->live_ports &= ~(1 << p);
+		else
+			sw->live_ports |= (1 << p);
+		state->state = info->state;
+	}
+
+	if (linked && media_disconnected == port->linked->state)
+		port->linked = linked;
+
+#ifdef DBG_LINK
+	if (change)
+		dbp_link(port, sw, change);
+#endif
+	if (change) {
+		sw->link_ports |= change;
+		schedule_work(&port->link_update);
+	}
+	return change;
+}  /* port_get_link_speed */
+
+/**
+ * port_set_link_speed - set port speed
+ * @port:	The port instance.
+ *
+ * This routine sets the link speed of the switch ports.
+ */
+static void port_set_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	u16 data;
+	u16 ctrl;
+	u16 local;
+	u16 status;
+	u32 adv;
+	u32 cfg;
+	int i;
+	int p;
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+		info = &sw->port_info[p];
+
+		/* Port does not have PHY. */
+		if (!info->phy)
+			continue;
+
+		/* Host port is not at either end. */
+		if (p == sw->HOST_PORT)
+			continue;
+
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		if (PHY_INTERFACE_MODE_SGMII == info->interface) {
+			port_set_sgmii_speed(port, p);
+			continue;
+		}
+
+		if (sw->features & GIGABIT_SUPPORT)
+			port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+		else
+			ctrl = 0;
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &local);
+		if (!(local & PORT_AUTO_NEG_SYM_PAUSE) &&
+		    (PHY_FLOW_CTRL == port->flow_ctrl ||
+		    PHY_RX_ONLY == port->flow_ctrl))
+			dbg_msg(" no sym pause: %d %04x\n", p, local);
+		if ((local & PORT_AUTO_NEG_ASYM_PAUSE) &&
+		    (PHY_TX_ONLY != port->flow_ctrl &&
+		    PHY_RX_ONLY != port->flow_ctrl))
+			dbg_msg(" has asym pause: %d %04x\n", p, local);
+		adv = ctrl;
+		adv <<= 16;
+		adv |= local;
+		port_r16(sw, p, P_SPEED_STATUS, &status);
+		port_r16(sw, p, P_NEG_RESTART_CTRL, &data);
+
+		cfg = 0;
+
+		/*
+		 * Do not need to restart auto-negotiation if desired settings
+		 * are same.
+		 */
+		if ((data & PORT_AUTO_NEG_ENABLE) &&
+		    (status &
+		    (PORT_STAT_SPEED_1000MBIT |
+		     PORT_STAT_SPEED_100MBIT |
+		     PORT_STAT_SPEED_10MBIT)))
+			cfg = adv;
+
+		/* Need auto-negotiation restart. */
+		if (sw->info->port_cfg[p].setup_time) {
+			cfg = 0;
+			sw->info->port_cfg[p].setup_time = 0;
+		}
+
+		local = port_advertised_flow_ctrl(port, local);
+
+		if (sw->features & GIGABIT_SUPPORT)
+			ctrl |= PORT_AUTO_NEG_1000BT_FD | PORT_AUTO_NEG_1000BT;
+		local |= PORT_AUTO_NEG_100BTX_FD | PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD | PORT_AUTO_NEG_10BT;
+
+		/* Check if manual configuration is specified by the user. */
+		if (port->speed || port->duplex) {
+			if (port->speed && port->speed != 1000)
+				ctrl &= ~(PORT_AUTO_NEG_1000BT_FD |
+					PORT_AUTO_NEG_1000BT);
+			if (10 == port->speed)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX);
+			else if (100 == port->speed)
+				local &= ~(PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			else if (1000 == port->speed)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			if (1 == port->duplex) {
+				ctrl &= ~PORT_AUTO_NEG_1000BT_FD;
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_10BT_FD);
+			} else if (2 == port->duplex) {
+				ctrl &= ~PORT_AUTO_NEG_1000BT;
+				local &= ~(PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT);
+			}
+		}
+		adv = ctrl;
+		adv <<= 16;
+		adv |= local;
+		if (adv != cfg) {
+			if (sw->features & GIGABIT_SUPPORT)
+				port_w16(sw, p, REG_PORT_PHY_1000_CTRL, ctrl);
+			port_w16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, local);
+			port_r16(sw, p, P_NEG_RESTART_CTRL, &data);
+			data |= PORT_AUTO_NEG_ENABLE;
+			sw->info->port_cfg[p].phy_ctrl = data;
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w16(sw, p, P_NEG_RESTART_CTRL, data);
+
+			/* Link is going down. */
+			sw->port_state[p].state = media_disconnected;
+		}
+	}
+}  /* port_set_link_speed */
+
+/**
+ * port_force_link_speed - force port speed
+ * @port:	The port instance.
+ *
+ * This routine forces the link speed of the switch ports.
+ */
+static void port_force_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	u16 data;
+	int i;
+	int p;
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+		info = &sw->port_info[p];
+
+		/* Port does not have PHY. */
+		if (!info->phy)
+			continue;
+
+		/* Host port is not at either end. */
+		if (p == sw->HOST_PORT)
+			continue;
+
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		if (PHY_INTERFACE_MODE_SGMII == info->interface) {
+			continue;
+		}
+
+		port_r16(sw, p, P_PHY_CTRL, &data);
+		data &= ~(PORT_AUTO_NEG_ENABLE |
+			PORT_SPEED_100MBIT | PORT_SPEED_1000MBIT);
+		if (100 == port->speed)
+			data |= PORT_SPEED_100MBIT;
+		else if (1000 == port->speed)
+			data |= PORT_SPEED_1000MBIT;
+		if (1 == port->duplex)
+			data &= ~PORT_FULL_DUPLEX;
+		else if (2 == port->duplex)
+			data |= PORT_FULL_DUPLEX;
+		port_w16(sw, p, P_PHY_CTRL, data);
+		sw->info->port_cfg[p].phy_ctrl = data;
+	}
+}  /* port_force_link_speed */
+
+static void port_mmd_read(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+		MMD_SETUP(PORT_MMD_OP_INDEX, devid));
+	port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, reg);
+	if (len > 1)
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_INCR_RW, devid));
+	else
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_NO_INCR, devid));
+	while (len) {
+		port_r16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, buf);
+		buf++;
+		len--;
+	}
+}  /* port_mmd_read */
+
+static void port_mmd_write(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+		MMD_SETUP(PORT_MMD_OP_INDEX, devid));
+	port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, reg);
+	if (len > 1)
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_INCR_W, devid));
+	else
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_NO_INCR, devid));
+	while (len) {
+		port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, *buf);
+		buf++;
+		len--;
+	}
+}  /* port_mmd_write */
+
+static void port_chk_sqi(struct ksz_sw *sw, uint port)
+{
+	u16 sqi[4];
+	int val;
+	int val2;
+	int i;
+	int n;
+	int cnt = 10;
+	int num = 4;
+
+	if (sw->port_info[port].state == media_disconnected)
+		return;
+	if (sw->port_info[port].tx_rate / TX_RATE_UNIT != 1000)
+		num = 1;
+	val = 0;
+	for (n = 0; n < cnt; n++) {
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, MMD_DSP_SQI_CHAN_A,
+			sqi, num);
+		val2 = 0;
+		for (i = 0; i < num; i++) {
+			if (sqi[i] & DSP_SQI_ERR_DETECTED)
+dbg_msg(" ?? %s %d %04x\n", __func__, i, sqi[i]);
+			if (sqi[i] & DSP_SQI_ERR_DETECTED)
+				break;
+			val2 += sqi[i] & DSP_SQI_AVG_ERR;
+		}
+		val2 /= num;
+		val += val2;
+	}
+	val /= cnt;
+	val >>= 8;
+	if (val > 15)
+		val = 15;
+	val = 15 - val;
+	sw->port_info[port].sqi = val;
+}  /* port_chk_sqi */
+
+struct ksz_phy_settings {
+	u16 mmd;
+	u16 reg;
+	u16 val;
+};
+
+static struct ksz_phy_settings ksz9893_phy_settings[] = {
+#if 0
+	{ MMD_DEVICE_ID_AFED, 0x3a, 0x5744 },
+	{ MMD_DEVICE_ID_AFED, 0x04, 0x00c0 },
+	{ MMD_DEVICE_ID_AFED, 0x06, 0x2000 },
+	{ MMD_DEVICE_ID_AFED, 0x08, 0x2000 },
+	{ MMD_DEVICE_ID_AFED, 0x00, 0x1002 },
+
+	{ 0x03, 0x0b, 0x0050 },
+	{ 0x03, 0x0e, 0x0025 },
+	{ MMD_DEVICE_ID_AFED, 0x37, 0x03f2 },
+
+	{ MMD_DEVICE_ID_DSP, 0xd2, 0x7500 },
+	{ MMD_DEVICE_ID_DSP, 0xc8, 0x0005 },
+	{ MMD_DEVICE_ID_DSP, 0xcb, 0x0790 },
+	{ MMD_DEVICE_ID_DSP, 0xcc, 0x6010 },
+
+	{ MMD_DEVICE_ID_COMMON, 0x46, 0x10f8 },
+#else
+	{ MMD_DEVICE_ID_DSP, 0xa0, 0x3fff },
+#endif
+};
+
+static void port_setup_eee(struct ksz_sw *sw, uint port)
+{
+	u16 val[0x20];
+
+	if (sw->features & NEW_CAP) {
+#ifdef DEBUG_PHY
+dbg_msg("%s %d\n", __func__, port);
+if (0 == port) {
+int i;
+		port_r16(sw, port, REG_PORT_PHY_CTRL, val);
+dbg_msg("%04x=%04x\n", REG_PORT_PHY_CTRL, val[0]);
+		port_r16(sw, port, REG_PORT_PHY_REMOTE_LB_LED, val);
+dbg_msg("%04x=%04x\n", REG_PORT_PHY_REMOTE_LB_LED, val[0]);
+
+dbg_msg(" %x\n", MMD_DEVICE_ID_DSP);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCE, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCE, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCC, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCC, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCA, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCA, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCB, val, 1);
+dbg_msg(" %04x=%04x\n", 0xCB, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xC8, val, 1);
+dbg_msg(" %04x=%04x\n", 0xC8, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xD9, val, 1);
+dbg_msg(" %04x=%04x\n", 0xD9, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xC9, val, 1);
+dbg_msg(" %04x=%04x\n", 0xC9, val[0]);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x79, &val[0x09],
+			18);
+for (i = 0; i < 18; i++)
+dbg_msg("%04x ", val[0x09 + i]);
+dbg_msg("\n");
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x8F, val, 1);
+dbg_msg(" %04x=%04x\n", 0x8F, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x9D, val, 1);
+dbg_msg(" %04x=%04x\n", 0x9D, val[0]);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x75, val, 1);
+dbg_msg(" %04x=%04x\n", 0x75, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xD3, val, 1);
+dbg_msg(" %04x=%04x\n", 0xD3, val[0]);
+
+dbg_msg(" %x\n", MMD_DEVICE_ID_AFED);
+		port_mmd_read(sw, port, 0x1C, 0x0, val, 1);
+dbg_msg(" %04x=%04x\n", 0x0, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x4, val, 1);
+dbg_msg(" %04x=%04x\n", 0x4, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x6, val, 1);
+dbg_msg(" %04x=%04x\n", 0x6, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x9, val, 1);
+dbg_msg(" %04x=%04x\n", 0x9, val[0]);
+
+		port_mmd_read(sw, port, 0x1C, 0x13, &val[0x13], 12);
+for (i = 0; i < 12; i++)
+dbg_msg("%04x ", val[0x13 + i]);
+dbg_msg("\n");
+}
+#endif
+
+		port_w16(sw, port, REG_PORT_PHY_CTRL, 0x2100);
+		port_w16(sw, port, REG_PORT_PHY_REMOTE_LB_LED, 0x00f0);
+
+		val[0] = 0x0100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCE, val, 1);
+		val[0] = 0x0ff0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCC, val, 1);
+		val[0] = 0x0141;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCA, val, 1);
+		val[0] = 0x0fcf;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCB, val, 1);
+		val[0] = 0x0010;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xC8, val, 1);
+		val[0] = 0x0100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xD9, val, 1);
+		val[0] = 0x0280;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xC9, val, 1);
+
+		val[0x09] = 0x010A;
+		val[0x0a] = 0x00ED;
+		val[0x0b] = 0x00D3;
+		val[0x0c] = 0x00BC;
+		val[0x0d] = 0x00A8;
+		val[0x0e] = 0x0096;
+		val[0x0f] = 0x0085;
+		val[0x10] = 0x0077;
+		val[0x11] = 0x006A;
+		val[0x12] = 0x005E;
+		val[0x13] = 0x0054;
+		val[0x14] = 0x004B;
+		val[0x15] = 0x0043;
+		val[0x16] = 0x003C;
+		val[0x17] = 0x0035;
+		val[0x18] = 0x002F;
+		val[0x19] = 0x002A;
+		val[0x1a] = 0x0026;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x79, &val[0x09],
+			18);
+
+		val[0] = 0x6032;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x8F, val, 1);
+		val[0] = 0x248C;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x9D, val, 1);
+
+		val[0] = 0x0060;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x75, val, 1);
+		val[0] = 0x7777;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xD3, val, 1);
+
+		val[0] = 0x9400;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x0, val, 1);
+/*
+ * THa  2016/10/20
+ * Use value 0x00E2 for improved 100BTX PMD Output Amplitude.
+ */
+		val[0] = 0x00e2;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x4, val, 1);
+		val[0] = 0x3100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x6, val, 1);
+		val[0] = 0xe01c;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x9, val, 1);
+
+		val[0x13] = 0x6eff;
+		val[0x14] = 0xe6ff;
+		val[0x15] = 0x6eff;
+		val[0x16] = 0xe6ff;
+		val[0x17] = 0x00ff;
+		val[0x18] = 0x43ff;
+		val[0x19] = 0xc3ff;
+		val[0x1a] = 0x6fff;
+		val[0x1b] = 0x07ff;
+		val[0x1c] = 0x0fff;
+		val[0x1d] = 0xe7ff;
+		val[0x1e] = 0xefff;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x13, &val[0x13],
+			12);
+
+		if (port == sw->HOST_PORT)
+			port_w16(sw, port, REG_PORT_PHY_CTRL, 0x1140);
+		else {
+			/* Avoid empty interrupt at beginning. */
+			port_w16(sw, port, REG_PORT_PHY_CTRL, 0x0140);
+			sw->info->port_cfg[port].setup_time = 4000;
+		}
+	} else {
+		memset(val, 0, sizeof(val));
+		port_mmd_write(sw, port, 0x1C, 0x10, val, 0x11);
+	}
+}  /* port_setup_eee */
+
+static void port_setup_9893(struct ksz_sw *sw, uint port)
+{
+	u16 val[1];
+	int i;
+	struct ksz_phy_settings *set;
+
+#if 0
+	port_w16(sw, port, REG_PORT_PHY_CTRL, 0x2100);
+#endif
+
+	for (i = 0; i < sizeof(ksz9893_phy_settings) /
+	     sizeof(struct ksz_phy_settings); i++) {
+		set = &ksz9893_phy_settings[i];
+		val[0] = set->val;
+		port_mmd_write(sw, port, set->mmd, set->reg, val, 1);
+	}
+
+#if 0
+	if (port == sw->HOST_PORT)
+		port_w16(sw, port, REG_PORT_PHY_CTRL, 0x1140);
+	else {
+		/* Avoid empty interrupt at beginning. */
+		port_w16(sw, port, REG_PORT_PHY_CTRL, 0x0140);
+		sw->info->port_cfg[port].setup_time = 4000;
+	}
+#endif
+}  /* port_setup_9893 */
+
+/**
+ * sw_enable - enable the switch
+ * @sw:		The switch instance.
+ *
+ * This routine enables the switch with a specific configuration.
+ */
+static void sw_enable(struct ksz_sw *sw)
+{
+	uint port;
+	int state = STP_STATE_FORWARDING;
+
+	if (sw->features & (DSA_SUPPORT))
+		state = STP_STATE_SIMPLE;
+
+	/* Manually change default membership when not all ports are used. */
+	if (sw->features & USE_FEWER_PORTS) {
+		for (port = 0; port < sw->mib_port_cnt; port++) {
+			port = chk_last_port(sw, port);
+			sw->info->port_cfg[port].vid_member = sw->PORT_MASK;
+		}
+		for (port = sw->phy_port_cnt; port < sw->port_cnt; port++) {
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		}
+		sw_cfg_port_base_vlan(sw, sw->HOST_PORT, sw->PORT_MASK);
+	}
+	if ((sw->dev_count > 1 && !sw->dev_offset) ||
+	    (sw->features & (STP_SUPPORT | DSA_SUPPORT))) {
+		u16 member;
+
+		for (port = 0; port < sw->mib_port_cnt; port++) {
+			if (skip_host_port(sw, port))
+				continue;
+			member = (1 << port);
+			if (sw->features & SW_VLAN_DEV) {
+				int q;
+
+				for (q = 0; q < sw->eth_cnt; q++)
+					if (sw->eth_maps[q].port <= port &&
+					    port < sw->eth_maps[q].port +
+					    sw->eth_maps[q].cnt) {
+						member = sw->eth_maps[q].mask;
+						break;
+					}
+			}
+			sw->info->port_cfg[port].vid_member = member;
+		}
+	} else if (1 == sw->eth_cnt) {
+		u16 member;
+
+		for (port = 0; port < sw->mib_port_cnt; port++) {
+			if (skip_host_port(sw, port))
+				continue;
+			member = 0;
+			if (sw->eth_maps[0].mask & (1 << port))
+				member = sw->eth_maps[0].mask;
+			sw->info->port_cfg[port].vid_member = member;
+		}
+	}
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		if (skip_host_port(sw, port))
+			continue;
+		if (sw->dev_count > 1 ||
+		    (sw->eth_maps[0].mask &&
+		    !(sw->eth_maps[0].mask & (1 << port))))
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		else if ((sw->eth_maps[0].mask & (1 << port)) &&
+			 (sw->eth_maps[0].proto & HSR_HW))
+			port_set_stp_state(sw, port, STP_STATE_SIMPLE);
+		else
+			port_set_stp_state(sw, port, state);
+	}
+	if (sw->dev_count > 1 && !sw->dev_offset && sw->eth_cnt < 2)
+		port_set_stp_state(sw, sw->HOST_PORT, STP_STATE_SIMPLE);
+	else
+		port_set_stp_state(sw, sw->HOST_PORT, state);
+
+	/*
+	 * There may be some entries in the dynamic MAC table before the
+	 * the learning is turned off.  Once the entries in the table the
+	 * switch may keep updating them even learning is off.
+	 */
+	if (sw->dev_count > 1)
+		sw_flush_dyn_mac_table(sw, sw->mib_port_cnt);
+}  /* sw_enable */
+
+static void sw_init_cached_regs(struct ksz_sw *sw)
+{
+	sw->cached.ptp_clk_ctrl = sw->reg->r16(sw, REG_PTP_CLK_CTRL);
+	sw->cached.ptp_unit_index = sw->reg->r32(sw, REG_PTP_UNIT_INDEX__4);
+}
+
+/**
+ * sw_init - initialize the switch
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the hardware switch engine for default operation.
+ */
+static void sw_init(struct ksz_sw *sw)
+{
+	memset(sw->tx_pad, 0, 60);
+	sw->tx_start = 0;
+	sw_init_cached_regs(sw);
+	sw->open_ports = sw->PORT_MASK & ~sw->HOST_PORT;
+
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+	sw_init_phy_addr(sw);
+#endif
+
+	sw_init_broad_storm(sw);
+
+	sw_init_prio(sw);
+
+	sw_init_prio_rate(sw);
+
+	sw_init_vlan(sw);
+
+	sw_init_acl(sw);
+#if 0
+	if (!sw_chk(sw, REG_SWITCH_CTRL_1,
+			SWITCH_TX_FLOW_CTRL | SWITCH_RX_FLOW_CTRL))
+		sw->overrides |= PAUSE_FLOW_CTRL;
+#endif
+}  /* sw_init */
+
+/**
+ * sw_setup - setup the switch
+ * @sw:		The switch instance.
+ *
+ * This routine setup the hardware switch engine for default operation.
+ */
+static void sw_setup(struct ksz_sw *sw)
+{
+	uint port;
+
+	/* Starting from stopped state will flush the ACL table. */
+	sw_cfg(sw, REG_SW_OPERATION, SW_START, 1);
+
+	sw->port_intr_mask = sw->PORT_MASK;
+	sw->intr_mask = TRIG_TS_INT | APB_TIMEOUT_INT;
+	sw_set_global_ctrl(sw);
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_RESV_MCAST_ENABLE, 1);
+#if 0
+	sw->reg->w16(sw, REG_SW_ISP_TPID__2, 0x88A8);
+#endif
+/*
+ * THa  2015/12/03
+ * The new chip does not require legal packet check to be disabled for the tail
+ * tagging to work, but it still counts packets as oversized.
+ */
+#if 0
+	if (!(sw->features & NEW_CAP))
+#endif
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE, 1);
+	if (sw->features & IS_9893) {
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH, 0);
+		sw->reg->w16(sw, REG_AVB_STRATEGY__2,
+			SW_SHAPING_CREDIT_ACCT |
+			SW_POLICING_CREDIT_ACCT);
+	}
+
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		sw->info->port_cfg[port].intr_mask = 0;
+		port_cfg_back_pressure(sw, port, 1);
+		if (port < sw->phy_port_cnt)
+			port_cfg_force_flow_ctrl(sw, port, 0);
+#ifndef CONFIG_KSZ_DLR
+		if (!(sw->features & ACL_CORRUPT_BUG))
+#endif
+			sw->info->port_cfg[port].intr_mask |= PORT_ACL_INT;
+		if (port == sw->HOST_PORT)
+			continue;
+
+#ifdef CONFIG_1588_PTP
+		sw->info->port_cfg[port].intr_mask |= PORT_PTP_INT;
+#endif
+	}
+/*
+ * THa  2015/10/01
+ * Increasing wait time in this register avoids the PTP transmit problem.
+ */
+	dbg_msg("eee txq wait: %04x\n",
+		sw->reg->r16(sw, REG_SW_EEE_TXQ_WAIT_TIME__2));
+	sw->reg->w16(sw, REG_SW_EEE_TXQ_WAIT_TIME__2, 0x0040);
+
+	for (port = 0; port < sw->phy_port_cnt; port++) {
+		u16 val = 0;
+
+		if (skip_host_port(sw, port))
+			continue;
+		if (sw->features & IS_9893)
+			port_setup_9893(sw, port);
+		else
+			port_setup_eee(sw, port);
+#ifdef NO_EEE
+		/* Disable EEE for now. */
+		port_mmd_read(sw, port, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+/*
+ * THa  2015/09/30
+ * EEE in gigabit causes PTP messages not to be sent immediately.
+ * Just advertise EEE in 100 causes link not to be established.
+ * EEE in 100 has too much PTP jitter.
+ */
+		val = 0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+#endif
+
+		/*
+		 * Switch actually cannot do auto-negotiation with old 10Mbit
+		 * hub.
+		 */
+		port_r16(sw, port, P_PHY_CTRL, &val);
+		val &= ~PORT_FULL_DUPLEX;
+		port_w16(sw, port, P_PHY_CTRL, val);
+
+		if (sw->features & PHY_INTR_BUG)
+			continue;
+/*
+ * THa  2015/10/07
+ * The S2 chip has a bug that writing to the 0xN13E register will cause the
+ * 100Mbit link to be unstable, with blinking LED that can only transmit.
+ * Enabling EEE will recover this error condition.
+ */
+#if 1
+		/* Enable port PHY interrupt. */
+		sw->info->port_cfg[port].intr_mask |= PORT_PHY_INT;
+
+#if 0
+		port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &val);
+
+		/* Normally it should be low? */
+		val |= PORT_INT_PIN_HIGH;
+		port_w16(sw, port, REG_PORT_PHY_PHY_CTRL, val);
+#endif
+		val = LINK_DOWN_INT | LINK_UP_INT;
+
+#if 1
+/*
+ * THa  2014/06/25
+ * SPI cannot just read PHY interrupt status register to clear interrupts.
+ * No way to clear the interrupts so cannot enable them here.
+ */
+		if (!(sw->features & NEW_CAP))
+			val = 0;
+#endif
+		do {
+			u32 data;
+
+			port_r32(sw, port, REG_PORT_PHY_INT_ENABLE & ~3, &data);
+			data &= 0xffff00ff;
+			data |= val << 8;
+			port_w32(sw, port, REG_PORT_PHY_INT_ENABLE & ~3, data);
+		} while (0);
+
+#if 0
+/*
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+		port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &val);
+#endif
+
+/*
+ * THa  2014/11/18
+ * The new KSZ9893 chip has a bug related to IBA.  When writing PHY related
+ * registers with SPI while IBA is enabled APB interrupt can be triggered.
+ * Somehow the hardware expects those writes be 32-bit.  By itself this bug is
+ * just an annoyance, but port 2 has additional problem.
+ * After IBA are run the next initialization triggers a more serioud bug.
+ * Instead of generating APB interrupt the PHY_PHY_CTRL register 0x213E is
+ * reset to 0 by the hardware.  Writing to the PHY_INT_ENABLE register, even
+ * with a zero value, will generate the PHY interrupt with no actual PHY
+ * status.  As the interrupt pin is no longer high, the interrupt will keep
+ * coming until it is masked out or the interrupt pin is set to high again.
+ */
+#endif
+	}
+#if 0
+	for (port = 0; port < sw->phy_port_cnt; port++) {
+#ifdef CONFIG_KSZ_IBA
+		if (port == sw->HOST_PORT) {
+			port_w8(sw, port, REG_PORT_MAC_CTRL_2, 0);
+			continue;
+		}
+#endif
+		port_w8(sw, port, REG_PORT_MAC_CTRL_2,
+#if 1
+			PORT_100BT_EEE_DISABLE |
+#endif
+			PORT_1000BT_EEE_DISABLE);
+	}
+#endif
+	port = 6;
+	if (PHY_INTERFACE_MODE_SGMII == sw->port_info[port].interface &&
+	    sw->port_info[port].phy) {
+		port_sgmii_setup(sw, port, false, 2, 1);
+		sw->info->port_cfg[port].intr_mask |= PORT_SGMII_INT;
+	}
+
+	sw_setup_broad_storm(sw);
+
+	sw_setup_prio(sw);
+
+	sw_setup_mirror(sw);
+
+	sw->info->multi_sys = MULTI_MAC_TABLE_ENTRIES;
+	sw->info->multi_net = SWITCH_MAC_TABLE_ENTRIES;
+#ifdef CONFIG_KSZ_STP
+	sw_setup_stp(sw);
+#endif
+	sw_setup_multi(sw);
+#ifdef CONFIG_1588_PTP
+	sw_setup_ptp(sw);
+#endif
+#ifdef CONFIG_KSZ_IBA
+	if (!sw->info->iba.use_iba)
+		sw_setup_iba(sw);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		sw_setup_dlr(sw);
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		sw_setup_hsr(sw);
+#endif
+	sw_setup_acl(sw);
+}  /* sw_setup */
+
+static void sw_reset(struct ksz_sw *sw)
+{
+	sw->overrides &= ~VLAN_SET;
+	sw_reset_setup(sw);
+	if (sw->features & NEW_CAP) {
+		uint p;
+		uint q;
+		u8 byte_before;
+		u8 byte_after;
+
+		sw_cfg(sw, REG_SW_OPERATION, SW_RESET, 1);
+		delay_micro(1);
+
+		/* Turn off SPI auto edge detection. */
+		sw->reg->w8(sw, REG_SW_GLOBAL_SERIAL_CTRL_0, 0);
+
+#if 1
+		if (sw->overrides & USE_802_1X_AUTH)
+			sw_cfg(sw, REG_SW_OPERATION, SW_START, 0);
+#endif
+
+/*
+ * THa  2016/10/03
+ * SGMII registers are not reset by hardware reset.
+ */
+		p = 6;
+		if (PHY_INTERFACE_MODE_SGMII == sw->port_info[p].interface) {
+			u16 ctrl;
+
+			port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+			ctrl |= SR_MII_RESET;
+			port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+			ctrl &= ~SR_MII_RESET;
+			port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+		}
+/*
+ * THa  2015/10/07
+ * The S2 chip has a bug that writing to the 0xN13E register will cause the
+ * 100Mbit link to be unstable.
+ * One way to workaround is to use 25 MHz clock speed for register access.
+ */
+#if 0
+		/* PHY interrupt level is not in normal position. */
+		for (p = 0; p < sw->phy_port_cnt; p++) {
+			u16 val;
+
+			port_r16(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, &val);
+			val |= PORT_REG_CLK_SPEED_25_MHZ;
+			port_w16(sw, p, REG_PORT_PHY_DIGITAL_DEBUG_1, val);
+			delay_milli(1);
+
+			port_r16(sw, p, REG_PORT_PHY_PHY_CTRL, &val);
+
+			val |= PORT_INT_PIN_HIGH;
+/*
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+			port_w16(sw, p, REG_PORT_PHY_PHY_CTRL, val);
+		}
+#endif
+		for (p = sw->phy_port_cnt; p < sw->mib_port_cnt; p++) {
+			q = p;
+			p = chk_last_port(sw, p);
+			port_r(sw, p, REG_PORT_XMII_CTRL_1, &byte_before);
+			byte_after = byte_before ^(PORT_MII_NOT_1GBIT |
+				PORT_MII_MAC_MODE | PORT_MII_SEL_M);
+			port_w(sw, p, REG_PORT_XMII_CTRL_1, byte_after);
+			port_w(sw, p, REG_PORT_XMII_CTRL_1, byte_before);
+			port_w16(sw, p, REG_PORT_XMII_CTRL_0,
+				sw->cached.xmii[q - sw->phy_port_cnt]);
+		}
+		sw_reset_acl(sw);
+		sw->overrides &= ~TAIL_TAGGING;
+		sw->overrides &= ~PTP_TAG;
+		sw->overrides &= ~TAG_REMOVE;
+		sw_dis_intr(sw);
+		return;
+	}
+
+	/* There is no global reset function yet. */
+	sw_dis_vlan(sw);
+	do {
+		int p;
+
+		/* Need to turn on ACL to write to ACL table. */
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			p = chk_last_port(sw, p);
+			port_cfg_acl(sw, p, 1);
+			port_w16(sw, p, REG_PORT_ACL_BYTE_EN_MSB, 0xffff);
+		}
+		sw_reset_acl_hw(sw);
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (skip_host_port(sw, p))
+				continue;
+			if (sw->dev_count > 1)
+				sw_cfg_port_base_vlan(sw, p, sw->PORT_MASK);
+			port_set_stp_state(sw, p, STP_STATE_FORWARDING);
+		}
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			p = chk_last_port(sw, p);
+			sw_cfg_def_vid(sw, p, 1);
+			port_cfg_tail_tag(sw, p, 0);
+			port_set_authen_mode(sw, p, 0);
+/*
+ * THa  2014/06/18
+ * Cannot read ACL in next bootup if there are lots of traffic going through
+ * the port.
+ */
+			if (sw->features & ACL_CORRUPT_BUG)
+				port_cfg_acl(sw, p, 1);
+			else
+				port_cfg_acl(sw, p, 0);
+		}
+	} while (0);
+	sw->overrides &= ~TAIL_TAGGING;
+	sw->overrides &= ~PTP_TAG;
+	sw->overrides &= ~TAG_REMOVE;
+}  /* sw_reset */
+
+static int sw_chk_reg(struct ksz_sw *sw, u32 reg, size_t count)
+{
+	size_t i;
+
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE) {
+		if (!check_sw_reg_range(reg))
+			return false;
+	}
+	return true;
+}
+
+static int sw_reg_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	if (sw_chk_reg(sw, reg, count)) {
+		sw_r(sw, reg, buf, count);
+		return count;
+	}
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		*addr = 0;
+		if (check_sw_reg_range(reg))
+			*addr = SW_R(sw, reg);
+	}
+	return i;
+}
+
+static int sw_reg_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	if (sw_chk_reg(sw, reg, count)) {
+		sw_w(sw, reg, buf, count);
+		return count;
+	}
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		if (check_sw_reg_range(reg))
+			SW_W(sw, reg, *addr);
+	}
+	return i;
+}
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.lock			= sw_lock,
+	.unlock			= sw_unlock,
+
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r24			= sw_r24,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w24			= sw_w24,
+	.w32			= sw_w32,
+
+	.r			= sw_r,
+	.w			= sw_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+
+	.r_dyn_mac_hw		= sw_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= sw_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= sw_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= sw_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= sw_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= sw_r_sta_mac_hw,
+	.w_sta_mac_hw		= sw_w_sta_mac_hw,
+	.r_vlan_hw		= sw_r_vlan_hw,
+	.w_vlan_hw		= sw_w_vlan_hw,
+	.r_mib_cnt_hw		= sw_r_mib_cnt_hw,
+	.r_acl_hw		= sw_r_acl_hw,
+	.w_acl_hw		= sw_w_acl_hw,
+
+#ifdef CONFIG_KSZ_HSR
+	.r_hsr_hw		= sw_r_hsr_hw,
+	.w_hsr_hw		= sw_w_hsr_hw,
+	.start_hsr_hw		= sw_start_hsr_hw,
+	.g_hsr_hw		= sw_g_hsr_hw,
+	.stop_hsr_hw		= sw_stop_hsr_hw,
+#endif
+};
+
+#ifdef CONFIG_KSZ_IBA
+/**
+ * sw_set_spi - use SPI for access
+ * @sw:		The switch instance.
+ * @iba:	The IBA instance.
+ *
+ * This routine uses default hardware access like SPI for register access.
+ */
+static void sw_set_spi(struct ksz_sw *sw, struct ksz_iba_info *iba)
+{
+	sw->reg = &sw_reg_ops;
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg = &ptp_reg_ops;
+	}
+#endif
+	iba->use_iba = 0;
+}  /* sw_set_spi */
+
+/**
+ * sw_set_ops - try to use SPI for access
+ * @sw:		The switch instance.
+ * @iba:	The IBA instance.
+ *
+ * This routine tries to use IBA for register access.
+ */
+static void sw_set_ops(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_ops);
+	struct ksz_iba_info *iba = &sw->info->iba;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = NULL;
+#endif
+
+	if (sw->reg == &sw_iba_ops)
+		return;
+
+	/* Catch bug if triggered. */
+	if (!iba->dev) {
+		dbg_msg("No IBA dev\n");
+		return;
+	}
+	if (sw->net_ops->get_ready && !sw->net_ops->get_ready(iba->dev)) {
+		schedule_delayed_work(&sw->set_ops, 1);
+		return;
+	}
+	if (sw->HOST_PORT < sw->phy_port_cnt &&
+	    sw->port_info[sw->HOST_PORT].state != media_connected) {
+		schedule_delayed_work(&sw->set_ops, 1);
+		return;
+	}
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		ptp = &sw->ptp_hw;
+#endif
+	mutex_lock(&sw->lock);
+	mutex_lock(sw->hwlock);
+	mutex_lock(sw->reglock);
+	if (netif_running(iba->dev)) {
+#ifdef CONFIG_1588_PTP
+		if (ptp)
+			ptp->reg = &ptp_iba_ops;
+#endif
+		sw->reg = &sw_iba_ops;
+		iba->cnt = 0;
+		iba->use_iba = 1;
+	}
+	if (iba->use_iba) {
+		u32 id;
+
+		sw->intr_using += 3;
+		iba->use_iba |= 0x80;
+		id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+		iba->use_iba &= ~0x80;
+dbg_msg("id = %08x\n", id);
+#if 1
+/*
+ * THa  2016/01/03
+ * KSZ9563 S1 does not respond the very first time when using RGMII.
+ */
+		if (id == 0xdeadbeaf && !(sw->features & NEW_CAP)) {
+			id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+dbg_msg("id = %08x\n", id);
+		}
+#endif
+		sw->intr_using -= 3;
+		if (id == 0xdeadbeaf)
+			sw_set_spi(sw, iba);
+	}
+	mutex_unlock(sw->reglock);
+	mutex_unlock(sw->hwlock);
+	mutex_unlock(&sw->lock);
+
+	if (!iba->use_iba)
+		return;
+printk(KERN_INFO "Using IBA\n");
+#if 1
+/*
+ * THa  2014/06/25
+ * SPI cannot just read PHY interrupt status register to clear interrupts.
+ *
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+	if (!(sw->features & NEW_CAP)) {
+		uint port;
+		u8 val = LINK_DOWN_INT | LINK_UP_INT;
+
+		sw->ops->acquire(sw);
+		for (port = 0; port < sw->phy_port_cnt; port++) {
+			port_w8(sw, port, REG_PORT_PHY_INT_ENABLE, val);
+		}
+		sw->ops->release(sw);
+	}
+#endif
+}  /* sw_set_ops */
+
+/**
+ * sw_set_dev - try to use SPI for access
+ * @sw:		The switch instance.
+ * @dev:	The network device.
+ * @mac_addr:	The MAC address used.
+ *
+ * This routine setup the IBA with the network device and its MAC address if
+ * the device exists.  Otherwise it uses the default access like SPI.
+ */
+static void sw_set_dev(struct ksz_sw *sw, struct net_device *dev, u8 *mac_addr)
+{
+	struct ksz_iba_info *iba = &sw->info->iba;
+	int delay_tick = 2;
+
+	if (!iba->use_iba && sw->HOST_PORT < sw->phy_port_cnt)
+		delay_tick = 10;
+	if (!dev) {
+		if (sw->reg != &sw_reg_ops) {
+#ifdef CONFIG_1588_PTP
+			struct ptp_info *ptp = NULL;
+
+			if (sw->features & PTP_HW)
+				ptp = &sw->ptp_hw;
+#endif
+			mutex_lock(&sw->lock);
+			mutex_lock(sw->hwlock);
+			mutex_lock(sw->reglock);
+			sw_set_spi(sw, iba);
+			mutex_unlock(sw->reglock);
+			mutex_unlock(sw->hwlock);
+			mutex_unlock(&sw->lock);
+		}
+	} else if (sw->features & IBA_SUPPORT)
+		schedule_delayed_work(&sw->set_ops, delay_tick);
+	mutex_lock(sw->hwlock);
+	iba->dev = dev;
+	prepare_iba(iba, iba->dst, mac_addr);
+	mutex_unlock(sw->hwlock);
+}  /* sw_set_dev */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void sw_set_mrp(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_mrp);
+
+	if (sw->HOST_PORT < sw->phy_port_cnt &&
+	    !netif_carrier_ok(sw->main_dev)) {
+		schedule_delayed_work(&sw->set_mrp, 50);
+		return;
+	}
+
+	mrp_start(&sw->mrp);
+}  /* sw_set_mrp */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Microchip LinkMD routines
+ */
+
+enum {
+	CABLE_UNKNOWN,
+	CABLE_GOOD,
+	CABLE_CROSSED,
+	CABLE_REVERSED,
+	CABLE_CROSSED_REVERSED,
+	CABLE_OPEN,
+	CABLE_SHORT
+};
+
+#define STATUS_FULL_DUPLEX		0x01
+#define STATUS_CROSSOVER		0x02
+#define STATUS_REVERSED			0x04
+
+#define LINK_10MBPS_FULL		0x00000001
+#define LINK_10MBPS_HALF		0x00000002
+#define LINK_100MBPS_FULL		0x00000004
+#define LINK_100MBPS_HALF		0x00000008
+#define LINK_1GBPS_FULL			0x00000010
+#define LINK_1GBPS_HALF			0x00000020
+#define LINK_10GBPS_FULL		0x00000040
+#define LINK_10GBPS_HALF		0x00000080
+#define LINK_SYM_PAUSE			0x00000100
+#define LINK_ASYM_PAUSE			0x00000200
+
+#define LINK_AUTO_MDIX			0x00010000
+#define LINK_MDIX			0x00020000
+#define LINK_AUTO_POLARITY		0x00040000
+
+#define CABLE_LEN_MAXIMUM		15000
+#define CABLE_LEN_MULTIPLIER		8
+
+#define PHY_RESET_TIMEOUT		10
+
+/**
+ * sw_get_link_md -
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to get the LinkMD status.
+ */
+static void sw_get_link_md(struct ksz_sw *sw, uint port)
+{
+	u16 ctrl;
+	u16 data;
+	u16 giga;
+	u16 link;
+	u16 len;
+	int i;
+	int timeout;
+	struct ksz_port_info *port_info = &sw->port_info[port];
+
+	port_r16(sw, port, P_LINK_STATUS, &link);
+
+	/* Read second time in case the status is not latched. */
+	if (!(link & PORT_LINK_STATUS))
+		port_r16(sw, port, P_LINK_STATUS, &link);
+	port_r16(sw, port, REG_PORT_PHY_DIGITAL_STATUS, &data);
+	port_info->status[0] = CABLE_UNKNOWN;
+	if (link & PORT_LINK_STATUS) {
+		int stat = 0;
+
+		port_info->status[0] = CABLE_GOOD;
+		port_info->length[0] = 1;
+		port_info->status[1] = CABLE_GOOD;
+		port_info->length[1] = 1;
+		port_info->status[2] = CABLE_GOOD;
+		port_info->length[2] = 1;
+		port_info->status[3] = CABLE_GOOD;
+		port_info->length[3] = 1;
+		port_info->status[4] = CABLE_GOOD;
+		port_info->length[4] = 1;
+
+		if (!(data & PORT_PHY_STAT_MDI))
+			stat |= STATUS_CROSSOVER;
+#if 0
+		if (data & PORT_REVERSED_POLARITY)
+			stat |= STATUS_REVERSED;
+#endif
+		if ((stat & (STATUS_CROSSOVER | STATUS_REVERSED)) ==
+				(STATUS_CROSSOVER | STATUS_REVERSED))
+			port_info->status[0] = CABLE_CROSSED_REVERSED;
+		else if ((stat & STATUS_CROSSOVER) == STATUS_CROSSOVER)
+			port_info->status[0] = CABLE_CROSSED;
+		else if ((stat & STATUS_REVERSED) == STATUS_REVERSED)
+			port_info->status[0] = CABLE_REVERSED;
+		return;
+	}
+
+	/* Put in 1000 Mbps mode. */
+	port_r16(sw, port, P_PHY_CTRL, &ctrl);
+	data = PORT_FULL_DUPLEX | PORT_SPEED_1000MBIT;
+	port_w16(sw, port, P_PHY_CTRL, data);
+	port_r16(sw, port, REG_PORT_PHY_1000_CTRL, &giga);
+	data = PORT_AUTO_NEG_MANUAL;
+	port_w16(sw, port, REG_PORT_PHY_1000_CTRL, data);
+
+	port_r16(sw, port, REG_PORT_PHY_LINK_MD, &data);
+
+	/* Disable transmitter. */
+	data |= PORT_TX_DISABLE;
+	port_w16(sw, port, REG_PORT_PHY_LINK_MD, data);
+
+	/* Wait at most 1 second.*/
+	delay_milli(100);
+
+	/* Enable transmitter. */
+	data &= ~PORT_TX_DISABLE;
+	port_w16(sw, port, REG_PORT_PHY_LINK_MD, data);
+
+	for (i = 1; i <= 4; i++) {
+
+		/* Start cable diagnostic test. */
+		data |= PORT_START_CABLE_DIAG;
+		data |= (i - 1) << PORT_CABLE_DIAG_PAIR_S;
+		port_w16(sw, port, REG_PORT_PHY_LINK_MD, data);
+		timeout = PHY_RESET_TIMEOUT;
+		do {
+			if (!--timeout)
+				break;
+			delay_milli(10);
+			port_r16(sw, port, REG_PORT_PHY_LINK_MD, &data);
+		} while ((data & PORT_START_CABLE_DIAG));
+
+		port_info->length[i] = 0;
+		port_info->status[i] = CABLE_UNKNOWN;
+
+		if (!(data & PORT_START_CABLE_DIAG)) {
+			len = data & PORT_CABLE_FAULT_COUNTER;
+			if (len >= 0x22)
+				len -= 0x22;
+			else
+				len = 0;
+			len *= CABLE_LEN_MULTIPLIER;
+			len += 5;
+			len /= 10;
+			port_info->length[i] = len;
+			data >>= PORT_CABLE_DIAG_RESULT_S;
+			data &= PORT_CABLE_DIAG_RESULT_M;
+			switch (data) {
+			case PORT_CABLE_STAT_NORMAL:
+				port_info->status[i] = CABLE_GOOD;
+				port_info->length[i] = 1;
+				break;
+			case PORT_CABLE_STAT_OPEN:
+				port_info->status[i] = CABLE_OPEN;
+				break;
+			case PORT_CABLE_STAT_SHORT:
+				port_info->status[i] = CABLE_SHORT;
+				break;
+			}
+		}
+	}
+
+	port_w16(sw, port, REG_PORT_PHY_1000_CTRL, giga);
+	port_w16(sw, port, P_PHY_CTRL, ctrl);
+	if (ctrl & PORT_AUTO_NEG_ENABLE) {
+		ctrl |= PORT_AUTO_NEG_RESTART;
+		port_w16(sw, port, P_NEG_RESTART_CTRL, ctrl);
+	}
+
+	port_info->length[0] = port_info->length[1];
+	port_info->status[0] = port_info->status[1];
+	for (i = 2; i < 5; i++) {
+		if (CABLE_GOOD == port_info->status[0]) {
+			if (port_info->status[i] != CABLE_GOOD) {
+				port_info->status[0] = port_info->status[i];
+				port_info->length[0] = port_info->length[i];
+				break;
+			}
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void get_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	u64 *counter)
+{
+	int i;
+	int mib;
+	uint port;
+	struct ksz_port_mib *port_mib;
+
+	memset(counter, 0, sizeof(u64) * TOTAL_SWITCH_COUNTER_NUM);
+	for (i = 0, port = first; i < cnt; i++, port++) {
+		if (cnt > 1 && port == sw->HOST_PORT)
+			continue;
+		port_mib = &sw->port_mib[port];
+		for (mib = port_mib->mib_start; mib < sw->mib_cnt; mib++)
+			counter[mib] += port_mib->counter[mib];
+	}
+}
+
+static struct {
+	int rx;
+	int tx;
+} mib_display[TOTAL_SWITCH_COUNTER_NUM / 2] = {
+	{ MIB_RX_TOTAL, MIB_TX_TOTAL },
+	{ MIB_RX_HI_PRIO, MIB_TX_HI_PRIO },
+	{ MIB_RX_PAUSE, MIB_TX_PAUSE },
+	{ MIB_RX_BROADCAST, MIB_TX_BROADCAST },
+	{ MIB_RX_MULTICAST, MIB_TX_MULTICAST },
+	{ MIB_RX_UNICAST, MIB_TX_UNICAST },
+	{ MIB_RX_DROPS, MIB_TX_DROPS },
+	{ MIB_RX_OCTET_64, MIB_RX_OCTET_65_127 },
+	{ MIB_RX_OCTET_128_255, MIB_RX_OCTET_256_511 },
+	{ MIB_RX_OCTET_512_1023, MIB_RX_OCTET_1024_1522 },
+	{ MIB_RX_OCTET_1523_2000, MIB_RX_OCTET_2001 },
+	{ MIB_RX_UNDERSIZE, MIB_RX_OVERSIZE },
+	{ MIB_RX_FRAGMENT, MIB_RX_JABBER },
+	{ MIB_RX_SYMBOL_ERR, MIB_RX_CRC_ERR },
+	{ MIB_RX_ALIGNMENT_ERR, MIB_RX_CTRL_8808 },
+	{ MIB_TX_LATE_COLLISION, MIB_TX_DEFERRED },
+	{ MIB_TX_TOTAL_COLLISION, MIB_TX_EXCESS_COLLISION },
+	{ MIB_TX_SINGLE_COLLISION, MIB_TX_MULTI_COLLISION },
+};
+
+static int display_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	char *buf)
+{
+	int mib;
+	int n;
+	int len = 0;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	get_sw_mib_counters(sw, first, cnt, counter);
+	for (mib = 0; mib < TOTAL_SWITCH_COUNTER_NUM / 2; mib++) {
+		int rx = mib_display[mib].rx;
+		int tx = mib_display[mib].tx;
+		if (buf)
+			len += sprintf(buf + len,
+				"%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+		else
+			printk(KERN_INFO "%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+	}
+	for (n = 0, mib = first; n < cnt; n++, mib++) {
+		int j;
+
+		for (j = 0; j < 2; j++) {
+			if (sw->port_mib[mib].rate[j].peak) {
+				u32 num;
+				u32 frac;
+
+				num = sw->port_mib[mib].rate[j].peak / 10;
+				frac = sw->port_mib[mib].rate[j].peak % 10;
+				if (buf)
+					len += sprintf(buf + len,
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				else
+					printk(KERN_INFO
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				sw->port_mib[mib].rate[j].peak = 0;
+			}
+		}
+	}
+	return len;
+}  /* display_sw_mib_counters */
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_sw_info(int cnt, char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	if (TOTAL_PORT_NUM != cnt)
+		return len;
+
+	len += sprintf(buf + len, "\ndynamic_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's dynamic MAC table\n");
+	len += sprintf(buf + len, "static_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's static MAC table\n");
+	len += sprintf(buf + len, "vlan_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's VLAN table\n");
+
+	len += sprintf(buf + len, "\naging:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable aging\n");
+	len += sprintf(buf + len, "fast_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable fast aging\n");
+	len += sprintf(buf + len, "link_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable link change auto aging\n");
+
+	len += sprintf(buf + len, "\nbcast_per:\t");
+	len += sprintf(buf + len,
+		"set broadcast storm percentage\n");
+	len += sprintf(buf + len, "mcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable multicast storm protection\n");
+	len += sprintf(buf + len, "diffserv_map:\t");
+	len += sprintf(buf + len,
+		"set DiffServ value.  Use \"decimal=hexadecimal\" format\n");
+	len += sprintf(buf + len, "p_802_1p_map:\t");
+	len += sprintf(buf + len,
+		"set 802.1p value.  Use \"decimal=hexadecimal\" format\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nvlan:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1Q VLAN\n");
+	len += sprintf(buf + len, "null_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to replace null vid\n");
+	len += sprintf(buf + len, "macaddr:\t");
+	len += sprintf(buf + len,
+		"set switch MAC address\n");
+	len += sprintf(buf + len, "mirror_mode:\t");
+	len += sprintf(buf + len,
+		"set to 1 to use mirror rx AND tx mode\n");
+
+	len += sprintf(buf + len, "\nigmp_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IGMP snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_option:");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD option snooping\n");
+
+	len += sprintf(buf + len, "\naggr_backoff:\t");
+	len += sprintf(buf + len,
+		"disable/enable aggressive backoff in half-duplex mode\n");
+	len += sprintf(buf + len, "no_exc_drop:\t");
+	len += sprintf(buf + len,
+		"disable/enable no excessive collision drop\n");
+	len += sprintf(buf + len, "buf_reserve:\t");
+	len += sprintf(buf + len,
+		"disable/enable buffer reserve\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nhuge_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable huge packet support\n");
+	len += sprintf(buf + len, "legal_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet\n");
+	len += sprintf(buf + len, "length_check:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet length check\n");
+
+	len += sprintf(buf + len, "\nback_pressure:\t");
+	len += sprintf(buf + len,
+		"set back pressure mode\n");
+	len += sprintf(buf + len, "sw_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port flow control\n");
+	len += sprintf(buf + len, "sw_half_duplex:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port half-duplex mode\n");
+#ifdef SWITCH_10_MBIT
+	len += sprintf(buf + len, "sw_10_mbit:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port 10Mbit mode\n");
+#endif
+	len += sprintf(buf + len, "rx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable receive flow control\n");
+	len += sprintf(buf + len, "tx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable transmit flow control\n");
+	len += sprintf(buf + len, "fair_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable fair flow control mode\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vlan_bound:\t");
+	len += sprintf(buf + len,
+		"disable/enable unicast VLAN boundary\n");
+
+	len += sprintf(buf + len, "pass_pause:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass PAUSE frames for debugging\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nswitch port settings:\n");
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len,
+		"display the port's duplex setting\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"display the port's link speed\n");
+	len += sprintf(buf + len, "linkmd:\t\t");
+	len += sprintf(buf + len,
+		"write to start LinkMD test.  read for result\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the port's MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set default VID value\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set VLAN membership\n");
+
+	len += sprintf(buf + len, "bcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable/enable broadcast storm protection\n");
+	len += sprintf(buf + len, "diffserv:\t");
+	len += sprintf(buf + len,
+		"disable/enable DiffServ priority\n");
+	len += sprintf(buf + len, "p_802_1p:\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1p priority\n");
+
+	len += sprintf(buf + len, "port_based:\t");
+	len += sprintf(buf + len,
+		"disable/enable port-based priority\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "prio_queue:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue\n");
+	len += sprintf(buf + len, "tx_p0_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 control\n");
+	len += sprintf(buf + len, "tx_p1_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 control\n");
+	len += sprintf(buf + len, "tx_p2_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 control\n");
+	len += sprintf(buf + len, "tx_p3_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 control\n");
+	len += sprintf(buf + len, "tx_p0_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 ratio\n");
+	len += sprintf(buf + len, "tx_p1_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 ratio\n");
+	len += sprintf(buf + len, "tx_p2_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 ratio\n");
+	len += sprintf(buf + len, "tx_p3_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 ratio\n");
+	len += sprintf(buf + len, "prio_rate:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue rate limiting\n");
+	len += sprintf(buf + len, "rx_limit:\t");
+	len += sprintf(buf + len,
+		"set rx rate limiting mode\n");
+	len += sprintf(buf + len, "cnt_ifg:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count IPG\n");
+	len += sprintf(buf + len, "cnt_pre:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count preamble\n");
+	len += sprintf(buf + len, "rx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 3 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 3 rate in 64Kbps unit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\npass_all:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass all frames for debugging\n");
+	len += sprintf(buf + len, "tail_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable tail tagging\n");
+	len += sprintf(buf + len, "rx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable rx\n");
+	len += sprintf(buf + len, "tx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable tx\n");
+	len += sprintf(buf + len, "learn:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable learning\n");
+
+	len += sprintf(buf + len, "mirror_port:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror port\n");
+	len += sprintf(buf + len, "mirror_rx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror receive\n");
+	len += sprintf(buf + len, "mirror_tx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror transmit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nnon_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to discard non-VID packets\n");
+	len += sprintf(buf + len, "ingress:\t");
+	len += sprintf(buf + len,
+		"disable/enable ingress VLAN filtering\n");
+	len += sprintf(buf + len, "drop_tagged:\t");
+	len += sprintf(buf + len,
+		"disable/enable drop tagged packet feature\n");
+	len += sprintf(buf + len, "replace_prio:\t");
+	len += sprintf(buf + len,
+		"disable/enable replace 802.1p priority feature\n");
+	len += sprintf(buf + len, "back_pressure:\t");
+	len += sprintf(buf + len,
+		"disable/enable back pressure in half-duplex mode\n");
+	len += sprintf(buf + len, "force_flow_ctrl:");
+	len += sprintf(buf + len,
+		"set to 1 to force flow control\n");
+	len += sprintf(buf + len, "\nmacaddr:\t");
+	len += sprintf(buf + len,
+		"set port MAC address\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nstatic MAC table:\n");
+	len += sprintf(buf + len, "addr:\t\t");
+	len += sprintf(buf + len,
+		"set MAC address\n");
+	len += sprintf(buf + len, "ports:\t\t");
+	len += sprintf(buf + len,
+		"set destination ports\n");
+	len += sprintf(buf + len, "override:\t");
+	len += sprintf(buf + len,
+		"set override bit\n");
+	len += sprintf(buf + len, "use_fid:\t");
+	len += sprintf(buf + len,
+		"set use FID bit\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	len += sprintf(buf + len, "\nVLAN table:\n");
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set VID\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set membership\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	return len;
+}  /* display_sw_info */
+
+static ssize_t sysfs_sw_read(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, ssize_t len, char *buf)
+{
+	int i;
+	int j;
+	u16 map;
+	struct ksz_sw_info *info = sw->info;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_sw_info(sw->mib_port_cnt, buf, len);
+		break;
+	case PROC_SW_VERSION:
+		len += sprintf(buf + len, "%s  %s\n",
+			SW_DRV_VERSION, SW_DRV_RELDATE);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->duplex);
+		if (media_connected == port->linked->state) {
+			if (1 == port->linked->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port->linked->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->speed);
+		if (media_connected == port->linked->state)
+			len += sprintf(buf + len, "%u\n",
+				port->linked->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u\n", port->force_link);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->flow_ctrl);
+		switch (port->flow_ctrl) {
+		case PHY_FLOW_CTRL:
+			len += sprintf(buf + len, "flow control\n");
+			break;
+		case PHY_TX_ONLY:
+			len += sprintf(buf + len, "tx only\n");
+			break;
+		case PHY_RX_ONLY:
+			len += sprintf(buf + len, "rx only\n");
+			break;
+		default:
+			len += sprintf(buf + len, "no flow control\n");
+			break;
+		}
+		break;
+	case PROC_SET_SW_MIB:
+		if (!port)
+			break;
+		len += display_sw_mib_counters(sw, port->first_port,
+			port->mib_port_cnt, buf + len);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		len += sprintf(buf + len, "%u%%\n", info->broad_per);
+		break;
+	case PROC_SET_DIFFSERV:
+		for (i = 0; i < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * KS_PRIO_IN_REG);
+			map = info->diffserv[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->diffserv[i]);
+		}
+		break;
+	case PROC_SET_802_1P:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * KS_PRIO_IN_REG);
+			map = info->p_802_1p[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->p_802_1p[i]);
+		}
+		break;
+	case PROC_SET_SW_VID:
+		len += sprintf(buf + len, "0x%04x\n", sw->vid);
+		break;
+	case PROC_GET_HOST_PORT:
+		len += sprintf(buf + len, "%u\n", sw->HOST_PORT + 1);
+		break;
+	case PROC_GET_PORTS:
+	{
+		uint ports = sw->mib_port_cnt;
+
+#if 0
+		if (sw->eth_cnt > 1)
+			ports = sw->eth_maps[0].cnt + 1;
+		if (sw->features & HSR_HW)
+			ports = 3;
+#else
+		if (1 == sw->eth_cnt)
+			ports = sw->eth_maps[0].cnt + 1;
+#endif
+		len += sprintf(buf + len, "%u\n", ports);
+		break;
+	}
+	case PROC_GET_DEV_START:
+	{
+		int start = 0;
+
+		if (sw->dev_offset)
+			start = 100;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_VLAN_START:
+	{
+		int start = 0;
+
+		if (sw->features & VLAN_PORT)
+			start = VLAN_PORT_START;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_AVB:
+#if 1
+		len += sprintf(buf + len, "%u\n",
+			!!(sw->features & AVB_SUPPORT));
+#else
+		len += sprintf(buf + len, "0\n");
+#endif
+		break;
+	case PROC_GET_STP:
+#if 0
+		len += sprintf(buf + len, "%u\n",
+			!!(sw->features & STP_SUPPORT));
+#else
+		len += sprintf(buf + len, "0\n");
+#endif
+		break;
+	case PROC_GET_TWO_DEV:
+		i = 0;
+		if (2 == sw->dev_count && (sw->features & SW_VLAN_DEV))
+			i = 1;
+		len += sprintf(buf + len, "%d\n", i);
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", sw->features);
+		len += sprintf(buf + len, "\t%08x = STP support\n",
+			STP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port forwarding\n",
+			VLAN_PORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port remove tag\n",
+			VLAN_PORT_REMOVE_TAG);
+		len += sprintf(buf + len, "\t%08x = VLAN port tag tailing\n",
+			VLAN_PORT_TAGGING);
+		len += sprintf(buf + len, "\t%08x = VLAN dev forwarding\n",
+			SW_VLAN_DEV);
+		len += sprintf(buf + len, "\t%08x = MRP support\n",
+			MRP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = Gigabit support\n",
+			GIGABIT_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = IBA support\n",
+			IBA_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = new capabilities\n",
+			NEW_CAP);
+		len += sprintf(buf + len, "\t%08x = AVB support\n",
+			AVB_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = Redundancy support\n",
+			REDUNDANCY_SUPPORT);
+#ifdef CONFIG_KSZ_DLR
+		len += sprintf(buf + len, "\t%08x = DLR support\n",
+			DLR_HW);
+#endif
+#ifdef CONFIG_KSZ_HSR
+		len += sprintf(buf + len, "\t%08x = HSR support\n",
+			HSR_HW);
+		len += sprintf(buf + len, "\t%08x = HSR RedBox support\n",
+			HSR_REDBOX);
+#endif
+		len += sprintf(buf + len, "\t%08x = DSA support\n",
+			DSA_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = different MAC addresses\n",
+			DIFF_MAC_ADDR);
+		len += sprintf(buf + len, "\t%08x = QuietWire\n",
+			QW_HW);
+#ifdef CONFIG_1588_PTP
+		len += sprintf(buf + len, "\t%08x = 1588 PTP\n",
+			PTP_HW);
+#endif
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", sw->overrides);
+		len += sprintf(buf + len, "\t%08x = flow control\n",
+			PAUSE_FLOW_CTRL);
+		len += sprintf(buf + len, "\t%08x = fast aging\n",
+			FAST_AGING);
+		len += sprintf(buf + len, "\t%08x = have >2 ports\n",
+			HAVE_MORE_THAN_2_PORTS);
+		len += sprintf(buf + len, "\t%08x = unknown mcast blocked\n",
+			UNK_MCAST_BLOCK);
+#ifdef CONFIG_KSZ_IBA
+		len += sprintf(buf + len, "\t%08x = IBA test\n",
+			IBA_TEST);
+#endif
+		len += sprintf(buf + len, "\t%08x = ACL intr monitor\n",
+			ACL_INTR_MONITOR);
+		len += sprintf(buf + len, "\t%08x = 802.1X Authentication\n",
+			USE_802_1X_AUTH);
+		len += sprintf(buf + len, "\t%08x = ptp tag\n",
+			PTP_TAG);
+		len += sprintf(buf + len, "\t%08x = tag is removed\n",
+			TAG_REMOVE);
+		len += sprintf(buf + len, "\t%08x = tail tagging\n",
+			TAIL_TAGGING);
+		break;
+	case PROC_SET_AUTHEN:
+		len += sprintf(buf + len, "%u\n",
+			!!(sw->overrides & USE_802_1X_AUTH));
+		break;
+	case PROC_DYNAMIC:
+		len = sw_d_dyn_mac_table(sw, buf, len);
+		break;
+	case PROC_STATIC:
+		len = sw_d_sta_mac_table(sw, buf, len);
+		len = sw_d_mac_table(sw, buf, len);
+		break;
+	case PROC_VLAN:
+		len = sw_d_vlan_table(sw, buf, len);
+		break;
+	case PROC_HSR:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW)
+			len = sw_d_hsr_table(sw, buf, len);
+#endif
+		break;
+	}
+	return len;
+}  /* sysfs_sw_read */
+
+static ssize_t sysfs_sw_read_hw(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	u8 data[8];
+	u32 val;
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	switch (proc_num) {
+	case PROC_SET_AGING:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_1, SW_AGING_ENABLE);
+		break;
+	case PROC_SET_FAST_AGING:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING);
+		break;
+	case PROC_SET_LINK_AGING:
+		chk = sw_chk(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		chk = !sw_chk(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_5,
+			SW_OUT_RATE_LIMIT_QUEUE_BASED);
+		break;
+	case PROC_ENABLE_VLAN:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID);
+		break;
+	case PROC_SET_DROP_INVALID_VID:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_0, SW_DROP_INVALID_VID);
+		break;
+	case PROC_SET_MAC_ADDR:
+		sw_get_addr(sw, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X\n",
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_MIRROR_MODE:
+		chk = sw_chk_mirror_rx_tx(sw);
+		if (sw->verbose) {
+			if (chk)
+				strcpy(note, " (rx and tx)");
+			else
+				strcpy(note, " (rx or tx)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP);
+		break;
+#ifdef SW_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IPV6_MLD_SNOOP);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IPV6_MLD_OPTION);
+		break;
+#endif
+	case PROC_SET_AGGR_BACKOFF:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_0, SW_AGGR_BACKOFF);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, NO_EXC_COLLISION_DROP);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		val = sw->reg->r32(sw, REG_SW_QM_CTRL__4);
+		chk = !!(val & UNICAST_VLAN_BOUNDARY);
+		break;
+	case PROC_SET_DOUBLE_TAG:
+		chk = sw_chk(sw, REG_SW_OPERATION, SW_DOUBLE_TAG);
+		break;
+	case PROC_SET_ISP_TAG:
+		chk = sw->reg->r16(sw, REG_SW_ISP_TPID__2);
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_HSR_TAG:
+		chk = sw->reg->r16(sw, REG_SW_HSR_TPID__2);
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_HSR_REDBOX_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			chk = hsr->ops->get_redbox_id(hsr);
+		}
+#endif
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_HSR_NET_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			chk = hsr->ops->get_net_id(hsr);
+		}
+#endif
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_MTU:
+		chk = sw->reg->r16(sw, REG_SW_MTU__2);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		chk = !!(val & SW_UNK_UCAST_ENABLE);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val &= ~SW_UNK_UCAST_ENABLE;
+		chk = val;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		chk = !!(val & SW_UNK_MCAST_ENABLE);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val &= ~SW_UNK_MCAST_ENABLE;
+		chk = val;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		chk = !!(val & SW_UNK_VID_ENABLE);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val &= ~SW_UNK_VID_ENABLE;
+		chk = val;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_JUMBO_PACKET:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, SW_JUMBO_PACKET);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		chk = !sw_chk(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, SW_BACK_PRESSURE);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, FAIR_FLOW_CTRL);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_4, SW_PASS_PAUSE);
+		break;
+	case PROC_ENABLE_PME:
+		chk = sw_chk(sw, REG_SW_PME_CTRL, PME_ENABLE);
+		break;
+	case PROC_ENABLE_PME_POLARITY:
+		chk = sw_chk(sw, REG_SW_PME_CTRL, PME_POLARITY);
+		break;
+	case PROC_SET_NO_COLOR:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_NO_COLOR_S);
+		break;
+	case PROC_SET_COLOR_RED:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_RED_COLOR_S);
+		break;
+	case PROC_SET_COLOR_YELLOW:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_YELLOW_COLOR_S);
+		break;
+	case PROC_SET_COLOR_GREEN:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_GREEN_COLOR_S);
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_sw_read_hw */
+
+static int sysfs_sw_write(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, int num, const char *buf)
+{
+	int changes;
+	int count;
+	unsigned int val;
+	u8 data[8];
+	int processed = true;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		sw_init(sw);
+		sw->verbose = !!num;
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		if (num <= 2)
+			port->duplex = (u8) num;
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		if (!(sw->features & GIGABIT_SUPPORT) && num == 1000)
+			break;
+		if (0 == num || 10 == num || 100 == num || 1000 == num)
+			port->speed = (u16) num;
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		port->force_link = (u8) num;
+		if (port->force_link) {
+			port_force_link_speed(port);
+			sw->phy_intr = sw->PORT_MASK;
+			port_get_link_speed(port);
+			sw->phy_intr = 0;
+		} else
+			port_set_link_speed(port);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		if (num <= PHY_RX_ONLY)
+			port->flow_ctrl = (u8) num;
+		break;
+	case PROC_SET_SW_MIB:
+		if (num >= 1 && num <= 2) {
+			sw_freeze_mib(sw, num - 1);
+			break;
+		}
+		for (count = 0; count < sw->mib_port_cnt; count++) {
+			struct ksz_port_mib *mib;
+
+			count = chk_last_port(sw, count);
+			mib = &sw->port_mib[count];
+			memset((void *) mib->counter, 0, sizeof(u64) *
+				TOTAL_SWITCH_COUNTER_NUM);
+			mib->rate[0].last = mib->rate[1].last = 0;
+			mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+			mib->rate[0].peak = mib->rate[1].peak = 0;
+		}
+		break;
+	case PROC_SET_SW_REG:
+		count = sscanf(buf, "%x=%x", (unsigned int *) &num, &val);
+		if (1 == count)
+			printk(KERN_INFO SW_SIZE_STR"\n",
+				SW_R(sw, num));
+		else if (2 == count)
+			SW_W(sw, num, val);
+		break;
+	case PROC_SET_SW_VID:
+		sw->vid = num;
+		break;
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = sw->features ^ num;
+		sw->features = num;
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		sw->overrides = num;
+		break;
+	case PROC_SET_AUTHEN:
+		if (num)
+			sw->overrides |= USE_802_1X_AUTH;
+		else if (sw->overrides & USE_802_1X_AUTH) {
+			struct ksz_acl_table *acl;
+			int acl_on;
+			int acl_rule;
+			uint port;
+
+			for (port = 0; port < sw->mib_port_cnt; port++) {
+				if (port == sw->HOST_PORT)
+					continue;
+				port_set_authen_mode(sw, port,
+					PORT_AUTHEN_PASS);
+				sw_cfg_port_base_vlan(sw, port, sw->PORT_MASK);
+
+				acl_on = port_chk_acl(sw, port);
+				if (!acl_on)
+					port_cfg_acl(sw, port, true);
+				acl_rule = 0;
+				acl = &sw->info->port_cfg[port].
+					acl_info[acl_rule];
+				acl->ruleset = 0;
+				sw->ops->release(sw);
+				sw_w_acl_ruleset(sw, port, acl_rule, acl);
+				sw->ops->acquire(sw);
+				acl_rule++;
+				acl++;
+				acl->ruleset = 0;
+				sw->ops->release(sw);
+				sw_w_acl_ruleset(sw, port, acl_rule, acl);
+				sw->ops->acquire(sw);
+				acl_rule++;
+				acl++;
+				acl->ruleset = 0;
+				sw->ops->release(sw);
+				sw_w_acl_ruleset(sw, port, acl_rule, acl);
+				sw->ops->acquire(sw);
+				if (!acl_on)
+					port_cfg_acl(sw, port, false);
+			}
+			sw->overrides &= ~USE_802_1X_AUTH;
+		}
+		break;
+	case PROC_DYNAMIC:
+		sw_flush_dyn_mac_table(sw, sw->mib_port_cnt);
+		break;
+	case PROC_STATIC:
+		sw_clr_sta_mac_table(sw);
+		break;
+	case PROC_SET_AGING:
+		sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_AGING_ENABLE, num);
+		break;
+	case PROC_SET_FAST_AGING:
+		sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING, num);
+		break;
+	case PROC_SET_LINK_AGING:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING, num);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		hw_cfg_broad_storm(sw, num);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE,
+			!num);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		sw_cfg(sw, REG_SW_MAC_CTRL_5, SW_OUT_RATE_LIMIT_QUEUE_BASED,
+			num);
+		break;
+	case PROC_SET_DIFFSERV:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_tos_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_SET_802_1P:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_802_1p_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_ENABLE_VLAN:
+		if (!num)
+			sw_dis_vlan(sw);
+		else
+			sw_ena_vlan(sw);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID, num);
+		break;
+	case PROC_SET_DROP_INVALID_VID:
+		sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_DROP_INVALID_VID, num);
+		break;
+	case PROC_SET_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			sw_set_addr(sw, data);
+		}
+		break;
+	}
+	case PROC_SET_MIRROR_MODE:
+		sw_cfg_mirror_rx_tx(sw, num);
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP, num);
+		break;
+#ifdef SW_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IPV6_MLD_SNOOP, num);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IPV6_MLD_OPTION, num);
+		break;
+#endif
+	case PROC_SET_AGGR_BACKOFF:
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_AGGR_BACKOFF, num);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, NO_EXC_COLLISION_DROP, num);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		val = sw->reg->r32(sw, REG_SW_QM_CTRL__4);
+		if (num)
+			val |= UNICAST_VLAN_BOUNDARY;
+		else
+			val &= ~UNICAST_VLAN_BOUNDARY;
+		sw->reg->w32(sw, REG_SW_QM_CTRL__4, val);
+		break;
+	case PROC_SET_DOUBLE_TAG:
+		sw_cfg(sw, REG_SW_OPERATION, SW_DOUBLE_TAG, num);
+		break;
+	case PROC_SET_ISP_TAG:
+		sw->reg->w16(sw, REG_SW_ISP_TPID__2, (u16) num);
+		break;
+	case PROC_SET_HSR_TAG:
+		sw->reg->w16(sw, REG_SW_HSR_TPID__2, (u16) num);
+		break;
+	case PROC_SET_HSR_REDBOX_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			if (0 <= num && num <= 7)
+				hsr->ops->set_redbox_id(hsr, (u8) num);
+		}
+#endif
+		break;
+	case PROC_SET_HSR_NET_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			if (0 <= num && num <= 7)
+				hsr->ops->set_net_id(hsr, (u8) num);
+		}
+#endif
+		break;
+	case PROC_SET_MTU:
+		if (2000 <= num && num <= 9000)
+			sw->reg->w16(sw, REG_SW_MTU__2, (u16) num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		sw_cfg(sw, REG_SW_LUE_UNK_UCAST_CTRL__4,
+			SW_UNK_UCAST_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORTS:
+		num &= ~SW_UNK_UCAST_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val &= SW_UNK_UCAST_ENABLE;
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4, val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		sw_cfg(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			SW_UNK_MCAST_ENABLE >> 24, num);
+		if (num) {
+			val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+			val &= ~SW_UNK_MCAST_ENABLE;
+			if (val == sw->HOST_MASK)
+				sw->overrides |= UNK_MCAST_BLOCK;
+		} else {
+			sw->overrides &= ~UNK_MCAST_BLOCK;
+		}
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORTS:
+		num &= ~SW_UNK_MCAST_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val &= SW_UNK_MCAST_ENABLE;
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4, val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		sw_cfg(sw, REG_SW_LUE_UNK_VID_CTRL__4,
+			SW_UNK_VID_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORTS:
+		num &= ~SW_UNK_VID_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val &= SW_UNK_VID_ENABLE;
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_VID_CTRL__4, val);
+		break;
+	case PROC_SET_JUMBO_PACKET:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_JUMBO_PACKET, num);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE,
+			!num);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH, num);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_BACK_PRESSURE, num);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, FAIR_FLOW_CTRL, num);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		sw_cfg(sw, REG_SW_MAC_CTRL_4, SW_PASS_PAUSE, num);
+		break;
+	case PROC_ENABLE_PME:
+		sw_cfg(sw, REG_SW_PME_CTRL, PME_ENABLE, num);
+		break;
+	case PROC_ENABLE_PME_POLARITY:
+		sw_cfg(sw, REG_SW_PME_CTRL, PME_POLARITY, num);
+		break;
+	case PROC_SET_NO_COLOR:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_NO_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_RED:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_RED_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_YELLOW:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_YELLOW_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_GREEN:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_GREEN_COLOR_S, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_sw_write */
+
+static ssize_t sysfs_port_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *port_cfg;
+	struct ksz_port_info *port_info;
+	int i;
+	int j;
+	u32 map;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = chk_last_port(sw, port);
+	port_cfg = &sw->info->port_cfg[port];
+	port_info = &sw->port_info[port];
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+		if (media_connected == port_info->state) {
+			if (1 == port_info->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port_info->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_SPEED:
+		if (media_connected == port_info->state)
+			len += sprintf(buf + len, "%u\n",
+				port_info->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_MAC_OPERATIONAL:
+		chk = !!(sw->dev_ports & (1 << port));
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_VLAN_RESTRICTED:
+		chk = port_cfg->restricted;
+		if (sw->verbose)
+			strcpy(note, " (0 = normal, 1 = restricted)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_VLAN_UNTAGGED:
+	{
+		int index;
+		int bit;
+
+		index = sw->vlan_index / VID_IN_DATA;
+		bit = sw->vlan_index % VID_IN_DATA;
+		chk = !!(port_cfg->untagged[index] & (1 << bit));
+		if (sw->verbose)
+			strcpy(note, " (0 = tagged, 1 = untagged)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+		len += display_sw_mib_counters(sw, port, 1, buf + len);
+		break;
+	case PROC_SET_LINK_MD:
+		len += sprintf(buf + len, "%u:%u %u:%u %u:%u %u:%u %u:%u\n",
+			port_info->length[0], port_info->status[0],
+			port_info->length[1], port_info->status[1],
+			port_info->length[2], port_info->status[2],
+			port_info->length[3], port_info->status[3],
+			port_info->length[4], port_info->status[4]);
+		if (sw->verbose)
+			len += sprintf(buf + len,
+				"(%d=unknown; %d=normal; %d=open; %d=short)\n",
+				CABLE_UNKNOWN, CABLE_GOOD, CABLE_OPEN,
+				CABLE_SHORT);
+		break;
+	case PROC_SET_SQI:
+		chk = port_info->sqi;
+		if (sw->verbose)
+			strcpy(note, " (0 = worst, 15 = best)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_PORT_BASED:
+		chk = port_cfg->port_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_DEF_VID:
+		chk = port_cfg->vid;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MEMBER:
+		chk = port_cfg->member;
+		type = SHOW_HELP_HEX_2;
+		break;
+	case PROC_SET_LIMIT:
+		chk = ((port_cfg->rate_limit >> PORT_IN_LIMIT_MODE_S) &
+			PORT_IN_LIMIT_MODE_M);
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (flooded unicast)");
+				break;
+			case 2:
+				strcpy(note, " (multicast)");
+				break;
+			case 3:
+				strcpy(note, " (broadcast)");
+				break;
+			default:
+				strcpy(note, " (all)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_LIMIT_PORT_BASED:
+		chk = ((port_cfg->rate_limit >> PORT_IN_PORT_BASED_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		chk = ((port_cfg->rate_limit >> PORT_RATE_PACKET_BASED_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_FLOW_CTRL:
+		chk = ((port_cfg->rate_limit >> PORT_IN_FLOW_CTRL_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_CNT_IFG:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_IFG_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_CNT_PRE:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_PREAMBLE_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_RX_P0_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[0] : port_cfg->rx_rate[0],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P1_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[1] : port_cfg->rx_rate[1],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P2_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[2] : port_cfg->rx_rate[2],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P3_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[3] : port_cfg->rx_rate[3],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P4_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[4] : port_cfg->rx_rate[4],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P5_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[5] : port_cfg->rx_rate[5],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P6_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[6] : port_cfg->rx_rate[6],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P7_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[7] : port_cfg->rx_rate[7],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[0] : port_cfg->tx_rate[0],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[1] : port_cfg->tx_rate[1],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[2] : port_cfg->tx_rate[2],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[3] : port_cfg->tx_rate[3],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_COLOR_MAP:
+		for (i = 0; i < DIFFSERV_ENTRIES / 16; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * 16);
+			map = port_cfg->color_map[i];
+			for (j = 0; j < 16; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & POLICE_COLOR_MAP_M);
+				map >>= POLICE_COLOR_MAP_S;
+			}
+			len += sprintf(buf + len, "\t%08x\n",
+				port_cfg->color_map[i]);
+		}
+		break;
+	case PROC_SET_TC_MAP:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / 8; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * 8);
+			map = port_cfg->tc_map[i];
+			for (j = 0; j < 8; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & PORT_TC_MAP_M);
+				map >>= PORT_TC_MAP_S;
+			}
+			len += sprintf(buf + len, "\t%08x\n",
+				port_cfg->tc_map[i]);
+		}
+		break;
+	case PROC_SET_MMD_ID:
+		chk = port_cfg->mmd_id;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_MMD_REG:
+		chk = port_cfg->mmd_reg;
+		type = SHOW_HELP_HEX;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read */
+
+static ssize_t sysfs_port_read_hw(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	u16 val;
+	struct ksz_port_cfg *cfg;
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	port = chk_last_port(sw, port);
+	cfg = &sw->info->port_cfg[port];
+	switch (proc_num) {
+	case PROC_ENABLE_BROADCAST_STORM:
+		chk = port_chk_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		chk = port_chk_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		chk = port_chk_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_VLAN_PRIO:
+		chk = port_chk_vlan_prio(sw, port);
+		break;
+	case PROC_ENABLE_MAC_PRIO:
+		chk = port_chk_mac_prio(sw, port);
+		break;
+	case PROC_ENABLE_ACL_PRIO:
+		chk = port_chk_acl_prio(sw, port);
+		break;
+	case PROC_SET_HIGHEST_PRIO:
+		chk = port_chk_highest_prio(sw, port);
+		break;
+	case PROC_SET_OR_PRIO:
+		chk = port_chk_or_prio(sw, port);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		chk = port_get_prio_queue(sw, port);
+		if (sw->verbose) {
+			if (chk < 3)
+				sprintf(note, " (%u)", (1 << chk));
+			else
+				strcpy(note, " (invalid)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_REPLACE_VID:
+		chk = port_chk32(sw, port, REG_PORT_MTI_QUEUE_CTRL_0__4,
+			MTI_PVID_REPLACE);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		chk = port_chk_replace_prio(sw, port);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		chk = sw_chk_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		chk = sw_chk_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DROP_NON_VLAN:
+		chk = port_chk_drop_non_vlan(sw, port);
+		break;
+	case PROC_SET_DROP_TAG:
+		chk = port_chk_drop_tag(sw, port);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		chk = port_chk_mirror_sniffer(sw, port);
+		break;
+	case PROC_SET_MIRROR_RX:
+		chk = port_chk_mirror_rx(sw, port);
+		break;
+	case PROC_SET_MIRROR_TX:
+		chk = port_chk_mirror_tx(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		chk = port_chk_dis_non_vid(sw, port);
+		break;
+	case PROC_SET_INGRESS:
+		chk = port_chk_in_filter(sw, port);
+		break;
+	case PROC_SET_MAC_BASED_802_1X:
+		chk = port_chk(sw, port, REG_PORT_LUE_CTRL,
+			PORT_MAC_BASED_802_1X);
+		break;
+	case PROC_SET_SRC_ADDR_FILTER:
+		chk = port_chk(sw, port, REG_PORT_LUE_CTRL,
+			PORT_SRC_ADDR_FILTER);
+		break;
+	case PROC_SET_VLAN_LOOKUP_0:
+		chk = port_chk(sw, port, REG_PORT_LUE_CTRL,
+			PORT_VLAN_LOOKUP_VID_0);
+		break;
+	case PROC_SET_MSTP:
+		chk = port_chk_mstp(sw, port);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX:
+		chk = port_chk_rx(sw, port);
+		break;
+	case PROC_SET_TX:
+		chk = port_chk_tx(sw, port);
+		break;
+	case PROC_SET_LEARN:
+		chk = !port_chk_dis_learn(sw, port);
+		break;
+	case PROC_SET_POWER:
+		chk = port_chk_power(sw, port);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		chk = port_chk_back_pressure(sw, port);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		chk = port_chk_force_flow_ctrl(sw, port);
+		break;
+	case PROC_SET_PASS_ALL:
+		chk = port_chk(sw, port, REG_PORT_MAC_CTRL_1, PORT_PASS_ALL);
+		break;
+	case PROC_SET_TAIL_TAG:
+		chk = port_chk_tail_tag(sw, port);
+		break;
+	case PROC_SET_CUSTOM_VID:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_CUSTOM_VID, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	}
+	case PROC_SET_SR_1_VID:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_1_VID, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	}
+	case PROC_SET_SR_2_VID:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_2_VID, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	}
+	case PROC_SET_SR_1_TYPE:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_1_TYPE, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	}
+	case PROC_SET_SR_2_TYPE:
+	{
+		u16 data;
+
+		port_r16(sw, port, REG_PORT_AVB_SR_2_TYPE, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	}
+	case PROC_SET_PME_CTRL:
+		chk = port_r_s(sw, port, REG_PORT_PME_CTRL, 7, 0);
+		len += sprintf(buf + len, "%02x:\n", chk);
+		len += sprintf(buf + len, "\t%02x = Magic Packet detect\n",
+			PME_WOL_MAGICPKT);
+		len += sprintf(buf + len, "\t%02x = link up detect\n",
+			PME_WOL_LINKUP);
+		len += sprintf(buf + len, "\t%02x = energy detect\n",
+			PME_WOL_ENERGY);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_PME_STATUS:
+		chk = port_r_s(sw, port, REG_PORT_PME_STATUS, 7, 0);
+		len += sprintf(buf + len, "%02x:\n", chk);
+		len += sprintf(buf + len, "\t%02x = Magic Packet detect\n",
+			PME_WOL_MAGICPKT);
+		len += sprintf(buf + len, "\t%02x = link up detect\n",
+			PME_WOL_LINKUP);
+		len += sprintf(buf + len, "\t%02x = energy detect\n",
+			PME_WOL_ENERGY);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_authen_mode(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_ACL:
+		chk = port_chk_acl(sw, port);
+		break;
+	case PROC_SET_P_INDEX:
+		chk = cfg->p_index;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_Q_INDEX:
+		chk = cfg->q_index;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_POLICE_PACKET_TYPE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_police_packet_type(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_NON_DSCP_COLOR:
+		len += sprintf(buf + len, "%u\n",
+			port_get_non_dscp_color(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_ENABLE_PORT_BASED_POLICING:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_port_based_policing(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_ENABLE_POLICE_DROP_ALL:
+		chk = port_chk_police_drop_all(sw, port);
+		break;
+	case PROC_ENABLE_COLOR_MARK:
+		chk = port_chk_color_mark(sw, port);
+		break;
+	case PROC_ENABLE_COLOR_REMAP:
+		chk = port_chk_color_remap(sw, port);
+		break;
+	case PROC_ENABLE_DROP_SRP:
+		chk = port_chk_drop_srp(sw, port);
+		break;
+	case PROC_ENABLE_COLOR_AWARE:
+		chk = port_chk_color_aware(sw, port);
+		break;
+	case PROC_ENABLE_POLICE:
+		chk = port_chk_police(sw, port);
+		break;
+	case PROC_SET_Q_CIR:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_cir(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_Q_PIR:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_pir(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_Q_CBS:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_cbs(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_Q_PBS:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_pbs(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_MAX_THRESHOLD:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_max(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_MIN_THRESHOLD:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_min(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_MULTIPLIER:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_multiplier(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_WRED_AVG_SIZE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_avg_size(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_Q_MAX_THRESHOLD:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_max(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_Q_MIN_THRESHOLD:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_min(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_Q_MULTIPLIER:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_multiplier(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_WRED_Q_AVG_SIZE:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_q_avg_size(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_RANDOM_DROP:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_random_drop(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_GYR:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_gyr(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_YR:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_yr(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_R:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_r(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_ALL:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_wred_drop_all(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_WRED_PMON:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u\n",
+			port_get_wred_pmon(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QUEUE_SCHEDULING:
+		chk = port_get_schedule_mode(sw, port, cfg->q_index);
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (strict priority)");
+				break;
+			case 2:
+				strcpy(note, " (WRR)");
+				break;
+			default:
+				strcpy(note, " (invalid)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_QUEUE_SHAPING:
+		chk = port_get_shaping(sw, port, cfg->q_index);
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (off)");
+				break;
+			case 1:
+				strcpy(note, " (on)");
+				break;
+			case 2:
+				strcpy(note, " (time aware)");
+				break;
+			default:
+				strcpy(note, " (invalid)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+#ifdef MTI_PREEMPT_ENABLE
+	case PROC_SET_PREEMPT:
+		len += sprintf(buf + len, "%u\n",
+			port_chk_preempt(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+#endif
+	case PROC_SET_TX_RATIO:
+		chk = port_get_tx_ratio(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_CREDIT_HI_WATER_MARK:
+		chk = port_get_hi_water_mark(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_CREDIT_LO_WATER_MARK:
+		chk = port_get_lo_water_mark(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_CREDIT_INCREMENT:
+		chk = port_get_increment(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_SRP:
+		chk = port_get_srp(sw, port);
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (off)");
+				break;
+			case 1:
+				strcpy(note, " (insert SR_1 tag)");
+				break;
+			case 2:
+				strcpy(note, " (insert SR_2 tag)");
+				break;
+			default:
+				strcpy(note, " (invalid)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_QM_DROP:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_drop(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_BURST_SIZE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_burst_size(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_RESV_SPACE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_resv_space(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_HI_WATER_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_hi_water_mark(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_LO_WATER_MARK:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_lo_water_mark(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_QM_TX_USED:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_tx_used(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_QM_TX_AVAIL:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_tx_avail(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_QM_TX_CALCULATED:
+		len += sprintf(buf + len, "%u\n",
+			port_get_qm_tx_calculated(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_RX_FLOW_CTRL:
+		chk = port_chk(sw, port, REG_PORT_STATUS_0,
+			PORT_RX_FLOW_CTRL);
+		break;
+	case PROC_GET_TX_FLOW_CTRL:
+		chk = port_chk(sw, port, REG_PORT_STATUS_0,
+			PORT_TX_FLOW_CTRL);
+		break;
+	case PROC_SET_MMD_VAL:
+		if (PHY_INTERFACE_MODE_SGMII == sw->port_info[port].interface)
+			port_sgmii_r(sw, port, cfg->mmd_id, cfg->mmd_reg,
+				&val, 1);
+		else
+			port_mmd_read(sw, port, cfg->mmd_id, cfg->mmd_reg,
+				&val, 1);
+		chk = val;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MAC_LOOPBACK:
+		chk = port_chk_mac_loopback(sw, port);
+		break;
+	case PROC_SET_PHY_LOOPBACK:
+		chk = port_chk_phy_loopback(sw, port);
+		break;
+	case PROC_SET_REMOTE_LOOPBACK:
+		chk = port_chk_remote_loopback(sw, port);
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read_hw */
+
+static int sysfs_port_write(struct ksz_sw *sw, int proc_num, uint port,
+	int num, const char *buf)
+{
+	int count;
+	unsigned int val;
+	u16 mmd_val;
+	int processed = true;
+	struct ksz_port_cfg *cfg;
+	struct ksz_port_info *port_info;
+
+	port = chk_last_port(sw, port);
+	port_info = &sw->port_info[port];
+	cfg = &sw->info->port_cfg[port];
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+	case PROC_SET_PORT_SPEED:
+	{
+		struct ksz_port phy_port;
+
+		if (sw->HOST_PORT == port)
+			break;
+		if ((PROC_SET_PORT_DUPLEX == proc_num && num > 2) ||
+		    (PROC_SET_PORT_SPEED == proc_num &&
+		    num != 0 && num != 10 && num != 100 && num != 1000))
+			break;
+		if (!(sw->features & GIGABIT_SUPPORT) && num == 1000)
+			break;
+
+		phy_port.sw = sw;
+		phy_port.port_cnt = 1;
+		phy_port.first_port = port;
+		phy_port.flow_ctrl = port_info->own_flow_ctrl;
+		phy_port.duplex = port_info->own_duplex;
+		phy_port.speed = port_info->own_speed;
+		if (PROC_SET_PORT_DUPLEX == proc_num)
+			phy_port.duplex = (u8) num;
+		else
+			phy_port.speed = (u16) num;
+		port_set_link_speed(&phy_port);
+		break;
+	}
+	case PROC_SET_MAC_OPERATIONAL:
+#if 0
+		if (sw->overrides & USE_802_1X_AUTH)
+			break;
+#endif
+		count = sw->dev_ports & (1 << port);
+		if (count && !num) {
+			sw->dev_ports &= ~(1 << port);
+			num = 1;
+		} else if (!count && num) {
+			if (sw->dev_count > 1)
+				break;
+			sw->dev_ports |= (1 << port);
+			num = 2;
+		} else
+			num = 0;
+
+#ifdef CONFIG_KSZ_MRP
+		if (sw->features & MRP_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			if (2 == num) {
+				mrp->rx_ports |= (1 << port);
+				mrp->tx_ports |= (1 << port);
+			} else if (1 == num) {
+				mrp->rx_ports &= ~(1 << port);
+				mrp->tx_ports &= ~(1 << port);
+			}
+		}
+#endif
+		break;
+	case PROC_SET_VLAN_RESTRICTED:
+		cfg->restricted = !!num;
+		break;
+	case PROC_SET_VLAN_UNTAGGED:
+	{
+		int bit;
+		int index;
+
+		index = sw->vlan_index / VID_IN_DATA;
+		bit = sw->vlan_index % VID_IN_DATA;
+		count = !!(cfg->untagged[index] & (1 << bit));
+		if (count != !!num) {
+			struct ksz_vlan_table vlan;
+
+			if (num)
+				cfg->untagged[index] |= (1 << bit);
+			else
+				cfg->untagged[index] &= ~(1 << bit);
+			sw->ops->release(sw);
+			if (!sw_r_vlan_table(sw, sw->vlan_index, &vlan)) {
+				if (num)
+					vlan.untag |= (1 << port);
+				else
+					vlan.untag &= ~(1 << port);
+				sw_w_vlan_table(sw, sw->vlan_index, &vlan);
+			}
+			sw->ops->acquire(sw);
+		}
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+	{
+		struct ksz_port_mib *mib = &sw->port_mib[port];
+
+		if (num >= 1 && num <= 2) {
+			port_freeze_mib(sw, port, num - 1);
+			break;
+		}
+		memset((void *) mib->counter, 0, sizeof(u64) *
+			TOTAL_SWITCH_COUNTER_NUM);
+		mib->rate[0].last = mib->rate[1].last = 0;
+		mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+		mib->rate[0].peak = mib->rate[1].peak = 0;
+		break;
+	}
+	case PROC_ENABLE_BROADCAST_STORM:
+		if (!num)
+			sw_dis_broad_storm(sw, port);
+		else
+			sw_ena_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		if (!num)
+			sw_dis_diffserv(sw, port);
+		else
+			sw_ena_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		if (!num)
+			sw_dis_802_1p(sw, port);
+		else
+			sw_ena_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_VLAN_PRIO:
+		port_cfg_vlan_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_MAC_PRIO:
+		port_cfg_mac_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_ACL_PRIO:
+		port_cfg_acl_prio(sw, port, num);
+		break;
+	case PROC_SET_HIGHEST_PRIO:
+		port_cfg_highest_prio(sw, port, num);
+		break;
+	case PROC_SET_OR_PRIO:
+		port_cfg_or_prio(sw, port, num);
+		break;
+	case PROC_SET_PORT_BASED:
+		sw_cfg_port_based(sw, port, num);
+		break;
+	case PROC_SET_DEF_VID:
+		sw_cfg_def_vid(sw, port, num);
+		break;
+	case PROC_SET_MEMBER:
+		sw_cfg_port_base_vlan(sw, port, (u16) num);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		if (0 <= num && num <= 2)
+			sw_set_multi_queue(sw, port, num);
+		break;
+	case PROC_SET_REPLACE_VID:
+		sw_cfg_replace_null_vid(sw, port, num);
+		break;
+	case PROC_SET_LIMIT:
+		hw_cfg_rx_limit(sw, port, (u8) num);
+		break;
+	case PROC_SET_LIMIT_PORT_BASED:
+		hw_cfg_in_port_based(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		hw_cfg_rate_packet_based(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_FLOW_CTRL:
+		hw_cfg_in_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_CNT_IFG:
+		hw_cfg_cnt_ifg(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_CNT_PRE:
+		hw_cfg_cnt_pre(sw, port, num);
+		break;
+	case PROC_SET_RX_P0_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 0, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P1_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 1, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P2_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 2, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P3_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 3, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P4_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 4, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P5_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 5, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P6_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 6, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P7_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 7, num);
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		sw_cfg_replace_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		if (!num)
+			sw_dis_rx_prio_rate(sw, port);
+		else
+			sw_ena_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		if (!num)
+			sw_dis_tx_prio_rate(sw, port);
+		else
+			sw_ena_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_LINK_MD:
+		sw_get_link_md(sw, port);
+		break;
+	case PROC_SET_SQI:
+		port_chk_sqi(sw, port);
+		break;
+	case PROC_SET_COLOR_MAP:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			port_cfg_color_map(sw, port, (u8) num, (u32) val);
+		break;
+	case PROC_SET_TC_MAP:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			port_cfg_tc_map(sw, port, (u8) num, (u32) val);
+		break;
+	case PROC_SET_DROP_NON_VLAN:
+		port_cfg_drop_non_vlan(sw, port, num);
+		break;
+	case PROC_SET_DROP_TAG:
+		port_cfg_drop_tag(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		port_cfg_mirror_sniffer(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_RX:
+		port_cfg_mirror_rx(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_TX:
+		port_cfg_mirror_tx(sw, port, num);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		port_cfg_dis_non_vid(sw, port, num);
+		break;
+	case PROC_SET_INGRESS:
+		port_cfg_in_filter(sw, port, num);
+		break;
+	case PROC_SET_MAC_BASED_802_1X:
+		port_cfg(sw, port, REG_PORT_LUE_CTRL, PORT_MAC_BASED_802_1X,
+			num);
+		break;
+	case PROC_SET_SRC_ADDR_FILTER:
+		if (!num)
+			sw->open_ports |= (1 << port);
+		port_cfg(sw, port, REG_PORT_LUE_CTRL, PORT_SRC_ADDR_FILTER,
+			num);
+		if (num)
+			sw->open_ports &= ~(1 << port);
+		break;
+	case PROC_SET_VLAN_LOOKUP_0:
+		port_cfg(sw, port, REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+			num);
+		break;
+	case PROC_SET_MSTP:
+		port_cfg_mstp(sw, port, num);
+		break;
+	case PROC_SET_RX:
+		port_cfg_rx_special(sw, port, num);
+		break;
+	case PROC_SET_TX:
+		port_cfg_tx(sw, port, num);
+		break;
+	case PROC_SET_LEARN:
+		port_cfg_dis_learn(sw, port, !num);
+#if 1
+		if (!num)
+			sw_flush_dyn_mac_table(sw, port);
+#endif
+		break;
+	case PROC_SET_POWER:
+		port_cfg_power(sw, port, num);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		port_cfg_back_pressure(sw, port, num);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		port_cfg_force_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_PASS_ALL:
+		port_cfg(sw, port, REG_PORT_MAC_CTRL_1, PORT_PASS_ALL, num);
+		break;
+	case PROC_SET_TAIL_TAG:
+		port_cfg_tail_tag(sw, port, num);
+		break;
+	case PROC_SET_CUSTOM_VID:
+		port_w16(sw, port, REG_PORT_CUSTOM_VID, (u16) num);
+		break;
+	case PROC_SET_SR_1_VID:
+		port_w16(sw, port, REG_PORT_AVB_SR_1_VID, (u16) num);
+		break;
+	case PROC_SET_SR_2_VID:
+		port_w16(sw, port, REG_PORT_AVB_SR_2_VID, (u16) num);
+		break;
+	case PROC_SET_SR_1_TYPE:
+		port_w16(sw, port, REG_PORT_AVB_SR_1_TYPE, (u16) num);
+		break;
+	case PROC_SET_SR_2_TYPE:
+		port_w16(sw, port, REG_PORT_AVB_SR_2_TYPE, (u16) num);
+		break;
+	case PROC_SET_PME_CTRL:
+		port_w_s(sw, port, REG_PORT_PME_CTRL, 7, 0, (u8) num);
+		break;
+	case PROC_SET_PME_STATUS:
+		port_w_s(sw, port, REG_PORT_PME_STATUS, 7, 0, (u8) num);
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		if (num > PORT_AUTHEN_TRAP)
+			break;
+		if (num == PORT_AUTHEN_NORMAL)
+			num = PORT_AUTHEN_PASS;
+		port_set_authen_mode(sw, port, num);
+		if (sw->overrides & USE_802_1X_AUTH)
+			port_open(sw, port, PORT_AUTHEN_PASS == num);
+		break;
+	case PROC_SET_ACL:
+		port_cfg_acl(sw, port, num);
+		break;
+	case PROC_SET_P_INDEX:
+		if (0 <= num && num < sw->mib_port_cnt) {
+			num = chk_last_port(sw, num);
+			cfg->p_index = (u8) num;
+		}
+		break;
+	case PROC_SET_Q_INDEX:
+		if (0 <= num && num < PRIO_QUEUES)
+			cfg->q_index = (u8) num;
+		break;
+	case PROC_SET_POLICE_PACKET_TYPE:
+		port_set_police_packet_type(sw, port, num);
+		break;
+	case PROC_SET_NON_DSCP_COLOR:
+		port_set_non_dscp_color(sw, port, num);
+		break;
+	case PROC_ENABLE_PORT_BASED_POLICING:
+		port_cfg_port_based_policing(sw, port, num);
+		break;
+	case PROC_ENABLE_POLICE_DROP_ALL:
+		port_cfg_police_drop_all(sw, port, num);
+		break;
+	case PROC_ENABLE_COLOR_MARK:
+		port_cfg_color_mark(sw, port, num);
+		break;
+	case PROC_ENABLE_COLOR_REMAP:
+		port_cfg_color_remap(sw, port, num);
+		break;
+	case PROC_ENABLE_DROP_SRP:
+		port_cfg_drop_srp(sw, port, num);
+		break;
+	case PROC_ENABLE_COLOR_AWARE:
+		port_cfg_color_aware(sw, port, num);
+		break;
+	case PROC_ENABLE_POLICE:
+		port_cfg_police(sw, port, num);
+		break;
+	case PROC_SET_Q_CIR:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_cir(sw, port, num);
+		break;
+	case PROC_SET_Q_PIR:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_pir(sw, port, num);
+		break;
+	case PROC_SET_Q_CBS:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_cbs(sw, port, num);
+		break;
+	case PROC_SET_Q_PBS:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_pbs(sw, port, num);
+		break;
+	case PROC_SET_WRED_MAX_THRESHOLD:
+		port_set_wred_max(sw, port, num);
+		break;
+	case PROC_SET_WRED_MIN_THRESHOLD:
+		port_set_wred_min(sw, port, num);
+		break;
+	case PROC_SET_WRED_MULTIPLIER:
+		port_set_wred_multiplier(sw, port, num);
+		break;
+	case PROC_SET_WRED_Q_MAX_THRESHOLD:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_wred_q_max(sw, port, num);
+		break;
+	case PROC_SET_WRED_Q_MIN_THRESHOLD:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_wred_q_min(sw, port, num);
+		break;
+	case PROC_SET_WRED_Q_MULTIPLIER:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		port_set_wred_q_multiplier(sw, port, num);
+		break;
+	case PROC_SET_WRED_RANDOM_DROP:
+		port_cfg_wred_random_drop(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_GYR:
+		port_cfg_wred_drop_gyr(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_YR:
+		port_cfg_wred_drop_yr(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_R:
+		port_cfg_wred_drop_r(sw, port, num);
+		break;
+	case PROC_SET_WRED_DROP_ALL:
+		port_cfg_wred_drop_all(sw, port, num);
+		break;
+	case PROC_SET_QUEUE_SCHEDULING:
+		port_set_schedule_mode(sw, port, cfg->q_index, (u8) num);
+		break;
+	case PROC_SET_QUEUE_SHAPING:
+		port_set_shaping(sw, port, cfg->q_index, (u8) num);
+		break;
+#ifdef MTI_PREEMPT_ENABLE
+	case PROC_SET_PREEMPT:
+		port_cfg_preempt(sw, port, cfg->q_index, num);
+		break;
+#endif
+	case PROC_SET_TX_RATIO:
+		port_set_tx_ratio(sw, port, cfg->q_index, (u8) num);
+		break;
+	case PROC_SET_CREDIT_HI_WATER_MARK:
+		port_set_hi_water_mark(sw, port, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_CREDIT_LO_WATER_MARK:
+		port_set_lo_water_mark(sw, port, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_CREDIT_INCREMENT:
+		port_set_increment(sw, port, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_SRP:
+		port_set_srp(sw, port, (u8) num);
+		break;
+	case PROC_SET_QM_DROP:
+		port_set_qm_drop(sw, port, num);
+		break;
+	case PROC_SET_QM_BURST_SIZE:
+		port_set_qm_burst_size(sw, port, (u8) num);
+		break;
+	case PROC_SET_QM_RESV_SPACE:
+		port_set_qm_resv_space(sw, port, (u16) num);
+		break;
+	case PROC_SET_QM_HI_WATER_MARK:
+		port_set_qm_hi_water_mark(sw, port, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_QM_LO_WATER_MARK:
+		port_set_qm_lo_water_mark(sw, port, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_MMD_ID:
+		cfg->mmd_id = (u16) num;
+		break;
+	case PROC_SET_MMD_REG:
+		cfg->mmd_reg = (u16) num;
+		break;
+	case PROC_SET_MMD_VAL:
+		mmd_val = (u16) num;
+		if (PHY_INTERFACE_MODE_SGMII == sw->port_info[port].interface)
+			port_sgmii_w(sw, port, cfg->mmd_id, cfg->mmd_reg,
+				&mmd_val, 1);
+		else
+			port_mmd_write(sw, port, cfg->mmd_id, cfg->mmd_reg,
+				&mmd_val, 1);
+		break;
+	case PROC_SET_MAC_LOOPBACK:
+		port_cfg_mac_loopback(sw, port, num);
+		break;
+	case PROC_SET_PHY_LOOPBACK:
+		port_cfg_phy_loopback(sw, port, num);
+		break;
+	case PROC_SET_REMOTE_LOOPBACK:
+		port_cfg_remote_loopback(sw, port, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_port_write */
+
+static ssize_t sysfs_mac_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_mac_table *entry;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		len += sprintf(buf + len, "0x%03x\n", entry->fid);
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		len += sprintf(buf + len, "%u\n", entry->use_fid);
+		break;
+	case PROC_SET_STATIC_MSTP:
+		len += sprintf(buf + len, "%u\n", entry->mstp);
+		break;
+	case PROC_SET_STATIC_PRIO:
+		len += sprintf(buf + len, "%u\n", entry->prio);
+		break;
+	case PROC_SET_STATIC_SRC:
+		len += sprintf(buf + len, "%u\n", entry->src);
+		break;
+	case PROC_SET_STATIC_DST:
+		len += sprintf(buf + len, "%u\n", entry->dst);
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		len += sprintf(buf + len, "%u\n", entry->override);
+		break;
+	case PROC_SET_STATIC_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		len += sprintf(buf + len, "0x%04x\n", entry->ports);
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			entry->addr[0], entry->addr[1],
+			entry->addr[2], entry->addr[3],
+			entry->addr[4], entry->addr[5]);
+		break;
+	case PROC_SET_STATIC_TYPE:
+		len += sprintf(buf + len, "%u\n", sw->alu_type);
+		break;
+	case PROC_SET_STATIC_INDEX:
+		len += sprintf(buf + len, "0x%03x\n", sw->alu_index);
+		break;
+	case PROC_SET_STATIC_INFO:
+		if (sw->alu_dirty) {
+			if (2 == sw->alu_type) {
+				u8 mac_addr[ETH_ALEN];
+				u16 fid;
+				u16 mac_index;
+
+				memcpy(mac_addr, entry->addr, ETH_ALEN);
+				fid = entry->fid;
+				sw_r_dyn_mac_table(sw, sw->alu_index,
+					mac_addr, fid, entry, &mac_index);
+				if (!sw->alu_index && mac_index)
+					sw->alu_index = mac_index;
+			} else if (!entry->dirty)
+				sw_r_sta_mac_table(sw, sw->alu_index,
+					sw->alu_type, entry);
+			sw->alu_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x.%u: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  ",
+			sw->alu_index, sw->alu_type,
+			entry->addr[0], entry->addr[1], entry->addr[2],
+			entry->addr[3], entry->addr[4], entry->addr[5],
+			entry->ports, entry->mstp);
+		if (2 == sw->alu_type)
+			len += sprintf(buf + len,
+				"t:%u  s:%u  d:%u  o:%u  %02x  [%u]\n",
+				entry->prio, entry->src, entry->dst,
+				entry->override, entry->fid,
+				entry->dirty ? 2 : entry->valid);
+		else
+			len += sprintf(buf + len,
+				"p:%u  s:%u  d:%u  o:%u  %u:%02x  [%u]\n",
+				entry->prio, entry->src, entry->dst,
+				entry->override, entry->use_fid, entry->fid,
+				entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_mac_read */
+
+static int sysfs_mac_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_mac_table *entry;
+	int processed = true;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		if (0 <= num && num <= ALU_V_FID_M) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		if (num)
+			entry->use_fid = 1;
+		else
+			entry->use_fid = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_MSTP:
+		if (0 <= num && num <= ALU_V_MSTP_M) {
+			entry->mstp = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_PRIO:
+		if (0 <= num && num <= ALU_V_PRIO_AGE_CNT_M) {
+			entry->prio = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_SRC:
+		if (num)
+			entry->src = 1;
+		else
+			entry->src = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_DST:
+		if (num)
+			entry->dst = 1;
+		else
+			entry->dst = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		if (num)
+			entry->override = 1;
+		else
+			entry->override = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		if (2 == sw->alu_type)
+			sw_w_dyn_mac_table(sw, sw->alu_index,
+				entry->addr, entry->fid, entry);
+		else
+			sw_w_sta_mac_table(sw, sw->alu_index,
+				sw->alu_type, entry);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->ports = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->addr[i] = (u8) n[i];
+			entry->dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_STATIC_TYPE:
+		if (0 <= num && num < 3) {
+			sw->alu_type = num;
+			sw->alu_dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_INDEX:
+		if (0 <= num && num < 0x1000) {
+			sw->alu_index = num;
+			sw->alu_dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_INFO:
+		sw->alu_dirty = 1;
+		entry->dirty = 0;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mac_write */
+
+static ssize_t sysfs_vlan_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_vlan_table *entry;
+	int i;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_VLAN_PORTS:
+		len += sprintf(buf + len, "0x%04x\n", entry->ports);
+		break;
+	case PROC_SET_VLAN_UNTAG:
+		len += sprintf(buf + len, "0x%04x\n", entry->untag);
+		break;
+	case PROC_SET_VLAN_FID:
+		len += sprintf(buf + len, "0x%03x\n", entry->fid);
+		break;
+	case PROC_SET_VLAN_MSTP:
+		len += sprintf(buf + len, "0x%x\n", entry->mstp);
+		break;
+	case PROC_SET_VLAN_PRIO:
+		len += sprintf(buf + len, "0x%x\n", entry->prio);
+		break;
+	case PROC_SET_VLAN_OPTION:
+		len += sprintf(buf + len, "%u\n", entry->option);
+		break;
+	case PROC_SET_VLAN_VID:
+		len += sprintf(buf + len, "0x%03x\n", sw->vlan_index);
+		break;
+	case PROC_SET_VLAN_INFO:
+		if (sw->vlan_dirty) {
+			if (!entry->dirty)
+				sw_r_vlan_table(sw, sw->vlan_index, entry);
+			sw->vlan_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: 0x%03x  m:%x  p:%x  o:%u  %04x  %04x  [%u]\n",
+			sw->vlan_index, entry->fid, entry->mstp, entry->prio,
+			entry->option, entry->untag, entry->ports,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	case PROC_SET_VID_2_FID:
+		len += sprintf(buf + len,
+			"%03x=%02x\n", sw->vlan_index,
+				sw->info->vid2fid[sw->vlan_index]);
+		break;
+	case PROC_SET_FID_2_MSTID:
+		if (entry->fid) {
+			len += sprintf(buf + len,
+				"%02x=%u\n", entry->fid,
+					sw->info->fid2mstid[entry->fid]);
+			break;
+		}
+		for (i = 0; i < FID_ENTRIES; i++) {
+			if ((i % 32) == 0)
+				len += sprintf(buf + len,
+					"%02x: ", i);
+			len += sprintf(buf + len,
+				"%u ", sw->info->fid2mstid[i]);
+			if ((i % 32) == 31)
+				len += sprintf(buf + len,
+					"\n");
+		}
+		break;
+	}
+	return len;
+}  /* sysfs_vlan_read */
+
+static int sysfs_vlan_write(struct ksz_sw *sw, int proc_num, int num)
+{
+	struct ksz_vlan_table *entry;
+	int processed = true;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+#ifdef CONFIG_KSZ_MRP
+		if (sw->features & MRP_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			mrp->ops->setup_vlan(mrp, sw->vlan_index, entry);
+		}
+#endif
+		sw_w_vlan_table(sw, sw->vlan_index, entry);
+		sw->vlan_dirty = 0;
+		sw->overrides |= VLAN_SET;
+		break;
+	case PROC_SET_VLAN_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->ports = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_UNTAG:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->untag = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_FID:
+		if (0 <= num && num < FID_ENTRIES) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_VID:
+		if (0 <= num && num < 0x1000) {
+			sw->vlan_index = num;
+			sw->vlan_dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_MSTP:
+		if (0 <= num && num <= VLAN_MSTP_M) {
+			entry->mstp = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_PRIO:
+		if (0 <= num && num <= VLAN_PRIO_M) {
+			entry->prio = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_OPTION:
+		if (num)
+			entry->option = 1;
+		else
+			entry->option = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_VLAN_INFO:
+		sw->vlan_dirty = 1;
+		entry->dirty = 0;
+		break;
+	case PROC_SET_VID_2_FID:
+		if (0 <= num && num < FID_ENTRIES) {
+			sw->info->vid2fid[sw->vlan_index] = num;
+			sw->info->fid_updated = 1;
+		}
+		break;
+	case PROC_SET_FID_2_MSTID:
+		if (0 <= num && num < NUM_OF_MSTI) {
+			sw->info->fid2mstid[entry->fid] = num;
+			sw->info->fid_updated = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_vlan_write */
+
+#ifdef CONFIG_KSZ_HSR
+static ssize_t sysfs_hsr_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_hsr_table *entry;
+	u8 mac_addr[ETH_ALEN];
+	u8 path_id;
+	struct ksz_hsr_info *info = &sw->info->hsr;
+
+	entry = &sw->info->hsr_entry;
+	switch (proc_num) {
+	case PROC_SET_HSR_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_HSR_AGE_CNT:
+		len += sprintf(buf + len, "%u\n", entry->age_cnt);
+		break;
+	case PROC_SET_HSR_PATH_ID:
+		len += sprintf(buf + len, "%x\n", entry->path_id >> 1);
+		break;
+	case PROC_SET_HSR_SRC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			entry->src_mac[0], entry->src_mac[1],
+			entry->src_mac[2], entry->src_mac[3],
+			entry->src_mac[4], entry->src_mac[5]);
+		break;
+	case PROC_SET_HSR_INDEX:
+		len += sprintf(buf + len, "0x%03x\n", sw->hsr_index);
+		break;
+	case PROC_SET_HSR_INFO:
+
+		/* MAC address and path ID will be wiped out if emtpy entry. */
+		memcpy(mac_addr, entry->src_mac, ETH_ALEN);
+		path_id = entry->path_id;
+		if (sw->hsr_dirty) {
+			sw_r_hsr_table(sw, sw->hsr_index, entry);
+			sw->hsr_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c:%u %04x:%04x %04x:%04x %04x:%04x [%u]\n",
+			sw->hsr_index,
+			entry->src_mac[0], entry->src_mac[1], entry->src_mac[2],
+			entry->src_mac[3], entry->src_mac[4], entry->src_mac[5],
+			entry->path_id >> 1, entry->age_cnt,
+			entry->start_seq[0], entry->start_seq[1],
+			entry->exp_seq[0], entry->exp_seq[1],
+			entry->seq_cnt[0], entry->seq_cnt[1],
+			entry->dirty ? 2 : entry->valid);
+
+		memcpy(entry->src_mac, mac_addr, ETH_ALEN);
+		entry->path_id = path_id;
+		break;
+	case PROC_GET_HSR_STATE:
+		len += sprintf(buf + len,
+			"%u %u:%u %u:%u\n", info->ring,
+			info->p1_down, info->p2_down,
+			info->p1_lost, info->p2_lost);
+		break;
+	}
+	return len;
+}  /* sysfs_hsr_read */
+
+static int sysfs_hsr_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_hsr_table *entry;
+	int processed = true;
+
+	entry = &sw->info->hsr_entry;
+	switch (proc_num) {
+	case PROC_SET_HSR_PATH_ID:
+		if (0 <= num && num <= 7) {
+			entry->path_id = num << 1;
+			if (!sw->hsr_index)
+				sw->hsr_dirty = 1;
+		}
+		break;
+	case PROC_SET_HSR_AGE_CNT:
+		if (0 <= num && num <= 3) {
+			entry->age_cnt = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_HSR_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		sw_w_hsr_table(sw, sw->hsr_index, entry);
+		break;
+	case PROC_SET_HSR_SRC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->src_mac[i] = (u8) n[i];
+			if (!sw->hsr_index)
+				sw->hsr_dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_HSR_INDEX:
+		if (0 <= num && num < 0x200) {
+			sw->hsr_index = num;
+			sw->hsr_dirty = 1;
+		}
+		break;
+	case PROC_SET_HSR_INFO:
+		sw->hsr_dirty = 1;
+		entry->dirty = 0;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_hsr_write */
+#endif
+
+static ssize_t sysfs_acl_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	struct ksz_acl_table *ruleset;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = chk_last_port(sw, port);
+	cfg = &sw->info->port_cfg[port];
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	ruleset = &cfg->acl_info[cfg->acl_rule_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		chk = ruleset->first_rule;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_RULESET:
+		len += acl_ruleset_info(ruleset, cfg->acl_rule_index, buf, len);
+		break;
+	case PROC_SET_ACL_MODE:
+		chk = acl->mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (layer 2)");
+				break;
+			case 2:
+				strcpy(note, " (layer 3)");
+				break;
+			case 3:
+				strcpy(note, " (layer 4)");
+				break;
+			default:
+				strcpy(note, " (off)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_ENABLE:
+		chk = acl->enable;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (type; ip; tcp port)");
+				break;
+			case 2:
+				strcpy(note, " (mac; src/dst; udp port)");
+				break;
+			case 3:
+				strcpy(note, " (both; -; tcp seq)");
+				break;
+			default:
+				strcpy(note, " (count; -; protocol)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_SRC:
+		chk = acl->src;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		chk = acl->equal;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		chk = action->prio_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (higher)");
+				break;
+			case 2:
+				strcpy(note, " (lower)");
+				break;
+			case 3:
+				strcpy(note, " (replace)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_PRIO:
+		chk = action->prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		chk = action->vlan_prio_replace;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		chk = action->vlan_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		chk = action->map_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (or)");
+				break;
+			case 2:
+				strcpy(note, " (and)");
+				break;
+			case 3:
+				strcpy(note, " (replace)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_PORTS:
+		chk = action->ports;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5]);
+		break;
+	case PROC_SET_ACL_TYPE:
+		chk = acl->eth_type;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_ACL_CNT:
+		chk = acl->cnt;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MSEC:
+		chk = acl->msec;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		chk = acl->intr_mode;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+		len += sprintf(buf + len, "%u.%u.%u.%u\n",
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3]);
+		break;
+	case PROC_SET_ACL_IP_MASK:
+		len += sprintf(buf + len, "%u.%u.%u.%u\n",
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3]);
+		break;
+	case PROC_SET_ACL_PROTOCOL:
+		chk = acl->protocol;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		chk = acl->port_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (either)");
+				break;
+			case 2:
+				strcpy(note, " (in range)");
+				break;
+			case 3:
+				strcpy(note, " (out of range)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		chk = acl->max_port;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		chk = acl->min_port;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		chk = acl->seqnum;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		chk = acl->tcp_flag_enable;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		chk = acl->tcp_flag;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		chk = acl->tcp_flag_mask;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_INDEX:
+		chk = cfg->acl_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		chk = cfg->acl_act_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_ACTION:
+		len += acl_action_info(action, cfg->acl_act_index, buf, len);
+		break;
+	case PROC_SET_ACL_RULE_INDEX:
+		chk = cfg->acl_rule_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_INFO:
+		len += acl_info(acl, cfg->acl_index, buf, len);
+		break;
+	case PROC_GET_ACL_TABLE:
+		len = sw_d_acl_table(sw, port, buf, len);
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_acl_read */
+
+static int sysfs_acl_write(struct ksz_sw *sw, int proc_num, uint port, int num,
+	const char *buf)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	struct ksz_acl_table *ruleset;
+	int acl_on = 0;
+	int processed = true;
+
+	port = chk_last_port(sw, port);
+	cfg = &sw->info->port_cfg[port];
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	ruleset = &cfg->acl_info[cfg->acl_rule_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_RULESET:
+	case PROC_SET_ACL_MODE:
+	case PROC_SET_ACL_ACTION:
+	case PROC_SET_ACL_INFO:
+		sw->ops->acquire(sw);
+		acl_on = port_chk_acl(sw, port);
+		if (!acl_on)
+			port_cfg_acl(sw, port, true);
+		sw->ops->release(sw);
+		break;
+	}
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		ruleset->first_rule = (u16) num;
+		ruleset->ruleset_changed = 1;
+		break;
+	case PROC_SET_ACL_RULESET:
+		sscanf(buf, "%x", &num);
+		ruleset->ruleset = (u16) num;
+		sw_w_acl_ruleset(sw, port, cfg->acl_rule_index, ruleset);
+		break;
+	case PROC_SET_ACL_MODE:
+		if (0 <= num && num < 4) {
+			acl->mode = num;
+			sw_w_acl_rule(sw, port, cfg->acl_index, acl);
+		}
+		break;
+	case PROC_SET_ACL_ENABLE:
+		if (0 <= num && num < 4) {
+			acl->enable = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_SRC:
+		if (num)
+			acl->src = 1;
+		else
+			acl->src = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		if (num)
+			acl->equal = 1;
+		else
+			acl->equal = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		if (0 <= num && num < 4) {
+			action->prio_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PRIO:
+		if (0 <= num && num <= KS_PRIO_M) {
+			action->prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		if (num)
+			action->vlan_prio_replace = 1;
+		else
+			action->vlan_prio_replace = 0;
+		action->action_changed = 1;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		if (0 <= num && num <= KS_PRIO_M) {
+			action->vlan_prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		if (0 <= num && num < 4) {
+			action->map_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PORTS:
+		sscanf(buf, "%x", &num);
+		if (0 <= num && num <= sw->PORT_MASK) {
+			action->ports = (u16) num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				acl->mac[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_TYPE:
+		sscanf(buf, "%x", &num);
+		acl->eth_type = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_CNT:
+		if (0 <= num && num <= ACL_CNT_M) {
+			acl->cnt = (u16) num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MSEC:
+		if (num)
+			acl->msec = 1;
+		else
+			acl->msec = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		if (num)
+			acl->intr_mode = 1;
+		else
+			acl->intr_mode = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_addr[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_IP_MASK:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_mask[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_PROTOCOL:
+		acl->protocol = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		if (0 <= num && num < 4) {
+			acl->port_mode = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		acl->max_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		acl->min_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		acl->seqnum = num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		if (num)
+			acl->tcp_flag_enable = 1;
+		else
+			acl->tcp_flag_enable = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		acl->tcp_flag = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		acl->tcp_flag_mask = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_act_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION:
+		if (num)
+			sw_w_acl_action(sw, port, cfg->acl_act_index, action);
+		else
+			sw_r_acl_table(sw, port, cfg->acl_act_index, action);
+		break;
+	case PROC_SET_ACL_RULE_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_rule_index = num;
+		}
+		break;
+	case PROC_SET_ACL_INFO:
+		sw_r_acl_table(sw, port, cfg->acl_index, acl);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	switch (proc_num) {
+	case PROC_SET_ACL_RULESET:
+	case PROC_SET_ACL_MODE:
+	case PROC_SET_ACL_ACTION:
+	case PROC_SET_ACL_INFO:
+		if (!acl_on) {
+			sw->ops->acquire(sw);
+			port_cfg_acl(sw, port, false);
+			sw->ops->release(sw);
+		}
+		break;
+	}
+	return processed;
+}  /* sysfs_acl_write */
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef KSZSW_REGS_SIZE
+static ssize_t kszsw_registers_read(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	unsigned reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off > KSZSW_REGS_SIZE))
+		return 0;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->get(sw, reg, count, buf);
+	sw->ops->release(sw);
+	return i;
+}
+
+static ssize_t kszsw_registers_write(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	unsigned reg;
+	unsigned phy_reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off >= KSZSW_REGS_SIZE))
+		return -EFBIG;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	phy_reg = reg & 0x1FF;
+	sw->ops->acquire(sw);
+	if ((reg & 0xF000) && (0x120 <= phy_reg && phy_reg <= 0x13F) &&
+	    ((reg & 3) || (count & 3))) {
+		char *tmp;
+		size_t cnt = (count + 3) & ~3;
+		int start = phy_reg & 3;
+
+#ifdef CONFIG_KSZ_IBA
+		if (sw->info->iba.use_iba)
+			start = 4 - ((phy_reg + count) & 3);
+#endif
+		tmp = kzalloc(cnt, GFP_KERNEL);
+		if (!tmp) {
+			i = 0;
+			goto write_done;
+		}
+		reg &= ~3;
+		i = sw->reg->get(sw, reg, cnt, tmp);
+		memcpy(&tmp[start], buf, count);
+		i = sw->reg->set(sw, reg, cnt, tmp);
+		kfree(tmp);
+	} else
+		i = sw->reg->set(sw, reg, count, buf);
+write_done:
+	sw->ops->release(sw);
+	return i;
+}
+
+static struct bin_attribute kszsw_registers_attr = {
+	.attr = {
+		.name	= "registers",
+		.mode	= S_IRUSR | S_IWUSR,
+	},
+	.size	= KSZSW_REGS_SIZE,
+	.read	= kszsw_registers_read,
+	.write	= kszsw_registers_write,
+};
+#endif
+
+static void sw_cfg_mac(struct ksz_sw *sw, u8 index, const u8 *dest, u32 ports,
+	int override, int use_fid, u16 fid)
+{
+	struct ksz_mac_table mac;
+
+	memset(&mac, 0, sizeof(struct ksz_mac_table));
+	memcpy(mac.addr, dest, ETH_ALEN);
+	mac.ports = ports & sw->PORT_MASK;
+	mac.override = override;
+	mac.use_fid = use_fid;
+	mac.fid = fid;
+	mac.valid = ports != 0;
+	if (!mac.valid && mac.override) {
+		mac.override = 0;
+		mac.valid = 1;
+	}
+	sw_w_dyn_mac_table(sw, 0, mac.addr, mac.fid, &mac);
+}  /* sw_cfg_mac */
+
+static void sw_cfg_vlan(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_vlan_table vlan;
+
+	if (0xffff == ports)
+		ports = sw->PORT_MASK;
+	if (sw_r_vlan_table(sw, vid, &vlan)) {
+		memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+		vlan.vid = vid;
+		vlan.fid = fid;
+	}
+	vlan.ports = ports & sw->PORT_MASK;
+	vlan.valid = ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+}  /* sw_cfg_vlan */
+
+static u8 sw_alloc_mac(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_mac */
+
+static void sw_free_mac(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_mac */
+
+static u8 sw_alloc_vlan(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_vlan */
+
+static void sw_free_vlan(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_vlan */
+
+static u16 sw_alloc_fid(struct ksz_sw *sw, u16 vid)
+{
+#if 0
+	int x;
+	int y;
+	u16 fid;
+
+	if (sw->info->fid_cnt + 2 == FID_ENTRIES)
+		return 0;
+	fid = vid & (FID_ENTRIES - 1);
+	if (vid < 2)
+		fid = 1000;
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	while (sw->info->fid[x] & (1 << y)) {
+		++fid;
+		++y;
+		if (y >= FID_IN_DATA) {
+			y = 0;
+			++x;
+		}
+	}
+	sw->info->fid[x] |= (1 << y);
+	++sw->info->fid_cnt;
+	return fid;
+#else
+	return sw->info->vid2fid[vid];
+#endif
+}  /* sw_alloc_fid */
+
+static void sw_free_fid(struct ksz_sw *sw, u16 fid)
+{
+#if 0
+	int x;
+	int y;
+
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	if (sw->info->fid[x] & (1 << y)) {
+		sw->info->fid[x] &= ~(1 << y);
+		--sw->info->fid_cnt;
+	}
+#endif
+}  /* sw_free_fid */
+
+static const u8 *sw_get_br_id(struct ksz_sw *sw)
+{
+	u8 id[8];
+	const u8* ret = id;
+
+	memcpy(&id[2], sw->info->mac_addr, ETH_ALEN);
+	id[0] = 0x80;
+	id[1] = 0x00;
+
+#ifdef CONFIG_KSZ_STP
+	ret = stp_br_id(&sw->info->rstp);
+#endif
+	return ret;
+}  /* sw_get_br_id */
+
+static void sw_from_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_backup(mrp, p);
+	}
+#endif
+}  /* sw_from_backup */
+
+static void sw_to_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_backup(mrp, p);
+	}
+#endif
+}  /* sw_to_backup */
+
+static void sw_from_designated(struct ksz_sw *sw, uint p, bool alt)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_designated(mrp, p, alt);
+	}
+#endif
+}  /* sw_from_designated */
+
+static void sw_to_designated(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_designated(mrp, p);
+	}
+#endif
+}  /* sw_to_designated */
+
+static void sw_tc_detected(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->tc_detected(mrp, p);
+	}
+#endif
+}  /* sw_tc_detected */
+
+static int sw_get_tcDetected(struct ksz_sw *sw, uint p)
+{
+	int ret = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *info = &sw->info->rstp;
+
+		ret = info->ops->get_tcDetected(info, p);
+	}
+#endif
+	return ret;
+}  /* sw_get_tcDetected */
+
+#define FAMILY_ID_85			0x85
+#define FAMILY_ID_95			0x95
+#define FAMILY_ID_94			0x94
+#define CHIP_ID_9567_RNX		0x67
+#define CHIP_ID_9566_RNX		0x66
+#define CHIP_ID_9477_STX		0x77
+
+#define FAMILY_ID_88			0x88
+#define FAMILY_ID_98			0x98
+#define CHIP_ID_9893_RNX		0x93
+
+static int sw_get_id(struct ksz_sw *sw, u8 *id1, u8 *id2, char *name)
+{
+	int id;
+	int i;
+	int j;
+
+	id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+	i = id;
+	i >>= 8;
+	i &= 0xffff;
+	j = i & 0xff;
+	i >>= 8;
+	*id1 = (u8) i;
+	*id2 = (u8) j;
+	switch (i) {
+	case FAMILY_ID_95:
+		strcat(name, "95");
+		switch (j) {
+		case CHIP_ID_9567_RNX:
+			strcat(name, "67RNX");
+			break;
+		case CHIP_ID_9566_RNX:
+			strcat(name, "66RNX");
+			break;
+		case CHIP_ID_9893_RNX:
+			strcat(name, "63RNX");
+			break;
+		}
+		break;
+	case FAMILY_ID_94:
+		strcat(name, "94");
+		switch (j) {
+		case CHIP_ID_9477_STX:
+			strcat(name, "77STX");
+			break;
+		}
+		break;
+	case FAMILY_ID_98:
+	case 0x64:
+		strcat(name, "98");
+		switch (j) {
+		case CHIP_ID_9567_RNX:
+			strcat(name, "97RNX");
+			break;
+		case CHIP_ID_9566_RNX:
+			strcat(name, "96RNX");
+			break;
+		case CHIP_ID_9893_RNX:
+			strcat(name, "93RNX");
+			break;
+		}
+		break;
+	}
+	if (name[0] && !name[2])
+		strcat(name, "xx");
+	return id;
+}  /* sw_get_id */
+
+static void sw_cfg_tail_tag(struct ksz_sw *sw, bool enable)
+{
+	port_cfg_tail_tag(sw, sw->HOST_PORT, enable);
+}
+
+static void sw_cfg_each_port(struct ksz_sw *sw, uint p, bool cpu)
+{
+	if (cpu)
+		p = sw->HOST_PORT;
+	else {
+		if (p >= sw->HOST_PORT)
+			p++;
+		sw->info->port_cfg[p].vid_member = (1 << p);
+	}
+	port_set_stp_state(sw, p, STP_STATE_SIMPLE);
+}
+
+static int sw_port_to_phy_addr(struct ksz_sw *sw, uint p)
+{
+	if (p >= sw->HOST_PORT)
+		p++;
+	if (0 <= p && p <= sw->mib_port_cnt)
+		return p;
+	return -1;
+}
+
+static void sw_set_port_addr(struct ksz_sw *sw, uint p, u8 *addr)
+{
+}
+
+static void sw_cfg_src_filter(struct ksz_sw *sw, bool set)
+{
+	int p;
+
+	if (!(sw->features & NEW_CAP))
+		return;
+	for (p = 0; p < sw->mib_port_cnt; p++) {
+		if (skip_host_port(sw, p))
+			continue;
+		if (!set)
+			sw->open_ports |= (1 << p);
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_SRC_ADDR_FILTER, set);
+		if (set)
+			sw->open_ports &= ~(1 << p);
+	}
+}  /* sw_cfg_src_filter */
+
+static void sw_fwd_unk_mcast(struct ksz_sw *sw, bool set)
+{
+	if (set) {
+		sw->reg->w32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			sw->HOST_MASK | SW_UNK_MCAST_ENABLE);
+		sw->overrides |= UNK_MCAST_BLOCK;
+	} else {
+		sw_cfg(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			SW_UNK_MCAST_ENABLE >> 24, false);
+		sw->overrides &= ~UNK_MCAST_BLOCK;
+	}
+}  /* sw_fwd_unk_mcast */
+
+static void sw_fwd_unk_ucast(struct ksz_sw *sw)
+{
+	sw->reg->w32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4,
+		sw->HOST_MASK | SW_UNK_UCAST_ENABLE);
+}  /* sw_fwd_unk_ucast */
+
+static void sw_fwd_unk_vid(struct ksz_sw *sw)
+{
+	sw->reg->w32(sw, REG_SW_LUE_UNK_VID_CTRL__4,
+		sw->HOST_MASK | SW_UNK_VID_ENABLE);
+}  /* sw_fwd_unk_vid */
+
+static int sw_need_dest(struct ksz_sw *sw, u8 *addr)
+{
+	int need = 0;
+
+	if (addr[0] & 0x01) {
+		int i;
+		struct ksz_mac_table *entry;
+		struct ksz_alu_table *alu;
+
+		for (i = MULTI_MAC_TABLE_ENTRIES - 1; i >= 0; i--) {
+			alu = &sw->info->alu_table[i];
+			entry = &sw->info->mac_table[i];
+			if (alu->valid &&
+			    !memcmp(addr, entry->addr, ETH_ALEN)) {
+				if (alu->forward & FWD_HOST)
+					need = 1;
+				if (alu->forward & FWD_HOST_OVERRIDE)
+					need = 2;
+				break;
+			}
+		}
+		if (i < 0 && (sw->overrides & UNK_MCAST_BLOCK))
+			need = 1;
+	} else if (!memcmp(addr, sw->info->mac_addr, ETH_ALEN))
+		need = 1;
+	return need;
+}  /* sw_need_dest */
+
+static void sw_forward(struct ksz_sw *sw, u8 *addr, u8 *self, u16 proto,
+	int tag)
+{
+	int forward = 0;
+
+	/* Already set for PTP message. */
+	if (sw->info->forward)
+		return;
+
+	/* Check for multicast addresses that are not forwarding. */
+	if (addr[0] & 0x01) {
+		int i;
+		struct ksz_mac_table *entry;
+		struct ksz_alu_table *alu;
+
+		for (i = MULTI_MAC_TABLE_ENTRIES - 1; i >= 0; i--) {
+			alu = &sw->info->alu_table[i];
+			entry = &sw->info->mac_table[i];
+			if (alu->valid &&
+			    !memcmp(addr, entry->addr, ETH_ALEN)) {
+				forward = alu->forward;
+				if (!(forward & FWD_VLAN_DEV)) {
+					if (proto == 0x888E)
+						forward = FWD_STP_DEV |
+							  FWD_VLAN_DEV;
+				}
+				break;
+			}
+		}
+		if (!forward)
+			forward = FWD_MAIN_DEV | FWD_MCAST;
+
+	/* Check unicast address to host. */
+	} else if (!memcmp(addr, self, ETH_ALEN))
+		forward = FWD_HOST;
+	else
+		forward = FWD_MAIN_DEV | FWD_UCAST;
+	if (!tag)
+		forward &= ~FWD_VLAN_DEV;
+	sw->info->forward = forward;
+}  /* sw_forward */
+
+static void sw_tx_fwd(struct work_struct *work)
+{
+	int rc;
+	bool last;
+	struct sk_buff *skb;
+	struct ksz_sw *sw = container_of(work, struct ksz_sw, tx_fwd);
+	const struct net_device_ops *ops = sw->main_dev->netdev_ops;
+
+	last = skb_queue_empty(&sw->txq);
+	while (!last) {
+		skb = skb_dequeue(&sw->txq);
+		last = skb_queue_empty(&sw->txq);
+		if (!skb)
+			continue;
+		do {
+			rc = ops->ndo_start_xmit(skb, skb->dev);
+			if (NETDEV_TX_BUSY == rc) {
+				rc = wait_event_interruptible_timeout(sw->queue,
+					!netif_queue_stopped(sw->main_dev),
+					50 * HZ / 1000);
+
+				rc = NETDEV_TX_BUSY;
+			}
+		} while (NETDEV_TX_BUSY == rc);
+	}
+}  /* sw_tx_fwd */
+
+#if 1
+static u8 last_addr[6];
+#endif
+
+static struct net_device *sw_rx_dev(struct ksz_sw *sw, u8 *data, u32 *len,
+	int *tag, int *port)
+{
+	u16 proto;
+	u16* proto_loc;
+	struct net_device *dev;
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	int index = -1;
+	int vid = 0;
+	u16 prio = 0;
+#ifdef CONFIG_KSZ_DSA
+	u32 org_len = *len;
+#endif
+
+	proto_loc = &vlan->h_vlan_proto;
+	proto = htons(*proto_loc);
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+		vid = vlan_tci & VLAN_VID_MASK;
+		prio = vlan_tci & VLAN_PRIO_MASK;
+		prio >>= VLAN_PRIO_SHIFT;
+		proto_loc = &vlan->h_vlan_encapsulated_proto;
+		proto = htons(*proto_loc);
+#if 0
+dbg_msg(" 1 vid: %x %d %04x\n", vlan_tci, vid, proto);
+#endif
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			ptr += VLAN_HLEN;
+dbg_msg(" 2 vid: %x\n", vlan_tci);
+		}
+	}
+#ifdef CONFIG_KSZ_HSR
+	if (proto == ETH_P_HSR) {
+		proto_loc += HSR_HLEN / 2;
+		proto = htons(*proto_loc);
+	}
+#endif
+
+#if 1
+/*
+ * THa  2016/02/03
+ * A company switch is sending frames that causes the dropped count to
+ * increase.
+ */
+	if (proto == 0x8874 &&
+	    0x01 == vlan->h_source[0] &&
+	    0x80 == vlan->h_source[1] &&
+	    0xc2 == vlan->h_source[2] &&
+	    0xff == vlan->h_dest[0]) {
+		return NULL;
+	}
+#endif
+	if (eth->h_proto == htons(0x9100)) {
+		u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+dbg_msg(" 1 isp: %x\n", vlan_tci);
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			ptr += VLAN_HLEN;
+			vlan_tci = ntohs(vlan->h_vlan_TCI);
+dbg_msg(" 2 vid: %x\n", vlan_tci);
+		}
+	}
+	sw->info->forward = 0;
+
+	/* Get received port number. */
+	if (sw->overrides & TAIL_TAGGING) {
+		(*len)--;
+		*tag = data[*len];
+		sw->tag.timestamp = 0;
+		sw->tag.ports = *tag;
+		if (*tag & 0x80) {
+			u32 rx_ts;
+
+			memcpy(&rx_ts, &data[*len - 4], 4);
+			rx_ts = ntohl(rx_ts);
+			sw->tag.timestamp = rx_ts;
+			(*len) -= 4;
+
+#ifdef CONFIG_1588_PTP
+			/* PTP message cannot be forwarded normally. */
+			if (sw->features & PTP_HW) {
+				struct ptp_info *ptp = &sw->ptp_hw;
+
+				sw->info->forward = ptp->forward;
+			} else
+#endif
+			if (sw->features & SW_VLAN_DEV)
+				sw->info->forward = FWD_VLAN_DEV;
+			else
+				sw->info->forward = FWD_MAIN_DEV;
+		}
+		*tag &= ~0x80;
+
+		/* In case tagging is not working right. */
+		if (*tag >= sw->mib_port_cnt)
+			*tag = 0;
+
+		/* Save receiving port. */
+		*port = *tag;
+		index = sw->info->port_cfg[*tag].index;
+	}
+#ifdef CONFIG_KSZ_IBA
+	if (proto == IBA_TAG_TYPE)
+		return sw->netdev[0];
+#endif
+
+#ifdef CONFIG_KSZ_DSA
+	/* DSA has its function to read the tail tag. */
+	if (sw->features & DSA_SUPPORT) {
+		*len = org_len;
+		return sw->netdev[0];
+	}
+#endif
+
+	/* Determine network device from VLAN id. */
+	if (index < 0) {
+		index = 0;
+		if (vid && (sw->features & SW_VLAN_DEV)) {
+			int p;
+
+			for (p = 0; p < sw->eth_cnt; p++) {
+				if (vid == sw->eth_maps[p].vlan) {
+					*port = sw->eth_maps[p].port;
+					index = sw->info->port_cfg[*port].index;
+					break;
+				}
+			}
+		}
+	}
+	if (index >= sw->dev_count + sw->dev_offset) {
+		printk(KERN_INFO "  [%s] netdev not correct\n", __func__);
+		BUG();
+	}
+	dev = sw->netdev[index];
+	if (sw->features & VLAN_PORT_TAGGING) {
+		(*tag)++;
+		if (!(sw->vlan_id & (1 << *tag)))
+			*tag = 0;
+	}
+	sw_forward(sw, data, dev->dev_addr, proto, *tag);
+	if ((sw->overrides & UNK_MCAST_BLOCK) &&
+	    (sw->info->forward & (FWD_MCAST | FWD_KNOWN)) == FWD_MCAST) {
+		struct sk_buff *skb;
+		u16 ports = sw->PORT_MASK & ~(1 << *port) & ~sw->HOST_MASK;
+
+		ports &= sw->live_ports;
+		if (!ports)
+			return dev;
+		if (memcmp(data, ipv6_neigh_mcast, 3)) {
+			if (memcmp(data, last_addr, 6)) {
+dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x  %d=%d %x\n",
+data[0], data[1], data[2], data[3], data[4], data[5], *port, *len, ports);
+				memcpy(last_addr, data, 6);
+			}
+		}
+#ifdef CONFIG_KSZ_MRP
+		/* Do not forward IPv6 packets as the test tool confuses them
+		 * for AVB traffic.
+		 */
+		else if ((sw->features & MRP_SUPPORT) && fqtss_hack)
+			return dev;
+		if (memcmp(data, ipv6_neigh_mcast, 3) &&
+		    (sw->features & MRP_SUPPORT)) {
+			int rc;
+
+			rc = mrp_chk_mcast(&sw->mrp, vlan->h_dest, vid, prio,
+					   proto, *port);
+			if (rc != 1)
+				return dev;
+		}
+#endif
+		skb = alloc_skb(*len + 8, GFP_ATOMIC);
+		if (!skb)
+			return dev;
+		skb->dev = sw->main_dev;
+		skb_reset_network_header(skb);
+		skb_reset_transport_header(skb);
+		memcpy(skb->data, data, *len);
+		skb_put(skb, *len);
+		sw->net_ops->add_tail_tag(sw, skb, ports);
+		skb->protocol = htons(ETH_P_TRAILER);
+
+		skb_queue_tail(&sw->txq, skb);
+		schedule_work(&sw->tx_fwd);
+	}
+	return dev;
+}  /* sw_rx_dev */
+
+static int pkt_matched(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, void *ptr, int (*get_multi)(void *ptr),
+	u8 h_promiscuous)
+{
+	int drop = false;
+	u8 bcast_addr[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	if (skb->data[0] & 0x01) {
+		if (memcmp(skb->data, bcast_addr, ETH_ALEN) && !get_multi(ptr))
+			drop = sw_match_multi(sw,
+				sw->net_ops->get_priv_port(dev), skb->data);
+	} else if (h_promiscuous && memcmp(skb->data, dev->dev_addr, ETH_ALEN))
+		drop = true;
+	if (drop)
+		return 0;
+	return skb->len;
+}  /* pkt_matched */
+
+static int sw_match_pkt(struct ksz_sw *sw, struct net_device **dev,
+	void **priv, int (*get_promiscuous)(void *ptr),
+	int (*get_multi)(void *ptr), struct sk_buff *skb,
+	u8 h_promiscuous)
+{
+	int s_promiscuous;
+
+	/* This function will return the child device if matched, and the
+	 * parent device if it is matched.
+	 */
+	if (sw->dev_count <= 1)
+		return true;
+	s_promiscuous = get_promiscuous(*priv);
+	if (!s_promiscuous && !pkt_matched(sw, skb, *dev, *priv, get_multi,
+	    h_promiscuous)) {
+		int matched = false;
+
+		/* There is a parent network device. */
+		if (sw->dev_offset) {
+			matched = true;
+			*dev = sw->netdev[0];
+			*priv = netdev_priv(*dev);
+			s_promiscuous = get_promiscuous(*priv);
+			if (!s_promiscuous && !pkt_matched(sw, skb, *dev,
+			    *priv, get_multi, h_promiscuous))
+				matched = false;
+		}
+		return matched;
+	}
+	return true;
+}  /* sw_match_pkt */
+
+static struct net_device *sw_parent_rx(struct ksz_sw *sw,
+	struct net_device *dev, struct sk_buff *skb, int *forward,
+	struct net_device **parent_dev, struct sk_buff **parent_skb)
+{
+	if (sw->dev_offset && dev != sw->netdev[0]) {
+		*parent_dev = sw->netdev[0];
+		if (!*forward)
+			*forward = FWD_MAIN_DEV;
+		if ((*forward & (FWD_MAIN_DEV | FWD_STP_DEV)) ==
+		    (FWD_MAIN_DEV | FWD_STP_DEV))
+			*parent_skb = skb_clone(skb, GFP_ATOMIC);
+		else if (!(*forward & FWD_STP_DEV))
+			dev = *parent_dev;
+		else
+			*forward &= ~FWD_VLAN_DEV;
+	}
+	return dev;
+}  /* sw_parent_rx */
+
+static int sw_port_vlan_rx(struct ksz_sw *sw, struct net_device *dev,
+	struct net_device *parent_dev, struct sk_buff *skb, int forward,
+	int tag, void *ptr, void (*rx_tstamp)(void *ptr, struct sk_buff *skb))
+{
+	struct sk_buff *vlan_skb;
+	struct net_device *vlan_dev = dev;
+
+	/* Add VLAN tag manually. */
+	if (!(forward & FWD_VLAN_DEV))
+		return false;
+
+	if (!tag || !(sw->features & VLAN_PORT))
+		return false;
+
+	/* tag never equals sw->HOST_PORT + 1. */
+if (tag == sw->HOST_PORT + 1)
+BUG();
+	if (tag > sw->HOST_PORT)
+		--tag;
+	tag += VLAN_PORT_START;
+
+	/* Only forward to one network device. */
+	if (!(forward & FWD_MAIN_DEV)) {
+		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
+		return true;
+	}
+	vlan_skb = skb_clone(skb, GFP_ATOMIC);
+	if (!vlan_skb)
+		return false;
+	skb_reset_mac_header(vlan_skb);
+	__vlan_hwaccel_put_tag(vlan_skb, htons(ETH_P_8021Q), tag);
+#ifdef CONFIG_1588_PTP
+	do {
+		struct ptp_info *ptp = ptr;
+
+		if (rx_tstamp && (ptp->rx_en & 1))
+			rx_tstamp(ptp, vlan_skb);
+	} while (0);
+#endif
+	if (parent_dev && dev != parent_dev) {
+		vlan_dev = parent_dev;
+		vlan_skb->dev = vlan_dev;
+	}
+	vlan_skb->protocol = eth_type_trans(vlan_skb, vlan_dev);
+	netif_rx(vlan_skb);
+	return true;
+}  /* sw_port_vlan_rx */
+
+static int sw_drop_icmp(struct sk_buff *skb, int extra_skb)
+{
+	int drop = 0;
+
+	if (skb && extra_skb &&	skb->protocol == htons(ETH_P_IP)) {
+		struct iphdr *iph = (struct iphdr *) skb->data;
+
+		drop = (iph->protocol == IPPROTO_ICMP);
+	}
+	return drop;
+}  /* sw_drop_icmp */
+
+static int sw_drv_rx(struct ksz_sw *sw, struct sk_buff *skb, uint port)
+{
+	int ret = 1;
+
+#ifdef CONFIG_KSZ_IBA
+	if (sw->features & IBA_SUPPORT) {
+		ret = iba_rcv(&sw->info->iba, skb);
+		if (!ret)
+			return ret;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+		if (2 == sw->info->iba.use_iba) {
+			dev_kfree_skb_irq(skb);
+			return 0;
+		}
+#endif
+	}
+#endif
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		ret = stp_rcv(&sw->info->rstp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef PROC_MRP
+	if (sw->features & MRP_SUPPORT) {
+		ret = mrp_rcv(&sw->mrp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		ret = dlr_rcv(&sw->info->dlr, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		ret = hsr_rcv(&sw->info->hsr, skb, port);
+
+		/* It is an HSR frame or consumed. */
+		if (ret < 2)
+			return ret;
+		if (sw->features & HSR_REDBOX) {
+			ret = hsr_chk(&sw->info->hsr, skb, port);
+			if (ret < 2)
+				return ret;
+		}
+	}
+#endif
+
+	/* Need to remove VLAN tag if not using tail tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV) &&
+	    !(sw->overrides & TAIL_TAGGING)) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			int p;
+			int vid;
+			struct ethhdr *eth;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			for (p = 0; p < sw->eth_cnt; p++) {
+				if (vid == sw->eth_maps[p].vlan) {
+					eth = (struct ethhdr *)
+						skb_pull(skb, VLAN_HLEN);
+					memmove(eth, vlan, 12);
+					break;
+				}
+			}
+		}
+	}
+	return ret;
+}  /* sw_drv_rx */
+
+static int sw_get_mtu(struct ksz_sw *sw)
+{
+	int need_tail_tag = false;
+	int header = 0;
+	int mtu = 0;
+
+	if (sw->features & (PTP_HW | DLR_HW | HSR_HW))
+		need_tail_tag = true;
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT_TAGGING)
+		need_tail_tag = true;
+	if (sw->features & (STP_SUPPORT | DSA_SUPPORT))
+		need_tail_tag = true;
+	if (need_tail_tag) {
+		mtu += 2;
+		if (sw->TAIL_TAG_LOOKUP < 0x100)
+			mtu -= 1;
+		if (sw->features & PTP_HW)
+			mtu += 4;
+	}
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		header = HSR_HLEN;
+#endif
+	if (sw->features & SW_VLAN_DEV)
+		if (header < VLAN_HLEN)
+			header = VLAN_HLEN;
+	mtu += header;
+dbg_msg("mtu: %d\n", mtu);
+	return mtu;
+}  /* sw_get_mtu */
+
+static int sw_get_tx_len(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+	int *header)
+{
+	int len = skb->len;
+	int hlen = 0;
+
+	if (sw->features & SW_VLAN_DEV)
+		hlen = VLAN_HLEN;
+#ifdef CONFIG_KSZ_HSR
+	do {
+		int i;
+
+		i = sw->info->port_cfg[port].index;
+		if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW))
+			hlen = HSR_HLEN;
+	} while (0);
+#endif
+	*header += hlen;
+	if (!(sw->overrides & TAIL_TAGGING))
+		return len;
+	if (len < 60)
+		len = 60;
+	len += 2;
+	if (sw->overrides & PTP_TAG)
+		len += 4;
+	return len;
+}  /* sw_get_tx_len */
+
+static void sw_add_tail_tag(struct ksz_sw *sw, struct sk_buff *skb, uint ports)
+{
+	struct ksz_sw_tx_tag tx_tag;
+	u8 *trailer;
+	u8 *tag;
+	int len = 2;
+	int ptp_len = 0;
+
+	/* PTP is enabled and so requires extra 4 bytes. */
+	if (sw->overrides & PTP_TAG)
+		ptp_len = 4;
+	len += ptp_len;
+	if (sw->TAIL_TAG_LOOKUP < 0x100)
+		len--;
+	trailer = skb_put(skb, len);
+	memset(&tx_tag, 0, sizeof(struct ksz_sw_tx_tag));
+	tx_tag.ports = ports & sw->PORT_MASK;
+	if (!tx_tag.ports)
+		tx_tag.ports = sw->TAIL_TAG_LOOKUP;
+	else if (ports & 0x80000000)
+		tx_tag.ports |= sw->TAIL_TAG_OVERRIDE;
+	tx_tag.ports = htons(tx_tag.ports);
+	tag = (u8 *) &tx_tag;
+	memcpy(trailer, &tag[4 - ptp_len], ptp_len + 2);
+	if (sw->TAIL_TAG_LOOKUP < 0x100)
+		trailer[ptp_len] = trailer[ptp_len + 1];
+}  /* sw_add_tail_tag */
+
+static int sw_get_tail_tag(u8 *trailer, int *port)
+{
+	int len = 1;
+
+	if (*trailer & 0x80)
+		len += 4;
+	*trailer &= ~0x80;
+	*port = *trailer;
+	return len;
+}  /* sw_get_tail_tag */
+
+static int sw_get_phys_port(struct ksz_sw *sw, uint port)
+{
+	if (port >= sw->HOST_PORT)
+		port++;
+	return port;
+}  /* sw_get_phy_port */
+
+static int sw_get_virt_port(struct ksz_sw *sw, uint port)
+{
+	if (port >= sw->HOST_PORT)
+		port--;
+	return port;
+}  /* sw_get_virt_port */
+
+static void sw_add_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM) {
+			if (vid > sw->HOST_PORT)
+				vid++;
+			sw->vlan_id |= (1 << vid);
+		}
+	}
+}  /* sw_add_vid */
+
+static void sw_kill_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM) {
+			if (vid > sw->HOST_PORT)
+				vid++;
+			sw->vlan_id &= ~(1 << vid);
+		}
+	}
+}  /* sw_kill_vid */
+
+static int append_tag(u16 lookup, u8 *pad, u8 *tag, int len, int ptp_len,
+	int addlen)
+{
+	memcpy(&pad[len], &tag[4 - ptp_len], ptp_len + 2);
+
+	/* Only one byte for the tag. */
+	if (lookup < 0x100) {
+		pad[len + ptp_len] = pad[len + ptp_len + 1];
+		addlen--;
+	}
+	return addlen;
+}
+
+static int add_frag(void *from, char *to, int offset, int len, int odd,
+	struct sk_buff *skb)
+{
+	memcpy(to + offset, from, len);
+	return 0;
+}
+
+static struct sk_buff *sw_ins_vlan(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb)
+{
+#ifdef CONFIG_KSZ_IBA
+	if (skb->protocol == htons(ETH_P_IBA))
+		return skb;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	do {
+		int i = sw->info->port_cfg[port].index;
+
+		if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW))
+			return skb;
+	} while (0);
+#endif
+
+	/* Need to insert VLAN tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV)) {
+		u16 vid;
+		struct vlan_ethhdr *vlan;
+		struct ethhdr *eth;
+		struct sk_buff *nskb;
+		int i = sw->info->port_cfg[port].index;
+
+		/*
+		 * Bridge uses clones of socket buffer to send to both
+		 * devices!
+		 */
+		nskb = skb_copy(skb, GFP_ATOMIC);
+		if (!nskb)
+			return skb;
+		dev_kfree_skb_irq(skb);
+		skb = nskb;
+		eth = (struct ethhdr *) skb->data;
+		if (sw->eth_cnt && (sw->eth_maps[i].proto & (DLR_HW | HSR_HW)))
+			memcpy(eth->h_source, sw->info->mac_addr, ETH_ALEN);
+
+		vid = sw->info->port_cfg[port].vid;
+		vlan = (struct vlan_ethhdr *) skb_push(skb, VLAN_HLEN);
+		memmove(vlan, eth, 12);
+		vlan->h_vlan_TCI = htons(vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+	}
+	return skb;
+}  /* sw_ins_vlan */
+
+#ifdef CONFIG_KSZ_HSR
+static struct sk_buff *sw_ins_hsr(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb, u16 *ports)
+{
+	int i = sw->info->port_cfg[port].index;
+
+#ifdef CONFIG_KSZ_IBA
+	if (skb->protocol == htons(ETH_P_IBA))
+		return skb;
+#endif
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW)) {
+		struct ksz_hsr_info *info = &sw->info->hsr;
+		struct hsr_port *from =
+			hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+#ifdef CONFIG_1588_PTP
+		struct ptp_msg *msg;
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (ptp->tx_msg_parsed)
+			msg = ptp->tx_msg;
+		else {
+			msg = check_ptp_msg(skb->data, NULL);
+			ptp->tx_msg_parsed = true;
+			ptp->tx_msg = msg;
+		}
+		if (msg) {
+			if (msg->hdr.messageType != SYNC_MSG &&
+			    msg->hdr.messageType != ANNOUNCE_MSG)
+				return skb;
+			if (!(*ports & sw->TAIL_TAG_LOOKUP)) {
+				if (!(*ports & (1 << info->ports[0]))) {
+					dev_kfree_skb_irq(skb);
+					return NULL;
+				}
+			}
+		}
+#endif
+		if (!hsr_forward_skb(skb, from))
+			return NULL;
+		memcpy(&skb->data[6], info->src_addr, ETH_ALEN);
+		*ports = info->member;
+	}
+	return skb;
+}  /* sw_ins_hsr */
+#endif
+
+static struct sk_buff *sw_check_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct ksz_port *priv, void *ptr,
+	int (*update_msg)(u8 *data, u32 port, u32 overrides))
+{
+	int len;
+	uint port;
+	struct sk_buff *org_skb;
+	struct ksz_sw_tx_tag tx_tag;
+	u8 *tag;
+	int update_dst = (sw->overrides & TAIL_TAGGING);
+	int ptp_len = 0;
+
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = ptr;
+#endif
+
+	if (!update_dst)
+		return sw_ins_vlan(sw, priv->first_port, skb);
+
+	if (skb->protocol == htons(ETH_P_TRAILER))
+		return skb;
+#ifdef CONFIG_KSZ_STP
+	if (skb->protocol == htons(STP_TAG_TYPE))
+		return skb;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (skb->protocol == htons(DLR_TAG_TYPE))
+		return skb;
+#endif
+#ifdef PROC_MRP
+	if (skb->protocol == htons(ETH_P_MSRP) ||
+	    skb->protocol == htons(ETH_P_MVRP) ||
+	    skb->protocol == htons(ETH_P_MMRP)) {
+
+		/* MRP frame from application has 2 end marks. */
+		if (skb->data[skb->len - 1] || skb->data[skb->len - 2])
+			return skb;
+
+		mrp_rcv(&sw->mrp, skb, sw->HOST_PORT);
+		return NULL;
+	}
+#endif
+
+	/* PTP is enabled and so requires extra 4 bytes. */
+	if (sw->overrides & PTP_TAG)
+		ptp_len = 4;
+
+	tx_tag.ports = 0;
+	tx_tag.timestamp = 0;
+
+#ifdef CONFIG_KSZ_IBA
+	if (skb->protocol == htons(ETH_P_IBA)) {
+		tx_tag.ports = sw->TAIL_TAG_LOOKUP;
+		len = skb->len;
+		goto add_tag;
+	}
+#endif
+
+	org_skb = skb;
+	port = 0;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port + 1;
+
+	do {
+		u16 prio;
+		u16 vid;
+		int i = sw->info->port_cfg[priv->first_port].index;
+		u32 features = sw->features;
+
+		if (sw->features & SW_VLAN_DEV)
+			features = sw->eth_maps[i].proto;
+		if (!(features & VLAN_PORT) || port || vlan_get_tag(skb, &vid))
+			break;
+		prio = vid & VLAN_PRIO_MASK;
+		vid &= VLAN_VID_MASK;
+		if (vid < VLAN_PORT_START)
+			break;
+		vid -= VLAN_PORT_START;
+		if (vid > sw->HOST_PORT)
+			vid++;
+		if (!vid || vid > sw->mib_port_cnt)
+			break;
+		port = vid;
+
+		if (sw->vid || prio) {
+			struct vlan_ethhdr *vlan =
+				(struct vlan_ethhdr *) skb->data;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vlan_tci &= ~VLAN_VID_MASK;
+			vlan_tci |= sw->vid;
+			vlan->h_vlan_TCI = htons(vlan_tci);
+
+		/* Need to remove VLAN tag manually. */
+		} else if (!(sw->overrides & TAG_REMOVE)) {
+			u8 *data;
+
+			len = VLAN_ETH_HLEN - 2;
+			data = &skb->data[len];
+			memmove(data - VLAN_HLEN, data, skb->len - len);
+			skb->len -= VLAN_HLEN;
+		}
+	} while (0);
+
+	if (port)
+		tx_tag.ports = 1 << (port - 1);
+
+	/* Socket buffer has no fragments. */
+	if (!skb_shinfo(skb)->nr_frags) {
+
+#ifdef NET_SKBUFF_DATA_USES_OFFSET
+		len = skb_end_pointer(skb) - skb->data;
+#else
+		len = skb->end - skb->data;
+#endif
+		if (skb->len + ptp_len + 2 > len || len < 60 + ptp_len + 2) {
+			len = (skb->len + ptp_len + 5) & ~3;
+			if (len < 68)
+				len = 68;
+			skb = dev_alloc_skb(len);
+			if (!skb)
+				return NULL;
+			memcpy(skb->data, org_skb->data, org_skb->len);
+			skb->len = org_skb->len;
+			copy_old_skb(org_skb, skb);
+		}
+		if (skb->len < 60) {
+			memset(&skb->data[skb->len], 0, 60 - skb->len);
+			skb->len = 60;
+		}
+		len = skb->len;
+	}
+	if (!tx_tag.ports)
+		tx_tag.ports = sw->TAIL_TAG_LOOKUP;
+
+#ifdef CONFIG_1588_PTP
+	if (ptp)
+		ptp_set_tx_info(ptp, skb->data, &tx_tag);
+#endif
+	if (sw->TAIL_TAG_LOOKUP == tx_tag.ports) {
+		/* Use VLAN for port forwarding if not specified directly. */
+		skb = sw_ins_vlan(sw, priv->first_port, skb);
+		if (len != skb->len)
+			len = skb->len;
+	}
+	do {
+		int dest;
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)skb->data;
+
+		/* The MAC table was programmed to forward only to host.
+		 * Need destination ports to send out.
+		 * The port may be blocked.  Need override to send out.
+		 */
+		dest = sw_need_dest(sw, skb->data);
+		if (dest > 1 && sw->TAIL_TAG_LOOKUP == tx_tag.ports) {
+			tx_tag.ports = sw->live_ports & sw->tx_ports[0];
+			tx_tag.ports &= ~sw->HOST_MASK;
+		}
+
+		/* Honor the VLAN priority to put in different queue. */
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			u16 prio;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+			struct ksz_port_cfg *cfg =
+				&sw->info->port_cfg[sw->HOST_PORT];
+			u32 queue = cfg->tc_map[0];
+
+			prio = vlan_tci & VLAN_PRIO_MASK;
+			prio >>= VLAN_PRIO_SHIFT;
+			queue >>= prio * PORT_TC_MAP_S;
+			queue &= PORT_TC_MAP_M;
+			if (sw->features & IS_9893)
+				tx_tag.ports |= (queue << 3);
+			else
+				tx_tag.ports |= (queue << 7);
+		}
+		if (tx_tag.ports && 2 == dest)
+			tx_tag.ports |= sw->TAIL_TAG_OVERRIDE;
+	} while (0);
+
+#ifdef CONFIG_KSZ_HSR
+	skb = sw_ins_hsr(sw, priv->first_port, skb, &tx_tag.ports);
+	if (!skb)
+		return NULL;
+	len = skb->len;
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+add_tag:
+#endif
+	tx_tag.ports = htons(tx_tag.ports);
+	tx_tag.timestamp = htonl(tx_tag.timestamp);
+	tag = (u8 *) &tx_tag;
+
+	/* Socket buffer has no fragments. */
+	if (!skb_shinfo(skb)->nr_frags) {
+		len = append_tag(sw->TAIL_TAG_LOOKUP, skb->data, tag, len,
+			ptp_len, ptp_len + 2);
+		skb_put(skb, len);
+	} else {
+		struct sock dummy;
+		struct sock *sk;
+
+		sk = skb->sk;
+		if (!sk) {
+			sk = &dummy;
+			sk->sk_allocation = GFP_KERNEL;
+			atomic_set(&sk->sk_wmem_alloc, 1);
+		}
+
+		/* Clear last tag. */
+		memset(&sw->tx_pad[sw->tx_start], 0, sizeof(tx_tag));
+		sw->tx_start = 0;
+		len = ptp_len + 2;
+		if (skb->len < 60) {
+			sw->tx_start = 60 - skb->len;
+			len += sw->tx_start;
+		}
+		len = append_tag(sw->TAIL_TAG_LOOKUP, sw->tx_pad, tag,
+			sw->tx_start, ptp_len, len);
+		skb_append_datato_frags(sk, skb, add_frag, sw->tx_pad, len);
+	}
+	return skb;
+}  /* sw_check_skb */
+
+static struct sk_buff *sw_check_tx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, struct ksz_port *priv)
+{
+	void *ptr = NULL;
+	int (*update_msg)(u8 *data, u32 port, u32 overrides) = NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptr = ptp;
+#if 0
+		update_msg = ptp->ops->update_msg;
+#endif
+	}
+#endif
+
+	return sw_check_skb(sw, skb, priv, ptr, update_msg);
+}  /* sw_check_tx */
+
+#ifdef CONFIG_KSZ_DSA
+static struct sk_buff *tail_xmit(struct sk_buff *skb, struct net_device *dev,
+	struct ksz_sw *sw)
+{
+	struct sk_buff *nskb;
+	int padlen;
+	int addlen = 8;
+
+	if (skb->protocol == htons(ETH_P_TRAILER))
+		return skb;
+
+	/*
+	 * We have to make sure that the trailer ends up as the very
+	 * last 4 bytes of the packet.  This means that we have to pad
+	 * the packet to the minimum ethernet frame size, if necessary,
+	 * before adding the trailer.
+	 */
+	padlen = 0;
+	if (skb->len < 60)
+		padlen = 60 - skb->len;
+
+	nskb = alloc_skb(NET_IP_ALIGN + skb->len + padlen + addlen, GFP_ATOMIC);
+	if (nskb == NULL) {
+		dev_kfree_skb_irq(skb);
+		return NULL;
+	}
+	skb_reserve(nskb, NET_IP_ALIGN);
+
+	skb_reset_mac_header(nskb);
+	skb_set_network_header(nskb, skb_network_header(skb) - skb->head);
+	skb_set_transport_header(nskb, skb_transport_header(skb) - skb->head);
+	skb_copy_and_csum_dev(skb, skb_put(nskb, skb->len));
+	nskb->dev = skb->dev;
+	dev_kfree_skb_irq(skb);
+
+	if (padlen) {
+		u8 *pad = skb_put(nskb, padlen);
+		memset(pad, 0, padlen);
+	}
+
+	sw->net_ops->add_tail_tag(sw, nskb, 0);
+
+	nskb->protocol = htons(ETH_P_TRAILER);
+
+	return nskb;
+}
+#endif
+
+static struct sk_buff *sw_final_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, struct ksz_port *port)
+{
+#ifdef CONFIG_KSZ_DSA
+	skb = tail_xmit(skb, dev, sw);
+	if (!skb)
+		return NULL;
+#endif
+
+	skb = sw->net_ops->check_tx(sw, dev, skb, port);
+	if (!skb)
+		return NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			ptp->ops->get_tx_tstamp(ptp, skb);
+	}
+#endif
+	return skb;
+}  /* sw_final_skb */
+
+static void sw_start(struct ksz_sw *sw, u8 *addr)
+{
+	int need_tail_tag = false;
+	int need_vlan = false;
+	int setup = true;
+
+#ifdef CONFIG_KSZ_IBA
+	if (2 <= sw->info->iba.use_iba)
+		setup = false;
+#endif
+	sw->ops->acquire(sw);
+	if (setup)
+		sw_setup(sw);
+	sw_enable(sw);
+
+	sw_set_addr(sw, addr);
+
+	/* STP has its own mechanism to handle looping. */
+	if (!(sw->features & STP_SUPPORT))
+		sw_cfg_src_filter(sw, true);
+	if (sw->features & (PTP_HW | DLR_HW | HSR_HW))
+		need_tail_tag = true;
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT) {
+		if (sw->features & VLAN_PORT_REMOVE_TAG) {
+			struct ksz_vlan_table entry;
+			int p;
+
+			memset(&entry, 0, sizeof(struct ksz_vlan_table));
+			sw->ops->release(sw);
+			entry.fid = VLAN_PORT_START;
+			entry.untag = sw->PORT_MASK;
+			entry.ports = sw->PORT_MASK;
+			entry.valid = 1;
+			sw_w_vlan_table(sw, VLAN_PORT_START, &entry);
+			for (p = 0; p < sw->mib_port_cnt - 1; p++) {
+				entry.fid = VLAN_PORT_START + p + 1;
+				entry.untag = (1 << p);
+				entry.ports = (1 << p) | (1 << sw->HOST_PORT);
+				entry.valid = 1;
+				sw_w_vlan_table(sw, VLAN_PORT_START + p + 1,
+					&entry);
+			}
+			sw->ops->acquire(sw);
+			for (p = 0; p < sw->mib_port_cnt; p++) {
+				p = chk_last_port(sw, p);
+				sw_cfg_def_vid(sw, p, VLAN_PORT_START);
+			}
+			need_vlan = true;
+			sw->overrides |= TAG_REMOVE;
+		}
+		if (sw->features & VLAN_PORT_TAGGING)
+			need_tail_tag = true;
+	}
+	if (sw->features & SW_VLAN_DEV) {
+		struct ksz_vlan_table entry;
+		int i;
+		int p;
+		uint q;
+
+		memset(&entry, 0, sizeof(struct ksz_vlan_table));
+		for (p = 0; p < sw->eth_cnt; p++) {
+
+			/* Not really using VLAN. */
+			if (1 == sw->eth_maps[p].vlan)
+				continue;
+			sw->ops->release(sw);
+
+			/*
+			 * Setting FID allows same MAC address in different
+			 * VLANs.
+			 */
+			entry.fid = sw->eth_maps[p].vlan & (FID_ENTRIES - 1);
+			entry.untag = sw->eth_maps[p].mask;
+
+			/* Use tail tag to determine the network device. */
+			if (need_tail_tag)
+				entry.untag |= sw->HOST_MASK;
+			entry.ports = sw->HOST_MASK | sw->eth_maps[p].mask;
+			entry.valid = 1;
+			sw_w_vlan_table(sw, sw->eth_maps[p].vlan, &entry);
+			sw->ops->acquire(sw);
+			for (i = 0, q = sw->eth_maps[p].port;
+			     i < sw->eth_maps[p].cnt; i++, q++) {
+				if (q == sw->HOST_PORT)
+					continue;
+				sw_cfg_def_vid(sw, q, sw->eth_maps[p].vlan);
+			}
+			need_vlan = true;
+		}
+	}
+	if (sw->features & (STP_SUPPORT | DSA_SUPPORT))
+		need_tail_tag = true;
+	if (sw->features & (DLR_HW | MRP_SUPPORT))
+		need_vlan = true;
+
+#ifdef CONFIG_KSZ_MSTP
+	if (sw->features & STP_SUPPORT)
+		need_vlan = true;
+#endif
+	if (need_vlan)
+		sw_ena_vlan(sw);
+	if (need_tail_tag) {
+		port_cfg_tail_tag(sw, sw->HOST_PORT, 1);
+if (!(sw->overrides & TAIL_TAGGING))
+dbg_msg(" ! tail tag not set\n");
+	}
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_start(stp);
+	} else
+		stp_set_addr(&sw->info->rstp, sw->info->mac_addr);
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg->start(ptp, true);
+if (!(sw->overrides & PTP_TAG))
+dbg_msg(" ! ptp tag not set\n");
+	}
+#endif
+}  /* sw_start */
+
+static int sw_stop(struct ksz_sw *sw, int complete)
+{
+	int reset = false;
+	int hw_access = true;
+
+#ifdef CONFIG_KSZ_IBA
+	if (2 <= sw->info->iba.use_iba) {
+		hw_access = false;
+	}
+#endif
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_stop(stp, hw_access);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp_stop(mrp);
+	}
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		reset = ptp->ops->stop(ptp, hw_access);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	if (2 <= sw->info->iba.use_iba) {
+		if (sw->info->iba.use_iba < 4) {
+			sw->ops->acquire(sw);
+			sw_cfg(sw, REG_SW_OPERATION, SW_RESET, 1);
+			sw->ops->release(sw);
+
+			/* Indicate no more hardware access. */
+			sw->info->iba.use_iba = 4;
+		}
+		return reset;
+	}
+#endif
+
+	sw->ops->acquire(sw);
+	if (!reset)
+		sw_reset(sw);
+	reset = true;
+	sw_init(sw);
+
+	/* Clean out static MAC table when the switch shutdown. */
+	if (complete)
+		sw_clr_sta_mac_table(sw);
+	sw->ops->release(sw);
+	return reset;
+}  /* sw_stop */
+
+static void sw_init_mib(struct ksz_sw *sw)
+{
+	int i;
+
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		i = chk_last_port(sw, i);
+		sw->port_mib[i].mib_start = 0;
+		if (sw->next_jiffies < jiffies)
+			sw->next_jiffies = jiffies + HZ * 2;
+		else
+			sw->next_jiffies += MIB_READ_INTERVAL;
+		sw->counter[i].time = sw->next_jiffies;
+		sw->port_state[i].state = media_disconnected;
+		port_init_cnt(sw, i);
+	}
+	for (i = sw->phy_port_cnt; i < sw->mib_port_cnt; i++) {
+		i = chk_last_port(sw, i);
+		if (sw->port_info[i].phy)
+			continue;
+		sw->port_state[i].state = media_connected;
+	}
+	sw->port_state[sw->HOST_PORT].state = media_connected;
+}  /* sw_init_mib */
+
+static int sw_open_dev(struct ksz_sw *sw, struct net_device *dev, u8 *addr)
+{
+	int mode = 0;
+
+	sw_init_mib(sw);
+
+	sw->main_dev = dev;
+	sw->net_ops->start(sw, addr);
+	if (sw->features & AVB_SUPPORT)
+		mode |= 1;
+	if (sw->dev_count > 1)
+		mode |= 1;
+	if (sw->features & DIFF_MAC_ADDR)
+		mode |= 2;
+	return mode;
+}  /* sw_open_dev */
+
+static void sw_open_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port, u8 *state)
+{
+	int i;
+	int p;
+	struct ksz_port_info *info;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_info *mrp = &sw->mrp;
+	u16 mrp_ports = 0;
+#endif
+
+	for (i = 0, p = port->first_port; i < port->port_cnt; i++, p++) {
+		info = &sw->port_info[p];
+		if (!info->phy)
+			continue;
+
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info->link = 0xFF;
+		info->state = media_unknown;
+		info->report = true;
+		if (port->port_cnt == 1) {
+			if (sw->net_ops->get_priv_port) {
+				struct ksz_port *sw_port =
+					sw->net_ops->get_priv_port(
+						sw->netdev[0]);
+
+				port->speed = sw_port->speed;
+				port->duplex = sw_port->duplex;
+				port->flow_ctrl = sw_port->flow_ctrl;
+			}
+			if (info->own_speed != port->speed ||
+			    info->own_duplex != port->duplex) {
+				if (info->own_speed)
+					port->speed = info->own_speed;
+				if (info->own_duplex)
+					port->duplex = info->own_duplex;
+			}
+		}
+	}
+
+	sw->ops->acquire(sw);
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		mrp_ports = mrp->tx_ports;
+	}
+#endif
+
+	/* Need to open the port in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		*state = STP_STATE_SIMPLE;
+		if (sw->dev_offset && !(sw->features & STP_SUPPORT)) {
+			*state = STP_STATE_FORWARDING;
+		}
+		if (sw->features & SW_VLAN_DEV) {
+			i = sw->info->port_cfg[port->first_port].index;
+			if (!(sw->eth_maps[i].proto & HSR_HW))
+				*state = STP_STATE_FORWARDING;
+		}
+		for (i = 0, p = port->first_port; i < port->port_cnt;
+		     i++, p++) {
+			if (p == sw->HOST_PORT)
+				continue;
+			sw->dev_ports |= (1 << p);
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT) {
+				stp_enable_port(&sw->info->rstp, p, state);
+			}
+#endif
+			port_set_stp_state(sw, p, *state);
+		}
+	} else if (!sw->dev_count) {
+		sw->dev_ports = sw->PORT_MASK;
+	}
+
+	sw->phy_intr = sw->PORT_MASK;
+	if (port->force_link)
+		port_force_link_speed(port);
+	else
+		port_set_link_speed(port);
+	port_get_link_speed(port);
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+	if (sw->dev_offset && dev == sw->netdev[0]) {
+		struct ksz_port *sw_port = NULL;
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		if (sw->net_ops->get_priv_port)
+			sw_port = sw->net_ops->get_priv_port(dev);
+		if (sw_port)
+			hw_priv->phy_id = sw_port->linked->phy_id;
+	}
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (dev != sw->netdev[i])
+			continue;
+#ifdef CONFIG_KSZ_DLR
+		if (sw->eth_maps[i].proto & DLR_HW) {
+			struct ksz_dlr_info *info = &sw->info->dlr;
+
+			if (info->ports[0] == port->first_port)
+				prep_dlr(info, dev, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (sw->eth_maps[i].proto & HSR_HW) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			if (info->ports[0] == port->first_port)
+				prep_hsr(info, dev, dev->dev_addr);
+		}
+#endif
+	}
+}  /* sw_open_port */
+
+static void sw_close_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	int i;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_info *mrp = &sw->mrp;
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	if (2 <= sw->info->iba.use_iba && dev == sw->main_dev)
+		return;
+#endif
+
+	/* Need to shut the port manually in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		int p;
+
+		sw->ops->acquire(sw);
+		for (i = 0, p = port->first_port; i < port->port_cnt;
+		     i++, p++) {
+			if (p == sw->HOST_PORT)
+				continue;
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT)
+				stp_disable_port(&sw->info->rstp, p);
+#endif
+#ifdef CONFIG_KSZ_MRP
+			if (sw->features & MRP_SUPPORT) {
+				mrp_close_port(mrp, p);
+			}
+#endif
+			sw->dev_ports &= ~(1 << p);
+			port_set_stp_state(sw, p, STP_STATE_DISABLED);
+		}
+		sw->ops->release(sw);
+	} else if (!sw->dev_count) {
+#ifdef CONFIG_KSZ_MRP
+		int p;
+
+		if (sw->features & MRP_SUPPORT) {
+			for (p = 0; p < sw->mib_port_cnt; p++) {
+				mrp_close_port(mrp, p);
+			}
+		}
+#endif
+		sw->dev_ports = 0;
+	}
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (dev != sw->netdev[i])
+			continue;
+#ifdef CONFIG_KSZ_HSR
+		if (sw->eth_maps[i].proto & HSR_HW) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			if (info->ports[0] == port->first_port)
+				stop_hsr(info);
+		}
+#endif
+	}
+	sw_reset_multi(sw, port);
+}  /* sw_close_port */
+
+static void sw_open(struct ksz_sw *sw)
+{
+	sw->running = true;
+#ifdef CONFIG_KSZ_IBA
+	if (!sw->info->iba.use_iba)
+		sw_set_dev(sw, sw->main_dev, sw->main_dev->dev_addr);
+#endif
+	sw_setup_reserved_multicast(sw);
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+		u32 speed = info->tx_rate / TX_RATE_UNIT;
+
+		mrp_set_speed(&sw->mrp, 0, speed, true);
+		mrp_open(&sw->mrp);
+		schedule_delayed_work(&sw->set_mrp, 100);
+	}
+#endif
+	/* Timer may already be started by the SPI device. */
+	if (!sw->monitor_timer_info->max)
+		ksz_start_timer(sw->monitor_timer_info,
+			sw->monitor_timer_info->period);
+}  /* sw_open */
+
+static void sw_close(struct ksz_sw *sw)
+{
+	int hw_access = true;
+
+#ifdef CONFIG_KSZ_IBA
+	if (2 <= sw->info->iba.use_iba) {
+		hw_access = false;
+	}
+#endif
+	sw->running = false;
+	flush_work(&sw->set_addr);
+	if (hw_access) {
+#ifdef CONFIG_KSZ_IBA
+		sw_set_dev(sw, NULL, sw->main_dev->dev_addr);
+#endif
+	}
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		mrp_close(&sw->mrp, hw_access);
+#endif
+	if (!hw_access) {
+		struct sw_priv *hw_priv = sw->dev;
+
+		ksz_stop_timer(&hw_priv->mib_timer_info);
+	}
+	ksz_stop_timer(sw->monitor_timer_info);
+	cancel_delayed_work_sync(sw->link_read);
+}  /* sw_close */
+
+static void sw_delayed_set_addr(struct work_struct *work)
+{
+	struct ksz_sw *sw = container_of(work, struct ksz_sw, set_addr);
+
+	sw->ops->acquire(sw);
+	sw_set_addr(sw, sw->netdev[0]->dev_addr);
+	sw->ops->release(sw);
+}  /* sw_delayed_set_addr */
+
+static u8 sw_set_mac_addr(struct ksz_sw *sw, struct net_device *dev,
+	u8 promiscuous, uint port)
+{
+	int n;
+#ifdef CONFIG_KSZ_IBA
+	u8 promisc = promiscuous;
+#endif
+
+	/* See if different MAC addresses are used. */
+	if (sw->dev_count > 1) {
+		int i;
+		int dev_count = sw->dev_count + sw->dev_offset;
+
+		for (i = 0; i < dev_count; i++) {
+			if (dev == sw->netdev[i])
+				continue;
+			if (memcmp(sw->netdev[i]->dev_addr,
+			    dev->dev_addr, ETH_ALEN))
+				break;
+		}
+		if (sw->features & DIFF_MAC_ADDR) {
+
+			/* All addresses the same. */
+			if (i == dev_count) {
+				sw->features &= ~DIFF_MAC_ADDR;
+				--promiscuous;
+			} else if (sw->dev_offset) {
+				int i;
+				int p;
+
+				for (i = 0, p = 0; i < sw->mib_port_cnt; i++) {
+					if (i == sw->HOST_PORT) {
+						n = 0;
+					} else {
+						++p;
+						n = p;
+					}
+					inc_mac_addr(sw->netdev[n]->dev_addr,
+						dev->dev_addr, n);
+				}
+			}
+		} else {
+			if (dev == sw->netdev[0] && i < dev_count) {
+
+				/* Make MAC address the same in all devices. */
+				for (i = 1; i < dev_count; i++) {
+					memcpy(sw->netdev[i]->dev_addr,
+						dev->dev_addr, ETH_ALEN);
+				}
+			} else {
+				if (i < dev_count) {
+					sw->features |= DIFF_MAC_ADDR;
+					++promiscuous;
+				}
+			}
+		}
+	}
+	if (dev == sw->netdev[0])
+		schedule_work(&sw->set_addr);
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->netdev[n] != dev)
+			continue;
+#ifdef CONFIG_KSZ_STP
+		if (sw->features & STP_SUPPORT) {
+			struct ksz_stp_info *stp = &sw->info->rstp;
+
+			stp->ops->change_addr(stp, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_DLR
+		if (sw->eth_maps[n].proto & DLR_HW) {
+			dlr_change_addr(&sw->info->dlr, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (sw->eth_maps[n].proto & (HSR_HW | HSR_REDBOX)) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			hsr->ops->change_addr(hsr, dev);
+		}
+#endif
+	}
+	if (dev != sw->netdev[0])
+		return promiscuous;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->set_identity(ptp, dev->dev_addr);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	if (netif_running(dev)) {
+		sw_set_dev(sw, dev, dev->dev_addr);
+
+		/* A hack to accept IBA response. */
+		if (!promisc)
+			promiscuous = 2;
+	}
+#endif
+#ifdef CAPTURE_IBA
+promiscuous = 1;
+#endif
+	return promiscuous;
+}  /* sw_set_mac_addr */
+
+static struct ksz_sw *sw_priv;
+
+static struct sw_dev_info *alloc_sw_dev_info(unsigned int minor)
+{
+	struct sw_dev_info *info;
+
+	info = kzalloc(sizeof(struct sw_dev_info), GFP_KERNEL);
+	if (info) {
+		info->sw = sw_priv;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+		info->read_max = 60000;
+		info->read_buf = kzalloc(info->read_max, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = sw_priv->dev_list[minor];
+		sw_priv->dev_list[minor] = info;
+	}
+	return info;
+}  /* alloc_sw_dev_info */
+
+static void free_sw_dev_info(struct sw_dev_info *info)
+{
+	if (info) {
+		struct ksz_sw *sw = info->sw;
+		unsigned int minor = info->minor;
+		struct sw_dev_info *prev = sw->dev_list[minor];
+
+		if (prev == info) {
+			sw->dev_list[minor] = info->next;
+		} else {
+			while (prev && prev->next != info)
+				prev = prev->next;
+			if (prev)
+				prev->next = info->next;
+		}
+		if (sw->dev_info == info) {
+			sw->dev_info = NULL;
+			sw->notifications = 0;
+		}
+#ifdef CONFIG_KSZ_MRP_
+		do {
+			struct mrp_info *mrp = &sw->mrp;
+
+			if (mrp->dev_info == info) {
+				mrp->dev_info = NULL;
+				mrp->notifications = 0;
+			}
+		} while (0);
+#endif
+#ifdef CONFIG_KSZ_DLR
+		do {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			if (dlr->dev_info == info) {
+				dlr->dev_info = NULL;
+				dlr->notifications = 0;
+			}
+		} while (0);
+#endif
+#ifdef CONFIG_KSZ_HSR
+		do {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			if (hsr->dev_info == info) {
+				hsr->dev_info = NULL;
+				hsr->notifications = 0;
+			}
+		} while (0);
+#endif
+		kfree(info->read_buf);
+		kfree(info->write_buf);
+		kfree(info);
+	}
+}  /* free_sw_dev_info */
+
+static int sw_dev_open(struct inode *inode, struct file *filp)
+{
+	struct sw_dev_info *info = (struct sw_dev_info *)
+		filp->private_data;
+	unsigned int minor = MINOR(inode->i_rdev);
+
+	if (minor > 1)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_sw_dev_info(minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* sw_dev_open */
+
+static int sw_dev_release(struct inode *inode, struct file *filp)
+{
+	struct sw_dev_info *info = (struct sw_dev_info *)
+		filp->private_data;
+
+	free_sw_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* sw_dev_release */
+
+static int sw_get_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	int i;
+	int n;
+	int p;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_cfg);
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, p++) {
+			cfg->on_off = 0;
+			if (cfg->set & SP_LEARN) {
+				if (!port_chk_dis_learn(sw, p))
+					cfg->on_off |= SP_LEARN;
+			}
+			if (cfg->set & SP_RX) {
+				if (port_chk_rx(sw, p))
+					cfg->on_off |= SP_RX;
+			}
+			if (cfg->set & SP_TX) {
+				if (port_chk_tx(sw, p))
+					cfg->on_off |= SP_TX;
+			}
+			if (p == sw->HOST_PORT)
+				continue;
+			if (cfg->set & SP_PHY_POWER) {
+				if (port_chk_power(sw, p))
+					cfg->on_off |= SP_PHY_POWER;
+			}
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_attrib */
+
+static int sw_set_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	int len;
+	int i;
+	int n;
+	int p;
+
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (size < len)
+			goto not_enough;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, p++) {
+			if (cfg->set & SP_LEARN)
+				port_cfg_dis_learn(sw, p,
+					!(cfg->on_off & SP_LEARN));
+			if (cfg->set & SP_RX)
+				port_cfg_rx_special(sw, p,
+					!!(cfg->on_off & SP_RX));
+			if (cfg->set & SP_TX)
+				port_cfg_tx(sw, p,
+					!!(cfg->on_off & SP_TX));
+			if (p == sw->HOST_PORT)
+				continue;
+			if (cfg->set & SP_PHY_POWER)
+				port_cfg_power(sw, p,
+					!!(cfg->on_off & SP_PHY_POWER));
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+
+not_enough:
+	*req_size = len + SIZEOF_ksz_request;
+	return DEV_IOC_INVALID_LEN;
+}  /* sw_set_attrib */
+
+static int base_dev_req(struct ksz_sw *sw, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 0;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+				sw->dev_info = info;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			msg->module = DEV_MOD_BASE;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			sw_setup_msg(info, msg, 8, NULL, NULL);
+			sw->notifications = 0;
+			sw->dev_info = NULL;
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				sw->notifications = *notify;
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_set_attrib(sw, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_get_attrib(sw, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* base_dev_req */
+
+static int sw_dev_req(struct ksz_sw *sw, int start, char *arg,
+	struct sw_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int maincmd;
+	int req_size;
+	int err = 0;
+	int result = DEV_IOC_OK;
+
+	/* Check request size. */
+	get_user_data(&req_size, &req->size, info);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+	    &result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	result = -EOPNOTSUPP;
+	get_user_data(&maincmd, &req->cmd, info);
+	maincmd >>= 16;
+	switch (maincmd) {
+	case DEV_MOD_BASE:
+		err = base_dev_req(sw, arg, info);
+		result = 0;
+		break;
+#ifdef CONFIG_1588_PTP
+	case DEV_MOD_PTP:
+		if (sw->features & PTP_HW) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			err = ptp->ops->dev_req(ptp, start, arg,
+				(struct ptp_dev_info *) info);
+			result = 0;
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_MRP
+	case DEV_MOD_MRP:
+		if (sw->features & MRP_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			err = mrp->ops->dev_req(mrp, start, arg);
+			result = 0;
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	case DEV_MOD_DLR:
+		if (sw->features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			err = dlr->ops->dev_req(dlr, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	case DEV_MOD_HSR:
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			err = hsr->ops->dev_req(hsr, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+	default:
+		break;
+	}
+
+	/* Processed by specific module. */
+	if (!result)
+		return err;
+	if (result < 0)
+		goto dev_ioctl_done;
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+dev_ioctl_done:
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+	return err;
+}  /* sw_dev_req */
+
+static ssize_t sw_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct sw_dev_info *info = (struct sw_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* sw_dev_read */
+
+#ifdef HAVE_UNLOCKED_IOCTL
+static long sw_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+#else
+static int sw_dev_ioctl(struct inode *inode, struct file *filp,
+	unsigned int cmd, unsigned long arg)
+#endif
+{
+	struct sw_dev_info *info = (struct sw_dev_info *)
+		filp->private_data;
+	struct ksz_sw *sw = info->sw;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok(VERIFY_WRITE, (void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok(VERIFY_READ, (void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = sw_dev_req(sw, 0, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* sw_dev_ioctl */
+
+static ssize_t sw_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct sw_dev_info *info = (struct sw_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	size = 0;
+	result = size;
+	rc = 0;
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* sw_dev_write */
+
+static const struct file_operations sw_dev_fops = {
+	.read		= sw_dev_read,
+	.write		= sw_dev_write,
+#ifdef HAVE_UNLOCKED_IOCTL
+	.unlocked_ioctl	= sw_dev_ioctl,
+#else
+	.ioctl		= sw_dev_ioctl,
+#endif
+	.open		= sw_dev_open,
+	.release	= sw_dev_release,
+};
+
+static struct class *sw_class;
+
+static int init_sw_dev(int dev_major, char *dev_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &sw_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	sw_class = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(sw_class)) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(sw_class, NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	return dev_major;
+}  /* init_sw_dev */
+
+static void exit_sw_dev(int dev_major, char *dev_name)
+{
+	device_destroy(sw_class, MKDEV(dev_major, 0));
+	class_destroy(sw_class);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_sw_dev */
+
+static void sw_init_dev(struct ksz_sw *sw)
+{
+	sw_priv = sw;
+	sprintf(sw->dev_name, "sw_dev");
+	sw->dev_major = init_sw_dev(0, sw->dev_name);
+	sw->msg_buf = kzalloc(MAX_SW_LEN, GFP_KERNEL);
+}  /* sw_init_dev */
+
+static void sw_exit_dev(struct ksz_sw *sw)
+{
+	kfree(sw->msg_buf);
+	if (sw->dev_major >= 0)
+		exit_sw_dev(sw->dev_major, sw->dev_name);
+}  /* sw_exit_dev */
+
+
+#if defined(CONFIG_KSZ_DLR) && !defined(CONFIG_KSZ_MSRP) && !defined(CONFIG_KSZ_HSR)
+#define USE_DLR
+#endif
+
+#if defined(CONFIG_KSZ_HSR) && !defined(CONFIG_KSZ_MSRP) && !defined(CONFIG_KSZ_DLR)
+#define USE_HSR
+#define USE_HSR_REDBOX
+#endif
+
+#if 0
+#define USE_DLR
+#if 0
+#define USE_DLR_FORWARD
+#endif
+#endif
+#if 0
+#define USE_HSR
+#if 0
+#define USE_HSR_REDBOX
+#endif
+#endif
+
+/*
+ * This enables multiple network device mode for the switch, which contains at
+ * least two physical ports.  Some users like to take control of the ports for
+ * running Spanning Tree Protocol.  The driver will create an additional eth?
+ * device for each port depending on the mode.
+ *
+ * Some limitations are the network devices cannot have different MTU and
+ * multicast hash tables.
+ */
+#if defined(USE_HSR_REDBOX) || defined(USE_DLR_FORWARD)
+static int multi_dev = 1;
+#else
+static int multi_dev;
+#endif
+
+/*
+ * As most users select multiple network device mode to use Spanning Tree
+ * Protocol, this enables a feature in which most unicast and multicast packets
+ * are forwarded inside the switch and not passed to the host.  Only packets
+ * that need the host's attention are passed to it.  This prevents the host
+ * wasting CPU time to examine each and every incoming packets and do the
+ * forwarding itself.
+ *
+ * As the hack requires the private bridge header, the driver cannot compile
+ * with just the kernel headers.
+ *
+ * Enabling STP support also turns on multiple network device mode.
+ */
+static int stp;
+
+/*
+ * This enables fast aging in the switch.  Not sure what situation requires
+ * that.  However, fast aging is used to flush the dynamic MAC table when STP
+ * support is enabled.
+ */
+static int fast_aging;
+
+static int authen;
+
+#if defined(USE_DLR) || defined(USE_HSR)
+static int avb;
+#else
+static int avb = 1;
+#endif
+
+#if defined(USE_DLR) || defined(USE_HSR)
+static int eth1_ports = 0x3;
+#else
+static int eth1_ports;
+#endif
+static int eth2_ports;
+static int eth3_ports;
+static int eth4_ports;
+static int eth5_ports;
+static int eth6_ports;
+
+#if defined(USE_HSR_REDBOX) || defined(USE_DLR_FORWARD)
+static int eth1_vlan = FID_ENTRIES - 2;
+#else
+static int eth1_vlan;
+#endif
+static int eth2_vlan;
+static int eth3_vlan;
+static int eth4_vlan;
+static int eth5_vlan;
+static int eth6_vlan;
+
+#if defined(USE_DLR)
+static char *eth1_proto = "dlr";
+#endif
+#if defined(USE_HSR)
+static char *eth1_proto = "hsr";
+#endif
+#if !defined(USE_DLR) && !defined(USE_HSR)
+static char *eth1_proto = " ";
+#endif
+#if defined(USE_HSR_REDBOX)
+static char *eth2_proto = "redbox";
+#else
+static char *eth2_proto = " ";
+#endif
+static char *eth3_proto = " ";
+static char *eth4_proto = " ";
+static char *eth5_proto = " ";
+static char *eth6_proto = " ";
+
+static int *eth_ports[] = {
+	&eth1_ports,
+	&eth2_ports,
+	&eth3_ports,
+	&eth4_ports,
+	&eth5_ports,
+	&eth6_ports,
+	NULL
+};
+
+static int *eth_vlans[] = {
+	&eth1_vlan,
+	&eth2_vlan,
+	&eth3_vlan,
+	&eth4_vlan,
+	&eth5_vlan,
+	&eth6_vlan,
+	NULL
+};
+
+static char **eth_proto[] = {
+	&eth1_proto,
+	&eth2_proto,
+	&eth3_proto,
+	&eth4_proto,
+	&eth5_proto,
+	&eth6_proto,
+	NULL
+};
+
+#ifdef CONFIG_KSZ_IBA
+static int iba = 1;
+#endif
+
+static void sw_setup_mode(struct ksz_sw *sw, int mode)
+{
+	switch (mode) {
+	case 1:
+		sw->features &= ~PTP_HW;
+		avb = 0;
+		break;
+	case 3:
+		avb = 0;
+		authen = 1;
+		break;
+	case 4:
+		eth1_ports = 0x3;
+		eth1_proto = "dlr";
+		avb = 0;
+		break;
+	case 6:
+		eth2_proto = "redbox";
+	case 5:
+		eth1_ports = 0x3;
+		eth1_vlan = 2;
+		eth1_proto = "hsr";
+		avb = 0;
+		multi_dev = 1;
+		break;
+	case 7:
+		stp = 1;
+		avb = 0;
+		break;
+	case 2:
+	default:
+		break;
+	}
+}  /* sw_setup_mode */
+
+static void sw_setup_zone(struct ksz_sw *sw)
+{
+	int c;
+	int f;
+	int limit;
+	int m;
+	uint p;
+	uint q;
+	int w;
+	int *v;
+	char **s;
+	uint features;
+	uint used = 0;
+	int last_vlan = 0;
+	uint ports = sw->PORT_MASK;
+
+#ifdef DEBUG
+	sw->verbose = 1;
+#endif
+	if (!avb)
+		sw->features &= ~AVB_SUPPORT;
+	if (sw->multi_dev > 2)
+		goto setup_next;
+	for (p = 0; p < sw->mib_port_cnt - 1; p++) {
+		v = eth_ports[p];
+
+		/* No more port setting. */
+		if (!v || !*v)
+			break;
+		m = *v;
+
+		/* Find out how the ports are to be used. */
+		limit = 0;
+		w = last_vlan;
+		features = 0;
+		s = eth_proto[p];
+		if (!strcmp(*s, "dlr")) {
+#ifdef CONFIG_KSZ_DLR
+			features = DLR_HW;
+#ifdef CONFIG_1588_PTP
+			if (sw->features & PTP_HW) {
+				features |= VLAN_PORT;
+				sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+			}
+#endif
+#endif
+			limit = 2;
+			w = 1;
+		} else if (!strcmp(*s, "hsr")) {
+#ifdef CONFIG_KSZ_HSR
+			features = HSR_HW;
+#ifdef CONFIG_1588_PTP
+			if (sw->features & PTP_HW) {
+				features |= VLAN_PORT;
+				sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+			}
+#endif
+#endif
+			limit = 2;
+			w = 1;
+		} else if (!strcmp(*s, "redbox")) {
+			features = HSR_REDBOX;
+			w = 1;
+		} else if (!p) {
+#ifdef CONFIG_1588_PTP
+			if (sw->features & PTP_HW) {
+				features |= VLAN_PORT;
+				sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+			}
+#endif
+		}
+
+		v = eth_vlans[p];
+		if (!w && (!v || !*v))
+			break;
+		if (*v)
+			w = *v;
+
+		for (q = 0; q < p; q++) {
+			m &= ~sw->eth_maps[q].mask;
+			if (w > 1 && w == sw->eth_maps[q].vlan)
+				w = last_vlan + 1;
+		}
+		c = 0;
+		f = -1;
+		for (q = 0; q < sw->mib_port_cnt; q++) {
+			if (skip_host_port(sw, q))
+				continue;
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = q;
+				else if (q - 1 == sw->HOST_PORT)
+					++c;
+				++c;
+
+				/* Limit to certain ports. */
+				if (limit && c >= limit) {
+					if (!(used & features)) {
+						used |= features;
+						++q;
+						break;
+					}
+					features = 0;
+				}
+			} else if (f >= 0)
+				break;
+		}
+		if (!c)
+			continue;
+		m &= (1 << q) - 1;
+		sw->eth_maps[p].cnt = c;
+		sw->eth_maps[p].mask = m;
+		sw->eth_maps[p].port = f;
+		sw->eth_maps[p].phy_id = f + 1;
+		sw->eth_maps[p].vlan = w & (4096 - 1);
+		sw->eth_maps[p].proto = features;
+#ifdef CONFIG_KSZ_DLR
+		if (features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			dlr->ports[0] = f++;
+			if (f == sw->HOST_PORT)
+				f++;
+			dlr->ports[1] = f;
+			dlr->member = (1 << dlr->ports[0]) |
+				(1 << dlr->ports[1]);
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			hsr->ports[0] = f++;
+			if (f == sw->HOST_PORT)
+				f++;
+			hsr->ports[1] = f;
+			hsr->member = (1 << hsr->ports[0]) |
+				(1 << hsr->ports[1]);
+		}
+#endif
+		if (last_vlan < w)
+			last_vlan = w;
+		ports &= ~m;
+	}
+
+	/* No VLAN devices specified. */
+	if (!p)
+		goto setup_next;
+
+	/* Not all ports are used. */
+	ports &= ~sw->HOST_MASK;
+	if (sw->multi_dev != 1)
+		ports = 0;
+	features = 0;
+#ifdef CONFIG_KSZ_HSR
+	if (ports && (sw->features & HSR_HW)) {
+		if ((used & HSR_HW) && !(used & HSR_REDBOX)) {
+			s = eth_proto[1];
+			if (!strcmp(*s, "redbox")) {
+				features = HSR_REDBOX;
+				used |= HSR_REDBOX;
+			}
+		}
+		if (used & HSR_REDBOX)
+			sw->features |= HSR_REDBOX;
+	}
+#endif
+	while (ports) {
+		m = ports;
+		c = 0;
+		f = -1;
+		for (q = 0; q < sw->mib_port_cnt; q++) {
+			if (skip_host_port(sw, q))
+				continue;
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = q;
+				else if (q - 1 == sw->HOST_PORT)
+					++c;
+				++c;
+			} else if (f >= 0)
+				break;
+		}
+		m &= (1 << q) - 1;
+		sw->eth_maps[p].cnt = c;
+		sw->eth_maps[p].mask = m;
+		sw->eth_maps[p].port = f;
+		sw->eth_maps[p].phy_id = f + 1;
+		sw->eth_maps[p].vlan = ++last_vlan & (4096 - 1);
+		sw->eth_maps[p].proto = features;
+		ports &= ~m;
+		p++;
+	}
+	if (p > 1)
+		sw->features |= SW_VLAN_DEV;
+	sw->eth_cnt = p;
+	for (p = 0; p < sw->eth_cnt; p++) {
+		dbg_msg("%d: %d:%d %04x %03x %08x\n",
+			p, sw->eth_maps[p].port, sw->eth_maps[p].cnt,
+			sw->eth_maps[p].mask, sw->eth_maps[p].vlan,
+			sw->eth_maps[p].proto);
+	}
+
+setup_next:
+#ifdef CONFIG_KSZ_DLR
+	if (!(used & DLR_HW))
+		sw->features &= ~DLR_HW;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (!(used & HSR_HW))
+		sw->features &= ~HSR_HW;
+#endif
+	if (sw->features & (DLR_HW | HSR_HW)) {
+		sw->features &= ~AVB_SUPPORT;
+		sw->features &= ~STP_SUPPORT;
+		sw->overrides &= ~USE_802_1X_AUTH;
+	}
+	return;
+}  /* sw_setup_zone */
+
+static int phy_offset;
+
+static void sw_setup_special(struct ksz_sw *sw, int *port_cnt,
+	int *mib_port_cnt, int *dev_cnt)
+{
+	phy_offset = 0;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+	dbg_msg("%s %d %d %d\n", __func__,
+		sw->stp, sw->multi_dev, sw->fast_aging);
+#ifdef CONFIG_KSZ_IBA
+	if (iba)
+		sw->features |= IBA_SUPPORT;
+#endif
+
+	/* Multiple network device interfaces are required. */
+	if (1 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt - 1;
+		sw->phy_offset = 1;
+	} else if (2 == sw->multi_dev)
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	else if (3 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt - 1;
+		sw->dev_offset = 1;
+	} else if (4 == sw->multi_dev)
+		sw->features |= VLAN_PORT;
+	else if (5 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt - 1;
+		sw->dev_offset = 1;
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	}
+
+	/* Single network device has multiple ports. */
+	if (1 == sw->dev_count) {
+		*port_cnt = sw->mib_port_cnt - 1;
+		*mib_port_cnt = sw->mib_port_cnt - 1;
+		if (0 < sw->HOST_PORT && sw->HOST_PORT < sw->mib_port_cnt - 1) {
+			(*port_cnt)++;
+			(*mib_port_cnt)++;
+		}
+	}
+	if (1 == sw->multi_dev && (sw->features & SW_VLAN_DEV))
+		sw->dev_count = sw->eth_cnt;
+	*dev_cnt = sw->dev_count;
+	if (3 == sw->multi_dev || 5 == sw->multi_dev)
+		(*dev_cnt)++;
+#ifdef CONFIG_1588_PTP
+	if (sw->features & VLAN_PORT) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->overrides |= PTP_PORT_FORWARD;
+	}
+#endif
+}  /* sw_setup_special */
+
+static void sw_leave_dev(struct ksz_sw *sw)
+{
+	int i;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT)
+		leave_stp(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		leave_mrp(&sw->mrp);
+#endif
+	for (i = 0; i < sw->dev_count; i++)
+		sw->netdev[i] = NULL;
+	sw->eth_cnt = 0;
+	sw->dev_count = 0;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+}  /* sw_leave_dev */
+
+static int sw_setup_dev(struct ksz_sw *sw, struct net_device *dev,
+	char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+	uint mib_port_cnt)
+{
+	int cnt;
+	int p;
+	int pi;
+	int phy_id;
+	u32 features;
+	int header = 0;
+
+	if (!phy_offset)
+		phy_offset = sw->phy_offset;
+
+	/* dev_offset is ether 0 or 1. */
+	p = i;
+	if (p)
+		p -= sw->dev_offset;
+	if (p >= sw->HOST_PORT)
+		p++;
+
+	if (sw->dev_offset) {
+		/*
+		 * First device associated with switch has been
+		 * created.
+		 */
+		if (i) {
+			snprintf(dev->name, IFNAMSIZ, "%s.10%%d", dev_name);
+			memcpy(dev->dev_addr, sw->port_info[p].mac_addr,
+				ETH_ALEN);
+		} else {
+			port_cnt = sw->mib_port_cnt - 1;
+			mib_port_cnt = sw->mib_port_cnt - 1;
+			if (0 < sw->HOST_PORT &&
+			    sw->HOST_PORT < sw->mib_port_cnt - 1) {
+				port_cnt++;
+				mib_port_cnt++;
+			}
+			sw->ops->acquire(sw);
+			sw_set_addr(sw, dev->dev_addr);
+			sw->ops->release(sw);
+		}
+	}
+
+	if (1 == sw->multi_dev && (sw->features & SW_VLAN_DEV)) {
+		port_cnt = sw->eth_maps[i].cnt;
+		p = sw->eth_maps[i].port;
+		mib_port_cnt = port_cnt;
+	}
+
+	port->port_cnt = port_cnt;
+	port->mib_port_cnt = mib_port_cnt;
+	port->first_port = p;
+	port->flow_ctrl = PHY_FLOW_CTRL;
+
+	/* S1 chips do not work well with asymmetric PAUSE. */
+	if ((sw->features & NEW_CAP) && (sw->features & GIGABIT_SUPPORT))
+		port->flow_ctrl = PHY_RX_ONLY;
+
+#ifdef CONFIG_KSZ_STP
+	if (!i && (sw->features & STP_SUPPORT))
+		prep_stp_mcast(dev);
+#endif
+
+#ifdef CONFIG_KSZ_DLR
+	/* Cannot flow control because of beacon timeout. */
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & DLR_HW)) {
+		if ((sw->features & NEW_CAP) &&
+		    (sw->features & GIGABIT_SUPPORT))
+			port->flow_ctrl = PHY_TX_ONLY;
+		prep_dlr_mcast(dev);
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW)) {
+#if 0
+		port->flow_ctrl = PHY_TX_ONLY;
+#endif
+		setup_hsr(&sw->info->hsr, dev);
+		if (header < HSR_HLEN)
+			header = HSR_HLEN;
+	}
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_REDBOX)) {
+		setup_hsr_redbox(&sw->info->hsr, dev);
+	}
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (!i && (sw->features & MRP_SUPPORT))
+		setup_mrp(&sw->mrp, dev);
+#endif
+	if (sw->features & AVB_SUPPORT)
+		port->flow_ctrl = PHY_NO_FLOW_CTRL;
+
+	/* Point to port under netdev. */
+	if (phy_offset)
+		phy_id = port->first_port + phy_offset;
+	else
+		phy_id = 0;
+
+	/* Replace virtual port with one from network device. */
+	do {
+		struct phy_device *phydev;
+		struct phy_priv *priv;
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		if (hw_priv->bus)
+			phydev = hw_priv->bus->phy_map[phy_id];
+		else
+			phydev = &sw->phy_map[phy_id];
+		priv = phydev->priv;
+		priv->port = port;
+	} while (0);
+	if (!phy_offset)
+		phy_offset = 1;
+
+	port->sw = sw;
+	port->linked = &sw->port_info[port->first_port];
+
+	for (cnt = 0, pi = p; cnt < port_cnt; cnt++, pi++) {
+		if (sw->port_info[pi].phy)
+			sw->port_info[pi].state = media_disconnected;
+		if (pi == sw->HOST_PORT)
+			continue;
+		sw->info->port_cfg[pi].index = i;
+	}
+	sw->netdev[i] = dev;
+	if (sw->dev_count > 1 && i && !(sw->features & DIFF_MAC_ADDR)) {
+		if (memcmp(dev->dev_addr, sw->netdev[0]->dev_addr, ETH_ALEN))
+			sw->features |= DIFF_MAC_ADDR;
+	}
+
+	INIT_WORK(&port->link_update, link_update_work);
+	features = sw->features;
+	if (sw->features & SW_VLAN_DEV)
+		features = sw->eth_maps[i].proto;
+
+#ifndef CONFIG_KSZ_DSA
+	if (features & VLAN_PORT)
+		dev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+#endif
+
+	/* Needed for inserting VLAN tag. */
+	if (sw->features & SW_VLAN_DEV)
+		if (header < VLAN_HLEN)
+			header = VLAN_HLEN;
+	dev->hard_header_len += header;
+
+	return phy_id;
+}  /* sw_setup_dev */
+
+static u8 sw_get_priv_state(struct net_device *dev)
+{
+	return STP_STATE_SIMPLE;
+}
+
+static void sw_set_priv_state(struct net_device *dev, u8 state)
+{
+}
+
+static int netdev_chk_running(struct net_device *dev)
+{
+	return netif_running(dev);
+}
+
+static int netdev_chk_stopped(struct net_device *dev)
+{
+	return netif_running(dev) && netif_queue_stopped(dev);
+}
+
+static void netdev_start_queue(struct net_device *dev)
+{
+	dev->trans_start = jiffies;
+	netif_start_queue(dev);
+}
+
+static void netdev_stop_queue(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+}
+
+static void netdev_wake_queue(struct net_device *dev)
+{
+	netif_wake_queue(dev);
+}
+
+static void sw_netdev_oper(struct ksz_sw *sw, struct net_device *dev,
+	int (*netdev_chk)(struct net_device *dev),
+	void (*netdev_oper)(struct net_device *dev))
+{
+	uint port;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		netdev_oper(dev);
+		return;
+	}
+	for (port = 0; port < dev_count; port++) {
+		dev = sw->netdev[port];
+		if (!dev)
+			continue;
+		if (!netdev_chk || netdev_chk(dev))
+			netdev_oper(dev);
+	}
+}  /* sw_netdev_oper */
+
+static void sw_netdev_open_port(struct ksz_sw *sw, struct net_device *dev)
+{
+	struct ksz_port *port;
+	u8 state;
+	int p;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		port = sw->net_ops->get_priv_port(dev);
+		state = sw->net_ops->get_state(dev);
+		sw->net_ops->open_port(sw, dev, port, &state);
+		sw->net_ops->set_state(dev, state);
+		return;
+	}
+	for (p = 0; p < dev_count; p++) {
+		dev = sw->netdev[p];
+		if (!dev)
+			continue;
+		port = sw->net_ops->get_priv_port(dev);
+		state = sw->net_ops->get_state(dev);
+		sw->net_ops->open_port(sw, dev, port, &state);
+		sw->net_ops->set_state(dev, state);
+	}
+}  /* sw_netdev_open_port */
+
+static void sw_netdev_start_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_start_queue);
+}
+
+static void sw_netdev_stop_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_stop_queue);
+}
+
+static void sw_netdev_wake_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_stopped, netdev_wake_queue);
+}
+
+static struct ksz_sw_net_ops sw_net_ops = {
+	.setup_special		= sw_setup_special,
+	.setup_dev		= sw_setup_dev,
+	.leave_dev		= sw_leave_dev,
+	.get_state		= sw_get_priv_state,
+	.set_state		= sw_set_priv_state,
+
+	.start			= sw_start,
+	.stop			= sw_stop,
+	.open_dev		= sw_open_dev,
+	.open_port		= sw_open_port,
+	.close_port		= sw_close_port,
+	.open			= sw_open,
+	.close			= sw_close,
+
+	.netdev_start_queue	= sw_netdev_start_queue,
+	.netdev_stop_queue	= sw_netdev_stop_queue,
+	.netdev_wake_queue	= sw_netdev_wake_queue,
+	.netdev_open_port	= sw_netdev_open_port,
+
+	.set_mac_addr		= sw_set_mac_addr,
+
+	.get_mtu		= sw_get_mtu,
+	.get_tx_len		= sw_get_tx_len,
+	.add_tail_tag		= sw_add_tail_tag,
+	.get_tail_tag		= sw_get_tail_tag,
+	.get_phys_port		= sw_get_phys_port,
+	.get_virt_port		= sw_get_virt_port,
+	.add_vid		= sw_add_vid,
+	.kill_vid		= sw_kill_vid,
+	.check_tx		= sw_check_tx,
+	.rx_dev			= sw_rx_dev,
+	.match_pkt		= sw_match_pkt,
+	.parent_rx		= sw_parent_rx,
+	.port_vlan_rx		= sw_port_vlan_rx,
+	.drop_icmp		= sw_drop_icmp,
+	.final_skb		= sw_final_skb,
+	.drv_rx			= sw_drv_rx,
+	.set_multi		= sw_set_multi,
+
+};
+
+static struct ksz_sw_ops sw_ops = {
+	.init			= sw_init_dev,
+	.exit			= sw_exit_dev,
+	.dev_req		= sw_dev_req,
+
+	.acquire		= sw_acquire,
+	.release		= sw_release,
+
+	.chk			= sw_chk,
+	.cfg			= sw_cfg,
+
+	.port_get_link_speed	= port_get_link_speed,
+	.port_set_link_speed	= port_set_link_speed,
+	.port_force_link_speed	= port_force_link_speed,
+
+	.port_r_cnt		= port_r_cnt,
+	.get_mib_counters	= get_sw_mib_counters,
+
+	.sysfs_read		= sysfs_sw_read,
+	.sysfs_read_hw		= sysfs_sw_read_hw,
+	.sysfs_write		= sysfs_sw_write,
+	.sysfs_port_read	= sysfs_port_read,
+	.sysfs_port_read_hw	= sysfs_port_read_hw,
+	.sysfs_port_write	= sysfs_port_write,
+	.sysfs_mac_read		= sysfs_mac_read,
+	.sysfs_mac_write	= sysfs_mac_write,
+	.sysfs_vlan_read	= sysfs_vlan_read,
+	.sysfs_vlan_write	= sysfs_vlan_write,
+
+#ifdef CONFIG_KSZ_STP
+	.sysfs_stp_read		= sysfs_stp_read,
+	.sysfs_stp_write	= sysfs_stp_write,
+	.sysfs_stp_port_read	= sysfs_stp_port_read,
+	.sysfs_stp_port_write	= sysfs_stp_port_write,
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	.sysfs_mrp_read		= sysfs_mrp_read,
+	.sysfs_mrp_write	= sysfs_mrp_write,
+	.sysfs_mrp_port_read	= sysfs_mrp_port_read,
+	.sysfs_mrp_port_write	= sysfs_mrp_port_write,
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	.sysfs_hsr_read		= sysfs_hsr_read,
+	.sysfs_hsr_write	= sysfs_hsr_write,
+#endif
+
+	.sysfs_acl_read		= sysfs_acl_read,
+	.sysfs_acl_write	= sysfs_acl_write,
+
+	.cfg_mac		= sw_cfg_mac,
+	.cfg_vlan		= sw_cfg_vlan,
+	.alloc_mac		= sw_alloc_mac,
+	.free_mac		= sw_free_mac,
+	.alloc_vlan		= sw_alloc_vlan,
+	.free_vlan		= sw_free_vlan,
+	.alloc_fid		= sw_alloc_fid,
+	.free_fid		= sw_free_fid,
+
+	.get_br_id		= sw_get_br_id,
+	.from_backup		= sw_from_backup,
+	.to_backup		= sw_to_backup,
+	.from_designated	= sw_from_designated,
+	.to_designated		= sw_to_designated,
+	.tc_detected		= sw_tc_detected,
+	.get_tcDetected		= sw_get_tcDetected,
+
+	.get_id			= sw_get_id,
+	.cfg_tail_tag		= sw_cfg_tail_tag,
+	.cfg_each_port		= sw_cfg_each_port,
+	.port_to_phy_addr	= sw_port_to_phy_addr,
+	.set_port_addr		= sw_set_port_addr,
+
+	.cfg_src_filter		= sw_cfg_src_filter,
+	.flush_table		= sw_flush_dyn_mac_table,
+	.fwd_unk_mcast		= sw_fwd_unk_mcast,
+	.fwd_unk_ucast		= sw_fwd_unk_ucast,
+	.fwd_unk_vid		= sw_fwd_unk_vid,
+
+	.port_freeze_mib	= port_freeze_mib,
+	.freeze_mib		= sw_freeze_mib,
+};
+
+static int sw_proc_intr(struct ksz_sw *sw)
+{
+	u32 intr_mask;
+	u32 status;
+	u16 port_intr_mask;
+	u8 port_status;
+	uint port;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = &sw->ptp_hw;
+#endif
+	u32 intr_status[2];
+	int cnt = 0;
+	static int dbg_intr_status = 5;
+	static int dbg_intr_cnt = 0;
+
+	intr_mask = sw->intr_mask;
+	status = sw->reg->r32(sw, REG_SW_INT_STATUS__4);
+	if (status)
+		cnt++;
+	intr_status[0] = status;
+	status &= sw->intr_mask;
+#ifdef CONFIG_1588_PTP
+	if (status & TRIG_TS_INT) {
+		if (ptp->started)
+			ptp->ops->proc_intr(ptp);
+		else
+			sw->intr_mask &= ~TRIG_TS_INT;
+	}
+#endif
+	if (status & APB_TIMEOUT_INT) {
+dbg_msg(" apb: %08x\n", sw->reg->r32(sw, REG_SW_APB_TIMEOUT_ADDR__4));
+		sw->reg->w32(sw, REG_SW_APB_TIMEOUT_ADDR__4,
+			APB_TIMEOUT_ACKNOWLEDGE);
+		status = sw->reg->r32(sw, REG_SW_INT_STATUS__4);
+		if (status & APB_TIMEOUT_INT)
+			sw->intr_mask &= ~APB_TIMEOUT_INT;
+	}
+	if (intr_mask != sw->intr_mask)
+		sw->reg->w32(sw, REG_SW_INT_MASK__4,
+			~sw->intr_mask & SWITCH_INT_MASK);
+
+	intr_mask = sw->port_intr_mask;
+	status = sw->reg->r32(sw, REG_SW_PORT_INT_STATUS__4);
+	if (status)
+		cnt++;
+	intr_status[1] = status;
+#if 0
+dbg_msg("port irq: %08x; %08x\n", status, sw->port_intr_mask);
+#endif
+	status &= sw->port_intr_mask;
+	for (port = 0; port < sw->mib_port_cnt; port++) {
+		port = chk_last_port(sw, port);
+		if (status & 1) {
+			struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+			int stop_phy_irq = false;
+
+			if (!(sw->features & NEW_CAP))
+				stop_phy_irq = true;
+			port_intr_mask = cfg->intr_mask;
+			port_r(sw, port, REG_PORT_INT_STATUS, &port_status);
+			port_status &= cfg->intr_mask;
+			if (port_status & PORT_SGMII_INT) {
+				u16 data = 0;
+
+				port_sgmii_w(sw, port, SR_MII,
+					MMD_SR_MII_AUTO_NEG_STATUS, &data, 1);
+				sw->phy_intr |= (1 << port);
+			}
+			if (port_status & PORT_PHY_INT) {
+				u8 val;
+
+				/* The status is cleared after read. */
+				port_r8(sw, port, REG_PORT_PHY_INT_STATUS,
+					&val);
+				if (val & (LINK_DOWN_INT | LINK_UP_INT))
+					sw->phy_intr |= (1 << port);
+
+				if (sw->features & PHY_INTR_BUG) {
+/*
+ * THa  2014/11/18
+ * KSZ9893 IBA/SPI register reset bug.
+ */
+if (!val) {
+	u16 data;
+
+	port_r8(sw, port, REG_PORT_PHY_INT_ENABLE, &val);
+dbg_msg("!! %x %x %x %x\n", status, port_status, cfg->intr_mask, val);
+	port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &data);
+dbg_msg(" p1: %x\n", data);
+	if (!(data & PORT_INT_PIN_HIGH)) {
+		port_w16(sw, port, REG_PORT_PHY_PHY_CTRL, 0x4300);
+		port_r16(sw, port, REG_PORT_PHY_PHY_CTRL, &data);
+dbg_msg(" p2: %x\n", data);
+stop_phy_irq = true;
+	}
+}
+#ifndef CONFIG_KSZ_IBA
+				if (stop_phy_irq) {
+				port_w(sw, port, REG_PORT_INT_MASK,
+					(~cfg->intr_mask & PORT_INT_MASK) |
+					PORT_PHY_INT);
+				cfg->intr_mask &= ~PORT_PHY_INT;
+				}
+#endif
+				}
+			}
+#ifdef CONFIG_1588_PTP
+			if (port_status & PORT_PTP_INT) {
+				if (ptp->started)
+					proc_ptp_tx_intr(ptp, port);
+				else
+					cfg->intr_mask &= ~PORT_PTP_INT;
+			}
+#endif
+			if (port_status & PORT_ACL_INT) {
+				if (sw->features & NEW_CAP)
+					port_w(sw, port, REG_PORT_INT_STATUS,
+						PORT_ACL_INT);
+				else {
+					port_w(sw, port, REG_PORT_INT_MASK,
+						(~cfg->intr_mask &
+						PORT_INT_MASK) |
+						PORT_ACL_INT);
+					port_w(sw, port, REG_PORT_INT_MASK,
+						~cfg->intr_mask &
+						PORT_INT_MASK);
+				}
+#ifdef CONFIG_KSZ_DLR
+				if (sw->features & DLR_HW)
+					dlr_timeout(&sw->info->dlr, port);
+#endif
+				if (sw->overrides & ACL_INTR_MONITOR)
+					printk(KERN_INFO "  acl: %d %lx\n",
+						port, jiffies);
+			}
+			if (port_intr_mask != cfg->intr_mask)
+				port_w(sw, port, REG_PORT_INT_MASK,
+					~cfg->intr_mask & PORT_INT_MASK);
+		}
+		status >>= 1;
+	}
+	if (sw->phy_intr)
+		schedule_delayed_work(sw->link_read, 0);
+	if (intr_mask != sw->port_intr_mask)
+		sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4,
+			~sw->port_intr_mask & sw->PORT_MASK);
+	if (sw->intr_cnt && cnt) {
+		dbg_intr_cnt = cnt;
+#if 0
+		dbg_msg("intr: %08x %08x\n", intr_status[0], intr_status[1]);
+#endif
+	}
+	if (!sw->intr_cnt && cnt)
+		dbg_intr_cnt = 0;
+	sw->intr_cnt += cnt;
+	if (!sw->intr_cnt && !dbg_intr_cnt && dbg_intr_status) {
+		dbg_msg("no intr status\n");
+		dbg_intr_status--;
+	}
+	return cnt;
+}  /* sw_proc_intr */
+
+/* -------------------------------------------------------------------------- */
+
+static void link_update_work(struct work_struct *work)
+{
+	struct ksz_port *port =
+		container_of(work, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct phy_device *phydev;
+	struct ksz_port_info *info;
+	int i;
+	int link;
+	u32 speed;
+	bool duplex;
+
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ops->link_change(dlr,
+			sw->port_info[dlr->ports[0]].state == media_connected,
+			sw->port_info[dlr->ports[1]].state == media_connected);
+	}
+#endif
+
+	/* This only matters when one phy device is used for the switch. */
+	if (1 == sw->dev_count) {
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		if (hw_priv->phy_id != port->linked->phy_id)
+			hw_priv->phy_id = port->linked->phy_id;
+	}
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		info = &sw->port_info[i];
+
+		if (!info->report)
+			continue;
+		info->report = false;
+		phydev = sw->phy[i + 1];
+		phydev->link = (info->state == media_connected);
+		phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		phydev->duplex = (info->duplex == 2);
+	}
+
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (sw->eth_maps[i].port == port->first_port) {
+			if (sw->eth_maps[i].phy_id != port->linked->phy_id)
+				sw->eth_maps[i].phy_id = port->linked->phy_id;
+			break;
+		}
+	}
+
+	info = port->linked;
+	phydev = sw->phy[port->first_port + 1];
+	phydev->link = (info->state == media_connected);
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	phydev->duplex = (info->duplex == 2);
+	i = sw->info->port_cfg[port->first_port].index;
+	if (phydev->attached_dev || sw->netdev[i]) {
+		struct net_device *dev;
+
+		if (phydev->attached_dev)
+			dev = phydev->attached_dev;
+		else
+			dev = sw->netdev[i];
+		link = netif_carrier_ok(dev);
+		if (link != phydev->link) {
+			if (phydev->link)
+				netif_carrier_on(dev);
+			else
+				netif_carrier_off(dev);
+			if (netif_msg_link(sw))
+				pr_info("%s link %s\n",
+					dev->name,
+					phydev->link ? "on" : "off");
+		}
+	}
+
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->netdev[0]) {
+		phydev = sw->netdev[0]->phydev;
+		if (!phydev)
+			phydev = sw->phydev;
+		if (sw->net_ops->get_priv_port)
+			port = sw->net_ops->get_priv_port(sw->netdev[0]);
+	} else
+		phydev = sw->phydev;
+	if (phydev->attached_dev || sw->netdev[0]) {
+		struct net_device *dev;
+		int phy_link = 0;
+
+		if (phydev->attached_dev)
+			dev = phydev->attached_dev;
+		else
+			dev = sw->netdev[0];
+
+		/* phydev settings may be changed by ethtool. */
+		info = &sw->port_info[sw->HOST_PORT];
+		phydev->link = (info->state == media_connected);
+		phydev->speed = info->tx_rate / TX_RATE_UNIT;
+		phydev->duplex = (info->duplex == 2);
+		phydev->pause = 1;
+		if (phydev->link)
+			phy_link = (port->linked->state == media_connected);
+		link = netif_carrier_ok(dev);
+		if (link != phy_link) {
+			if (phy_link)
+				netif_carrier_on(dev);
+			else
+				netif_carrier_off(dev);
+			if (netif_msg_link(sw))
+				pr_info("%s link %s\n",
+					dev->name,
+					phy_link ? "on" : "off");
+		}
+		if (phydev->adjust_link && phydev->attached_dev)
+			phydev->adjust_link(phydev->attached_dev);
+	}
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		stp->ops->link_change(stp, true);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+		hsr->ops->link_change(hsr,
+			sw->port_info[hsr->ports[0]].state == media_connected,
+			sw->port_info[hsr->ports[1]].state == media_connected);
+		if (hsr->ports[0] <= port->first_port &&
+		    port->first_port <= hsr->ports[1])
+			hsr->ops->check_announce(hsr);
+	}
+#endif
+
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		if (!(sw->link_ports & (1 << i)))
+			continue;
+		info = &sw->port_info[i];
+		speed = (media_connected == info->state) ?
+			info->tx_rate / TX_RATE_UNIT : 0;
+		duplex = (info->duplex == 2);
+#ifdef CONFIG_1588_PTP
+		if (sw->features & PTP_HW) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			ptp->linked[i] = speed;
+			if (speed)
+				ptp->linked[i] |= 0x80000000;
+		}
+#endif
+#ifdef CONFIG_KSZ_MSRP
+		if (sw->features & MRP_SUPPORT) {
+			int n;
+			struct mrp_info *mrp = &sw->mrp;
+
+			n = sw_get_net_port(sw, 0, mrp->ports, i);
+			if (n <= mrp->ports)
+				mrp_set_speed(mrp, n, speed, duplex);
+		}
+#endif
+	}
+	if (sw->link_ports) {
+
+#ifdef CONFIG_1588_PTP
+		if (sw->features & PTP_HW) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			if (ptp->started)
+				set_latency(&ptp->set_latency);
+		}
+#endif
+	}
+	sw->link_ports = 0;
+}  /* link_update_work */
+
+static void sw_dis_intr(struct ksz_sw *sw)
+{
+	sw->reg->w32(sw, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4, sw->PORT_MASK);
+}  /* sw_dis_intr */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	int i;
+
+	sw->reg->w32(sw, REG_SW_INT_MASK__4,
+		~sw->intr_mask & SWITCH_INT_MASK);
+	sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4,
+		~sw->port_intr_mask & sw->PORT_MASK);
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		i = chk_last_port(sw, i);
+		port_w(sw, i, REG_PORT_INT_MASK,
+			~sw->info->port_cfg[i].intr_mask & PORT_INT_MASK);
+	}
+}  /* sw_ena_intr */
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *ks = seq->private;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&ks->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = HW_R(ks, i + j * SW_SIZE);
+		mutex_unlock(&ks->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, "\n");
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @ks:		The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void create_debugfs(struct sw_priv *ks)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "%s",
+		 dev_name(ks->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root\n");
+		return;
+	}
+
+	ks->debug_root = root;
+	ks->debug_file = debugfs_create_file("state", 0444, root,
+		ks, &state_fops);
+	if (IS_ERR(ks->debug_file))
+		pr_err("cannot create debugfs state file\n");
+}
+
+static void delete_debugfs(struct sw_priv *ks)
+{
+	debugfs_remove(ks->debug_file);
+	debugfs_remove(ks->debug_root);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs_9897.c"
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_sysfs.c"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr_sysfs.c"
+#endif
+
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct sw_priv *ks = phy_dat;
+
+	ks->sw.intr_using += 1;
+	ks->irq_work.func(&ks->irq_work);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct sw_priv *ks =
+		container_of(work, struct sw_priv, irq_work);
+	struct ksz_sw *sw = &ks->sw;
+	int intr;
+
+	/* Fake interrupt can be triggered once. */
+	if (ks->intr_working & 0x80000000) {
+		ks->intr_working |= 1;
+		if (ks->sw.info->port_cfg[0].intr_mask & PORT_PHY_INT)
+			ks->intr_working |= 2;
+	}
+	ks->intr_working |= 0x80000000;
+	mutex_lock(&sw->lock);
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	sw->intr_cnt = 0;
+	do {
+		intr = sw_proc_intr(sw);
+	} while (intr);
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	mutex_unlock(&ks->hwlock);
+	mutex_unlock(&sw->lock);
+	sw->intr_using = 0;
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	int err = 0;
+
+	INIT_WORK(&ks->irq_work, sw_change);
+
+	err = request_threaded_irq(ks->irq, NULL, sw_interrupt,
+		ks->intr_mode, name, ks);
+	if (err < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)\n",
+			name,
+			ks->irq);
+		return 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	free_irq(ks->irq, ks);
+	cancel_work_sync(&ks->irq_work);
+}  /* sw_stop_interrupt */
+
+static int sw_init_phy_priv(struct sw_priv *ks, struct phy_device *phydev,
+	int i)
+{
+	struct phy_priv *phydata;
+	struct ksz_port *port;
+	int p = i;
+
+	if (!p)
+		p = 1;
+
+	phydata = kzalloc(sizeof(struct phy_priv), GFP_KERNEL);
+	if (!phydata)
+		return -ENOMEM;
+	port = &ks->ports[i];
+	phydata->port = port;
+	port->sw = &ks->sw;
+	port->first_port = p - 1;
+	port->port_cnt = 1;
+	port->mib_port_cnt = 1;
+	port->flow_ctrl = PHY_FLOW_CTRL;
+	port->linked = &ks->sw.port_info[port->first_port];
+	INIT_WORK(&port->link_update, link_update_work);
+	phydata->state = phydev->state;
+	phydev->priv = phydata;
+	return 0;
+}  /* sw_init_phy_priv */
+
+static void sw_init_phydev(struct ksz_sw *sw, struct phy_device *phydev)
+{
+	struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+
+	phydev->interface = sw->interface;
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	phydev->duplex = (info->duplex == 2);
+	phydev->pause = 1;
+}  /* sw_init_phydev */
+
+#ifndef CONFIG_KSZ_NO_MDIO_BUS
+#define KSZ989X_SW_ID		0x9897
+#define KSZ889X_SW_ID		0x8897
+#define PHY_ID_KSZ989X_SW	((KSZ9897_ID_HI << 16) | KSZ989X_SW_ID)
+#define PHY_ID_KSZ889X_SW	((KSZ9897_ID_HI << 16) | KSZ889X_SW_ID)
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver[] = {
+{
+	.phy_id		= PHY_ID_KSZ989X_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Microchip KSZ989X Switch",
+	.features	= (PHY_GBIT_FEATURES |
+				SUPPORTED_Pause | SUPPORTED_Asym_Pause),
+	.flags		= PHY_HAS_MAGICANEG | PHY_HAS_INTERRUPT,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.driver		= { .owner = THIS_MODULE, },
+}, {
+	.phy_id		= PHY_ID_KSZ889X_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Microchip KSZ889X Switch",
+	.features	= (PHY_BASIC_FEATURES | SUPPORTED_Pause),
+	.flags		= PHY_HAS_MAGICANEG | PHY_HAS_INTERRUPT,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+	.driver		= { .owner = THIS_MODULE, },
+}
+};
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy_id, u16 phy, u16 reg, u16 *val)
+{
+	u16 data;
+	u16 ret;
+	int n;
+	int id = phy;
+
+	/* Get the represented PHY id when using multiple ports. */
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->eth_maps[n].port + 1 == phy) {
+			phy = sw->eth_maps[n].phy_id;
+			break;
+		}
+	}
+
+	if (0 == phy || phy > sw->phy_port_cnt)
+		phy = phy_id;
+	port_r16(sw, phy - 1, P_PHY_CTRL + reg * 2, &data);
+	ret = data;
+
+	/* Use unique switch id to differentiate from regular PHY. */
+	if (3 == reg) {
+		if (sw->features & GIGABIT_SUPPORT)
+			ret = KSZ989X_SW_ID;
+		else
+			ret = KSZ889X_SW_ID;
+	}
+	if (id > sw->phy_port_cnt) {
+		switch (reg) {
+		case 0:
+			ret = 0x1140;
+			break;
+		case 1:
+			ret = 0x796d;
+			break;
+		case 4:
+			ret = 0x05e1;
+			break;
+		case 5:
+			ret = 0xc5e1;
+			break;
+		case 9:
+			ret = 0x0700;
+			break;
+		case 10:
+			if (0 == id)
+				id = sw->HOST_PORT;
+			else
+				id--;
+			if (sw->port_info[id].tx_rate >= 1000 * TX_RATE_UNIT)
+				ret = 0x7800;
+			else
+				ret = 0;
+			break;
+		}
+	}
+	if (1 == reg && !(ret & PORT_LINK_STATUS)) {
+		port_r16(sw, phy - 1, P_SPEED_STATUS, &data);
+		if ((ret & PORT_AUTO_NEG_ACKNOWLEDGE) &&
+		    (data & (PORT_STAT_SPEED_1000MBIT |
+		    PORT_STAT_SPEED_100MBIT |
+		    PORT_STAT_SPEED_10MBIT)))
+			ret |= PORT_LINK_STATUS;
+	}
+	*val = ret;
+}  /* sw_r_phy */
+
+static int ksz_mii_addr(int *reg, int *bank)
+{
+	int ret;
+
+	ret = (*reg & 0xC000) >> ADDR_SHIFT;
+	*bank = (*reg & 0x3000) >> BANK_SHIFT;
+	*reg &= 0x0FFF;
+	return ret;
+}
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int addr;
+	int bank;
+	int ret = 0xffff;
+
+	if (phy_id > sw->mib_port_cnt)
+		return 0xffff;
+
+	addr = ksz_mii_addr(&regnum, &bank);
+
+	sw->ops->acquire(sw);
+
+	switch (addr) {
+	case ADDR_8:
+		ret = sw->reg->r8(sw, regnum);
+		break;
+	case ADDR_16:
+		ret = sw->reg->r16(sw, regnum);
+		break;
+	case ADDR_32:
+		ret = sw->reg->r32(sw, regnum);
+		break;
+	default:
+		if (regnum < 11) {
+			u16 data;
+
+			sw_r_phy(sw, ks->phy_id, phy_id, regnum, &data);
+			ret = data;
+		} else
+			ret = 0;
+	}
+	sw->ops->release(sw);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	static int last_reg;
+	static int last_val;
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int addr;
+	int bank;
+	int reg;
+
+	if (phy_id > sw->mib_port_cnt)
+		return -EINVAL;
+
+	reg = regnum;
+	addr = ksz_mii_addr(&regnum, &bank);
+
+	sw->ops->acquire(sw);
+
+	switch (addr) {
+	case ADDR_8:
+		sw->reg->w8(sw, regnum, val);
+		break;
+	case ADDR_16:
+		sw->reg->w16(sw, regnum, val);
+		break;
+	case ADDR_32:
+		/*
+		 * The phy_write interface allows only 16-bit value.  Break
+		 * the 32-bit write into two calls for SPI efficiency.
+		 */
+
+		/* Previous write to high word. */
+		if (last_reg == reg + 2) {
+			last_val <<= 16;
+			last_val |= val;
+			sw->reg->w32(sw, regnum, last_val);
+			last_reg = 0;
+		} else {
+			/* Somebody has written to different address! */
+			if (last_reg) {
+				int last_bank;
+
+				addr = ksz_mii_addr(&last_reg, &last_bank);
+				sw->reg->w16(sw, last_reg, last_val);
+				last_reg = 0;
+			}
+
+			/* Cache the 16-bit write to high word. */
+			if (reg & 3) {
+				last_reg = reg;
+				last_val = val;
+
+			/* Did not find the previous write to high word.*/
+			} else
+				sw->reg->w16(sw, regnum, val);
+		}
+		break;
+	default:
+		if (regnum < 11) {
+			int i;
+			int first;
+			int last;
+
+			if (0 == phy_id) {
+				first = 0;
+				last = sw->phy_port_cnt;
+			} else {
+				int n;
+				int f;
+				int l;
+
+				first = phy_id - 1;
+				last = phy_id;
+				for (n = 0; n < sw->eth_cnt; n++) {
+					f = sw->eth_maps[n].port + 1;
+					l = f + sw->eth_maps[n].cnt;
+					if (f <= phy_id && phy_id < l) {
+						first = sw->eth_maps[n].port;
+						last = first +
+							sw->eth_maps[n].cnt;
+						break;
+					}
+				}
+				if (last > sw->phy_port_cnt)
+					last = sw->phy_port_cnt;
+			}
+
+			/* PHY device driver resets or powers down the PHY. */
+			if (0 == regnum &&
+			    (val & (PORT_PHY_RESET | PORT_POWER_DOWN)))
+				break;
+			for (i = first; i < last; i++) {
+				if (i == sw->HOST_PORT)
+					continue;
+				port_w16(sw, i, P_PHY_CTRL + regnum * 2, val);
+			}
+			if (0 == regnum &&
+			    !(val & PORT_AUTO_NEG_ENABLE))
+				schedule_delayed_work(&ks->link_read, 1);
+		}
+		break;
+	}
+	sw->ops->release(sw);
+	return 0;
+}  /* ksz_mii_write */
+
+static int driver_installed;
+
+static int ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	int err;
+	int i;
+
+	pdev = platform_device_register_simple("Switch MII bus", ks->sw.id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	if (!driver_installed) {
+		err = phy_drivers_register(kszsw_phy_driver,
+			ARRAY_SIZE(kszsw_phy_driver));
+		if (err)
+			goto mii_init_free_mii_bus;
+		driver_installed = true;
+	}
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.mib_port_cnt + 1)) - 1);
+	bus->priv = ks;
+	bus->irq = ks->bus_irqs;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = ks->irq;
+
+	ks->phy_id = 1;
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	if (!bus->phy_map[0]) {
+		printk(KERN_WARNING "No PHY detected\n");
+		mdiobus_unregister(bus);
+		err = -ENODEV;
+		goto mii_init_free_mii_bus;
+	}
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i]) {
+			err = sw_init_phy_priv(ks, bus->phy_map[i], i);
+			if (err)
+				goto mii_init_free_mii_bus;
+		}
+
+	ks->bus = bus;
+	ks->pdev = pdev;
+	ks->phydev = bus->phy_map[0];
+	sw_init_phydev(&ks->sw, ks->phydev);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i])
+			kfree(bus->phy_map[i]->priv);
+	if (driver_installed) {
+		phy_drivers_unregister(kszsw_phy_driver,
+			ARRAY_SIZE(kszsw_phy_driver));
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+
+mii_init_reg:
+	platform_device_unregister(pdev);
+
+	return err;
+}  /* ksz_mii_init */
+
+static void ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct platform_device *pdev = ks->pdev;
+	struct mii_bus *bus = ks->bus;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		if (bus->phy_map[i]) {
+			struct ksz_port *port;
+
+			port = &ks->ports[i];
+			flush_work(&port->link_update);
+			kfree(bus->phy_map[i]->priv);
+		}
+	mdiobus_unregister(bus);
+	if (driver_installed) {
+		phy_drivers_unregister(kszsw_phy_driver,
+			ARRAY_SIZE(kszsw_phy_driver));
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+	platform_device_unregister(pdev);
+}  /* ksz_mii_exit */
+#endif
+
+/* driver bus management functions */
+
+static void determine_rate(struct ksz_sw *sw, struct ksz_port_mib *mib)
+{
+	int j;
+
+	for (j = 0; j < 2; j++) {
+		if (mib->rate[j].last) {
+			unsigned long diff = jiffies - mib->rate[j].last;
+			u64 cnt = mib->counter[MIB_RX_TOTAL + j] -
+				mib->rate[j].last_cnt;
+
+			if (cnt > 1000000 && diff >= 100) {
+				u32 rem;
+				u64 rate = cnt;
+
+				rate *= 8;
+				diff *= 10 * 100;
+				rate = div_u64_rem(rate, diff, &rem);
+				mib->rate[j].last = jiffies;
+				mib->rate[j].last_cnt =
+					mib->counter[MIB_RX_TOTAL + j];
+				if (mib->rate[j].peak < (u32) rate)
+					mib->rate[j].peak = (u32) rate;
+			}
+		} else
+			mib->rate[j].last = jiffies;
+	}
+}  /* determine_rate */
+
+static void ksz9897_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	int i;
+	int cnt = 0;
+
+	/* Find out how many ports are connected. */
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		i = chk_last_port(sw, i);
+		if (media_connected == sw->port_state[i].state)
+			++cnt;
+	}
+	if (!cnt)
+		cnt++;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += MIB_READ_INTERVAL * cnt;
+	}
+
+	/* Restart auto-negotiation for those ports used in port_setup_eee. */
+	do {
+		struct ksz_port_cfg *cfg;
+		u32 period = hw_priv->mib_timer_info.period * 1000 / HZ;
+
+		for (i = 0; i < sw->phy_port_cnt; i++) {
+			cfg = &sw->info->port_cfg[i];
+			if (cfg->setup_time) {
+				if (cfg->setup_time > period)
+					cfg->setup_time -= period;
+				else {
+					sw->ops->acquire(sw);
+					port_w16(sw, i, REG_PORT_PHY_CTRL,
+						0x1140);
+					sw->ops->release(sw);
+					cfg->setup_time = 0;
+				}
+			}
+		}
+	} while (0);
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		i = chk_last_port(sw, i);
+		mib = &sw->port_mib[i];
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[i].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, i))
+				return;
+			hw_priv->counter[i].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				hw_priv->counter[i].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[i].counter);
+				if (i != sw->HOST_PORT)
+					determine_rate(sw, mib);
+			}
+		} else if (time_after_eq(jiffies, hw_priv->counter[i].time)) {
+			hw_priv->counter[i].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[i].state) {
+				hw_priv->counter[i].read = 1;
+				sw->next_jiffies += MIB_READ_INTERVAL;
+			}
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[i].link_down) {
+			sw->port_state[i].link_down = 0;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[i].read = 1;
+		}
+	}
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		mrp_chk_blocked_addr(&sw->mrp);
+#endif
+}  /* ksz9897_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct phy_device *phydev;
+	struct ksz_port *port = NULL;
+	struct ksz_port *sw_port = NULL;
+	int i;
+	int changes = 0;
+	int s = 1;
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+	if (sw->dev_offset) {
+		struct phy_priv *phydata;
+		struct net_device *dev = sw->netdev[0];
+
+		phydev = sw->phydev;
+		phydata = phydev->priv;
+		if (dev && sw->net_ops->get_priv_port)
+			sw_port = sw->net_ops->get_priv_port(dev);
+		else
+			sw_port = phydata->port;
+	}
+
+	/*
+	 * Only check port which has interrupts triggered.
+	 * If no interrupt poll all the ports with PHY.
+	 */
+	if (!sw->phy_intr) {
+		sw->phy_intr = (1 << sw->phy_port_cnt) - 1;
+		for (i = sw->phy_port_cnt; i < sw->mib_port_cnt; i++)
+			if (sw->port_info[i].phy)
+				sw->phy_intr |= (1 << i);
+	}
+	sw->ops->acquire(sw);
+	for (i = sw->dev_offset; i < sw->dev_count + sw->dev_offset; i++) {
+		struct phy_priv *phydata;
+		struct net_device *dev = sw->netdev[i];
+		int j = i + s;
+
+		if (j > sw->HOST_PORT)
+			j++;
+		phydev = sw->phy[j];
+		if (sw->features & SW_VLAN_DEV)
+			phydev = sw->phy[sw->eth_maps[i].port + 1];
+		phydata = phydev->priv;
+		if (dev && sw->net_ops->get_priv_port)
+			port = sw->net_ops->get_priv_port(dev);
+		else
+			port = phydata->port;
+		changes |= port_get_link_speed(port);
+
+		/* Copy all port information for user access. */
+		if (port != phydata->port) {
+			copy_port_status(port, phydata->port);
+			if (phydata != hw_priv->phydev->priv) {
+				phydata = hw_priv->phydev->priv;
+				copy_port_status(port, phydata->port);
+			}
+		}
+	}
+	if (sw->phy_intr && sw->port_info[sw->HOST_PORT].phy) {
+		struct phy_priv *phydata;
+
+		phydev = sw->phy[sw->HOST_PORT + 1];
+		phydata = phydev->priv;
+		port = phydata->port;
+		port_get_link_speed(port);
+	}
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+
+	if (!sw->dev_offset || (media_connected == sw_port->linked->state))
+		changes = 0;
+
+	/* Not to read PHY registers unnecessarily if no link change. */
+	if (!changes)
+		return;
+
+	for (i = sw->dev_offset; i < sw->dev_count + sw->dev_offset; i++) {
+		struct phy_priv *phydata;
+		struct net_device *dev = sw->netdev[i];
+
+		phydev = sw->phy[i + s];
+		if (sw->features & SW_VLAN_DEV)
+			phydev = sw->phy[sw->eth_maps[i].port + 1];
+		phydata = phydev->priv;
+		if (dev && sw->net_ops->get_priv_port)
+			port = sw->net_ops->get_priv_port(dev);
+		else
+			port = phydata->port;
+		if (port->first_port < sw->phy_port_cnt) {
+			if (media_connected == port->linked->state) {
+				sw_port->linked = port->linked;
+				hw_priv->phy_id = port->linked->phy_id;
+				break;
+			}
+		}
+	}
+}  /* link_read_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz9897_mib_monitor(unsigned long ptr)
+{
+	struct sw_priv *hw_priv = (struct sw_priv *) ptr;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz9897_mib_monitor */
+
+static void ksz9897_dev_monitor(unsigned long ptr)
+{
+	struct sw_priv *hw_priv = (struct sw_priv *) ptr;
+
+#ifndef CONFIG_KSZ_NO_MDIO_BUS
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	int i;
+
+	for (i = 0; i <= TOTAL_PORT_NUM; i++) {
+		phydev = hw_priv->bus->phy_map[i];
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		if (priv->state != phydev->state) {
+			priv->state = phydev->state;
+			if (PHY_UP == phydev->state ||
+			    PHY_RESUMING == phydev->state)
+				schedule_work(&priv->port->link_update);
+		}
+	}
+#endif
+	if (!(hw_priv->intr_working & 2))
+		schedule_delayed_work(&hw_priv->link_read, 0);
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz9897_dev_monitor */
+
+#ifdef CONFIG_KSZ_DSA
+#include "ksz_dsa.c"
+#endif
+
+static int intr_mode;
+static int sw_host_port;
+static int ports;
+
+
+static int sw_device_present;
+
+static int ksz_probe_prep(struct sw_priv *ks, void *dev)
+{
+	struct ksz_sw *sw;
+
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+	intr_mode = 1;
+#endif
+
+	if (1 == intr_mode)
+		ks->intr_mode = IRQF_TRIGGER_LOW | IRQF_ONESHOT;
+	else if (2 == intr_mode)
+		ks->intr_mode = IRQF_TRIGGER_FALLING;
+	ks->intr_mode |= IRQF_ONESHOT;
+
+	dev_set_drvdata(ks->dev, ks);
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	mutex_init(&sw->acllock);
+	mutex_init(&sw->alulock);
+	mutex_init(&sw->vlanlock);
+	mutex_init(&sw->hsrlock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+	sw->dev = ks;
+
+	sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+	if (!sw->info)
+		return -ENOMEM;
+
+	sw->reg = &sw_reg_ops;
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+	init_waitqueue_head(&sw->queue);
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (dev) {
+		sw->netdev[0] = dev;
+		sw->features |= IBA_SUPPORT;
+		sw->info->iba.use_iba = 2;
+	}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	ksz_iba_init(&sw->info->iba, sw);
+	INIT_DELAYED_WORK(&sw->set_ops, sw_set_ops);
+#endif
+
+	return 0;
+}  /* ksz_probe_prep */
+
+static void xmii_hack(struct ksz_sw *sw, int pi, u16 *data, u16 orig,
+	u8 *data_hi)
+{
+	*data_hi ^= (PORT_MII_NOT_1GBIT | PORT_MII_MAC_MODE | PORT_MII_SEL_M);
+	port_w8(sw, pi, REG_PORT_XMII_CTRL_1, *data_hi);
+	*data = orig;
+	port_w8(sw, pi, REG_PORT_XMII_CTRL_1, *data_hi);
+}  /* xmii_hack */
+
+static int ksz_probe_next(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+	struct ksz_port *port;
+	struct ksz_port_info *info;
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	u32 id;
+	u32 id1;
+	u32 id2;
+	int cnt;
+	int i;
+	uint mib_port_count;
+	uint phy_port_count;
+	uint pi;
+	uint port_count;
+	int reset = true;
+	int ret = -ENODEV;
+	int cfg = 0;
+
+#ifdef CONFIG_KSZ_IBA
+	if (sw->info->iba.use_iba)
+		reset = false;
+#endif
+
+	sw->ops->acquire(sw);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	cfg = sw->reg->r8(sw, REG_SW_GLOBAL_SERIAL_CTRL_0);
+	if ((cfg & 0xc) == 0xc)
+		cfg >>= 5;
+	else
+		cfg = 0;
+
+	/* Turn off SPI auto edge detection. */
+	sw->reg->w8(sw, REG_SW_GLOBAL_SERIAL_CTRL_0, 0);
+#endif
+
+	/* simple check for a valid chip being connected to the bus */
+	id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+	sw->ops->release(sw);
+	id1 = id;
+	id1 >>= 8;
+	id1 &= 0xffff;
+	id2 = id1 & 0xff;
+	id1 >>= 8;
+dbg_msg("%02x %02x\n", id1, id2);
+	if (id1 != FAMILY_ID_95 && id1 != FAMILY_ID_98 &&
+	    id1 != FAMILY_ID_94 && id1 != FAMILY_ID_85 && id1 != 0x64) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)\n", id);
+		ret = -ENODEV;
+		goto err_mii;
+	}
+	dev_info(ks->dev, "chip id 0x%08x\n", id);
+
+	port_count = 1;
+	mib_port_count = 1;
+	phy_port_count = 1;
+#ifdef CONFIG_1588_PTP
+	if ((FAMILY_ID_95 & 0x0f) == (id1 & 0x0f)) {
+		sw->features |= PTP_HW;
+		sw->features |= ACL_CORRUPT_BUG;
+	}
+	if (0x64 == id1) {
+		sw->features |= PTP_HW;
+#if 1
+		sw->features |= SETUP_PHY;
+#endif
+	}
+#endif
+	if ((FAMILY_ID_85 & 0xf0) == (id1 & 0xf0))
+		sw->features |= QW_HW;
+	if ((CHIP_ID_67 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 7;
+		mib_port_count = 7;
+		phy_port_count = 5;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+	} else if ((CHIP_ID_66 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 6;
+		mib_port_count = 6;
+		phy_port_count = 5;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+	} else if ((CHIP_ID_63 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 3;
+		mib_port_count = 3;
+		phy_port_count = 2;
+		sw->TAIL_TAG_LOOKUP = (1 << (3 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (3 + 2));
+		sw->features |= IS_9893;
+	}
+	sw->features |= NO_GLOBAL_RESET;
+	sw->features |= PHY_INTR_BUG;
+
+	sw->id = sw_device_present;
+
+	/* Check for S2 revision. */
+	sw->ops->acquire(sw);
+	id = sw->reg->r8(sw, REG_GLOBAL_OPTIONS);
+	sw->ops->release(sw);
+	if (id) {
+		u16 val;
+
+		sw->revision = 1;
+
+		sw->ops->acquire(sw);
+		port_r16(sw, 0, REG_PORT_INT_STATUS & ~1, &val);
+		sw->ops->release(sw);
+		if (!(val & PORT_PHY_INT)) {
+			sw->revision = 2;
+			sw->features &= ~PHY_INTR_BUG;
+		}
+
+		sw->features &= ~ACL_CORRUPT_BUG;
+		sw->features &= ~SETUP_PHY;
+		sw->features &= ~NO_GLOBAL_RESET;
+		sw->features |= NEW_CAP;
+
+		/* Only new KSZ9897 changes XMII definitions. */
+		if ((CHIP_ID_63 & 0x0f) != (id2 & 0x0f)) {
+			sw->features |= NEW_XMII;
+			if (id & SW_REDUNDANCY_ABLE) {
+				sw->features |= REDUNDANCY_SUPPORT;
+#ifdef CONFIG_KSZ_HSR
+				sw->features |= HSR_HW;
+#endif
+			}
+		} else {
+			if (id & SW_QW_ABLE)
+				sw->features |= QW_HW;
+			else
+				sw->features |= GIGABIT_SUPPORT;
+		}
+		if (id & SW_AVB_ABLE) {
+			sw->features |= AVB_SUPPORT;
+#ifdef CONFIG_1588_PTP
+			sw->features |= PTP_HW;
+#endif
+		}
+
+		/* DLR can be used if supervisor is not needed. */
+#ifdef CONFIG_KSZ_DLR
+		sw->features |= DLR_HW;
+#endif
+
+		switch (id & 0x0f) {
+		case SW_9477_SL_5_2:
+			/* Last port is SGMII. */
+			if (!sw_host_port)
+				sw_host_port = 6;
+			break;
+		}
+		if (id & SW_GIGABIT_ABLE)
+			sw->features |= GIGABIT_SUPPORT;
+
+		if (sw->features & IS_9893)
+dbg_msg("avb=%d  qw=%d  giga=%d\n",
+!!(id & SW_AVB_ABLE), !!(id & SW_QW_ABLE), !!(id & SW_GIGABIT_ABLE));
+		else
+dbg_msg("avb=%d  rr=%d  giga=%d\n",
+!!(id & SW_AVB_ABLE), !!(id & SW_REDUNDANCY_ABLE), !!(id & SW_GIGABIT_ABLE));
+	} else if ((FAMILY_ID_95 & 0x0f) == (id1 & 0x0f))
+		sw->features |= AVB_SUPPORT;
+	if ((sw->features & (HSR_HW | DLR_HW)) && port_count > 3)
+		sw->overrides |= HAVE_MORE_THAN_2_PORTS;
+
+	sw->PORT_MASK = (1 << mib_port_count) - 1;
+	if (sw_host_port < 0 || sw_host_port > mib_port_count)
+		sw_host_port = 0;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (!sw_host_port)
+		sw_host_port = port_count;
+#endif
+
+	/* Select the host port. */
+	if (sw_host_port > 0 && sw_host_port <= port_count)
+		sw->HOST_PORT = sw_host_port - 1;
+	else
+		sw_host_port = 0;
+
+	/* Want fewer ports used. */
+	if (ports > 0 && ports < port_count) {
+		mib_port_count = ports;
+		if (ports < phy_port_count)
+			phy_port_count = ports;
+		sw->PORT_MASK = (1 << ports) - 1;
+		if (sw_host_port > 0) {
+
+			/* Host port not in working ports. */
+			if (sw_host_port >= ports + 1)
+				++mib_port_count;
+
+			if (sw_host_port > ports + 1) {
+				sw->last_port = ports;
+				sw->features |= USE_FEWER_PORTS;
+dbg_msg("fewer: %x %x\n", phy_port_count, mib_port_count);
+			}
+		}
+		port_count = mib_port_count;
+	}
+	if (!sw_host_port)
+		sw->HOST_PORT = port_count - 1;
+
+	sw->dev_count = 1;
+
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = mib_port_count;
+	sw->phy_port_cnt = phy_port_count;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+	sw->PORT_MASK |= sw->HOST_MASK;
+dbg_msg("mask: %x %x; %x %x\n", sw->HOST_MASK, sw->PORT_MASK,
+sw->TAIL_TAG_LOOKUP, sw->TAIL_TAG_OVERRIDE);
+
+#if 0
+	/* Host port not in the middle of working ports. */
+	if (sw->HOST_PORT == port_count - 1 || !sw->HOST_PORT ||
+	    (sw->features & USE_FEWER_PORTS)) {
+#else
+	if (sw->HOST_PORT >= port_count - 1) {
+#endif
+		port_count = port_count - 1;
+		mib_port_count = mib_port_count - 1;
+	}
+	sw->port_cnt = port_count;
+dbg_msg("port: %x %x %x\n", sw->port_cnt, sw->mib_port_cnt, sw->phy_port_cnt);
+
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+
+	for (cnt = 0, pi = 0; cnt < phy_port_count; cnt++, pi++) {
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info = &sw->port_info[pi];
+		info->link = 0xFF;
+		info->state = media_disconnected;
+		info->phy = true;
+		info->report = true;
+		info->phy_id = pi + 1;
+	}
+	sw->interface = PHY_INTERFACE_MODE_RGMII;
+	sw->ops->acquire(sw);
+	for (; cnt < sw->mib_port_cnt; cnt++, pi++) {
+		u16 data;
+		u16 orig;
+		u8 *data_lo;
+		u8 *data_hi;
+		int speed;
+		phy_interface_t phy;
+		int gbit;
+		int mode;
+
+		if (sw->TAIL_TAG_LOOKUP < 0x100)
+			mode = 2;
+		else
+			mode = 5;
+		if (sw->features & USE_FEWER_PORTS)
+			pi = sw->HOST_PORT;
+		info = &sw->port_info[pi];
+		info->phy_id = pi + 1;
+		if (pi < mode) {
+#ifdef CONFIG_KSZ_IBA_ONLY
+			if (pi == sw->HOST_PORT) {
+				info->state = media_connected;
+				sw->live_ports |= (1 << pi);
+				info->tx_rate = 1000 * TX_RATE_UNIT;
+				info->duplex = 2;
+			}
+#endif
+			break;
+		}
+		port_r16(sw, pi, REG_PORT_XMII_CTRL_0, &data);
+		orig = data;
+		data_hi = (u8 *) &data;
+		data_lo = data_hi + 1;
+
+/**
+ * THa  2015/08/27
+ * Port 6 or 7 may never start transmiting and cause flow control problem in
+ * the receive port.
+ * Not guaranteed to work all the time.
+ */
+		if (sw->features & NEW_CAP) {
+			if (reset)
+				xmii_hack(sw, pi, &data, orig, data_hi);
+		} else
+			*data_hi &= ~(PORT_RGMII_ID_IG_ENABLE |
+				PORT_RGMII_ID_EG_ENABLE);
+#ifdef USE_10_MBIT_MODE
+		*data_lo &= ~PORT_MII_100MBIT;
+#endif
+#ifdef USE_HALF_DUPLEX
+		*data_lo &= ~PORT_MII_FULL_DUPLEX;
+#endif
+#ifdef USE_RGMII_MODE
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+#endif
+#ifdef USE_MII_MODE
+		sw_set_gbit(sw, false, data_hi);
+		sw_set_xmii(sw, 0, data_hi);
+#endif
+#ifdef USE_GMII_MODE
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 2, data_hi);
+#endif
+#ifdef USE_GMII_100_MODE
+		sw_set_gbit(sw, false, data_hi);
+#endif
+#ifdef USE_RMII_MODE
+		sw_set_gbit(sw, false, data_hi);
+		sw_set_xmii(sw, 1, data_hi);
+#endif
+/* Strap options may not valid after reset. */
+#if 1
+if (PORT_RMII_SEL == (*data_hi & PORT_MII_SEL_M)) {
+dbg_msg("?%02x\n", *data_hi);
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+}
+#endif
+		gbit = sw_get_gbit(sw, *data_hi);
+		mode = sw_get_xmii(sw, *data_hi);
+		switch (mode) {
+		case 2:
+			phy = PHY_INTERFACE_MODE_GMII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 1000;
+			if (gbit)
+				break;
+		case 0:
+			phy = PHY_INTERFACE_MODE_MII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			break;
+		case 1:
+			phy = PHY_INTERFACE_MODE_RMII;
+			if (sw->HOST_PORT == pi)
+				sw->interface = phy;
+			info->interface = phy;
+			speed = 100;
+			break;
+		default:
+			phy = PHY_INTERFACE_MODE_RGMII;
+			if (*data_hi & PORT_RGMII_ID_IG_ENABLE)
+				phy = PHY_INTERFACE_MODE_RGMII_RXID;
+			if (*data_hi & PORT_RGMII_ID_EG_ENABLE) {
+				if (PHY_INTERFACE_MODE_RGMII_RXID == phy)
+					phy = PHY_INTERFACE_MODE_RGMII_ID;
+				else
+					phy = PHY_INTERFACE_MODE_RGMII_TXID;
+			}
+			if (sw->HOST_PORT == pi) {
+				if ((id & 0x0f) == SW_9477_SL_5_2)
+					phy = PHY_INTERFACE_MODE_RGMII;
+				sw->interface = phy;
+			}
+			info->interface = phy;
+			speed = 100;
+			if (gbit)
+				speed = 1000;
+			break;
+		}
+		if (*data_lo & PORT_SGMII_SEL) {
+			info->interface = PHY_INTERFACE_MODE_SGMII;
+			info->link = 0xFF;
+			info->state = media_disconnected;
+			info->phy = port_sgmii_detect(sw, pi);
+			if (info->phy)
+				info->report = true;
+			info->phy_id = pi + 1;
+		}
+		if (sw->HOST_PORT == pi)
+dbg_msg("host: %d %d\n", sw->HOST_PORT, sw->interface);
+#if defined(CONFIG_MACB) || defined(CONFIG_MACB_MODULE)
+		if (sw->HOST_PORT == pi &&
+		    phy != PHY_INTERFACE_MODE_RGMII_TXID) {
+			phy = PHY_INTERFACE_MODE_RGMII_TXID;
+			sw->interface = phy;
+			info->interface = phy;
+		}
+#endif
+		if (info->phy)
+			info->state = media_disconnected;
+		else {
+			info->state = media_connected;
+			sw->live_ports |= (1 << pi);
+		}
+		if (!(*data_lo & PORT_MII_100MBIT))
+			info->tx_rate = 10 * TX_RATE_UNIT;
+		else
+			info->tx_rate = speed * TX_RATE_UNIT;
+		if (*data_lo & PORT_MII_FULL_DUPLEX)
+			info->duplex = 2;
+		else
+			info->duplex = 1;
+		info->flow_ctrl = 0x33;
+		sw->cached.xmii[cnt - sw->phy_port_cnt] = (*data_lo << 8) |
+			*data_hi;
+dbg_msg("xmii: %04x %02x %02x; %u %u\n", orig, *data_lo, *data_hi,
+info->tx_rate / TX_RATE_UNIT, info->duplex);
+	}
+	sw->ops->release(sw);
+
+#ifndef CONFIG_KSZ_NO_MDIO_BUS
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+
+#else
+	for (i = 0; i <= sw->mib_port_cnt; i++)
+		sw_init_phy_priv(ks, &sw->phy_map[i], i);
+	ks->phydev = &sw->phy_map[0];
+	sw_init_phydev(sw, ks->phydev);
+#endif
+
+	/* Try to enable different mode from one set in U-Boot. */
+	sw_setup_mode(sw, cfg);
+
+#ifdef CONFIG_KSZ_DSA
+	avb = 0;
+	authen = 0;
+	multi_dev = 0;
+#endif
+	if (authen)
+		sw->overrides |= USE_802_1X_AUTH;
+	if (!(sw->features & AVB_SUPPORT))
+		avb = 0;
+
+#ifndef CONFIG_KSZ_DSA
+	if (!multi_dev && (avb || sw->features & (PTP_HW) ||
+	    sw->overrides & (USE_802_1X_AUTH))) {
+		multi_dev = 3;
+		if (!(sw->overrides & (USE_802_1X_AUTH)))
+			stp = 1;
+	}
+#endif
+
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+	if (sw->stp)
+		sw->features |= STP_SUPPORT;
+	if (sw->fast_aging)
+		sw->overrides |= FAST_AGING;
+	sw_setup_zone(sw);
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & AVB_SUPPORT) {
+		sw->features |= MRP_SUPPORT;
+	}
+#endif
+
+	sw->phydev = ks->phydev;
+	sw->counter = ks->counter;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+
+	sw_setup_mib(sw);
+	sw_init_mib(sw);
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	create_debugfs(ks);
+#endif
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_init(&sw->info->rstp, sw);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_init(&sw->info->dlr, sw);
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		ksz_hsr_init(&sw->info->hsr, sw);
+#endif
+	sw->ops->acquire(sw);
+
+	/* Turn off PTP in case the feature is not enabled. */
+	if (reset)
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, 0);
+
+	if (reset)
+		sw_reset(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw->ops->release(sw);
+	sw->ops->init(sw);
+
+#if !defined(CONFIG_KSZ9897_EMBEDDED)
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		init_dlr_sysfs(ks->dev);
+#endif
+#endif
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+	if (ret < 0)
+		goto err_mii;
+	sema_init(&ks->proc_sem, 1);
+
+	if (ks->bus) {
+		for (i = 0; i <= sw->mib_port_cnt; i++)
+			sw->phy[i] = ks->bus->phy_map[i];
+	} else {
+		for (i = 0; i <= sw->mib_port_cnt; i++)
+			sw->phy[i] = &sw->phy_map[i];
+	}
+	phydev = sw->phy[0];
+	priv = phydev->priv;
+	port = priv->port;
+	port->port_cnt = port_count;
+	port->mib_port_cnt = mib_port_count;
+	port->flow_ctrl = PHY_TX_ONLY;
+
+#ifdef NO_ATTACHED_DEV
+	sw->ops->acquire(sw);
+	phydev = sw->phydev;
+	priv = phydev->priv;
+	port = priv->port;
+	port_set_link_speed(port);
+	sw->ops->release(sw);
+#endif
+
+	INIT_WORK(&sw->set_addr, sw_delayed_set_addr);
+	INIT_WORK(&sw->tx_fwd, sw_tx_fwd);
+	skb_queue_head_init(&sw->txq);
+
+	INIT_WORK(&ks->mib_read, ksz9897_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz9897_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz9897_dev_monitor, ks);
+
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!(sw->multi_dev & 1) && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period * 10);
+
+	sw_device_present++;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ports = sw->mib_port_cnt - 0;
+#if 1
+		if (1 == sw->multi_dev && (sw->features & SW_VLAN_DEV))
+			ptp->ports = sw->eth_maps[0].cnt + 1;
+#endif
+		ptp->reg = &ptp_reg_ops;
+		ptp->ops = &ptp_ops;
+		ptp->parent = ks->dev;
+		ptp->ops->init(ptp, sw->info->mac_addr);
+#ifdef NO_ATTACHED_DEV
+		ptp->reg->start(ptp, true);
+#endif
+		init_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+	}
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		INIT_DELAYED_WORK(&sw->set_mrp, sw_set_mrp);
+		mrp->ops = &mrp_ops;
+		mrp->ops->init(mrp);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_DSA
+	ksz_dsa_init();
+#endif
+
+	if (ks->irq <= 0)
+		return 0;
+	sw->ops->acquire(sw);
+	sw->reg->w32(sw, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4, sw->PORT_MASK);
+	sw->reg->w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		i = chk_last_port(sw, i);
+		port_w(sw, i, REG_PORT_INT_MASK, 0xff);
+		port_w16(sw, i, REG_PTP_PORT_TX_INT_STATUS__2, 0xffff);
+	}
+	sw->ops->release(sw);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0)
+		printk(KERN_WARNING "No switch interrupt\n");
+	else {
+		sw->ops->acquire(sw);
+		sw_ena_intr(sw);
+		sw->ops->release(sw);
+	}
+
+	return 0;
+
+err_mii:
+
+#ifdef CONFIG_KSZ_IBA
+	ksz_iba_exit(&sw->info->iba);
+#endif
+	kfree(sw->info);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	kfree(ks->hw_dev);
+#endif
+	kfree(ks);
+
+	return ret;
+}  /* ksz_probe_next */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+static int ksz_probe(struct sw_priv *ks)
+{
+	int ret = -ENODEV;
+
+	ret = ksz_probe_prep(ks, NULL);
+	if (ret)
+		return ret;
+
+	return ksz_probe_next(ks);
+}
+#endif
+
+static int ksz_remove(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+
+#ifdef CONFIG_KSZ_DSA
+	ksz_dsa_cleanup();
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		cancel_delayed_work_sync(&sw->set_mrp);
+		mrp->ops->exit(mrp);
+	}
+#endif
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		exit_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+#ifdef NO_ATTACHED_DEV
+		ptp->ops->stop(ptp);
+#endif
+		ptp->ops->exit(ptp);
+	}
+#endif
+
+	if (ks->irq > 0) {
+#ifndef CONFIG_KSZ_IBA_ONLY
+		sw->ops->acquire(sw);
+		sw->reg->w32(sw, REG_SW_INT_MASK__4, 0xffffffff);
+		sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4, 0xffffffff);
+		sw->ops->release(sw);
+#endif
+		sw_stop_interrupt(ks);
+	}
+
+#ifndef CONFIG_KSZ_NO_MDIO_BUS
+	ksz_mii_exit(ks);
+#endif
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+
+#if !defined(CONFIG_KSZ9897_EMBEDDED)
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		exit_dlr_sysfs(ks->dev);
+#endif
+	exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+	sw->ops->exit(sw);
+	cancel_delayed_work_sync(&ks->link_read);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	delete_debugfs(ks);
+#endif
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_exit(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_exit(&sw->info->dlr);
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		ksz_hsr_exit(&sw->info->hsr);
+#endif
+#ifdef CONFIG_KSZ_IBA
+	ksz_iba_exit(&sw->info->iba);
+#endif
+	kfree(sw->info);
+#ifndef CONFIG_KSZ_IBA_ONLY
+	kfree(ks->hw_dev);
+#endif
+	kfree(ks);
+
+	return 0;
+}  /* ksz_remove */
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_sw_9897.h b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_9897.h
new file mode 100644
index 0000000..47e9d50
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_9897.h
@@ -0,0 +1,1308 @@
+/**
+ * Microchip KSZ9897 switch common header
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_SW_9897_H
+#define KSZ_SW_9897_H
+
+
+/* These definitions should be defined before this header file. */
+#ifndef PRIO_QUEUES
+#define PRIO_QUEUES			4
+#endif
+#define PRIO_QUEUES_M			(PRIO_QUEUES - 1)
+
+#ifndef KS_PRIO_IN_REG
+#define KS_PRIO_IN_REG			2
+#endif
+
+#ifndef TOTAL_PORT_NUM
+#define TOTAL_PORT_NUM			7
+#endif
+
+#ifndef SWITCH_COUNTER_NUM
+#define SWITCH_COUNTER_NUM		0x20
+#endif
+
+#ifndef SW_D
+#error "SW_D and other data bus parameters need to be defined."
+#endif
+
+/* Host port can be any one of them. */
+#define SWITCH_PORT_NUM			(TOTAL_PORT_NUM)
+
+
+struct sw_dev_info {
+	void *sw;
+	unsigned int minor;
+	u8 *write_buf;
+	u8 *read_buf;
+	size_t read_max;
+	size_t read_len;
+	size_t write_len;
+	struct semaphore sem;
+	struct mutex lock;
+	wait_queue_head_t wait_msg;
+	struct sw_dev_info *next;
+};
+
+
+struct ksz_vlan_table;
+
+#include "ksz_sw_api.h"
+#ifdef CONFIG_KSZ_MSTP
+#include "ksz_mstp.h"
+#elif defined(CONFIG_KSZ_STP)
+#include "ksz_stp.h"
+#endif
+#ifdef CONFIG_KSZ_IBA
+#include "ksz_iba.h"
+#endif
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_9897.h"
+#endif
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.h"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.h"
+#endif
+#ifdef CONFIG_KSZ_HSR
+#include "ksz_hsr.h"
+#endif
+
+
+#define LEARNED_MAC_TABLE_ENTRIES	1024
+#define STATIC_MAC_TABLE_ENTRIES	16
+#define RESERVED_MCAST_TABLE_ENTRIES	0x30
+#define ACTUAL_MCAST_TABLE_ENTRIES	8
+#define SWITCH_MAC_TABLE_ENTRIES	16
+#define MULTI_MAC_TABLE_ENTRIES		80
+
+/**
+ * struct ksz_mac_table - Static MAC table data structure
+ * @mac_addr:	MAC address to filter.
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @override:	Override setting.
+ * @use_fid:	FID use setting.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_mac_table {
+	u8 addr[ETH_ALEN];
+	u32 ports;
+	u16 fid;
+	u8 mstp;
+	u8 prio;
+	u8 src:1;
+	u8 dst:1;
+	u8 override:1;
+	u8 use_fid:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define FWD_HOST_OVERRIDE		BIT(0)
+#define FWD_HOST			BIT(1)
+#define FWD_STP_DEV			BIT(2)
+#define FWD_MAIN_DEV			BIT(3)
+#define FWD_VLAN_DEV			BIT(4)
+#define FWD_MCAST			BIT(5)
+#define FWD_UCAST			BIT(6)
+#define FWD_KNOWN			BIT(7)
+
+struct ksz_alu_table {
+	u16 index;
+	u16 owner;
+	u8 forward;
+	u8 type;
+	u8 valid:1;
+};
+
+#define VLAN_TABLE_ENTRIES		4096
+#define VID_IN_DATA			32
+#define FID_ENTRIES			128
+#define FID_IN_DATA			32
+#define NUM_OF_VID			4094
+#define NUM_OF_MSTI			8
+
+/**
+ * struct ksz_vlan_table - VLAN table data structure
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @untag:	Untag membership.
+ * @mstp:	MSTP number.
+ * @prio:	Priority
+ * @fo:		Forward option.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_vlan_table {
+	u16 vid;
+	u16 fid;
+	u32 ports;
+	u32 untag;
+	u8 mstp;
+	u8 prio;
+	u8 option:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * struct ksz_hsr_table - HSR table data structure
+ * @dst:	Destination MAC address.
+ * @src:	Source MAC address.
+ * @age_cnt:	Age count.
+ * @path_id:	Path ID.
+ * @start_seq:	Starting sequence number.
+ * @exp_seq:	Expected sequence number.
+ * @seq_cnt:	Out of sequence number count.
+ * @valid:	Valid setting indicating the entry is being used.
+ * @dirty:	Flag indicating structure has been changed.
+ */
+struct ksz_hsr_table {
+	u8 dst_mac[ETH_ALEN];
+	u8 src_mac[ETH_ALEN];
+	u8 age_cnt;
+	u8 path_id;
+	u16 start_seq[2];
+	u16 exp_seq[2];
+	u16 seq_cnt[2];
+	u8 valid:1;
+	u8 dirty:1;
+};
+#endif
+
+#define PRIO_802_1P_ENTRIES		8
+
+#define DIFFSERV_ENTRIES		64
+
+#define ACL_TABLE_ENTRIES		16
+
+struct ksz_acl_table {
+	u16 first_rule;
+	u16 ruleset;
+	u8 mac[ETH_ALEN];
+	u16 eth_type;
+	u8 protocol;
+	u8 ip4_addr[4];
+	u8 ip4_mask[4];
+	u32 seqnum;
+	u16 max_port;
+	u16 min_port;
+	u8 prio;
+	u8 vlan_prio;
+	u16 ports;
+	u16 cnt;
+	u8 tcp_flag_mask;
+	u8 tcp_flag;
+#if 0
+	u8 ip6_addr[16];
+	u8 ip6_mask[16];
+#endif
+	u32 mode:2;
+	u32 enable:2;
+	u32 src:1;
+	u32 equal:1;
+	u32 port_mode:2;
+	u32 tcp_flag_enable:1;
+	u32 msec:1;
+	u32 intr_mode:1;
+	u32 prio_mode:2;
+	u32 vlan_prio_replace:1;
+	u32 map_mode:2;
+	u32 changed:1;
+	u32 action_changed:1;
+	u32 ruleset_changed:1;
+	u32 action_selected:1;
+
+	u8 data[ACL_TABLE_LEN];
+};
+
+/**
+ * struct ksz_port_mib - Port MIB data structure
+ * @cnt_ptr:	Current pointer to MIB counter index.
+ * @mib_start:	The starting counter index.  Some ports do not start at 0.
+ * @counter:	64-bit MIB counter value.
+ * @dropped:	Temporary buffer to remember last read packet dropped values.
+ * @read_cnt:	Used to signal when to read the MIB counter.
+ * @read_max:	Used to indicate how often to read the MIB counter.
+ *
+ * MIB counters needs to be read periodically so that counters do not get
+ * overflowed and give incorrect values.  A right balance is needed to
+ * satisfy this condition and not waste too much CPU time.
+ */
+struct ksz_port_mib {
+	u8 cnt_ptr;
+	u8 mib_start;
+	u8 interval;
+	u8 reserved[1];
+
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+	struct {
+		unsigned long last;
+		u64 last_cnt;
+		u32 peak;
+	} rate[2];
+	unsigned long last_drop;
+	u64 drop;
+	u64 first_drop;
+};
+
+enum {
+	STP_STATE_DISABLED = 0,
+	STP_STATE_LISTENING,
+	STP_STATE_LEARNING,
+	STP_STATE_FORWARDING,
+	STP_STATE_BLOCKED,
+	STP_STATE_SIMPLE
+};
+
+/**
+ * struct ksz_port_cfg - Port configuration data structure
+ * @vid:	VID value.
+ * @member:	Port membership.
+ * @port_prio:	Port priority.
+ * @rate_ctrl:	Priority rate control.
+ * @rx_rate:	Receive priority rate.
+ * @tx_rate:	Transmit priority rate.
+ * @rate_limit: Priority rate limit value.
+ * @vid_member:	VLAN membership.
+ * @index:	Net device pointer.
+ * @stp_state:	Current Spanning Tree Protocol state.
+ */
+struct ksz_port_cfg {
+	u16 vid;
+	u16 member;
+	u8 rate_ctrl[PRIO_QUEUES];
+	u32 rx_packet[RX_PRIO_QUEUES];
+	u32 rx_rate[RX_PRIO_QUEUES];
+	u32 tx_packet[PRIO_QUEUES];
+	u32 tx_rate[PRIO_QUEUES];
+	u32 color_map[DIFFSERV_ENTRIES / 16];
+	u32 tc_map[PRIO_802_1P_ENTRIES / 8];
+	u32 untagged[VLAN_TABLE_ENTRIES / VID_IN_DATA];
+	u8 p_index;
+	u8 q_index;
+	u8 port_prio;
+	u8 rate_limit;
+	int packet_based;
+	u16 intr_mask;
+	u16 vid_member;
+	int index;
+	int stp_state[NUM_OF_MSTI];
+
+	struct ksz_acl_table acl_info[ACL_TABLE_ENTRIES];
+	u16 acl_index;
+	u16 acl_act_index;
+	u16 acl_rule_index;
+
+	u16 mmd_id;
+	u16 mmd_reg;
+
+	u32 enabled:1;
+	u32 ptp_enabled:1;
+	u32 asCapable:1;
+	u32 asCapable_set:1;
+	u32 avb_a:1;
+	u32 avb_b:1;
+	u32 restricted:1;
+	u32 freeze:1;
+
+	u16 phy_ctrl;
+	u16 phy_adv;
+	u16 phy_adv_g;
+	u8 phy_intr;
+
+	u8 mstp;
+
+	u32 setup_time;
+};
+
+/**
+ * struct ksz_sw_info - KSZ9897 switch information data structure
+ * @mac_table:	MAC table entries information.
+ * @alu_table:	ALU table entries information.
+ * @multi_net:	Network multicast addresses used.
+ * @multi_sys:	System multicast addresses used.
+ * @port_cfg:	Port configuration information.
+ * @rstp:	RSTP information.
+ * @iba:	IBA information.
+ * @dlr:	DLR information.
+ * @hsr:	HSR information.
+ * @hsr_entry:	HSR table entry information.
+ * @mac_entry:	MAC table entry information.
+ * @vlan_entry:	VLAN table entry information.
+ * @diffserv:	DiffServ priority settings.  Possible values from 6-bit of ToS
+ *		(bit7 ~ bit2) field.
+ * @p_802_1p:	802.1P priority settings.  Possible values from 3-bit of 802.1p
+ *		Tag priority field.
+ * @br_addr:	Bridge address.  Used for STP.
+ * @mac_addr:	Switch MAC address.
+ * @broad_per:	Broadcast storm percentage.
+ * @member:	Current port membership.  Used for STP.
+ * @phy_addr:	PHY address used by first port.
+ */
+struct ksz_sw_info {
+	struct ksz_mac_table mac_table[MULTI_MAC_TABLE_ENTRIES];
+	struct ksz_alu_table alu_table[MULTI_MAC_TABLE_ENTRIES];
+	int forward;
+	int multi_net;
+	int multi_sys;
+	struct ksz_port_cfg port_cfg[TOTAL_PORT_NUM];
+#ifdef CONFIG_KSZ_STP
+	struct ksz_stp_info rstp;
+#endif
+#ifdef CONFIG_KSZ_IBA
+	struct ksz_iba_info iba;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	struct ksz_dlr_info dlr;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	struct ksz_hsr_info hsr;
+	struct ksz_hsr_table hsr_entry;
+#endif
+	struct ksz_mac_table mac_entry;
+	struct ksz_vlan_table vlan_entry;
+
+	SW_D diffserv[DIFFSERV_ENTRIES / KS_PRIO_IN_REG];
+	SW_D p_802_1p[PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG];
+
+	u8 br_addr[ETH_ALEN];
+	u8 mac_addr[ETH_ALEN];
+
+	u8 vid2fid[VLAN_TABLE_ENTRIES];
+	u8 fid2mstid[FID_ENTRIES];
+	u32 vid[VLAN_TABLE_ENTRIES / VID_IN_DATA];
+	u32 fid[FID_ENTRIES / FID_IN_DATA];
+	u16 fid_cnt;
+
+	u8 broad_per;
+	u8 member[NUM_OF_MSTI];
+	u8 phy_addr;
+	u8 fid_updated;
+};
+
+/**
+ * struct ksz_port_state - Port state information data structure
+ * @state:	Connection status of the port.
+ * @link_down:	Indication the link has just gone down.
+ *
+ * It is pointless to read MIB counters when the port is disconnected.  The
+ * @state provides the connection status so that MIB counters are read only
+ * when the port is connected.  The @link_down indicates the port is just
+ * disconnected so that all MIB counters are read one last time to update the
+ * information.
+ */
+struct ksz_port_state {
+	uint state;
+	uint tx_rate;
+	u8 link_down;
+};
+
+#define TX_RATE_UNIT			10000
+
+/**
+ * struct ksz_port_info - Port information data structure
+ * @interface:	PHY interface.
+ * @state:	Connection status of the port.
+ * @tx_rate:	Transmit rate divided by 10000 to get Mbit.
+ * @duplex:	Duplex mode.
+ * @flow_ctrl:	Flow control.
+ * @link:	Link status.  Used to determine link.
+ * @advertised:	Advertised auto-negotiation setting.  Used to determine link.
+ * @partner:	Auto-negotiation partner setting.  Used to determine link.
+ * @status:	LinkMD status values.
+ * @length:	LinkMD length values.
+ * @sqi:	Signal Quality Indicator.
+ * @mac_addr:	MAC address of the port.
+ * @phy_id:	PHY id used by the port.
+ */
+struct ksz_port_info {
+	phy_interface_t interface;
+	uint state;
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+	u8 link_down;
+	u16 link;
+	u32 advertised;
+	u32 partner;
+	u32 status[5];
+	u32 length[5];
+	u16 sqi;
+	u8 mac_addr[ETH_ALEN];
+	u8 own_flow_ctrl;
+	u8 own_duplex;
+	u16 own_speed;
+	u8 phy_id;
+	u32 report:1;
+	u32 phy:1;
+};
+
+struct ksz_sw;
+struct ksz_port;
+
+struct ksz_sw_reg_ops {
+	void (*lock)(struct ksz_sw *sw);
+	void (*unlock)(struct ksz_sw *sw);
+
+	u8 (*r8)(struct ksz_sw *sw, unsigned reg);
+	u16 (*r16)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r24)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r32)(struct ksz_sw *sw, unsigned reg);
+	void (*w8)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w16)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w24)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w32)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	void (*r)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+	void (*w)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+
+	int (*get)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+	int (*set)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+
+	int (*r_dyn_mac_hw)(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+		u16 src_fid, struct ksz_mac_table *mac, u16 *entry);
+	int (*w_dyn_mac_hw)(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+		u16 src_fid, struct ksz_mac_table *mac);
+	int (*start_dyn_mac_hw)(struct ksz_sw *sw);
+	int (*g_dyn_mac_hw)(struct ksz_sw *sw, struct ksz_mac_table *mac);
+	u32 (*stop_dyn_mac_hw)(struct ksz_sw *sw);
+	int (*r_sta_mac_hw)(struct ksz_sw *sw, u32 ctrl[], int num,
+		struct ksz_mac_table *mac);
+	int (*w_sta_mac_hw)(struct ksz_sw *sw, u32 ctrl[], int num,
+		struct ksz_mac_table *mac);
+	int (*r_vlan_hw)(struct ksz_sw *sw, u32 data[], int num);
+	int (*w_vlan_hw)(struct ksz_sw *sw, u32 data[], int num);
+	int (*r_mib_cnt_hw)(struct ksz_sw *sw, uint port, u32 addr[],
+		int num, u32 data[]);
+	int (*r_acl_hw)(struct ksz_sw *sw, uint port, u16 addr, u8 data[]);
+	int (*w_acl_hw)(struct ksz_sw *sw, uint port, u16 addr, u8 data[]);
+
+#ifdef CONFIG_KSZ_HSR
+	int (*r_hsr_hw)(struct ksz_sw *sw, u16 addr,
+		struct ksz_hsr_table *hsr);
+	int (*w_hsr_hw)(struct ksz_sw *sw, u16 addr,
+		struct ksz_hsr_table *hsr);
+	int (*start_hsr_hw)(struct ksz_sw *sw);
+	int (*g_hsr_hw)(struct ksz_sw *sw, struct ksz_hsr_table *hsr);
+	u32 (*stop_hsr_hw)(struct ksz_sw *sw);
+#endif
+};
+
+struct ksz_sw_net_ops {
+	void (*setup_special)(struct ksz_sw *sw, int *port_cnt,
+		int *mib_port_cnt, int *dev_cnt);
+	int (*setup_dev)(struct ksz_sw *sw, struct net_device *dev,
+		char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+		uint mib_port_cnt);
+	void (*leave_dev)(struct ksz_sw *sw);
+	u8 (*get_state)(struct net_device *dev);
+	void (*set_state)(struct net_device *dev, u8 state);
+	struct ksz_port *(*get_priv_port)(struct net_device *dev);
+	int (*get_ready)(struct net_device *dev);
+
+	void (*start)(struct ksz_sw *sw, u8 *addr);
+	int (*stop)(struct ksz_sw *sw, int complete);
+	int (*open_dev)(struct ksz_sw *sw, struct net_device *dev, u8 *addr);
+	void (*open_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port, u8 *state);
+	void (*close_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*open)(struct ksz_sw *sw);
+	void (*close)(struct ksz_sw *sw);
+
+	void (*netdev_start_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_stop_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_wake_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_open_port)(struct ksz_sw *sw, struct net_device *dev);
+
+	u8 (*set_mac_addr)(struct ksz_sw *sw, struct net_device *dev,
+		u8 promiscuous, uint port);
+
+	int (*get_mtu)(struct ksz_sw *sw);
+	int (*get_tx_len)(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+		int *header);
+	void (*add_tail_tag)(struct ksz_sw *sw, struct sk_buff *skb, uint ports);
+	int (*get_tail_tag)(u8 *trailer, int *port);
+	int (*get_phys_port)(struct ksz_sw *sw, uint port);
+	int (*get_virt_port)(struct ksz_sw *sw, uint port);
+	void (*add_vid)(struct ksz_sw *sw, u16 vid);
+	void (*kill_vid)(struct ksz_sw *sw, u16 vid);
+	struct sk_buff *(*check_tx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, struct ksz_port *priv);
+	struct net_device *(*rx_dev)(struct ksz_sw *sw, u8 *data, u32 *len,
+		int *tag, int *port);
+	int (*match_pkt)(struct ksz_sw *sw, struct net_device **dev,
+		void **priv, int (*get_promiscuous)(void *ptr),
+		int (*get_multi)(void *ptr), struct sk_buff *skb,
+		u8 h_promiscuous);
+	struct net_device *(*parent_rx)(struct ksz_sw *sw,
+		struct net_device *dev, struct sk_buff *skb, int *forward,
+		struct net_device **parent_dev, struct sk_buff **parent_skb);
+	int (*port_vlan_rx)(struct ksz_sw *sw, struct net_device *dev,
+		struct net_device *parent_dev, struct sk_buff *skb,
+		int forward, int tag, void *ptr,
+		void (*rx_tstamp)(void *ptr, struct sk_buff *skb));
+	int (*drop_icmp)(struct sk_buff *skb, int extra_skb);
+	struct sk_buff *(*final_skb)(struct ksz_sw *sw, struct sk_buff *skb,
+		struct net_device *dev, struct ksz_port *port);
+	int (*drv_rx)(struct ksz_sw *sw, struct sk_buff *skb, uint port);
+	void (*set_multi)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *priv);
+
+};
+
+struct ksz_sw_ops {
+	void (*init)(struct ksz_sw *sw);
+	void (*exit)(struct ksz_sw *sw);
+	int (*dev_req)(struct ksz_sw *sw, int start, char *arg,
+		struct sw_dev_info *info);
+
+	int (*get_first_port)(struct ksz_sw *sw);
+
+	void (*acquire)(struct ksz_sw *sw);
+	void (*release)(struct ksz_sw *sw);
+
+	int (*chk)(struct ksz_sw *sw, u32 addr, SW_D bits);
+	void (*cfg)(struct ksz_sw *sw, u32 addr, SW_D bits, bool set);
+
+	int (*port_get_link_speed)(struct ksz_port *port);
+	void (*port_set_link_speed)(struct ksz_port *port);
+	void (*port_force_link_speed)(struct ksz_port *port);
+
+	int (*port_r_cnt)(struct ksz_sw *sw, uint port);
+	void (*get_mib_counters)(struct ksz_sw *sw, int first, int cnt,
+		u64 *counter);
+
+	ssize_t (*sysfs_read)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, ssize_t len, char *buf);
+	ssize_t (*sysfs_read_hw)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_write)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, int num, const char *buf);
+	ssize_t (*sysfs_port_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	ssize_t (*sysfs_port_read_hw)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+	ssize_t (*sysfs_mac_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mac_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_vlan_read)(struct ksz_sw *sw, int proc_num,
+		ssize_t len, char *buf);
+	int (*sysfs_vlan_write)(struct ksz_sw *sw, int proc_num, int num);
+
+#ifdef CONFIG_KSZ_STP
+	ssize_t (*sysfs_stp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_stp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_stp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_stp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	ssize_t (*sysfs_mrp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mrp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_mrp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_mrp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	ssize_t (*sysfs_hsr_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_hsr_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+#endif
+
+	ssize_t (*sysfs_acl_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	int (*sysfs_acl_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+
+	void (*cfg_mac)(struct ksz_sw *sw, u8 index, const u8 *dest, u32 ports,
+		int override, int use_fid, u16 fid);
+	void (*cfg_vlan)(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+		u32 ports);
+	u8 (*alloc_mac)(struct ksz_sw *sw);
+	void (*free_mac)(struct ksz_sw *sw, u8 index);
+	u8 (*alloc_vlan)(struct ksz_sw *sw);
+	void (*free_vlan)(struct ksz_sw *sw, u8 index);
+	u16 (*alloc_fid)(struct ksz_sw *sw, u16 vid);
+	void (*free_fid)(struct ksz_sw *sw, u16 fid);
+
+	const u8 *(*get_br_id)(struct ksz_sw *sw);
+	void (*from_backup)(struct ksz_sw *sw, uint p);
+	void (*to_backup)(struct ksz_sw *sw, uint p);
+	void (*from_designated)(struct ksz_sw *sw, uint p, bool alt);
+	void (*to_designated)(struct ksz_sw *sw, uint p);
+	void (*tc_detected)(struct ksz_sw *sw, uint p);
+	int (*get_tcDetected)(struct ksz_sw *sw, uint p);
+
+	int (*get_id)(struct ksz_sw *sw, u8 *id1, u8 *id2, char *name);
+	void (*cfg_tail_tag)(struct ksz_sw *sw, bool enable);
+	void (*cfg_each_port)(struct ksz_sw *sw, uint p, bool cpu);
+	int (*port_to_phy_addr)(struct ksz_sw *sw, uint p);
+	void (*set_port_addr)(struct ksz_sw *sw, uint p, u8 *addr);
+
+	void (*cfg_src_filter)(struct ksz_sw *sw, bool set);
+	void (*flush_table)(struct ksz_sw *sw, uint port);
+	void (*fwd_unk_mcast)(struct ksz_sw *sw, bool set);
+	void (*fwd_unk_ucast)(struct ksz_sw *sw);
+	void (*fwd_unk_vid)(struct ksz_sw *sw);
+
+	void (*port_freeze_mib)(struct ksz_sw *sw, uint port, bool freeze);
+	void (*freeze_mib)(struct ksz_sw *sw, bool freeze);
+};
+
+struct ksz_sw_tx_tag {
+	u32 timestamp;
+	u16 ports;
+};
+
+struct ksz_sw_cached_regs {
+	u32 ptp_unit_index;
+	u16 ptp_clk_ctrl;
+	u16 xmii[2];
+};
+
+/* Switch features and bug fixes. */
+#define STP_SUPPORT			(1 << 0)
+#define VLAN_PORT			(1 << 1)
+#define VLAN_PORT_REMOVE_TAG		(1 << 2)
+#define VLAN_PORT_TAGGING		(1 << 3)
+#define VLAN_PORT_START			200
+#define SW_VLAN_DEV			(1 << 4)
+#define MRP_SUPPORT			(1 << 5)
+
+#define ACL_CORRUPT_BUG			(1 << 8)
+#define NO_GLOBAL_RESET			(1 << 9)
+#define PHY_INTR_BUG			(1 << 10)
+#define IS_9893				(1 << 15)
+#define SETUP_PHY			(1 << 16)
+#define NEW_XMII			(1 << 17)
+#define USE_FEWER_PORTS			(1 << 18)
+#define GIGABIT_SUPPORT			(1 << 19)
+#define IBA_SUPPORT			(1 << 20)
+#define NEW_CAP				(1 << 21)
+#define AVB_SUPPORT			(1 << 22)
+#define REDUNDANCY_SUPPORT		(1 << 23)
+#define DLR_HW				(1 << 24)
+#define HSR_HW				(1 << 25)
+#define HSR_REDBOX			(1 << 26)
+#define DSA_SUPPORT			(1 << 28)
+#define DIFF_MAC_ADDR			(1 << 29)
+#define QW_HW				(1 << 30)
+#define PTP_HW				(1 << 31)
+
+/* Software overrides. */
+#define PAUSE_FLOW_CTRL			(1 << 0)
+#define FAST_AGING			(1 << 1)
+#define MCAST_FILTER			(1 << 2)
+#define HAVE_MORE_THAN_2_PORTS		(1 << 3)
+#define DLR_FORWARD			(1 << 4)
+#define UNK_MCAST_BLOCK			(1 << 5)
+
+#define BAD_SPI				(1 << 15)
+#define IBA_TEST			(1 << 16)
+#define ACL_INTR_MONITOR		(1 << 17)
+
+#define TAIL_PRP_0			(1 << 24)
+#define TAIL_PRP_1			(1 << 25)
+
+#define USE_802_1X_AUTH			(1 << 27)
+#define VLAN_SET			(1 << 28)
+#define PTP_TAG				(1 << 29)
+#define TAG_REMOVE			(1 << 30)
+#define TAIL_TAGGING			(1 << 31)
+
+/**
+ * struct ksz_sw - Virtual switch data structure
+ * @dev:		Pointer to hardware device.
+ * @phydev:		Pointer to PHY device interface.
+ * @interface:		The R/G/MII interface used.
+ * @msg_enable:		The message flags controlling driver output.
+ * @hwlock:		Pointer to hardware lock.
+ * @reglock:		Pointer to register lock.
+ * @acllock:		ACL table lock.
+ * @alulock:		ALU table lock.
+ * @vlanlock:		VLAN table lock.
+ * @hsrlock:		HSR table lock.
+ * @lock		Software lock to switch structure.
+ * @locked:		locked status.
+ * @info:		Pointer to switch information structure.
+ * @port_info:		Port information.
+ * @netdev:		Pointer to OS dependent network devices.
+ * @phy:		Pointer to OS dependent PHY devices.
+ * @dev_offset:		Indication of a switch associated network device.
+ * @phy_offset:		Indication of a port associated PHY device.
+ * @port_state:		Port state information.
+ * @port_mib:		Port MIB information.
+ * @mib_cnt:		Number of MIB counters this switch has.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @phy_port_cnt:	Number of ports with actual PHY.
+ * @port_cnt:		Number of ports to support.
+ * @monitor_timer_info:	Timer information for monitoring ports.
+ * @counter:		Pointer to OS dependent MIB counter information.
+ * @link_read:		Workqueue for link monitoring.
+ * @ops:		Switch function access.
+ * @reg:		Switch register access.
+ * @net_ops:		Network related switch function access.
+ * @HOST_PORT:		A predefined value indicating the host port.
+ * @HOST_MASK:		A predefined value indicating the host port mask.
+ * @PORT_MASK:		A predefined value indicating the port mask.
+ * @TAIL_TAG_LOOKUP:	A predefined value indicating tx tail tag lookup.
+ * @TAIL_TAG_OVERRIDE:	A predefined value indicating tx tail tag override.
+ * @live_ports:		Bitmap of ports with link enabled.
+ * @on_ports:		Bitmap of ports with 802.1X enabled.
+ * @rx_ports:		Bitmap of ports with receive enabled.
+ * @tx_ports:		Bitmap of ports with transmit enabled.
+ * @dev_count:		Number of network devices this switch supports.
+ * @id:			Hardware ID.  Used for display only.
+ * @vlan_id		Used for the VLAN port forwarding feature.
+ * @vid:		Used for the VLAN port forwarding feature.
+ * @revision:		Hardware revision number.
+ * @features:		Switch features to enable.
+ * @overrides:		Switch features to override.
+ * @multi_dev:		Used to specify multiple devices mode.
+ * @stp:		Used to enable STP.
+ * @fast_aging:		Used to enable fast aging.
+ */
+struct ksz_sw {
+	void *dev;
+	void *phydev;
+	phy_interface_t interface;
+	u32 msg_enable;
+	wait_queue_head_t queue;
+	struct sk_buff_head txq;
+	struct mutex *hwlock;
+	struct mutex *reglock;
+	struct mutex acllock;
+	struct mutex alulock;
+	struct mutex vlanlock;
+	struct mutex hsrlock;
+	struct mutex lock;
+	int intr_cnt;
+	int intr_using;
+
+	struct ksz_sw_info *info;
+	struct ksz_port_info port_info[SWITCH_PORT_NUM];
+	struct net_device *main_dev;
+	struct net_device *netdev[TOTAL_PORT_NUM];
+	struct phy_device phy_map[TOTAL_PORT_NUM + 1];
+	struct phy_device *phy[TOTAL_PORT_NUM + 1];
+	int dev_offset;
+	int phy_offset;
+	struct ksz_port_state port_state[TOTAL_PORT_NUM];
+	struct ksz_port_mib port_mib[TOTAL_PORT_NUM];
+	u8 mib_interval_start[4];
+	unsigned long next_jiffies;
+	int mib_cnt;
+	int mib_port_cnt;
+	int phy_port_cnt;
+	int dsa_port_cnt;
+	int port_cnt;
+	int last_port;
+	struct ksz_timer_info *monitor_timer_info;
+	struct ksz_counter_info *counter;
+	struct delayed_work *link_read;
+
+	const struct ksz_sw_ops *ops;
+	const struct ksz_sw_reg_ops *reg;
+	const struct ksz_sw_reg_ops *cur;
+	struct ksz_sw_net_ops *net_ops;
+	struct delayed_work set_ops;
+	struct delayed_work set_mrp;
+	struct work_struct set_addr;
+	struct work_struct tx_fwd;
+
+	int HOST_PORT;
+	u16 HOST_MASK;
+	u16 PORT_MASK;
+	u16 TAIL_TAG_LOOKUP;
+	u16 TAIL_TAG_OVERRIDE;
+	u32 intr_mask;
+	u32 port_intr_mask;
+	u32 phy_intr;
+	u16 dev_ports;
+	u16 link_ports;
+	u16 live_ports;
+	u16 on_ports;
+	u16 open_ports;
+	u16 rx_ports[NUM_OF_MSTI];
+	u16 tx_ports[NUM_OF_MSTI];
+	u8 tx_pad[60];
+	int tx_start;
+	struct ksz_sw_tx_tag tag;
+	struct ksz_sw_cached_regs cached;
+
+	int dev_major;
+	u8 *msg_buf;
+	struct sw_dev_info *dev_list[2];
+	struct sw_dev_info *dev_info;
+	uint notifications;
+	char dev_name[20];
+
+	int dev_count;
+	int id;
+	u32 vlan_id;
+	u16 vid;
+	u16 alu_index;
+	u8 alu_type;
+	u8 alu_dirty;
+	u16 vlan_index;
+	u16 hsr_index;
+	u8 hsr_dirty;
+	u8 vlan_dirty;
+	u8 verbose;
+	u8 running;
+
+	int revision;
+	uint features;
+	uint overrides;
+
+	struct napi_struct *napi;
+
+	int multi_dev;
+	int stp;
+	int fast_aging;
+	struct {
+		u16 cnt;
+		u16 mask;
+		u16 port;
+		u16 phy_id;
+		u16 vlan;
+		uint proto;
+	} eth_maps[SWITCH_PORT_NUM];
+	int eth_cnt;
+
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_info mrp;
+#endif
+
+#ifdef CONFIG_1588_PTP
+	/* PTP structure size can be variable. */
+	struct ptp_info ptp_hw;
+#endif
+};
+
+struct ksz_sw_sysfs {
+	struct ksz_dev_attr *ksz_port_attrs[TOTAL_PORT_NUM];
+	struct attribute **port_attrs[TOTAL_PORT_NUM];
+};
+
+/**
+ * struct ksz_port - Virtual port data structure
+ * @first_port:		Index of first port this port supports.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports this port supports.
+ * @flow_ctrl:		Flow control setting.  PHY_NO_FLOW_CTRL for no flow
+ *			control, and PHY_FLOW_CTRL for flow control.
+ *			PHY_TX_ONLY and PHY_RX_ONLY are not supported for 100
+ *			Mbit PHY.
+ * @duplex:		Duplex mode setting.  1 for half duplex, 2 for full
+ *			duplex, and 0 for auto, which normally results in full
+ *			duplex.
+ * @speed:		Speed setting.  10 for 10 Mbit, 100 for 100 Mbit, and
+ *			0 for auto, which normally results in 100 Mbit.
+ * @force_link:		Force link setting.  0 for auto-negotiation, and 1 for
+ *			force.
+ * @linked:		Pointer to port information linked to this port.
+ * @sw:			Pointer to virtual switch structure.
+ */
+struct ksz_port {
+	int first_port;
+	int mib_port_cnt;
+	int port_cnt;
+
+	u8 flow_ctrl;
+	u8 duplex;
+	u16 speed;
+	u8 force_link;
+
+	struct ksz_port_info *linked;
+
+	struct ksz_sw *sw;
+	struct work_struct link_update;
+};
+
+struct lan_attributes {
+	int info;
+	int version;
+	int duplex;
+	int speed;
+	int force;
+	int flow_ctrl;
+	int features;
+	int overrides;
+	int mib;
+	int reg;
+	int vid;
+	int dynamic_table;
+	int static_table;
+	int vlan_table;
+	int hsr_table;
+	int aging;
+	int fast_aging;
+	int link_aging;
+	int bcast_per;
+	int mcast_storm;
+	int tx_queue_based;
+	int diffserv_map;
+	int p_802_1p_map;
+	int vlan;
+	int null_vid;
+	int drop_inv_vid;
+	int macaddr;
+	int mirror_mode;
+	int igmp_snoop;
+	int ipv6_mld_snoop;
+	int ipv6_mld_option;
+	int aggr_backoff;
+	int no_exc_drop;
+	int jumbo_packet;
+	int legal_packet;
+	int length_check;
+	int back_pressure;
+	int sw_flow_ctrl;
+	int sw_half_duplex;
+	int sw_10_mbit;
+	int fair_flow_ctrl;
+	int vlan_bound;
+	int double_tag;
+	int isp;
+	int hsr;
+	int hsr_redbox_id;
+	int hsr_net_id;
+	int mtu;
+	int unk_ucast_fwd;
+	int unk_ucast_ports;
+	int unk_mcast_fwd;
+	int unk_mcast_ports;
+	int unk_vid_fwd;
+	int unk_vid_ports;
+	int pass_pause;
+	int pme;
+	int pme_polarity;
+
+	int host_port;
+	int ports;
+	int dev_start;
+	int vlan_start;
+	int avb;
+	int stp;
+	int two_dev;
+	int authen;
+
+	int alu_fid;
+	int alu_use_fid;
+	int alu_override;
+	int alu_valid;
+	int alu_mstp;
+	int alu_prio;
+	int alu_src;
+	int alu_dst;
+	int alu_ports;
+	int alu_addr;
+	int alu_type;
+	int alu_index;
+	int alu_info;
+
+	int vlan_valid;
+	int vlan_ports;
+	int vlan_untag;
+	int vlan_fid;
+	int vlan_mstp;
+	int vlan_prio;
+	int vlan_option;
+	int vlan_index;
+	int vlan_info;
+	int vid2fid;
+	int fid2mstid;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_br_info;
+	int stp_br_on;
+	int stp_br_prio;
+	int stp_br_fwd_delay;
+	int stp_br_max_age;
+	int stp_br_hello_time;
+	int stp_br_tx_hold;
+	int stp_version;
+#ifdef CONFIG_KSZ_MSTP
+	int stp_br_max_hops;
+	int stp_msti;
+	int stp_msti_vid;
+	int stp_mstp_cfg;
+	int stp_mstp_name;
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	int mrp_src_addr;
+
+#ifdef CONFIG_KSZ_MSRP
+	int msrp_info;
+	int msrpEnabled;
+	int msrp_sr_a;
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	int hsr_valid;
+	int hsr_age_cnt;
+	int hsr_path_id;
+	int hsr_addr;
+	int hsr_index;
+	int hsr_info;
+	int hsr_state;
+#endif
+
+	int no_color;
+	int color_red;
+	int color_yellow;
+	int color_green;
+};
+
+struct sw_attributes {
+	int mib;
+	int vid;
+	int member;
+	int bcast_storm;
+	int diffserv;
+	int p_802_1p;
+	int prio_vlan;
+	int prio_mac;
+	int prio_acl;
+	int prio_highest;
+	int prio_or;
+	int port_prio;
+	int non_vid;
+	int ingress;
+	int drop_non_vlan;
+	int drop_tagged;
+	int replace_vid;
+	int replace_prio;
+	int mac_802_1x;
+	int src_addr_filter;
+	int vlan_lookup_0;
+	int mstp;
+	int rx;
+	int tx;
+	int learn;
+	int power;
+	int prio_queue;
+	int rx_prio_rate;
+	int tx_prio_rate;
+	int limit;
+	int limit_port_based;
+	int limit_packet_based;
+	int limit_flow_ctrl;
+	int limit_cnt_ifg;
+	int limit_cnt_pre;
+	int rx_p0_rate;
+	int rx_p1_rate;
+	int rx_p2_rate;
+	int rx_p3_rate;
+	int rx_p4_rate;
+	int rx_p5_rate;
+	int rx_p6_rate;
+	int rx_p7_rate;
+	int tx_q0_rate;
+	int tx_q1_rate;
+	int tx_q2_rate;
+	int tx_q3_rate;
+	int color_map;
+	int tc_map;
+	int mirror_port;
+	int mirror_rx;
+	int mirror_tx;
+	int back_pressure;
+	int force_flow_ctrl;
+	int pass_all;
+	int tail_tag;
+
+	int cust_vid;
+	int sr_1_vid;
+	int sr_2_vid;
+	int sr_1_type;
+	int sr_2_type;
+
+	int pme_ctrl;
+	int pme_status;
+
+	int authen_mode;
+	int acl;
+	int acl_first_rule;
+	int acl_ruleset;
+	int acl_mode;
+	int acl_enable;
+	int acl_src;
+	int acl_equal;
+	int acl_addr;
+	int acl_type;
+	int acl_cnt;
+	int acl_msec;
+	int acl_intr_mode;
+	int acl_ip_addr;
+	int acl_ip_mask;
+	int acl_protocol;
+	int acl_seqnum;
+	int acl_port_mode;
+	int acl_max_port;
+	int acl_min_port;
+	int acl_tcp_flag_enable;
+	int acl_tcp_flag;
+	int acl_tcp_flag_mask;
+	int acl_prio_mode;
+	int acl_prio;
+	int acl_vlan_prio_replace;
+	int acl_vlan_prio;
+	int acl_map_mode;
+	int acl_ports;
+	int acl_index;
+	int acl_act_index;
+	int acl_act;
+	int acl_rule_index;
+	int acl_info;
+	int acl_table;
+
+	int p_index;
+	int q_index;
+	int police_type;
+	int non_dscp_color;
+	int police_drop_all;
+	int police_port_based;
+	int color_mark;
+	int color_remap;
+	int drop_srp;
+	int color_aware;
+	int police;
+
+	int q_cir;
+	int q_pir;
+	int q_cbs;
+	int q_pbs;
+
+	int wred_max;
+	int wred_min;
+	int wred_multiplier;
+	int wred_avg_size;
+	int wred_q_max;
+	int wred_q_min;
+	int wred_q_multiplier;
+	int wred_q_avg_size;
+	int wred_random_drop;
+	int wred_drop_gyr;
+	int wred_drop_yr;
+	int wred_drop_r;
+	int wred_drop_all;
+	int wred_q_pmon;
+
+	int q_scheduling;
+	int q_shaping;
+#ifdef MTI_PREEMPT_ENABLE
+	int preempt;
+#endif
+	int q_tx_ratio;
+	int q_credit_hi;
+	int q_credit_lo;
+	int q_credit_incr;
+	int srp;
+
+	int qm_drop;
+	int qm_burst;
+	int qm_resv_space;
+	int qm_hi;
+	int qm_lo;
+	int qm_tx_used;
+	int qm_tx_avail;
+	int qm_tx_calc;
+
+	int mmd_id;
+	int mmd_reg;
+	int mmd_val;
+
+	int rx_flow_ctrl;
+	int tx_flow_ctrl;
+
+	int duplex;
+	int speed;
+	int mac_oper;
+	int vlan_restricted;
+	int vlan_untagged;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_info;
+	int stp_on;
+	int stp_prio;
+	int stp_admin_path_cost;
+	int stp_path_cost;
+	int stp_admin_edge;
+	int stp_auto_edge;
+	int stp_mcheck;
+	int stp_admin_p2p;
+	int stp_auto_isolate;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	int mmrpEnabled;
+	int mmrp_mac;
+	int mmrp_svc;
+	int mmrp_reg;
+	int mvrpEnabled;
+	int mvrp_vid;
+	int mvrp_reg;
+
+#ifdef CONFIG_KSZ_MSRP
+	int asCapable;
+	int msrpEnabled;
+	int q_delta;
+	int q_admin_slope;
+	int q_oper_slope;
+	int q_alg;
+	int sr_0_rx_prio;
+	int sr_0_tx_prio;
+	int sr_0_boundary;
+	int sr_1_rx_prio;
+	int sr_1_tx_prio;
+	int sr_1_boundary;
+#endif
+#endif
+
+	int linkmd;
+	int sqi;
+	int mac_loopback;
+	int phy_loopback;
+	int remote_loopback;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_sw_api.h b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_api.h
new file mode 100644
index 0000000..04023b0
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_api.h
@@ -0,0 +1,74 @@
+/**
+ * Microchip switch driver API header
+ *
+ * Copyright (c) 2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SW_API_H
+#define KSZ_SW_API_H
+
+
+enum {
+	DEV_INFO_SW_LINK = DEV_INFO_LAST,
+	DEV_INFO_SW_ACL,
+};
+
+enum {
+	DEV_SW_CFG,
+};
+
+
+#define SW_INFO_LINK_CHANGE		(1 << 0)
+#define SW_INFO_ACL_INTR		(1 << 1)
+
+
+#define SP_RX				(1 << 0)
+#define SP_TX				(1 << 1)
+#define SP_LEARN			(1 << 2)
+#define SP_MIRROR_RX			(1 << 3)
+#define SP_MIRROR_TX			(1 << 4)
+#define SP_MIRROR_SNIFFER		(1 << 5)
+#define SP_PHY_POWER			(1 << 6)
+
+#define SP_BCAST_STORM			(1 << 16)
+#define SP_DIFFSERV			(1 << 17)
+#define SP_802_1P			(1 << 18)
+
+struct ksz_info_cfg {
+	uint set;
+	uint on_off;
+} __packed;
+
+#ifndef TX_RATE_UNIT
+#define TX_RATE_UNIT			10000
+#endif
+
+struct ksz_info_speed {
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+} __packed;
+
+union ksz_info_data {
+	struct ksz_info_cfg cfg;
+	struct ksz_info_speed speed;
+} __packed;
+
+struct ksz_info_opt {
+	u8 num;
+	u8 port;
+	union ksz_info_data data;
+} __packed;
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_sw_phy.h b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_phy.h
new file mode 100644
index 0000000..4fe238c
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_phy.h
@@ -0,0 +1,66 @@
+/**
+ * Microchip switch PHY common header
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_PHY_H
+#define KSZ_PHY_H
+
+#define ADDR_SHIFT			14
+#define ADDR_8				1
+#define ADDR_16				2
+#define ADDR_24				3
+#define ADDR_32				4
+
+#define BANK_SHIFT			12
+
+#define PHY_REG(addr, reg)		\
+	(((addr) << ADDR_SHIFT) | (reg))
+
+#define PHY_BANK_REG(addr, bank, reg)	\
+	(((addr) << ADDR_SHIFT) | ((bank) << BANK_SHIFT) | (reg))
+
+/* Use PHY access if no direct access. */
+#ifndef SW_R8
+#define SW_R8(s, r)	phy_read(s->phydev, PHY_REG(ADDR_8, r))
+#define SW_W8(s, r, v)	phy_write(s->phydev, PHY_REG(ADDR_8, r), v)
+#define SW_R16(s, r)	phy_read(s->phydev, PHY_REG(ADDR_16, r))
+#define SW_W16(s, r, v)	phy_write(s->phydev, PHY_REG(ADDR_16, r), v)
+#define SW_R32(s, r)	phy_read(s->phydev, PHY_REG(ADDR_32, r))
+#define SW_W32(s, r, v) \
+	do { \
+		phy_write(s->phydev, PHY_REG(ADDR_32, (r) + 2), (v) >> 16); \
+		phy_write(s->phydev, PHY_REG(ADDR_32, r), v); \
+	} while (0)
+#define SW_LOCK(s)				\
+	do {					\
+		mutex_lock(s->hwlock);		\
+	} while (0)
+#define SW_UNLOCK(s)				\
+	do {					\
+		mutex_unlock(s->hwlock);	\
+	} while (0)
+#endif
+
+struct phy_priv {
+	int ptp_irq;
+	int ptp_using;
+	struct ksz_port *port;
+	enum phy_state state;
+};
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_sw_sysfs_9897.c b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_sysfs_9897.c
new file mode 100644
index 0000000..88abfb2
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_sw_sysfs_9897.c
@@ -0,0 +1,1076 @@
+/**
+ * Microchip gigabit switch common sysfs code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2011-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_sysfs.h"
+
+
+static char *sw_name[] = {
+	"sw0",
+	"sw1",
+	"sw2",
+	"sw3",
+	"sw4",
+	"sw5",
+	"sw6",
+	"sw7",
+};
+
+static ssize_t netlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_port *uninitialized_var(port);
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = sw->ops->sysfs_read(sw, proc_num, port, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_mac_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_vlan_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	len = sw->ops->sysfs_mrp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	len = sw->ops->sysfs_hsr_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_read_hw(sw, proc_num, len, buf);
+	sw->ops->release(sw);
+
+netlan_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_port *uninitialized_var(port);
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_mac_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+
+	if (sw->ops->sysfs_vlan_write(sw, proc_num, num))
+		goto netlan_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->ops->sysfs_mrp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->ops->sysfs_hsr_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netlan_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netsw_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	uint port;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+	len = sw->ops->sysfs_acl_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	len = sw->ops->sysfs_mrp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_port_read_hw(sw, num, port, len, buf);
+	sw->ops->release(sw);
+
+netsw_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netsw_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	uint port;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_acl_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->ops->sysfs_mrp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_port_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netsw_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+#define LAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute lan_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETLAN_RD_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO, show_lan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETLAN_WR_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_lan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netlan_store(d, attr, buf, count,			\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO | S_IWUSR, show_lan_##name, store_lan_##name)
+
+#define SW_ATTR(_name, _mode, _show, _store) \
+struct device_attribute sw_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETSW_RD_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO, show_sw_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETSW_WR_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static ssize_t store_sw_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netsw_store(d, attr, buf, count,				\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO | S_IWUSR, show_sw_##name, store_sw_##name)
+
+NETLAN_WR_ENTRY(info);
+NETLAN_WR_ENTRY(version);
+NETLAN_WR_ENTRY(duplex);
+NETLAN_WR_ENTRY(speed);
+NETLAN_WR_ENTRY(force);
+NETLAN_WR_ENTRY(flow_ctrl);
+NETLAN_WR_ENTRY(mib);
+NETLAN_WR_ENTRY(reg);
+NETLAN_WR_ENTRY(vid);
+NETLAN_WR_ENTRY(features);
+NETLAN_WR_ENTRY(overrides);
+
+NETLAN_WR_ENTRY(dynamic_table);
+NETLAN_WR_ENTRY(static_table);
+NETLAN_RD_ENTRY(vlan_table);
+NETLAN_RD_ENTRY(hsr_table);
+NETLAN_WR_ENTRY(aging);
+NETLAN_WR_ENTRY(fast_aging);
+NETLAN_WR_ENTRY(link_aging);
+NETLAN_WR_ENTRY(bcast_per);
+NETLAN_WR_ENTRY(mcast_storm);
+NETLAN_WR_ENTRY(tx_queue_based);
+NETLAN_WR_ENTRY(diffserv_map);
+NETLAN_WR_ENTRY(p_802_1p_map);
+NETLAN_WR_ENTRY(vlan);
+NETLAN_WR_ENTRY(null_vid);
+NETLAN_WR_ENTRY(drop_inv_vid);
+NETLAN_WR_ENTRY(macaddr);
+NETLAN_WR_ENTRY(mirror_mode);
+NETLAN_WR_ENTRY(igmp_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_option);
+NETLAN_WR_ENTRY(aggr_backoff);
+NETLAN_WR_ENTRY(no_exc_drop);
+NETLAN_WR_ENTRY(jumbo_packet);
+NETLAN_WR_ENTRY(legal_packet);
+NETLAN_WR_ENTRY(length_check);
+NETLAN_WR_ENTRY(back_pressure);
+NETLAN_WR_ENTRY(sw_flow_ctrl);
+NETLAN_WR_ENTRY(sw_half_duplex);
+#ifdef SWITCH_10_MBIT
+NETLAN_WR_ENTRY(sw_10_mbit);
+#endif
+NETLAN_WR_ENTRY(fair_flow_ctrl);
+NETLAN_WR_ENTRY(vlan_bound);
+NETLAN_WR_ENTRY(double_tag);
+NETLAN_WR_ENTRY(isp);
+NETLAN_WR_ENTRY(hsr);
+NETLAN_WR_ENTRY(hsr_redbox_id);
+NETLAN_WR_ENTRY(hsr_net_id);
+NETLAN_WR_ENTRY(mtu);
+NETLAN_WR_ENTRY(unk_ucast_fwd);
+NETLAN_WR_ENTRY(unk_ucast_ports);
+NETLAN_WR_ENTRY(unk_mcast_fwd);
+NETLAN_WR_ENTRY(unk_mcast_ports);
+NETLAN_WR_ENTRY(unk_vid_fwd);
+NETLAN_WR_ENTRY(unk_vid_ports);
+NETLAN_WR_ENTRY(pass_pause);
+NETLAN_WR_ENTRY(pme);
+NETLAN_WR_ENTRY(pme_polarity);
+
+NETLAN_RD_ENTRY(host_port);
+NETLAN_RD_ENTRY(ports);
+NETLAN_RD_ENTRY(dev_start);
+NETLAN_RD_ENTRY(vlan_start);
+NETLAN_RD_ENTRY(avb);
+NETLAN_RD_ENTRY(stp);
+NETLAN_RD_ENTRY(two_dev);
+NETLAN_WR_ENTRY(authen);
+
+NETLAN_WR_ENTRY(alu_fid);
+NETLAN_WR_ENTRY(alu_use_fid);
+NETLAN_WR_ENTRY(alu_override);
+NETLAN_WR_ENTRY(alu_valid);
+NETLAN_WR_ENTRY(alu_mstp);
+NETLAN_WR_ENTRY(alu_prio);
+NETLAN_WR_ENTRY(alu_src);
+NETLAN_WR_ENTRY(alu_dst);
+NETLAN_WR_ENTRY(alu_ports);
+NETLAN_WR_ENTRY(alu_addr);
+NETLAN_WR_ENTRY(alu_type);
+NETLAN_WR_ENTRY(alu_index);
+NETLAN_WR_ENTRY(alu_info);
+
+NETLAN_WR_ENTRY(vlan_valid);
+NETLAN_WR_ENTRY(vlan_ports);
+NETLAN_WR_ENTRY(vlan_untag);
+NETLAN_WR_ENTRY(vlan_fid);
+NETLAN_WR_ENTRY(vlan_mstp);
+NETLAN_WR_ENTRY(vlan_prio);
+NETLAN_WR_ENTRY(vlan_option);
+NETLAN_WR_ENTRY(vlan_index);
+NETLAN_WR_ENTRY(vlan_info);
+NETLAN_WR_ENTRY(vid2fid);
+NETLAN_WR_ENTRY(fid2mstid);
+
+#ifdef CONFIG_KSZ_STP
+NETLAN_RD_ENTRY(stp_br_info);
+NETLAN_WR_ENTRY(stp_br_on);
+NETLAN_WR_ENTRY(stp_br_prio);
+NETLAN_WR_ENTRY(stp_br_fwd_delay);
+NETLAN_WR_ENTRY(stp_br_hello_time);
+NETLAN_WR_ENTRY(stp_br_max_age);
+NETLAN_WR_ENTRY(stp_br_tx_hold);
+NETLAN_WR_ENTRY(stp_version);
+#ifdef CONFIG_KSZ_MSTP
+NETLAN_WR_ENTRY(stp_br_max_hops);
+NETLAN_WR_ENTRY(stp_msti);
+NETLAN_RD_ENTRY(stp_msti_vid);
+NETLAN_RD_ENTRY(stp_mstp_cfg);
+NETLAN_WR_ENTRY(stp_mstp_name);
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+NETLAN_WR_ENTRY(mrp_src_addr);
+
+#ifdef CONFIG_KSZ_MSRP
+NETLAN_WR_ENTRY(msrp_info);
+NETLAN_WR_ENTRY(msrpEnabled);
+NETLAN_WR_ENTRY(msrp_sr_a);
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+NETLAN_WR_ENTRY(hsr_valid);
+NETLAN_WR_ENTRY(hsr_age_cnt);
+NETLAN_WR_ENTRY(hsr_path_id);
+NETLAN_WR_ENTRY(hsr_addr);
+NETLAN_WR_ENTRY(hsr_index);
+NETLAN_WR_ENTRY(hsr_info);
+NETLAN_RD_ENTRY(hsr_state);
+#endif
+
+NETLAN_WR_ENTRY(no_color);
+NETLAN_WR_ENTRY(color_red);
+NETLAN_WR_ENTRY(color_yellow);
+NETLAN_WR_ENTRY(color_green);
+
+NETSW_WR_ENTRY(mib);
+NETSW_WR_ENTRY(vid);
+NETSW_WR_ENTRY(member);
+NETSW_WR_ENTRY(bcast_storm);
+NETSW_WR_ENTRY(mstp);
+NETSW_WR_ENTRY(rx);
+NETSW_WR_ENTRY(tx);
+NETSW_WR_ENTRY(learn);
+NETSW_WR_ENTRY(power);
+NETSW_WR_ENTRY(mirror_port);
+NETSW_WR_ENTRY(mirror_rx);
+NETSW_WR_ENTRY(mirror_tx);
+NETSW_WR_ENTRY(diffserv);
+NETSW_WR_ENTRY(p_802_1p);
+NETSW_WR_ENTRY(prio_vlan);
+NETSW_WR_ENTRY(prio_mac);
+NETSW_WR_ENTRY(prio_acl);
+NETSW_WR_ENTRY(prio_highest);
+NETSW_WR_ENTRY(prio_or);
+NETSW_WR_ENTRY(port_prio);
+NETSW_WR_ENTRY(non_vid);
+NETSW_WR_ENTRY(drop_non_vlan);
+NETSW_WR_ENTRY(drop_tagged);
+NETSW_WR_ENTRY(ingress);
+NETSW_WR_ENTRY(replace_vid);
+NETSW_WR_ENTRY(replace_prio);
+NETSW_WR_ENTRY(mac_802_1x);
+NETSW_WR_ENTRY(src_addr_filter);
+NETSW_WR_ENTRY(vlan_lookup_0);
+NETSW_WR_ENTRY(prio_queue);
+NETSW_WR_ENTRY(rx_prio_rate);
+NETSW_WR_ENTRY(tx_prio_rate);
+NETSW_WR_ENTRY(limit);
+NETSW_WR_ENTRY(limit_port_based);
+NETSW_WR_ENTRY(limit_packet_based);
+NETSW_WR_ENTRY(limit_flow_ctrl);
+NETSW_WR_ENTRY(limit_cnt_ifg);
+NETSW_WR_ENTRY(limit_cnt_pre);
+NETSW_WR_ENTRY(rx_p0_rate);
+NETSW_WR_ENTRY(rx_p1_rate);
+NETSW_WR_ENTRY(rx_p2_rate);
+NETSW_WR_ENTRY(rx_p3_rate);
+NETSW_WR_ENTRY(rx_p4_rate);
+NETSW_WR_ENTRY(rx_p5_rate);
+NETSW_WR_ENTRY(rx_p6_rate);
+NETSW_WR_ENTRY(rx_p7_rate);
+NETSW_WR_ENTRY(tx_q0_rate);
+NETSW_WR_ENTRY(tx_q1_rate);
+NETSW_WR_ENTRY(tx_q2_rate);
+NETSW_WR_ENTRY(tx_q3_rate);
+NETSW_WR_ENTRY(color_map);
+NETSW_WR_ENTRY(tc_map);
+NETSW_WR_ENTRY(back_pressure);
+NETSW_WR_ENTRY(force_flow_ctrl);
+NETSW_WR_ENTRY(pass_all);
+NETSW_WR_ENTRY(tail_tag);
+
+NETSW_WR_ENTRY(cust_vid);
+NETSW_WR_ENTRY(sr_1_vid);
+NETSW_WR_ENTRY(sr_2_vid);
+NETSW_WR_ENTRY(sr_1_type);
+NETSW_WR_ENTRY(sr_2_type);
+
+NETSW_WR_ENTRY(pme_ctrl);
+NETSW_WR_ENTRY(pme_status);
+
+NETSW_WR_ENTRY(authen_mode);
+NETSW_WR_ENTRY(acl);
+NETSW_WR_ENTRY(acl_first_rule);
+NETSW_WR_ENTRY(acl_ruleset);
+NETSW_WR_ENTRY(acl_mode);
+NETSW_WR_ENTRY(acl_enable);
+NETSW_WR_ENTRY(acl_src);
+NETSW_WR_ENTRY(acl_equal);
+NETSW_WR_ENTRY(acl_addr);
+NETSW_WR_ENTRY(acl_type);
+NETSW_WR_ENTRY(acl_cnt);
+NETSW_WR_ENTRY(acl_msec);
+NETSW_WR_ENTRY(acl_intr_mode);
+NETSW_WR_ENTRY(acl_ip_addr);
+NETSW_WR_ENTRY(acl_ip_mask);
+NETSW_WR_ENTRY(acl_protocol);
+NETSW_WR_ENTRY(acl_seqnum);
+NETSW_WR_ENTRY(acl_port_mode);
+NETSW_WR_ENTRY(acl_max_port);
+NETSW_WR_ENTRY(acl_min_port);
+NETSW_WR_ENTRY(acl_tcp_flag_enable);
+NETSW_WR_ENTRY(acl_tcp_flag);
+NETSW_WR_ENTRY(acl_tcp_flag_mask);
+NETSW_WR_ENTRY(acl_prio_mode);
+NETSW_WR_ENTRY(acl_prio);
+NETSW_WR_ENTRY(acl_vlan_prio_replace);
+NETSW_WR_ENTRY(acl_vlan_prio);
+NETSW_WR_ENTRY(acl_map_mode);
+NETSW_WR_ENTRY(acl_ports);
+NETSW_WR_ENTRY(acl_index);
+NETSW_WR_ENTRY(acl_act_index);
+NETSW_WR_ENTRY(acl_act);
+NETSW_WR_ENTRY(acl_rule_index);
+NETSW_WR_ENTRY(acl_info);
+NETSW_RD_ENTRY(acl_table);
+
+NETSW_WR_ENTRY(p_index);
+NETSW_WR_ENTRY(q_index);
+NETSW_WR_ENTRY(police_type);
+NETSW_WR_ENTRY(non_dscp_color);
+NETSW_WR_ENTRY(police_drop_all);
+NETSW_WR_ENTRY(police_port_based);
+NETSW_WR_ENTRY(color_mark);
+NETSW_WR_ENTRY(color_remap);
+NETSW_WR_ENTRY(drop_srp);
+NETSW_WR_ENTRY(color_aware);
+NETSW_WR_ENTRY(police);
+NETSW_WR_ENTRY(q_cir);
+NETSW_WR_ENTRY(q_pir);
+NETSW_WR_ENTRY(q_cbs);
+NETSW_WR_ENTRY(q_pbs);
+NETSW_WR_ENTRY(wred_max);
+NETSW_WR_ENTRY(wred_min);
+NETSW_WR_ENTRY(wred_multiplier);
+NETSW_RD_ENTRY(wred_avg_size);
+NETSW_WR_ENTRY(wred_q_max);
+NETSW_WR_ENTRY(wred_q_min);
+NETSW_WR_ENTRY(wred_q_multiplier);
+NETSW_RD_ENTRY(wred_q_avg_size);
+NETSW_WR_ENTRY(wred_random_drop);
+NETSW_WR_ENTRY(wred_drop_gyr);
+NETSW_WR_ENTRY(wred_drop_yr);
+NETSW_WR_ENTRY(wred_drop_r);
+NETSW_WR_ENTRY(wred_drop_all);
+NETSW_RD_ENTRY(wred_q_pmon);
+
+NETSW_WR_ENTRY(q_scheduling);
+NETSW_WR_ENTRY(q_shaping);
+#ifdef MTI_PREEMPT_ENABLE
+NETSW_WR_ENTRY(preempt);
+#endif
+NETSW_WR_ENTRY(q_tx_ratio);
+NETSW_WR_ENTRY(q_credit_hi);
+NETSW_WR_ENTRY(q_credit_lo);
+NETSW_WR_ENTRY(q_credit_incr);
+NETSW_WR_ENTRY(srp);
+
+NETSW_WR_ENTRY(qm_drop);
+NETSW_WR_ENTRY(qm_burst);
+NETSW_WR_ENTRY(qm_resv_space);
+NETSW_WR_ENTRY(qm_hi);
+NETSW_WR_ENTRY(qm_lo);
+NETSW_RD_ENTRY(qm_tx_used);
+NETSW_RD_ENTRY(qm_tx_avail);
+NETSW_RD_ENTRY(qm_tx_calc);
+
+NETSW_WR_ENTRY(mmd_id);
+NETSW_WR_ENTRY(mmd_reg);
+NETSW_WR_ENTRY(mmd_val);
+
+NETSW_RD_ENTRY(rx_flow_ctrl);
+NETSW_RD_ENTRY(tx_flow_ctrl);
+
+NETSW_WR_ENTRY(duplex);
+NETSW_WR_ENTRY(speed);
+NETSW_WR_ENTRY(mac_oper);
+NETSW_WR_ENTRY(vlan_restricted);
+NETSW_WR_ENTRY(vlan_untagged);
+
+#ifdef CONFIG_KSZ_STP
+NETSW_RD_ENTRY(stp_info);
+NETSW_WR_ENTRY(stp_on);
+NETSW_WR_ENTRY(stp_prio);
+NETSW_WR_ENTRY(stp_admin_path_cost);
+NETSW_WR_ENTRY(stp_path_cost);
+NETSW_WR_ENTRY(stp_admin_edge);
+NETSW_WR_ENTRY(stp_auto_edge);
+NETSW_WR_ENTRY(stp_mcheck);
+NETSW_WR_ENTRY(stp_admin_p2p);
+#ifdef CONFIG_KSZ_MSTP
+NETSW_WR_ENTRY(stp_auto_isolate);
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+NETSW_WR_ENTRY(mmrpEnabled);
+NETSW_WR_ENTRY(mmrp_mac);
+NETSW_WR_ENTRY(mmrp_svc);
+NETSW_WR_ENTRY(mmrp_reg);
+NETSW_WR_ENTRY(mvrpEnabled);
+NETSW_WR_ENTRY(mvrp_vid);
+NETSW_WR_ENTRY(mvrp_reg);
+
+#ifdef CONFIG_KSZ_MSRP
+NETSW_WR_ENTRY(asCapable);
+NETSW_WR_ENTRY(msrpEnabled);
+NETSW_WR_ENTRY(q_delta);
+NETSW_WR_ENTRY(q_admin_slope);
+NETSW_RD_ENTRY(q_oper_slope);
+NETSW_WR_ENTRY(q_alg);
+NETSW_RD_ENTRY(sr_0_rx_prio);
+NETSW_WR_ENTRY(sr_0_tx_prio);
+NETSW_RD_ENTRY(sr_0_boundary);
+NETSW_RD_ENTRY(sr_1_rx_prio);
+NETSW_WR_ENTRY(sr_1_tx_prio);
+NETSW_RD_ENTRY(sr_1_boundary);
+#endif
+#endif
+
+NETSW_WR_ENTRY(linkmd);
+NETSW_WR_ENTRY(sqi);
+NETSW_WR_ENTRY(mac_loopback);
+NETSW_WR_ENTRY(phy_loopback);
+NETSW_WR_ENTRY(remote_loopback);
+
+static struct attribute *lan_attrs[] = {
+	&lan_attr_info.attr,
+	&lan_attr_version.attr,
+#ifdef USE_SPEED_LINK
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+#endif
+#ifdef USE_MIB
+	&lan_attr_mib.attr,
+#endif
+	&lan_attr_reg.attr,
+	&lan_attr_vid.attr,
+	&lan_attr_features.attr,
+	&lan_attr_overrides.attr,
+
+	&lan_attr_dynamic_table.attr,
+	&lan_attr_static_table.attr,
+	&lan_attr_vlan_table.attr,
+	&lan_attr_hsr_table.attr,
+	&lan_attr_aging.attr,
+	&lan_attr_fast_aging.attr,
+	&lan_attr_link_aging.attr,
+	&lan_attr_bcast_per.attr,
+	&lan_attr_mcast_storm.attr,
+	&lan_attr_tx_queue_based.attr,
+	&lan_attr_diffserv_map.attr,
+	&lan_attr_p_802_1p_map.attr,
+	&lan_attr_vlan.attr,
+	&lan_attr_null_vid.attr,
+	&lan_attr_drop_inv_vid.attr,
+	&lan_attr_macaddr.attr,
+	&lan_attr_mirror_mode.attr,
+	&lan_attr_igmp_snoop.attr,
+	&lan_attr_ipv6_mld_snoop.attr,
+	&lan_attr_ipv6_mld_option.attr,
+	&lan_attr_aggr_backoff.attr,
+	&lan_attr_no_exc_drop.attr,
+	&lan_attr_jumbo_packet.attr,
+	&lan_attr_legal_packet.attr,
+	&lan_attr_length_check.attr,
+	&lan_attr_back_pressure.attr,
+	&lan_attr_sw_flow_ctrl.attr,
+	&lan_attr_sw_half_duplex.attr,
+#ifdef SWITCH_10_MBIT
+	&lan_attr_sw_10_mbit.attr,
+#endif
+	&lan_attr_fair_flow_ctrl.attr,
+	&lan_attr_vlan_bound.attr,
+	&lan_attr_double_tag.attr,
+	&lan_attr_isp.attr,
+	&lan_attr_hsr.attr,
+	&lan_attr_hsr_redbox_id.attr,
+	&lan_attr_hsr_net_id.attr,
+	&lan_attr_mtu.attr,
+	&lan_attr_unk_ucast_fwd.attr,
+	&lan_attr_unk_ucast_ports.attr,
+	&lan_attr_unk_mcast_fwd.attr,
+	&lan_attr_unk_mcast_ports.attr,
+	&lan_attr_unk_vid_fwd.attr,
+	&lan_attr_unk_vid_ports.attr,
+	&lan_attr_pass_pause.attr,
+	&lan_attr_pme.attr,
+	&lan_attr_pme_polarity.attr,
+
+	&lan_attr_host_port.attr,
+	&lan_attr_ports.attr,
+	&lan_attr_dev_start.attr,
+	&lan_attr_vlan_start.attr,
+	&lan_attr_avb.attr,
+	&lan_attr_stp.attr,
+	&lan_attr_two_dev.attr,
+	&lan_attr_authen.attr,
+
+	&lan_attr_alu_fid.attr,
+	&lan_attr_alu_use_fid.attr,
+	&lan_attr_alu_override.attr,
+	&lan_attr_alu_valid.attr,
+	&lan_attr_alu_mstp.attr,
+	&lan_attr_alu_prio.attr,
+	&lan_attr_alu_src.attr,
+	&lan_attr_alu_dst.attr,
+	&lan_attr_alu_ports.attr,
+	&lan_attr_alu_addr.attr,
+	&lan_attr_alu_type.attr,
+	&lan_attr_alu_index.attr,
+	&lan_attr_alu_info.attr,
+
+	&lan_attr_vlan_valid.attr,
+	&lan_attr_vlan_ports.attr,
+	&lan_attr_vlan_untag.attr,
+	&lan_attr_vlan_fid.attr,
+	&lan_attr_vlan_mstp.attr,
+	&lan_attr_vlan_prio.attr,
+	&lan_attr_vlan_option.attr,
+	&lan_attr_vlan_index.attr,
+	&lan_attr_vlan_info.attr,
+	&lan_attr_vid2fid.attr,
+	&lan_attr_fid2mstid.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&lan_attr_stp_br_info.attr,
+	&lan_attr_stp_br_on.attr,
+	&lan_attr_stp_br_prio.attr,
+	&lan_attr_stp_br_fwd_delay.attr,
+	&lan_attr_stp_br_hello_time.attr,
+	&lan_attr_stp_br_max_age.attr,
+	&lan_attr_stp_br_tx_hold.attr,
+	&lan_attr_stp_version.attr,
+#ifdef CONFIG_KSZ_MSTP
+	&lan_attr_stp_br_max_hops.attr,
+	&lan_attr_stp_msti.attr,
+	&lan_attr_stp_msti_vid.attr,
+	&lan_attr_stp_mstp_cfg.attr,
+	&lan_attr_stp_mstp_name.attr,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	&lan_attr_mrp_src_addr.attr,
+
+#ifdef CONFIG_KSZ_MSRP
+	&lan_attr_msrp_info.attr,
+	&lan_attr_msrpEnabled.attr,
+	&lan_attr_msrp_sr_a.attr,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	&lan_attr_hsr_valid.attr,
+	&lan_attr_hsr_age_cnt.attr,
+	&lan_attr_hsr_path_id.attr,
+	&lan_attr_hsr_addr.attr,
+	&lan_attr_hsr_index.attr,
+	&lan_attr_hsr_info.attr,
+	&lan_attr_hsr_state.attr,
+#endif
+
+	&lan_attr_no_color.attr,
+	&lan_attr_color_red.attr,
+	&lan_attr_color_yellow.attr,
+	&lan_attr_color_green.attr,
+
+	NULL
+};
+
+static struct attribute *sw_attrs[] = {
+	&sw_attr_mib.attr,
+
+	&sw_attr_vid.attr,
+	&sw_attr_member.attr,
+	&sw_attr_bcast_storm.attr,
+	&sw_attr_mstp.attr,
+	&sw_attr_rx.attr,
+	&sw_attr_tx.attr,
+	&sw_attr_learn.attr,
+	&sw_attr_power.attr,
+	&sw_attr_mirror_port.attr,
+	&sw_attr_mirror_rx.attr,
+	&sw_attr_mirror_tx.attr,
+	&sw_attr_diffserv.attr,
+	&sw_attr_p_802_1p.attr,
+	&sw_attr_prio_vlan.attr,
+	&sw_attr_prio_mac.attr,
+	&sw_attr_prio_acl.attr,
+	&sw_attr_prio_highest.attr,
+	&sw_attr_prio_or.attr,
+	&sw_attr_port_prio.attr,
+	&sw_attr_non_vid.attr,
+	&sw_attr_drop_non_vlan.attr,
+	&sw_attr_drop_tagged.attr,
+	&sw_attr_ingress.attr,
+	&sw_attr_replace_vid.attr,
+	&sw_attr_replace_prio.attr,
+	&sw_attr_mac_802_1x.attr,
+	&sw_attr_src_addr_filter.attr,
+	&sw_attr_vlan_lookup_0.attr,
+	&sw_attr_prio_queue.attr,
+	&sw_attr_rx_prio_rate.attr,
+	&sw_attr_tx_prio_rate.attr,
+	&sw_attr_limit.attr,
+	&sw_attr_limit_port_based.attr,
+	&sw_attr_limit_packet_based.attr,
+	&sw_attr_limit_flow_ctrl.attr,
+	&sw_attr_limit_cnt_ifg.attr,
+	&sw_attr_limit_cnt_pre.attr,
+	&sw_attr_rx_p0_rate.attr,
+	&sw_attr_rx_p1_rate.attr,
+	&sw_attr_rx_p2_rate.attr,
+	&sw_attr_rx_p3_rate.attr,
+	&sw_attr_rx_p4_rate.attr,
+	&sw_attr_rx_p5_rate.attr,
+	&sw_attr_rx_p6_rate.attr,
+	&sw_attr_rx_p7_rate.attr,
+	&sw_attr_tx_q0_rate.attr,
+	&sw_attr_tx_q1_rate.attr,
+	&sw_attr_tx_q2_rate.attr,
+	&sw_attr_tx_q3_rate.attr,
+	&sw_attr_color_map.attr,
+	&sw_attr_tc_map.attr,
+	&sw_attr_back_pressure.attr,
+	&sw_attr_force_flow_ctrl.attr,
+	&sw_attr_pass_all.attr,
+	&sw_attr_tail_tag.attr,
+
+	&sw_attr_cust_vid.attr,
+	&sw_attr_sr_1_vid.attr,
+	&sw_attr_sr_2_vid.attr,
+	&sw_attr_sr_1_type.attr,
+	&sw_attr_sr_2_type.attr,
+
+	&sw_attr_pme_ctrl.attr,
+	&sw_attr_pme_status.attr,
+
+	&sw_attr_authen_mode.attr,
+	&sw_attr_acl.attr,
+	&sw_attr_acl_first_rule.attr,
+	&sw_attr_acl_ruleset.attr,
+	&sw_attr_acl_mode.attr,
+	&sw_attr_acl_enable.attr,
+	&sw_attr_acl_src.attr,
+	&sw_attr_acl_equal.attr,
+	&sw_attr_acl_addr.attr,
+	&sw_attr_acl_type.attr,
+	&sw_attr_acl_cnt.attr,
+	&sw_attr_acl_msec.attr,
+	&sw_attr_acl_intr_mode.attr,
+	&sw_attr_acl_ip_addr.attr,
+	&sw_attr_acl_ip_mask.attr,
+	&sw_attr_acl_protocol.attr,
+	&sw_attr_acl_seqnum.attr,
+	&sw_attr_acl_port_mode.attr,
+	&sw_attr_acl_max_port.attr,
+	&sw_attr_acl_min_port.attr,
+	&sw_attr_acl_tcp_flag_enable.attr,
+	&sw_attr_acl_tcp_flag.attr,
+	&sw_attr_acl_tcp_flag_mask.attr,
+	&sw_attr_acl_prio_mode.attr,
+	&sw_attr_acl_prio.attr,
+	&sw_attr_acl_vlan_prio_replace.attr,
+	&sw_attr_acl_vlan_prio.attr,
+	&sw_attr_acl_map_mode.attr,
+	&sw_attr_acl_ports.attr,
+	&sw_attr_acl_index.attr,
+	&sw_attr_acl_act_index.attr,
+	&sw_attr_acl_act.attr,
+	&sw_attr_acl_rule_index.attr,
+	&sw_attr_acl_info.attr,
+	&sw_attr_acl_table.attr,
+
+	&sw_attr_p_index.attr,
+	&sw_attr_q_index.attr,
+	&sw_attr_police_type.attr,
+	&sw_attr_non_dscp_color.attr,
+	&sw_attr_police_drop_all.attr,
+	&sw_attr_police_port_based.attr,
+	&sw_attr_color_mark.attr,
+	&sw_attr_color_remap.attr,
+	&sw_attr_drop_srp.attr,
+	&sw_attr_color_aware.attr,
+	&sw_attr_police.attr,
+
+	&sw_attr_q_cir.attr,
+	&sw_attr_q_pir.attr,
+	&sw_attr_q_cbs.attr,
+	&sw_attr_q_pbs.attr,
+
+	&sw_attr_wred_max.attr,
+	&sw_attr_wred_min.attr,
+	&sw_attr_wred_multiplier.attr,
+	&sw_attr_wred_avg_size.attr,
+	&sw_attr_wred_q_max.attr,
+	&sw_attr_wred_q_min.attr,
+	&sw_attr_wred_q_multiplier.attr,
+	&sw_attr_wred_q_avg_size.attr,
+	&sw_attr_wred_random_drop.attr,
+	&sw_attr_wred_drop_gyr.attr,
+	&sw_attr_wred_drop_yr.attr,
+	&sw_attr_wred_drop_r.attr,
+	&sw_attr_wred_drop_all.attr,
+	&sw_attr_wred_q_pmon.attr,
+
+	&sw_attr_q_scheduling.attr,
+	&sw_attr_q_shaping.attr,
+#ifdef MTI_PREEMPT_ENABLE
+	&sw_attr_preempt.attr,
+#endif
+	&sw_attr_q_tx_ratio.attr,
+	&sw_attr_q_credit_hi.attr,
+	&sw_attr_q_credit_lo.attr,
+	&sw_attr_q_credit_incr.attr,
+	&sw_attr_srp.attr,
+
+	&sw_attr_qm_drop.attr,
+	&sw_attr_qm_burst.attr,
+	&sw_attr_qm_resv_space.attr,
+	&sw_attr_qm_hi.attr,
+	&sw_attr_qm_lo.attr,
+	&sw_attr_qm_tx_used.attr,
+	&sw_attr_qm_tx_avail.attr,
+	&sw_attr_qm_tx_calc.attr,
+
+	&sw_attr_mmd_id.attr,
+	&sw_attr_mmd_reg.attr,
+	&sw_attr_mmd_val.attr,
+
+	&sw_attr_rx_flow_ctrl.attr,
+	&sw_attr_tx_flow_ctrl.attr,
+
+	&sw_attr_duplex.attr,
+	&sw_attr_speed.attr,
+	&sw_attr_mac_oper.attr,
+	&sw_attr_vlan_restricted.attr,
+	&sw_attr_vlan_untagged.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&sw_attr_stp_info.attr,
+	&sw_attr_stp_on.attr,
+	&sw_attr_stp_prio.attr,
+	&sw_attr_stp_admin_path_cost.attr,
+	&sw_attr_stp_path_cost.attr,
+	&sw_attr_stp_admin_edge.attr,
+	&sw_attr_stp_auto_edge.attr,
+	&sw_attr_stp_mcheck.attr,
+	&sw_attr_stp_admin_p2p.attr,
+#ifdef CONFIG_KSZ_MSTP
+	&sw_attr_stp_auto_isolate.attr,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	&sw_attr_mmrpEnabled.attr,
+	&sw_attr_mmrp_mac.attr,
+	&sw_attr_mmrp_svc.attr,
+	&sw_attr_mmrp_reg.attr,
+	&sw_attr_mvrpEnabled.attr,
+	&sw_attr_mvrp_vid.attr,
+	&sw_attr_mvrp_reg.attr,
+
+#ifdef CONFIG_KSZ_MSRP
+	&sw_attr_asCapable.attr,
+	&sw_attr_msrpEnabled.attr,
+	&sw_attr_q_delta.attr,
+	&sw_attr_q_admin_slope.attr,
+	&sw_attr_q_oper_slope.attr,
+	&sw_attr_q_alg.attr,
+	&sw_attr_sr_0_rx_prio.attr,
+	&sw_attr_sr_0_tx_prio.attr,
+	&sw_attr_sr_0_boundary.attr,
+	&sw_attr_sr_1_rx_prio.attr,
+	&sw_attr_sr_1_tx_prio.attr,
+	&sw_attr_sr_1_boundary.attr,
+#endif
+#endif
+
+	&sw_attr_linkmd.attr,
+	&sw_attr_sqi.attr,
+	&sw_attr_mac_loopback.attr,
+	&sw_attr_phy_loopback.attr,
+	&sw_attr_remote_loopback.attr,
+
+	NULL
+};
+
+static struct attribute_group lan_group = {
+	.name  = "sw",
+	.attrs  = lan_attrs,
+};
+
+static struct attribute_group sw_group = {
+	.name  = "sw0",
+	.attrs  = sw_attrs,
+};
+
+/* Kernel checking requires the attributes are in data segment. */
+#define SW_ATTRS_SIZE		(sizeof(sw_attrs) / sizeof(void *) - 1)
+
+#define MAX_SWITCHES		2
+
+static struct ksz_dev_attr ksz_sw_dev_attrs[(
+	SW_ATTRS_SIZE * TOTAL_PORT_NUM) * MAX_SWITCHES];
+static struct ksz_dev_attr *ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+static void exit_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int i;
+
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		sysfs_remove_group(&dev->kobj, &sw_group);
+		kfree(info->port_attrs[i]);
+		info->port_attrs[i] = NULL;
+		info->ksz_port_attrs[i] = NULL;
+	}
+
+	sysfs_remove_group(&dev->kobj, &lan_group);
+
+	ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+}
+
+static int init_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int err;
+	int i;
+
+	err = sysfs_create_group(&dev->kobj, &lan_group);
+	if (err)
+		return err;
+	for (i = 0; i < sw->mib_port_cnt; i++) {
+		if (i >= sw->phy_port_cnt)
+			err = alloc_dev_attr(sw_attrs,
+				sizeof(sw_attrs) / sizeof(void *), i,
+				&info->ksz_port_attrs[i], &info->port_attrs[i],
+				"0_linkmd", &ksz_sw_dev_attrs_ptr);
+		else
+			err = alloc_dev_attr(sw_attrs,
+				sizeof(sw_attrs) / sizeof(void *), i,
+				&info->ksz_port_attrs[i], &info->port_attrs[i],
+				NULL, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &sw_group);
+		if (err)
+			return err;
+	}
+	return err;
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/ksz_sysfs.h b/drivers/net/ethernet/micrel/ksz9897/ksz_sysfs.h
new file mode 100644
index 0000000..86bde98
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/ksz_sysfs.h
@@ -0,0 +1,189 @@
+/**
+ * Microchip switch common sysfs header
+ *
+ * Copyright (c) 2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SYSFS_H
+#define KSZ_SYSFS_H
+
+
+#ifndef get_private_data
+
+#ifndef get_sysfs_data
+static void get_sysfs_data_(struct net_device *dev,
+	struct semaphore **proc_sem, struct ksz_port **port)
+{}
+
+#define get_sysfs_data		get_sysfs_data_
+#endif
+
+static void get_private_data_(struct device *d, struct semaphore **proc_sem,
+	struct ksz_sw **sw, struct ksz_port **port)
+{
+	if (d->bus && (
+#if defined(__LINUX_SPI_H)
+	    d->bus == &spi_bus_type
+#endif
+#if defined(__LINUX_SPI_H) && defined(_LINUX_I2C_H)
+	    ||
+#endif
+#if defined(_LINUX_I2C_H)
+	    d->bus == &i2c_bus_type
+#endif
+#if !defined(__LINUX_SPI_H) && !defined(_LINUX_I2C_H)
+	    d->bus
+#endif
+	    )) {
+		struct sw_priv *hw_priv;
+
+		hw_priv = dev_get_drvdata(d);
+		if (port && hw_priv->phydev) {
+			struct phy_priv *phydata;
+
+			phydata = hw_priv->phydev->priv;
+			*port = phydata->port;
+		}
+		*proc_sem = &hw_priv->proc_sem;
+		*sw = &hw_priv->sw;
+	} else {
+		struct net_device *dev;
+		struct ksz_port *p = NULL;
+
+		dev = to_net_dev(d);
+		get_sysfs_data(dev, proc_sem, &p);
+		*sw = p->sw;
+		if (port)
+			*port = p;
+	}
+}
+
+#define get_private_data	get_private_data_
+#endif
+
+#ifndef get_num_val
+static int get_num_val_(const char *buf)
+{
+	int num = -1;
+
+	if ('0' == buf[0] && 'x' == buf[1])
+		sscanf(&buf[2], "%x", (unsigned int *) &num);
+	else if ('0' == buf[0] && 'b' == buf[1]) {
+		int i = 2;
+
+		num = 0;
+		while (buf[i]) {
+			num <<= 1;
+			num |= buf[i] - '0';
+			i++;
+		}
+	} else if ('0' == buf[0] && 'd' == buf[1])
+		sscanf(&buf[2], "%u", &num);
+	else
+		sscanf(buf, "%d", &num);
+	return num;
+}  /* get_num_val */
+
+#define get_num_val		get_num_val_
+#endif
+
+#ifdef USE_SHOW_HELP_
+enum {
+	SHOW_HELP_NONE,
+	SHOW_HELP_ON_OFF,
+	SHOW_HELP_NUM,
+	SHOW_HELP_HEX,
+	SHOW_HELP_HEX_2,
+	SHOW_HELP_HEX_4,
+	SHOW_HELP_HEX_8,
+	SHOW_HELP_SPECIAL,
+};
+
+static char *help_formats[] = {
+	"",
+	"%d%s\n",
+	"%u%s\n",
+	"0x%x%s\n",
+	"0x%02x%s\n",
+	"0x%04x%s\n",
+	"0x%08x%s\n",
+	"%d%s\n",
+};
+
+static char *display_strs[] = {
+	" (off)",
+	" (on)",
+};
+
+static char *show_on_off(uint on)
+{
+	if (on <= 1)
+		return display_strs[on];
+	return NULL;
+}  /* show_on_off */
+
+static ssize_t sysfs_show(ssize_t len, char *buf, int type, int chk, char *ptr,
+	int verbose)
+{
+	if (type) {
+		if (verbose) {
+			if (SHOW_HELP_ON_OFF == type)
+				ptr = show_on_off(chk);
+		}
+		len += sprintf(buf + len, help_formats[type], chk, ptr);
+	}
+	return len;
+}  /* sysfs_show */
+#endif
+
+static int alloc_dev_attr(struct attribute **attrs, size_t attr_size, int item,
+	struct ksz_dev_attr **ksz_attrs, struct attribute ***item_attrs,
+	char *item_name, struct ksz_dev_attr **attrs_ptr)
+{
+	struct attribute **attr_ptr;
+	struct device_attribute *dev_attr;
+	struct ksz_dev_attr *new_attr;
+
+	*item_attrs = kmalloc(attr_size * sizeof(void *), GFP_KERNEL);
+	if (!*item_attrs)
+		return -ENOMEM;
+
+	attr_size--;
+	attr_size *= sizeof(struct ksz_dev_attr);
+	*ksz_attrs = *attrs_ptr;
+	*attrs_ptr += attr_size / sizeof(struct ksz_dev_attr);
+
+	new_attr = *ksz_attrs;
+	attr_ptr = *item_attrs;
+	while (*attrs != NULL) {
+		if (item_name && !strcmp((*attrs)->name, item_name))
+			break;
+		dev_attr = container_of(*attrs, struct device_attribute, attr);
+		memcpy(new_attr, dev_attr, sizeof(struct device_attribute));
+		strncpy(new_attr->dev_name, (*attrs)->name, DEV_NAME_SIZE);
+		if (10 <= item && item <= 15)
+			new_attr->dev_name[0] = item - 10 + 'a';
+		else
+			new_attr->dev_name[0] = item + '0';
+		new_attr->dev_attr.attr.name = new_attr->dev_name;
+		*attr_ptr = &new_attr->dev_attr.attr;
+		new_attr++;
+		attr_ptr++;
+		attrs++;
+	}
+	*attr_ptr = NULL;
+	return 0;
+}
+
+#endif
diff --git a/drivers/net/ethernet/micrel/ksz9897/micrel_ptp.c b/drivers/net/ethernet/micrel/ksz9897/micrel_ptp.c
new file mode 100644
index 0000000..9533f2c
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/micrel_ptp.c
@@ -0,0 +1,284 @@
+/*
+ * PTP 1588 clock using Microchip PTP switch
+ *
+ * Copyright (C) 2010 OMICRON electronics GmbH
+ * Copyright (C) 2013-2015 Micrel, Inc.
+ * Copyright (C) 2015-2016 Microchip Technology Inc.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include <linux/device.h>
+#include <linux/hrtimer.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/timex.h>
+#include <linux/io.h>
+
+#include <linux/ptp_clock_kernel.h>
+
+#define N_ALARM		0
+#define N_EXT_TS	2
+
+struct micrel_ptp_info {
+	struct ptp_clock *clock;
+	struct ptp_clock_info caps;
+	struct ptp_info *ptp;
+
+	u32 clock_events;
+	u64 alarm_interval;
+	u64 alarm_value;
+};
+
+static void ptp_event_pps(struct micrel_ptp_info *info)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_PPS;
+	ptp_clock_event(info->clock, &event);
+}
+
+#if 0
+static void ptp_event_alarm(struct micrel_ptp_info *info)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_ALARM;
+	event.index = 0;
+	event.timestamp = info->alarm_value;
+	ptp_clock_event(info->clock, &event);
+}
+#endif
+
+static void ptp_event_trigger(struct micrel_ptp_info *info, int index,
+	u32 sec, u32 nsec)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_EXTTS;
+	event.index = index;
+	event.timestamp = sec;
+	event.timestamp *= NSEC_PER_SEC;
+	event.timestamp += nsec;
+	ptp_clock_event(info->clock, &event);
+}
+
+/*
+ * PTP clock operations
+ */
+
+static int ptp_adjfreq(struct ptp_clock_info *clock, s32 ppb)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	output = 1;
+	clk_opt.sec = clk_opt.nsec = 0;
+	clk_opt.drift = -ppb;
+	clk_opt.interval = NANOSEC_IN_SEC;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_adjtime(struct ptp_clock_info *clock, s64 delta)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int neg_adj = 0;
+	int err = 0;
+
+	if (delta < 0) {
+		neg_adj = 1;
+		delta = -delta;
+	}
+	clk_opt.sec = div_u64_rem(delta, NSEC_PER_SEC, &clk_opt.nsec);
+	if (!neg_adj)
+		output = 2;
+	else
+		output = 1;
+	clk_opt.interval = 0;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_gettime(struct ptp_clock_info *clock, struct timespec *ts)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	if (!ts)
+		return -info->ptp->drift;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_GET, DEV_PTP_CLK, 0,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	if (err)
+		return err;
+	ts->tv_sec = clk_opt.sec;
+	ts->tv_nsec = clk_opt.nsec;
+	return 0;
+}
+
+static int ptp_settime(struct ptp_clock_info *clock, const struct timespec *ts)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	output = 0;
+	clk_opt.sec = ts->tv_sec;
+	clk_opt.nsec = ts->tv_nsec;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_enable(struct ptp_clock_info *clock,
+	struct ptp_clock_request *rq, int on)
+{
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	struct ptp_info *ptp = info->ptp;
+	u32 bit;
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_EXTTS:
+		if (rq->extts.index >= 2)
+			return -EINVAL;
+		bit = 1 << rq->extts.index;
+		if (on)
+			ptp->clock_events |= bit;
+		else
+			ptp->clock_events &= ~bit;
+		return 0;
+
+	case PTP_CLK_REQ_PPS:
+		bit = 1 << 31;
+		if (on)
+			ptp->clock_events |= bit;
+		else
+			ptp->clock_events &= ~bit;
+		return 0;
+
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info ptp_caps = {
+	.owner		= THIS_MODULE,
+	/* Only 16 characters. */
+	.name		= "Microchip clock",
+	.max_adj	= MAX_DRIFT_CORR,
+	.n_alarm	= N_ALARM,
+	.n_ext_ts	= N_EXT_TS,
+	.n_per_out	= 0,
+	.pps		= 1,
+	.adjfreq	= ptp_adjfreq,
+	.adjtime	= ptp_adjtime,
+	.gettime	= ptp_gettime,
+	.settime	= ptp_settime,
+	.enable		= ptp_enable,
+};
+
+static int micrel_ptp_get_ts_info(struct ptp_info *ptp,
+	struct ethtool_ts_info *info)
+{
+	struct micrel_ptp_info *clock_info = ptp->clock_info;
+
+	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+		SOF_TIMESTAMPING_RX_HARDWARE |
+		SOF_TIMESTAMPING_RAW_HARDWARE;
+	if (clock_info->clock)
+		info->phc_index = ptp_clock_index(clock_info->clock);
+	else
+		info->phc_index = -1;
+	info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON);
+	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+		(1 << HWTSTAMP_FILTER_ALL);
+	return 0;
+}
+
+static int micrel_ptp_probe(struct ptp_info *ptp)
+{
+	struct micrel_ptp_info *info;
+#if 0
+	struct timespec now;
+#endif
+	int err = -ENOMEM;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info)
+		goto no_memory;
+
+	err = -ENODEV;
+
+	info->caps = ptp_caps;
+
+#if 0
+	getnstimeofday(&now);
+	ptp_settime(&info->caps, &now);
+#endif
+
+	info->clock = ptp_clock_register(&info->caps, ptp->parent);
+	if (IS_ERR(info->clock)) {
+		err = PTR_ERR(info->clock);
+		goto no_clock;
+	}
+	info->ptp = ptp;
+	ptp->clock_info = info;
+
+	return 0;
+
+no_clock:
+	kfree(info);
+no_memory:
+	return err;
+}
+
+static int micrel_ptp_remove(struct ptp_info *ptp)
+{
+	struct micrel_ptp_info *info = ptp->clock_info;
+
+	ptp_clock_unregister(info->clock);
+	kfree(info);
+	ptp->clock_info = NULL;
+
+	return 0;
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/mmrp.c b/drivers/net/ethernet/micrel/ksz9897/mmrp.c
new file mode 100644
index 0000000..4628863
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/mmrp.c
@@ -0,0 +1,177 @@
+/*
+ *	IEEE 802.1Q Multiple MAC Registration Protocol (MMRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/8021q/vlan_gvrp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+
+
+#ifndef ETH_P_MMRP
+#define ETH_P_MMRP		0x88F6
+#endif
+
+#define MRP_MMRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x20 }
+
+enum mmrp_attributes {
+	MMRP_ATTR_INVALID,
+	MMRP_ATTR_SVC,
+	MMRP_ATTR_MAC,
+	__MMRP_ATTR_MAX
+};
+#define MMRP_ATTR_MAX	(__MMRP_ATTR_MAX - 1)
+
+static struct mrp_application mac_mrp_app __read_mostly = {
+	.type		= MRP_APPLICATION_MMRP,
+	.maxattr	= MMRP_ATTR_MAX,
+	.pkttype.type	= htons(ETH_P_MMRP),
+	.group_address	= MRP_MMRP_ADDRESS,
+	.version	= 0,
+};
+
+static int mmrp_attr_chk(u8 attrtype, u8 attrlen)
+{
+	if (MMRP_ATTR_MAC == attrtype && ETH_ALEN == attrlen)
+		return 0;
+	else if (MMRP_ATTR_SVC == attrtype && 1 == attrlen)
+		return 0;
+	if (MMRP_ATTR_MAC != attrtype &&
+	    MMRP_ATTR_SVC != attrtype)
+		return 1;
+	return -1;
+}
+
+static int mmrp_rxpdu(struct mrp_applicant *app, u8 *data, int len)
+{
+	int rc;
+
+	app->dry_run = 1;
+	rc = mrp_rx(app, data, len);
+
+	/* Discard entire PDU if malformed. */
+	if (rc < 0)
+		return 0;
+	app->dry_run = 0;
+	return mrp_rx(app, data, len);
+}
+
+static int mmrp_txpdu(struct mrp_applicant *app)
+{
+	int bytes;
+	int err;
+	u8 *msg_buf;
+	u8 *msg_eof;
+	u8 *msg_ptr;
+
+	if (!app->normal)
+		return 0;
+
+	err = mrp_pdu_init(app);
+	if (err < 0)
+		return err;
+
+	msg_buf = app->pdu->data;
+	msg_eof = msg_buf + app->dev->mtu;
+	msg_buf++;
+	msg_ptr = msg_buf;
+
+	if (app->normal & (1 << MMRP_ATTR_MAC)) {
+		err = mrp_tx(app, msg_ptr, msg_eof, &bytes,
+			     MMRP_ATTR_MAC, ETH_ALEN, MMRP_MAC_OPT_MAX,
+			     MMRP_MAC_MIN);
+
+		/* LeaveAll for MAC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MMRP_ATTR_MAC - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+
+#if 1
+	if (app->normal & (1 << MMRP_ATTR_SVC)) {
+		err = mrp_tx(app, msg_ptr, msg_eof, &bytes,
+			     MMRP_ATTR_SVC, 1, MMRP_SVC_OPT_MAX, MMRP_SVC_MIN);
+
+		/* LeaveAll for SVC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MMRP_ATTR_SVC - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+#endif
+	app->lva.tx = 0;
+
+send:
+	return mrp_txpdu(app, msg_buf, msg_ptr, msg_eof);
+}
+
+static int mmrp_req_join_mac(struct mrp_applicant *app, u8 *addr, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, addr, ETH_ALEN, MMRP_ATTR_MAC,
+		new_decl);
+}
+
+#if 0
+static int mmrp_req_join_svc(struct mrp_applicant *app, u8 svc)
+{
+	return mrp_req_new(app, &svc, 1, MMRP_ATTR_SVC,
+		MRP_EVENT_JOIN);
+}
+#endif
+
+static int mmrp_req_leave_mac(struct mrp_applicant *app, u8 *addr)
+{
+	return mrp_req_leave(app, addr, ETH_ALEN, MMRP_ATTR_MAC);
+}
+
+#if 0
+static int mmrp_req_leave_svc(struct mrp_applicant *app, u8 svc)
+{
+	return mrp_req_leave(app, &svc, 1, MMRP_ATTR_SVC);
+}
+#endif
+
+static void mmrp_req_set_mac(struct mrp_applicant *app, u8 *addr,
+			enum mrp_registrar_state state)
+{
+	mrp_req_set(app, addr, ETH_ALEN, MMRP_ATTR_MAC, state);
+}
+
+static void mmrp_init_application(struct mrp_applicant *app,
+				  void (*acton)(struct mrp_applicant *app,
+				  struct mrp_attr *attr),
+				  void (*cleanup)(struct mrp_applicant *app))
+{
+	app->attrval_inc = mrp_attrvalue_inc;
+	app->attr_chk = mmrp_attr_chk;
+	app->attr_cmp = mrp_attr_cmp;
+	app->attr_valid = mrp_attr_valid;
+	app->attr_size = mrp_attr_size;
+	app->attr_len = mrp_attr_len;
+	app->attr_type = mrp_attr_type;
+	app->rxpdu = mmrp_rxpdu;
+	app->txpdu = mmrp_txpdu;
+	app->acton = acton;
+	app->cleanup = cleanup;
+	app->normal = (1 << MMRP_ATTR_MAC);
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/mrp.c b/drivers/net/ethernet/micrel/ksz9897/mrp.c
new file mode 100644
index 0000000..6a09cb5
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/mrp.c
@@ -0,0 +1,2055 @@
+/*
+ *	IEEE 802.1Q Multiple Registration Protocol (MRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/802/garp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+#include <linux/kernel.h>
+#include <linux/timer.h>
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include "mrp.h"
+#include <asm/unaligned.h>
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+#define prandom_u32()		prandom32(app->rnd)
+#endif
+
+
+static unsigned int mrp_join_time __read_mostly = 200;
+module_param(mrp_join_time, uint, 0644);
+MODULE_PARM_DESC(mrp_join_time, "Join time in ms (default 200ms)");
+
+static unsigned int mrp_periodic_time __read_mostly = 1000;
+module_param(mrp_periodic_time, uint, 0644);
+MODULE_PARM_DESC(mrp_periodic_time, "Periodic time in ms (default 1s)");
+
+static unsigned int mrp_leave_time __read_mostly = 1000;
+static unsigned int mrp_lva_time __read_mostly = 10000;
+
+MODULE_LICENSE("GPL");
+
+static const u8
+mrp_applicant_state_table[MRP_APPLICANT_MAX + 1][MRP_EVENT_MAX + 1] = {
+	[MRP_APPLICANT_VO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VO,
+	},
+	[MRP_APPLICANT_VP] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_AA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VP,
+	},
+	[MRP_APPLICANT_VN] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_AN,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VN,
+	},
+	[MRP_APPLICANT_AN] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_AN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AN,
+	},
+	[MRP_APPLICANT_AA] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AA,
+	},
+	[MRP_APPLICANT_QA] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_QA,
+	},
+	[MRP_APPLICANT_LA] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_LA,
+	},
+	[MRP_APPLICANT_AO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_AO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_AO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AO,
+	},
+	[MRP_APPLICANT_QO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_QO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_QO,
+	},
+	[MRP_APPLICANT_AP] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_AO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AP,
+	},
+	[MRP_APPLICANT_QP] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_QO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QP,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VP,
+	},
+	[MRP_APPLICANT_LO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_LO,
+	},
+};
+
+static const u8
+mrp_tx_action_table[MRP_APPLICANT_MAX + 1] = {
+	[MRP_APPLICANT_VO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_VP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_VN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AA] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QA] = MRP_TX_ACTION_S_JOIN_IN_OPTIONAL,
+	[MRP_APPLICANT_LA] = MRP_TX_ACTION_S_LV,
+	[MRP_APPLICANT_AO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_QO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_AP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QP] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_LO] = MRP_TX_ACTION_S_IN,
+};
+
+static const u8
+mrp_tx_la_action_table[MRP_APPLICANT_MAX + 1] = {
+	[MRP_APPLICANT_VO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_VP] = MRP_TX_ACTION_S_IN,
+	[MRP_APPLICANT_VN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AA] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QA] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_LA] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_AO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_QO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_AP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_LO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+};
+
+#ifdef DBG_MRP_APP
+static char *format_event(char *str, enum mrp_event event)
+{
+	switch (event) {
+	case MRP_EVENT_NEW:
+		strcpy(str, "New!");
+		break;
+	case MRP_EVENT_JOIN:
+		strcpy(str, "Join!");
+		break;
+	case MRP_EVENT_LV:
+		strcpy(str, "Lv!");
+		break;
+	case MRP_EVENT_TX:
+		strcpy(str, "tx!");
+		break;
+	case MRP_EVENT_TX_LA:
+		strcpy(str, "txLA!");
+		break;
+	case MRP_EVENT_TX_LAF:
+		strcpy(str, "txLAF!");
+		break;
+	case MRP_EVENT_R_NEW:
+		strcpy(str, "rNew!");
+		break;
+	case MRP_EVENT_R_JOIN_IN:
+		strcpy(str, "rJoinIn!");
+		break;
+	case MRP_EVENT_R_IN:
+		strcpy(str, "rIn!");
+		break;
+	case MRP_EVENT_R_JOIN_MT:
+		strcpy(str, "rJoinMt!");
+		break;
+	case MRP_EVENT_R_MT:
+		strcpy(str, "rMt!");
+		break;
+	case MRP_EVENT_R_LV:
+		strcpy(str, "rLv!");
+		break;
+	case MRP_EVENT_R_LA:
+		strcpy(str, "rLA!");
+		break;
+	case MRP_EVENT_REDECLARE:
+		strcpy(str, "Redec!");
+		break;
+	case MRP_EVENT_PERIODIC:
+		strcpy(str, "periodic!");
+		break;
+	case MRP_EVENT_PERIODIC_DISABLE:
+		strcpy(str, "peoridicDisable!");
+		break;
+	case MRP_EVENT_PERIODIC_ENABLE:
+		strcpy(str, "periodicEnable!");
+		break;
+	case MRP_EVENT_LV_TIMER:
+		strcpy(str, "lv!");
+		break;
+	case MRP_EVENT_LVA_TIMER:
+		strcpy(str, "lva!");
+		break;
+	case MRP_EVENT_FLUSH:
+		strcpy(str, "Flush!");
+		break;
+	default:
+		strcpy(str, "?");
+		break;
+	}
+	return str;
+}  /* format_event */
+
+static char *format_action(char *str, enum mrp_tx_action action)
+{
+	switch (action) {
+	case MRP_TX_ACTION_NONE:
+		strcpy(str, "-");
+		break;
+	case MRP_TX_ACTION_S_NEW:
+		strcpy(str, "sN");
+		break;
+	case MRP_TX_ACTION_S_JOIN_IN:
+		strcpy(str, "sJ");
+		break;
+	case MRP_TX_ACTION_S_JOIN_IN_OPTIONAL:
+		strcpy(str, "[sJ]");
+		break;
+	case MRP_TX_ACTION_S_IN:
+		strcpy(str, "s");
+		break;
+	case MRP_TX_ACTION_S_IN_OPTIONAL:
+		strcpy(str, "[s]");
+		break;
+	case MRP_TX_ACTION_S_LV:
+		strcpy(str, "sL");
+		break;
+	default:
+		strcpy(str, "?");
+		break;
+	}
+	return str;
+}  /* format_action */
+
+static char *format_app_state(char *str, enum mrp_applicant_state state)
+{
+	switch (state) {
+	case MRP_APPLICANT_VO:
+		strcpy(str, "VO");
+		break;
+	case MRP_APPLICANT_VP:
+		strcpy(str, "VP");
+		break;
+	case MRP_APPLICANT_VN:
+		strcpy(str, "VN");
+		break;
+	case MRP_APPLICANT_AN:
+		strcpy(str, "AN");
+		break;
+	case MRP_APPLICANT_AA:
+		strcpy(str, "AA");
+		break;
+	case MRP_APPLICANT_QA:
+		strcpy(str, "QA");
+		break;
+	case MRP_APPLICANT_LA:
+		strcpy(str, "LA");
+		break;
+	case MRP_APPLICANT_AO:
+		strcpy(str, "AO");
+		break;
+	case MRP_APPLICANT_QO:
+		strcpy(str, "QO");
+		break;
+	case MRP_APPLICANT_AP:
+		strcpy(str, "AP");
+		break;
+	case MRP_APPLICANT_QP:
+		strcpy(str, "QP");
+		break;
+	case MRP_APPLICANT_LO:
+		strcpy(str, "LO");
+		break;
+	default:
+		strcpy(str, "INV");
+		break;
+	}
+	return str;
+}  /* format_app_state */
+#endif
+
+#ifdef DBG_MRP_REG
+static char *format_reg_state(char *str, enum mrp_registrar_state state)
+{
+	switch (state) {
+	case MRP_REGISTRAR_MT:
+		strcpy(str, "MT");
+		break;
+	case MRP_REGISTRAR_IN:
+		strcpy(str, "IN");
+		break;
+	case MRP_REGISTRAR_LV:
+		strcpy(str, "LV");
+		break;
+	default:
+		strcpy(str, "INV");
+		break;
+	}
+	return str;
+}  /* format_reg_state */
+#endif
+
+static void mrp_attrvalue_inc(void *value, u8 len)
+{
+	u8 *v = (u8 *)value;
+
+	/* Add 1 to the last byte. If it becomes zero,
+	 * go to the previous byte and repeat.
+	 */
+	while (len > 0 && !++v[--len])
+		;
+}
+
+static int mrp_attr_cmp(const struct mrp_attr *attr,
+			 const void *value, u8 len, u8 type)
+{
+	if (attr->type != type)
+		return attr->type - type;
+	if (attr->len != len)
+		return attr->len - len;
+	return memcmp(attr->value, value, len);
+}
+
+static int mrp_attr_valid(u8 attrtype, const void *value)
+{
+	return true;
+}
+
+static u8 mrp_attr_size(u8 attrtype, u8 attrlen)
+{
+	return attrlen;
+}
+
+static u8 mrp_attr_len(u8 attrtype, u8 attrlen)
+{
+	return attrlen;
+}
+
+static u8 mrp_attr_type(struct mrp_attr *attr)
+{
+	return attr->type;
+}
+
+#if 0
+static int dbg_listener;
+static int dbg_talker;
+#endif
+
+static struct mrp_attr *mrp_attr_lookup(const struct mrp_applicant *app,
+					const void *value, u8 len, u8 type)
+{
+	struct rb_node *parent = app->mad.rb_node;
+	struct mrp_attr *attr;
+	int d;
+
+	while (parent) {
+		attr = rb_entry(parent, struct mrp_attr, node);
+		d = app->attr_cmp(attr, value, len, type);
+		if (d > 0)
+			parent = parent->rb_left;
+		else if (d < 0)
+			parent = parent->rb_right;
+		else
+			return attr;
+	}
+#if 0
+{
+const u8 *data = value;
+
+if ((len == 34 && !dbg_talker) || (len == 10 && !dbg_listener) ||
+(len != 34 && len != 10)) {
+dbg_msg("p:%d %d %d = ", app->port, app->app->type, type);
+for (d = 0; d < len; d++)
+dbg_msg("%02x ", data[d]);
+dbg_msg("\n");
+
+/* XMOS sends a talker with irrevant data. */
+if (34 == len && 0x20 == data[17] && 0x01 == data[19])
+++dbg_talker;
+if (10 == len)
+++dbg_listener;
+}
+}
+#endif
+	return NULL;
+}
+
+static struct mrp_attr *mrp_attr_create(struct mrp_applicant *app,
+					const void *value, u8 len, u8 type,
+					u8 size)
+{
+	struct rb_node *parent = NULL, **p = &app->mad.rb_node;
+	struct mrp_attr *attr;
+	int d;
+
+	while (*p) {
+		parent = *p;
+		attr = rb_entry(parent, struct mrp_attr, node);
+		d = app->attr_cmp(attr, value, len, type);
+		if (d > 0)
+			p = &parent->rb_left;
+		else if (d < 0)
+			p = &parent->rb_right;
+		else {
+			/* The attribute already exists; re-use it. */
+			return attr;
+		}
+	}
+	attr = kmalloc(sizeof(*attr) + size, GFP_ATOMIC);
+	if (!attr)
+		return attr;
+	attr->state = MRP_APPLICANT_VO;
+	attr->type  = type;
+	attr->len   = len;
+	memcpy(attr->value, value, len);
+
+	attr->new_state = MRP_APPLICANT_VO;
+	attr->action = MRP_TX_ACTION_NONE;
+	attr->fix_state = MRP_REGISTRAR_LV;
+	attr->reg_state = MRP_REGISTRAR_MT;
+	attr->notify = MRP_NOTIFY_NONE;
+	attr->aging = 0;
+
+	rb_link_node(&attr->node, parent, p);
+	rb_insert_color(&attr->node, &app->mad);
+	return attr;
+}
+
+static void mrp_attr_destroy(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+#if 0
+	u8 len = app->attr_len(attr->type, attr->len);
+	u8 type = app->attr_type(attr);
+dbg_msg("%s p:%d [%d=%d:%02x]\n", __func__, app->port, app->app->type,
+type, attr->value[len - 1]);
+#endif
+	rb_erase(&attr->node, &app->mad);
+	kfree(attr);
+}
+
+static int mrp_pdu_init(struct mrp_applicant *app)
+{
+	struct sk_buff *skb;
+	struct mrp_pdu_hdr *ph;
+
+	skb = alloc_skb(app->dev->mtu + LL_RESERVED_SPACE(app->dev) + 8,
+			GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	skb->dev = app->dev;
+	skb->protocol = app->app->pkttype.type;
+	skb_reserve(skb, LL_RESERVED_SPACE(app->dev));
+	skb_reset_network_header(skb);
+	skb_reset_transport_header(skb);
+
+	ph = (struct mrp_pdu_hdr *)__skb_put(skb, sizeof(*ph));
+	ph->version = app->app->version;
+
+	app->pdu = skb;
+	return 0;
+}
+
+static int mrp_applicant_chk(struct mrp_attr *attr)
+{
+	int ret = 0;
+#if 0
+	u8 len = attr->len;
+
+if (25 == len || 34 == len || 10 == len)
+	len = 8;
+dbg_msg("aging [%02x] %d\n", attr->value[len - 1], attr->aging);
+#endif
+	if (attr->new_state != attr->state) {
+		if (MRP_APPLICANT_VO == attr->new_state) {
+			if (!attr->aging && attr->fix_state == MRP_REGISTRAR_LV)
+				attr->aging = 4;
+		} else if (MRP_APPLICANT_LO != attr->new_state)
+			attr->aging = 0;
+		attr->state = attr->new_state;
+	}
+	if (MRP_REGISTRAR_MT != attr->reg_state)
+		attr->aging = 0;
+	if (attr->action != MRP_TX_ACTION_NONE) {
+		if (attr->aging > 1)
+			--attr->aging;
+		attr->action = MRP_TX_ACTION_NONE;
+	}
+	if (1 == attr->aging)
+		return 1;
+	return ret;
+}
+
+#define MRP_OPT_MAX(attrlen)		\
+	((((2 + (attrlen)) + 1) * 2 - (2 + (attrlen))) * 3)
+
+#define MMRP_SVC_OPT_MAX		MRP_OPT_MAX(1)
+#define MMRP_MAC_OPT_MAX		MRP_OPT_MAX(6)
+#define MVRP_VID_OPT_MAX		MRP_OPT_MAX(2)
+
+static void mrp_prepare_tx(struct mrp_applicant *app, u8 attrtype,
+	int firstval_max)
+{
+	u8 firstval[20];
+	int firstval_cnt = 0;
+	int firstval_opt = 0;
+	int opt_cnt = 0;
+	struct rb_node *node, *next;
+	struct rb_node *opt = NULL;
+	struct mrp_attr *attr;
+	struct mrp_attr *first = NULL;
+	struct mrp_attr dummy;
+	struct mrp_attr *last = &dummy;
+	enum mrp_tx_action last_action = MRP_TX_ACTION_NONE;
+
+	memset(firstval, 0, sizeof(firstval));
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attrtype != attr->type)
+			continue;
+		if (MRP_TX_ACTION_NONE == attr->action)
+{
+dbg_msg(" no send\n");
+			continue;
+}
+
+		/* At least one attribute that is trying to send something. */
+		if (last) {
+			last = attr;
+
+			/* Rememeber action so that it can be restored. */
+			last_action = attr->action;
+		}
+		if (firstval_cnt) {
+			int firstval_next = 1;
+
+			app->attrval_inc(firstval, attr->len);
+			if (memcmp(firstval, attr->value, attr->len))
+				firstval_next = 0;
+			if (!firstval_next)
+				first = NULL;
+			else {
+				++firstval_cnt;
+				if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL ==
+				    attr->action ||
+				    MRP_TX_ACTION_S_IN_OPTIONAL ==
+				    attr->action) {
+					if (!opt) {
+						opt = node;
+						opt_cnt = 1;
+						firstval_opt =
+							firstval_cnt % 3;
+						if (!firstval_opt)
+							firstval_opt = 3;
+					} else {
+						++opt_cnt;
+						++firstval_opt;
+					}
+
+					/* Exceed allowed encoding. */
+					if (firstval_opt >= firstval_max)
+						first = NULL;
+				} else {
+					opt = NULL;
+					opt_cnt = 0;
+					firstval_opt = 0;
+				}
+			}
+		}
+		if (!first) {
+			while (opt_cnt) {
+				first = rb_entry(opt, struct mrp_attr, node);
+				first->action = MRP_TX_ACTION_NONE;
+				opt = rb_next(opt);
+				--opt_cnt;
+			}
+			opt = NULL;
+			firstval_opt = 0;
+
+			/* Skip if not needed to send. */
+			if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action ||
+			    MRP_TX_ACTION_S_IN_OPTIONAL == attr->action) {
+				attr->action = MRP_TX_ACTION_NONE;
+				continue;
+			}
+			first = attr;
+			memcpy(firstval, attr->value, attr->len);
+			firstval_cnt = 1;
+
+			/* There is at least one attribute sending. */
+			last = NULL;
+		}
+	}
+
+	/* Need to send at least one for LeaveAll. */
+	if ((app->lva.tx & (1 << (attrtype - 1))) && last &&
+	    MRP_TX_ACTION_NONE == last->action)
+		last->action = last_action;
+}
+
+static u8 mrp_3pack_encode(u8 vect[], int *i)
+{
+	u8 val;
+
+	val = vect[0] * __MRP_VECATTR_EVENT_MAX * __MRP_VECATTR_EVENT_MAX;
+	val += vect[1] * __MRP_VECATTR_EVENT_MAX;
+	val += vect[2];
+	memset(vect, 0, 3);
+	*i = 0;
+	return val;
+}
+
+#ifdef CONFIG_KSZ_MSRP
+static u8 mrp_4pack_encode(u8 vect[])
+{
+	u8 val;
+
+	val = vect[0] * 64;
+	val += vect[1] * 16;
+	val += vect[2] * 4;
+	val += vect[3];
+	return val;
+}
+
+static void mrp_4pack_decode(u8 val, u8 vect[], int *i)
+{
+	vect[3] = val & 3;
+	val >>= 2;
+	vect[2] = val & 3;
+	val >>= 2;
+	vect[1] = val & 3;
+	val >>= 2;
+	vect[0] = val & 3;
+	*i = 0;
+}
+#endif
+
+static void mrp_encode_action(struct mrp_attr *attr, u8 *vectevt)
+{
+	switch (attr->action) {
+	case MRP_TX_ACTION_S_JOIN_IN:
+	case MRP_TX_ACTION_S_JOIN_IN_OPTIONAL:
+		*vectevt = MRP_REGISTRAR_IN == attr->reg_state ?
+			MRP_VECATTR_EVENT_JOIN_IN :
+			MRP_VECATTR_EVENT_JOIN_MT;
+		if (attr->fix_state != MRP_REGISTRAR_LV)
+			*vectevt = MRP_VECATTR_EVENT_JOIN_IN;
+		break;
+	case MRP_TX_ACTION_S_IN:
+	case MRP_TX_ACTION_S_IN_OPTIONAL:
+		*vectevt = MRP_REGISTRAR_IN == attr->reg_state ?
+			MRP_VECATTR_EVENT_IN :
+			MRP_VECATTR_EVENT_MT;
+		if (attr->fix_state != MRP_REGISTRAR_LV)
+			*vectevt = MRP_VECATTR_EVENT_IN;
+		break;
+	case MRP_TX_ACTION_S_NEW:
+		*vectevt = MRP_VECATTR_EVENT_NEW;
+		break;
+	case MRP_TX_ACTION_S_LV:
+		*vectevt = MRP_VECATTR_EVENT_LV;
+		break;
+	default:
+		break;
+	}
+}
+
+/* mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+#define MRP_MIN			(2 + 1 + 4)
+
+#define MVRP_VID_MIN		(2 + MRP_MIN)
+#define MMRP_SVC_MIN		(1 + MRP_MIN)
+#define MMRP_MAC_MIN		(6 + MRP_MIN)
+
+static int mrp_tx(struct mrp_applicant *app, u8 *msg_buf, u8 *msg_eof,
+	int *bytes, u8 attrtype, u8 attrlen, u8 attrmax, u8 msg_min)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	struct mrp_msg_hdr *mh;
+	struct mrp_vecattr_hdr *vah;
+	u8 firstval[20];
+	u8 vectevt[3];
+	int i = 0;
+	int len = 0;
+	int num_break = false;
+	int node_break = false;
+	u8 *msg_ptr = msg_buf;
+
+	*bytes = 0;
+
+	/* mrp_msg_hdr + mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+	if (msg_ptr > (msg_eof - (sizeof(struct mrp_msg_hdr) + msg_min)))
+		return -1;
+	memset(vectevt, 0, 3);
+
+	mrp_prepare_tx(app, attrtype, attrmax);
+
+	mh = (struct mrp_msg_hdr *) msg_ptr;
+	mh->attrtype = attrtype;
+	mh->attrlen = attrlen;
+	msg_ptr += sizeof(struct mrp_msg_hdr);
+
+	vah = (struct mrp_vecattr_hdr *) msg_ptr;
+	msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	node = app->last_node;
+	if (node) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attrtype != attr->type)
+			return 0;
+	} else
+		node = rb_first(&app->mad);
+	for (;
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+
+		/* Check if there is enough space for all required data. */
+		if (!len && msg_ptr > (msg_eof - msg_min)) {
+dbg_msg(" no space\n");
+			node_break = true;
+			break;
+		}
+
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attrtype != attr->type)
+			continue;
+
+		if (MRP_TX_ACTION_NONE == attr->action) {
+			if (mrp_applicant_chk(attr))
+				mrp_attr_destroy(app, attr);
+
+			/* In case there are events waiting. */
+			if (i)
+				i += 2;
+			goto val_end;
+		}
+		if (len) {
+			app->attrval_inc(firstval, attr->len);
+			if (memcmp(firstval, attr->value, attr->len)) {
+				/* Process with current node. */
+				next = node;
+				num_break = true;
+				if (i)
+					i += 2;
+				goto val_end;
+			}
+		}
+		if (!len) {
+			memcpy(firstval, attr->value, attr->len);
+			memcpy(vah->firstattrvalue, firstval, attr->len);
+			msg_ptr += attr->len;
+		}
+		mrp_encode_action(attr, &vectevt[i]);
+		if (mrp_applicant_chk(attr))
+			mrp_attr_destroy(app, attr);
+
+		++i;
+		++len;
+
+val_end:
+		if (len && i >= 3) {
+			*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+			if (msg_ptr > msg_eof - (1 + 4)) {
+				num_break = true;
+				node_break = true;
+			}
+		}
+		if (num_break) {
+			put_unaligned(cpu_to_be16(len), &vah->lenflags);
+			if (app->lva.tx & (1 << (attrtype - 1)))
+				vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+			len = 0;
+			num_break = false;
+
+			if (msg_ptr > (msg_eof - msg_min)) {
+				node_break = true;
+				break;
+			}
+
+			vah = (struct mrp_vecattr_hdr *) msg_ptr;
+			msg_ptr += sizeof(struct mrp_vecattr_hdr);
+		}
+		if (node_break)
+			break;
+	}
+	if (len) {
+		if (i)
+			*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+		put_unaligned(cpu_to_be16(len), &vah->lenflags);
+		if (app->lva.tx & (1 << (attrtype - 1)))
+			vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+
+	/* Need to send LeaveAll with zero number of attributes. */
+	} else if (app->lva.tx & (1 << (attrtype - 1)) && !node_break) {
+		memset(vah->firstattrvalue, 0, attrlen);
+		msg_ptr += attrlen;
+		put_unaligned(0, &vah->lenflags);
+		vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+	}
+	app->last_node = node;
+
+	if (msg_ptr == msg_buf +
+	    sizeof(struct mrp_msg_hdr) +
+	    sizeof(struct mrp_vecattr_hdr)) {
+		return 0;
+	}
+	*msg_ptr++ = MRP_END_MARK;
+	*msg_ptr++ = MRP_END_MARK;
+
+	*bytes = msg_ptr - msg_buf;
+
+	/* Not all attributes sent. */
+	if (node)
+		return -1;
+	return 0;
+}
+
+static void mrp_leave_timer_arm(struct mrp_applicant *app)
+{
+	unsigned long delay;
+
+	delay = mrp_leave_time;
+#if 1
+	if (mrp_10_1_2f_hack)
+		delay -= 200;
+	if (fqtss_hack)
+		delay = 10000;
+#endif
+
+	/* Extend the timer in case one is already scheduled. */
+	mod_timer(&app->leave_timer,
+		  jiffies + msecs_to_jiffies(delay));
+	app->timer_arm.leave = 1;
+}
+
+static int mrp_registrar_event(struct mrp_applicant *app, struct mrp_attr *attr,
+	enum mrp_event event)
+{
+	enum mrp_registrar_state state = attr->reg_state;
+	enum mrp_notification notify = MRP_NOTIFY_NONE;
+
+	switch (event) {
+	case MRP_EVENT_R_LV:
+	case MRP_EVENT_R_LA:
+	case MRP_EVENT_REDECLARE:
+		if (MRP_REGISTRAR_IN == attr->reg_state) {
+			mrp_leave_timer_arm(app);
+			state = MRP_REGISTRAR_LV;
+		}
+		break;
+	case MRP_EVENT_R_NEW:
+		state = MRP_REGISTRAR_IN;
+		notify = MRP_NOTIFY_NEW;
+		break;
+	case MRP_EVENT_R_JOIN_IN:
+	case MRP_EVENT_R_JOIN_MT:
+		if (MRP_REGISTRAR_MT == attr->reg_state) {
+			state = MRP_REGISTRAR_IN;
+			notify = MRP_NOTIFY_JOIN;
+		} else if (MRP_REGISTRAR_LV == attr->reg_state)
+			state = MRP_REGISTRAR_IN;
+		if (attr->changed)
+			notify = MRP_NOTIFY_JOIN;
+		break;
+	case MRP_EVENT_LV_TIMER:
+		if (MRP_REGISTRAR_LV == attr->reg_state) {
+			state = MRP_REGISTRAR_MT;
+			notify = MRP_NOTIFY_LV;
+		}
+		break;
+	case MRP_EVENT_FLUSH:
+		if (MRP_REGISTRAR_MT != attr->reg_state)
+			notify = MRP_NOTIFY_LV;
+		state = MRP_REGISTRAR_MT;
+		break;
+	case MRP_EVENT_R_MT:
+		break;
+	default:
+		break;
+	}
+#ifdef DBG_MRP_REG
+	if (state != attr->reg_state || (notify && notify != attr->notify)) {
+		char last_reg_str[10];
+		char reg_str[10];
+		u8 len = app->attr_len(attr->type, attr->len);
+		u8 type = app->attr_type(attr);
+
+		format_reg_state(reg_str, state);
+		format_reg_state(last_reg_str, attr->reg_state);
+		if (notify != attr->notify && notify)
+dbg_msg(" ==>");
+dbg_msg(" reg_event p:%d [%d=%d:%02x] %s:%s %d\n", app->port, app->app->type,
+type, attr->value[len - 1],
+	last_reg_str, reg_str, notify);
+	}
+#endif
+
+	/* Either fixed IN or MT */
+	if (attr->fix_state != MRP_REGISTRAR_LV) {
+		state = attr->fix_state;
+		notify = MRP_NOTIFY_NONE;
+	}
+	attr->reg_state = state;
+	attr->notify = notify;
+	attr->changed = 0;
+	return notify != MRP_NOTIFY_NONE;
+}
+
+static int mrp_attr_event(struct mrp_applicant *app,
+			   struct mrp_attr *attr, enum mrp_event event)
+{
+	enum mrp_applicant_state state;
+	int tx = false;
+	int tx_req = false;
+#ifdef DBG_MRP_APP
+	char ev_str[10];
+	char last_app_str[10];
+	char last_new_str[10];
+	char app_str[10];
+	char new_str[10];
+	char act_str[10];
+	enum mrp_applicant_state last_state = attr->state;
+	enum mrp_tx_action last_action = attr->action;
+
+	format_event(ev_str, event);
+	format_app_state(last_new_str, attr->new_state);
+	format_app_state(last_app_str, attr->state);
+#endif
+
+	/* Do not change state if already sent. */
+	if (MRP_EVENT_TX_LAF == event && MRP_TX_ACTION_NONE == attr->action)
+		return tx_req;
+
+	/* Note #4. */
+	if (MRP_EVENT_R_JOIN_IN == event) {
+		if ((MRP_APPLICANT_VO == attr->state ||
+		    MRP_APPLICANT_VP == attr->state) && app->p2p_mac)
+			goto registrar;
+
+	/* Note #5. */
+	} else if (MRP_EVENT_R_IN == event) {
+		if (MRP_APPLICANT_AA == attr->state && !app->p2p_mac)
+			goto registrar;
+	}
+
+	state = mrp_applicant_state_table[attr->state][event];
+	if (state == MRP_APPLICANT_INVALID) {
+		WARN_ON(1);
+		return tx_req;
+	}
+
+	/* Note #8. */
+	if (MRP_APPLICANT_AN == attr->state && MRP_EVENT_TX == event &&
+	    MRP_REGISTRAR_IN != attr->reg_state)
+		state = MRP_APPLICANT_AA;
+
+	if (event == MRP_EVENT_TX) {
+		attr->action = mrp_tx_action_table[attr->state];
+		tx = true;
+	}
+	if (event == MRP_EVENT_TX_LA) {
+		attr->action = mrp_tx_la_action_table[attr->state];
+		tx = true;
+	}
+#ifdef MRP_BASIC
+	if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action)
+		attr->action = MRP_TX_ACTION_S_JOIN_IN;
+	else if (MRP_TX_ACTION_S_IN_OPTIONAL == attr->action)
+		attr->action = MRP_TX_ACTION_S_IN;
+#endif
+	if (state != attr->state) {
+		switch (state) {
+		case MRP_APPLICANT_VN:
+		case MRP_APPLICANT_AN:
+		case MRP_APPLICANT_AA:
+		case MRP_APPLICANT_LA:
+		case MRP_APPLICANT_VP:
+		case MRP_APPLICANT_AP:
+		case MRP_APPLICANT_LO:
+#if 0
+if (MRP_EVENT_R_MT == event)
+dbg_msg("<%d:%d>", attr->state, state);
+#endif
+
+			/* Note #6. */
+			tx_req = true;
+			break;
+		case MRP_APPLICANT_VO:
+
+			/* Attribute is not being sent. */
+			if (!tx) {
+				if (!attr->aging)
+					attr->aging = 3;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+	if (tx && !tx_req &&
+	    (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action ||
+	    MRP_TX_ACTION_S_IN_OPTIONAL == attr->action))
+		tx = false;
+
+	/* Update state after attribute is sent. */
+	attr->new_state = state;
+	if (!tx)
+		attr->state = state;
+#ifdef DBG_MRP_APP
+	if (last_state != state || last_action != attr->action) {
+		u8 len = app->attr_len(attr->type, attr->len);
+		u8 type = app->attr_type(attr);
+
+		format_app_state(new_str, attr->new_state);
+		format_app_state(app_str, attr->state);
+		format_action(act_str, attr->action);
+if (MRP_EVENT_R_LV == event)
+dbg_msg(" --> ");
+else if (app->rx)
+dbg_msg(" -> ");
+dbg_msg("attr_event p:%d [%d=%d:%02x] %s:%s %s ", app->port, app->app->type,
+type, attr->value[len - 1],
+last_app_str, last_new_str, ev_str);
+dbg_msg(" %s:%s %s %d\n", app_str, new_str, act_str, tx_req);
+	}
+#endif
+
+registrar:
+	switch (event) {
+	case MRP_EVENT_R_NEW:
+	case MRP_EVENT_R_JOIN_IN:
+	case MRP_EVENT_R_IN:
+	case MRP_EVENT_R_JOIN_MT:
+	case MRP_EVENT_R_MT:
+	case MRP_EVENT_R_LV:
+	case MRP_EVENT_R_LA:
+	case MRP_EVENT_REDECLARE:
+	case MRP_EVENT_LV_TIMER:
+	case MRP_EVENT_LVA_TIMER:
+	case MRP_EVENT_FLUSH:
+		if (mrp_registrar_event(app, attr, event))
+			app->acton(app, attr);
+		break;
+	default:
+		break;
+	}
+
+	/* Attribute has been changed.  Need to send. */
+	if (!app->rx && attr->changed) {
+		attr->changed = 0;
+		tx_req |= 1;
+	}
+
+	return tx_req;
+}
+
+#if 1
+static int max_lva;
+#endif
+
+static void mrp_lva_timer_arm(struct mrp_applicant *app)
+{
+	unsigned long delay;
+	unsigned long fixed;
+	unsigned long random;
+
+	/* Extend the timer in case one is already scheduled. */
+	/* LeaveAll timer can be 1.5 longer. */
+	app->timer_arm.lva = 1;
+	random = mrp_lva_time / 2;
+	fixed = msecs_to_jiffies(mrp_lva_time);
+	delay = (u64)msecs_to_jiffies(random) * prandom_u32() >> 32;
+	delay += fixed;
+#if 1
+	if (max_lva) {
+		delay = 3000;
+		max_lva = 0;
+	}
+#endif
+	mod_timer(&app->lva_timer, jiffies + delay);
+#if 0
+if (app->port < 3 && app->app->type == 2)
+dbg_msg("  s lva: %d=%lu %lu\n", app->port, delay, jiffies);
+#endif
+}
+
+static int mrp_lva_timer_event(struct mrp_applicant *app, enum mrp_event event)
+{
+	enum mrp_timer_state state = app->lva.state;
+	int tx_lva = 0;
+	int tx_req = false;
+
+	switch (event) {
+	case MRP_EVENT_TX:
+		if (MRP_TIMER_ACTIVE == state) {
+			tx_lva = 0xf;
+			state = MRP_TIMER_PASSIVE;
+		}
+		break;
+	case MRP_EVENT_R_LA:
+		state = MRP_TIMER_PASSIVE;
+#if 0
+if (app->port < 3 && app->app->type == 2) {
+dbg_msg("  rla  ");
+}
+#endif
+#if 1
+		if (fqtss_34_2_3_hack)
+			max_lva = 1;
+#endif
+		mrp_lva_timer_arm(app);
+		break;
+	case MRP_EVENT_LVA_TIMER:
+		state = MRP_TIMER_ACTIVE;
+#if 0
+if (app->port < 3 && app->app->type == 2)
+dbg_msg("  timer  ");
+#endif
+		mrp_lva_timer_arm(app);
+		break;
+	default:
+		return tx_req;
+	}
+	if (state != app->lva.state && MRP_TIMER_ACTIVE == state)
+		tx_req = true;
+	app->lva.state = state;
+#if 0
+if (app->lva.tx && app->port < 2)
+dbg_msg(" lva: %d=%x %x\n", app->port, app->lva.tx, tx_lva);
+#endif
+#if 0
+if (!app->lva.tx)
+#endif
+	app->lva.tx = tx_lva;
+	return tx_req;
+}
+
+static void mrp_periodic_timer_arm(struct mrp_applicant *app)
+{
+	/* Ignore if one is already scheduled. */
+	if (app->timer_arm.periodic)
+		return;
+	mod_timer(&app->periodic_timer,
+		  jiffies + msecs_to_jiffies(mrp_periodic_time));
+	app->timer_arm.periodic = 1;
+}
+
+static void mrp_periodic_event(struct mrp_applicant *app, enum mrp_event event)
+{
+	enum mrp_timer_state state = app->periodic.state;
+
+	switch (event) {
+	case MRP_EVENT_PERIODIC:
+		if (MRP_TIMER_ACTIVE == state)
+			mrp_periodic_timer_arm(app);
+		break;
+	case MRP_EVENT_PERIODIC_DISABLE:
+		state = MRP_TIMER_PASSIVE;
+		break;
+	case MRP_EVENT_PERIODIC_ENABLE:
+		if (MRP_TIMER_PASSIVE == state) {
+			state = MRP_TIMER_ACTIVE;
+			mrp_periodic_timer_arm(app);
+		}
+	default:
+		break;
+	}
+	app->periodic.state = state;
+}
+
+static void mrp_join_timer_arm(struct mrp_applicant *app)
+{
+	unsigned long delay;
+	unsigned long fixed;
+	unsigned long random;
+
+	/* Ignore if one is already scheduled. */
+	if (app->timer_arm.join)
+		return;
+	app->timer_arm.join = 1;
+	random = mrp_join_time / 10;
+	fixed = msecs_to_jiffies(mrp_join_time);
+	delay = (u64)msecs_to_jiffies(random) * prandom_u32() >> 32;
+	delay += fixed;
+	mod_timer(&app->join_timer, jiffies + delay);
+}
+
+static void mrp_mad_event(struct mrp_applicant *app, enum mrp_event event)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	enum mrp_event tx_event;
+	int tx_req = 0;
+
+	/* Flush! means leavealltimer!. */
+	if (MRP_EVENT_FLUSH == event)
+		tx_req |= mrp_lva_timer_event(app, MRP_EVENT_LVA_TIMER);
+
+	/* Do this first so that tx! becomes txLA! if LeaveAll is indicated. */
+	if (MRP_EVENT_TX == event ||
+	    (MRP_EVENT_R_LA == event && !app->lva.rx) ||
+	    MRP_EVENT_LVA_TIMER == event)
+		tx_req |= mrp_lva_timer_event(app, event);
+#if 1
+	if (MRP_EVENT_TX == event && app->lva.tx)
+		event = MRP_EVENT_TX_LA;
+#endif
+#if 0
+/*
+ * Do this first or after txLAF! ?
+ * If first the registrar values will always be empty, prompting the other to
+ * send back the values.  On the other hand, the receiving side can flush the
+ * registrar value and get the current one immediately without waiting for
+ * another transmit.
+ */
+
+	if (MRP_EVENT_TX_LA == event) {
+		for (node = rb_first(&app->mad);
+		     next = node ? rb_next(node) : NULL, node != NULL;
+		     node = next) {
+			attr = rb_entry(node, struct mrp_attr, node);
+			tx_req |= mrp_attr_event(app, attr, MRP_EVENT_R_LA);
+		}
+	}
+#endif
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+
+		/* Act on specific attribute type. */
+		if (app->lva_type) {
+			u8 attrtype = app->attr_type(attr);
+
+			if (attrtype != app->lva_type)
+				continue;
+		}
+		tx_event = event;
+#if 0
+		if (MRP_EVENT_TX == event && app->lva.tx) {
+			u8 attrtype = app->attr_type(attr);
+
+			if (app->lva.tx & (attrtype - 1))
+				tx_event = MRP_EVENT_TX_LA;
+		}
+#endif
+		tx_req |= mrp_attr_event(app, attr, tx_event);
+	}
+	if (MRP_EVENT_PERIODIC == event)
+		mrp_periodic_event(app, MRP_EVENT_PERIODIC);
+	if (MRP_EVENT_TX == event || MRP_EVENT_TX_LA == event) {
+		int rc;
+
+		event = 0;
+#if 1
+		app->lva.rx = app->lva.tx;
+#endif
+		rc = app->txpdu(app);
+
+		/* Not all attributes sent. */
+		if (app->last_node) {
+			if (app->lva.tx)
+				event = MRP_EVENT_TX_LAF;
+			else
+				tx_req |= 2;
+		} else if (app->lva.rx) {
+#if 1
+			event = MRP_EVENT_R_LA;
+#endif
+		}
+		if (event) {
+			for (node = rb_first(&app->mad);
+			     next = node ? rb_next(node) : NULL, node != NULL;
+			     node = next) {
+				attr = rb_entry(node, struct mrp_attr, node);
+				tx_req |= mrp_attr_event(app, attr, event);
+			}
+		}
+	}
+	if (tx_req) {
+		mrp_join_timer_arm(app);
+
+		/* Not continuing previous tx!. */
+		if (!(tx_req & 2))
+			app->last_node = NULL;
+	}
+}
+
+static void mrp_join_timer_exec(struct mrp_applicant *app)
+{
+	spin_lock(&app->lock);
+	app->timer_arm.join = 0;
+	mrp_mad_event(app, MRP_EVENT_TX);
+	spin_unlock(&app->lock);
+}
+
+static void mrp_join_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, join_work);
+
+	mrp_join_timer_exec(app);
+}
+
+static void mrp_join_timer(unsigned long data)
+{
+	struct mrp_applicant *app = (struct mrp_applicant *)data;
+
+	/* Timer is running in interrupt context. */
+	schedule_work(&app->join_work);
+}
+
+static void mrp_periodic_timer_exec(struct mrp_applicant *app)
+{
+	spin_lock(&app->lock);
+	app->timer_arm.periodic = 0;
+	mrp_mad_event(app, MRP_EVENT_PERIODIC);
+	spin_unlock(&app->lock);
+}
+
+static void mrp_periodic_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, periodic_work);
+
+	mrp_periodic_timer_exec(app);
+}
+
+static void mrp_periodic_timer(unsigned long data)
+{
+	struct mrp_applicant *app = (struct mrp_applicant *)data;
+
+	/* Periodic timer is disabled. */
+	if (MRP_TIMER_PASSIVE == app->periodic.state) {
+		app->timer_arm.periodic = 0;
+		return;
+	}
+
+	/* Timer is running in interrupt context.  Not necessary. */
+	schedule_work(&app->periodic_work);
+}
+
+static void mrp_leave_timer_exec(struct mrp_applicant *app)
+{
+	spin_lock(&app->lock);
+	app->timer_arm.leave = 0;
+	mrp_mad_event(app, MRP_EVENT_LV_TIMER);
+	spin_unlock(&app->lock);
+	app->cleanup(app);
+}
+
+static void mrp_leave_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, leave_work);
+
+	mrp_leave_timer_exec(app);
+}
+
+static void mrp_leave_timer(unsigned long data)
+{
+	struct mrp_applicant *app = (struct mrp_applicant *)data;
+
+	/* Timer is running in interrupt context.. */
+	schedule_work(&app->leave_work);
+}
+
+static void mrp_lva_timer_exec(struct mrp_applicant *app)
+{
+	int tx_req;
+
+	spin_lock(&app->lock);
+	app->timer_arm.lva = 0;
+#if 0
+if (app->port < 3 && app->app->type == 2) {
+dbg_msg(" lva: %d=%lu %lu\n", app->port, jiffies - app->rla_jiffies, jiffies);
+app->rla_jiffies = jiffies;
+}
+#endif
+	tx_req = mrp_lva_timer_event(app, MRP_EVENT_LVA_TIMER);
+	if (tx_req)
+		mrp_join_timer_arm(app);
+	spin_unlock(&app->lock);
+}
+
+static void mrp_lva_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, lva_work);
+
+	mrp_lva_timer_exec(app);
+}
+
+static void mrp_lva_timer(unsigned long data)
+{
+	struct mrp_applicant *app = (struct mrp_applicant *)data;
+
+	/* Timer is running in interrupt context.  Not necessary. */
+	schedule_work(&app->lva_work);
+}
+
+static void mrp_update_attr(struct mrp_applicant *app, u8 *attrvalue,
+	u8 attrlen, u8 attrtype, u8 vaevent)
+{
+	enum mrp_event event;
+	struct mrp_attr *attr;
+
+	switch (vaevent) {
+	case MRP_VECATTR_EVENT_NEW:
+		event = MRP_EVENT_R_NEW;
+		break;
+	case MRP_VECATTR_EVENT_JOIN_IN:
+		event = MRP_EVENT_R_JOIN_IN;
+		break;
+	case MRP_VECATTR_EVENT_IN:
+		event = MRP_EVENT_R_IN;
+		break;
+	case MRP_VECATTR_EVENT_JOIN_MT:
+		event = MRP_EVENT_R_JOIN_MT;
+		break;
+	case MRP_VECATTR_EVENT_MT:
+		event = MRP_EVENT_R_MT;
+		break;
+	case MRP_VECATTR_EVENT_LV:
+		event = MRP_EVENT_R_LV;
+		break;
+	default:
+		return;
+	}
+#ifdef DBG_MRP_
+{
+	char ev_str[10];
+	u8 len = app->attr_len(attrtype, attrlen);
+
+	format_event(ev_str, event);
+dbg_msg("> ");
+#if 1
+dbg_msg("%s p:%d [%d=%d:%02x] %s\n", __func__, app->port, app->app->type,
+attrtype, attrvalue[len - 1], ev_str);
+#endif
+}
+#endif
+	attr = mrp_attr_lookup(app, attrvalue, attrlen, attrtype);
+	if (attr == NULL) {
+
+		/* No need to create an attribute. */
+		if (MRP_VECATTR_EVENT_MT == vaevent ||
+		    MRP_VECATTR_EVENT_LV == vaevent)
+{
+#if 1
+if (MRP_VECATTR_EVENT_LV == vaevent)
+dbg_msg(" not create\n");
+#endif
+			return;
+}
+		attr = mrp_attr_create(app, attrvalue, attrlen, attrtype,
+				       app->attr_size(attrtype, attrlen));
+		if (!attr)
+			return;
+	}
+
+#if 1
+	/* AVnu test tool sends Lv and does not expect receiving talker
+	 * declaration in MSRP.c.35.4.5b.
+	 */
+	/* MSRP does not follow standard MRP operation.
+	 * Ignore rLv! if not registered.
+	 */
+	if (event == MRP_EVENT_R_LV &&
+	    app->app->type == MRP_APPLICATION_MSRP &&
+	    (attr->fix_state != MRP_REGISTRAR_LV ||
+	    attr->reg_state != MRP_REGISTRAR_IN))
+		return;
+#endif
+
+	if (app->attr_upd) {
+		attr->changed = app->attr_upd(attrvalue, attr, false);
+	}
+
+	app->rx = 1;
+	if (mrp_attr_event(app, attr, event))
+		mrp_join_timer_arm(app);
+	app->rx = 0;
+}
+
+static int mrp_parse_end_mark(u8 **msg_ptr, u8 *msg_eof)
+{
+	u8 *data = *msg_ptr;
+
+	if (*msg_ptr > (msg_eof - 2))
+{
+dbg_msg(" e1 %02x %02x; %p %p\n", data[0], data[1], *msg_ptr, msg_eof);
+		return -2;
+}
+
+	/* End mark can terminte list. */
+	if (MRP_END_MARK == data[0] && MRP_END_MARK == data[1]) {
+		*msg_ptr += 2;
+		return 0;
+	}
+
+	/* Keep parsing. */
+	return 1;
+}
+
+static int mrp_parse_events(struct mrp_applicant *app,
+	u8 **msg_ptr, u8 *msg_eof, u8 attrlen, u8 attrtype)
+{
+	struct mrp_vecattr_hdr *vah;
+	u16 valen;
+	u16 LA;
+	u8 firstval[20];
+	u8 vaevent;
+	u8 vaevents;
+
+	vah = (struct mrp_vecattr_hdr *) *msg_ptr;
+	valen = get_unaligned(&vah->lenflags);
+	LA = valen & ~MRP_VECATTR_HDR_LEN_MASK;
+
+	/* Only LeaveAll is understood in this version. */
+	if (app->ver_diff <= 0 && LA && LA != MRP_VECATTR_HDR_FLAG_LA)
+		return -1;
+	LA &= MRP_VECATTR_HDR_FLAG_LA;
+	valen = be16_to_cpu(valen & MRP_VECATTR_HDR_LEN_MASK);
+
+	*msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	memcpy(firstval, vah->firstattrvalue, attrlen);
+	*msg_ptr += attrlen;
+
+	if (*msg_ptr + (valen + 2) / 3 > msg_eof)
+		return -1;
+
+	/* Do it once for each attribute type. */
+	if (!app->dry_run && LA && !(app->lva.rx & (1 << attrtype))) {
+		app->lva_type = attrtype;
+		mrp_mad_event(app, MRP_EVENT_R_LA);
+		app->lva_type = 0;
+		app->lva.rx |= (1 << attrtype);
+	}
+
+	/* In a VectorAttribute, the Vector contains events which are packed
+	 * three to a byte. We process one byte of the Vector at a time.
+	 */
+	while (valen > 0) {
+		vaevents = **msg_ptr;
+		*msg_ptr += sizeof(vaevents);
+
+		/* Extract and process the first event. */
+		vaevent = vaevents / (__MRP_VECATTR_EVENT_MAX *
+				      __MRP_VECATTR_EVENT_MAX);
+		if (vaevent >= __MRP_VECATTR_EVENT_MAX) {
+			/* The byte is malformed; stop processing. */
+			return -2;
+		}
+		if (!app->dry_run && app->attr_valid(attrtype, firstval))
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the second event. */
+		if (!--valen)
+			break;
+		vaevents %= (__MRP_VECATTR_EVENT_MAX *
+			     __MRP_VECATTR_EVENT_MAX);
+		vaevent = vaevents / __MRP_VECATTR_EVENT_MAX;
+		app->attrval_inc(firstval, attrlen);
+		if (!app->dry_run && app->attr_valid(attrtype, firstval))
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the third event. */
+		if (!--valen)
+			break;
+		vaevents %= __MRP_VECATTR_EVENT_MAX;
+		vaevent = vaevents;
+		app->attrval_inc(firstval, attrlen);
+		if (!app->dry_run && app->attr_valid(attrtype, firstval))
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+		app->attrval_inc(firstval, attrlen);
+		--valen;
+	}
+	return 0;
+}
+
+static int mrp_parse_msg(struct mrp_applicant *app, u8 **msg_ptr, u8 *msg_eof)
+{
+	struct mrp_msg_hdr *mh;
+	int rc;
+
+	/*
+	 * End mark terminates the message.  Happen when there are filler
+	 * bytes in the frame.
+	 */
+	mh = (struct mrp_msg_hdr *) *msg_ptr;
+	if (MRP_END_MARK == mh->attrtype &&
+	    MRP_END_MARK == mh->attrlen) {
+		return -1;
+	}
+
+	/* Check whether the attribute type is understood. */
+	rc = app->attr_chk(mh->attrtype, mh->attrlen);
+	if (rc < 0)
+		return -1;
+
+	/* Attribute not understood but not in future protocol. */
+	if (rc && (app->ver_diff <= 0 || !app->msg_cnt))
+		return -1;
+
+	*msg_ptr += sizeof(struct mrp_msg_hdr);
+	while (*msg_ptr < (msg_eof - 3)) {
+		rc = mrp_parse_events(app, msg_ptr, msg_eof,
+			mh->attrlen, mh->attrtype);
+		if (rc < 0)
+			return rc;
+
+		/* End mark terminates the vector list. */
+		rc = mrp_parse_end_mark(msg_ptr, msg_eof);
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int mrp_rx(struct mrp_applicant *app, u8 *data, int len)
+{
+	struct mrp_pdu_hdr *ph;
+	u8 *msg_ptr;
+	u8 *msg_eof;
+	int rc;
+
+	ph = (struct mrp_pdu_hdr *) data;
+	app->ver_diff = ph->version - app->app->version;
+	app->msg_cnt = 0;
+
+	msg_ptr = (u8 *)(ph + 1);
+	msg_eof = msg_ptr + len - sizeof(struct mrp_pdu_hdr);
+
+	while (msg_ptr < (msg_eof - 2)) {
+
+		/* rLA! applies to the attribute type declared. */
+		app->lva.rx = 0;
+
+		rc = mrp_parse_msg(app, &msg_ptr, msg_eof);
+		if (rc < 0) {
+			if (app->dry_run && rc == -2)
+				rc = 0;
+			return rc;
+		}
+		app->msg_cnt++;
+
+		/* End mark terminates the frame. */
+		rc = mrp_parse_end_mark(&msg_ptr, msg_eof);
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int mrp_txpdu(struct mrp_applicant *app, u8 *msg_buf, u8 *msg_ptr,
+	u8 *msg_eof)
+{
+	/* Have something to send. */
+	if (msg_ptr != msg_buf) {
+		int rc;
+#ifndef DBG_MRP_RX
+		struct mrp_info *mrp = app->parent;
+#endif
+
+		/* Reset txLA flags if not txLAF. */
+		if (!app->last_node)
+			app->lva.tx = 0;
+		if (msg_ptr <= (msg_eof - 2)) {
+			*msg_ptr++ = MRP_END_MARK;
+			*msg_ptr++ = MRP_END_MARK;
+		}
+		skb_put(app->pdu, msg_ptr - msg_buf);
+		dev_hard_header(app->pdu, app->dev,
+			ntohs(app->app->pkttype.type),
+			app->group_address, app->src_addr,
+			app->pdu->len);
+#if 0
+		if (app->app->type == 2 && 1 == app->port && app->rla_jiffies) {
+if (jiffies - app->rla_jiffies > 20)
+dbg_msg("p:%d %d %lu: \n", app->port, app->app->type, jiffies - app->rla_jiffies);
+app->rla_jiffies = 0;
+		}
+#endif
+#ifdef DBG_MRP_TX
+		for (rc = 0; rc < app->pdu->len; rc++) {
+			dbg_msg("%02x ", app->pdu->data[rc]);
+			if ((rc % 16) == 15)
+				dbg_msg("\n");
+		}
+		if ((rc % 16))
+			dbg_msg("\n");
+#endif
+#ifdef DBG_MRP_RX
+		app->rxpdu(app, &app->pdu->data[14], app->pdu->len - 14);
+		kfree_skb(app->pdu);
+#else
+#ifdef MRP_PASSTHRU
+		kfree_skb(app->pdu);
+#else
+		rc = proc_mrp_xmit(mrp, app->port, app->pdu);
+#endif
+#endif
+		app->pdu = NULL;
+	} else {
+		kfree_skb(app->pdu);
+		app->pdu = NULL;
+	}
+	return 0;
+}
+
+static int mrp_req_new(struct mrp_applicant *app,
+	const void *value, u8 len, u8 type, u8 event)
+{
+	struct mrp_attr *attr;
+
+	spin_lock_bh(&app->lock);
+
+	/* Create automatically returns found attribute. */
+	attr = mrp_attr_create(app, value, len, type,
+			       app->attr_size(type, len));
+	if (!attr) {
+		spin_unlock_bh(&app->lock);
+		return -ENOMEM;
+	}
+
+	/* Some data in the attribute can be changed. */
+	if (app->attr_upd)
+		attr->changed = app->attr_upd(value, attr, true);
+	if (mrp_attr_event(app, attr, event))
+		mrp_join_timer_arm(app);
+	spin_unlock_bh(&app->lock);
+	return 0;
+}
+
+static int mrp_req_leave(struct mrp_applicant *app,
+	const void *value, u8 len, u8 type)
+{
+	struct mrp_attr *attr;
+	int rc;
+
+	spin_lock_bh(&app->lock);
+	attr = mrp_attr_lookup(app, value, len, type);
+	if (!attr) {
+		spin_unlock_bh(&app->lock);
+		return 0;
+	}
+	rc = mrp_attr_event(app, attr, MRP_EVENT_LV);
+	if (rc)
+		mrp_join_timer_arm(app);
+	spin_unlock_bh(&app->lock);
+	return rc;
+}
+
+static void mrp_req_set(struct mrp_applicant *app,
+	const void *value, u8 len, u8 type, enum mrp_registrar_state state)
+{
+	struct mrp_attr *attr;
+
+	spin_lock_bh(&app->lock);
+
+	/* Create automatically returns found attribute. */
+	attr = mrp_attr_create(app, value, len, type,
+			       app->attr_size(type, len));
+	if (!attr) {
+		spin_unlock_bh(&app->lock);
+		return;
+	}
+	if (attr->fix_state == state) {
+		spin_unlock_bh(&app->lock);
+		return;
+	}
+	attr->fix_state = state;
+	if (state != MRP_REGISTRAR_LV) {
+		attr->reg_state = state;
+		attr->notify = (state == MRP_REGISTRAR_IN) ?
+			MRP_NOTIFY_NEW : MRP_NOTIFY_LV;
+	} else if (MRP_APPLICANT_VO == attr->state)
+		attr->aging = 2;
+	spin_unlock_bh(&app->lock);
+	if (state != MRP_REGISTRAR_LV)
+		app->acton(app, attr);
+}
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+static struct rnd_state rnd;
+#endif
+
+static int mrp_init_applicant(struct mrp_port *port, void *mrp, u8 num,
+			      struct net_device *dev,
+			      struct mrp_application *appl)
+{
+	struct mrp_applicant *app;
+	int err;
+
+	err = -ENOMEM;
+	app = kzalloc(sizeof(*app), GFP_KERNEL);
+	if (!app)
+		goto err2;
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+	app->rnd = &rnd;
+#endif
+	app->parent = mrp;
+	app->port = num;
+	app->dev = dev;
+	app->app = appl;
+	app->mad = RB_ROOT;
+	app->p2p_mac = 1;
+	app->group_address = appl->group_address;
+	inc_mac_addr(app->src_addr, dev->dev_addr, 0);
+#if 0
+	inc_mac_addr(app->src_addr, dev->dev_addr, num + 1);
+#endif
+	spin_lock_init(&app->lock);
+	skb_queue_head_init(&app->queue);
+	rcu_assign_pointer(port->applicants[appl->type], app);
+	INIT_WORK(&app->join_work, mrp_join_timer_work);
+	setup_timer(&app->join_timer, mrp_join_timer, (unsigned long)app);
+
+	INIT_WORK(&app->periodic_work, mrp_periodic_timer_work);
+	setup_timer(&app->periodic_timer, mrp_periodic_timer,
+		    (unsigned long)app);
+	app->periodic.state = MRP_TIMER_PASSIVE;
+
+	INIT_WORK(&app->leave_work, mrp_leave_timer_work);
+	setup_timer(&app->leave_timer, mrp_leave_timer, (unsigned long)app);
+
+	INIT_WORK(&app->lva_work, mrp_lva_timer_work);
+	setup_timer(&app->lva_timer, mrp_lva_timer, (unsigned long)app);
+	app->lva.state = MRP_TIMER_PASSIVE;
+	mrp_lva_timer_arm(app);
+	return 0;
+
+err2:
+	return err;
+}
+
+static void mrp_uninit_applicant(struct mrp_port *port,
+	struct mrp_application *appl)
+{
+	struct mrp_applicant *app = rtnl_dereference(
+		port->applicants[appl->type]);
+
+	RCU_INIT_POINTER(port->applicants[appl->type], NULL);
+
+	/* Delete timer and generate a final TX event to flush out
+	 * all pending messages before the applicant is gone.
+	 */
+
+	/* TX event actually may arm several timers. */
+	spin_lock_bh(&app->lock);
+	mrp_mad_event(app, MRP_EVENT_TX);
+	spin_unlock_bh(&app->lock);
+
+	del_timer_sync(&app->join_timer);
+	del_timer_sync(&app->periodic_timer);
+	del_timer_sync(&app->leave_timer);
+	del_timer_sync(&app->lva_timer);
+	flush_work(&app->join_work);
+	flush_work(&app->periodic_work);
+	flush_work(&app->leave_work);
+	flush_work(&app->lva_work);
+
+	kfree_rcu(app, rcu);
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/mrp.h b/drivers/net/ethernet/micrel/ksz9897/mrp.h
new file mode 100644
index 0000000..9181dac
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/mrp.h
@@ -0,0 +1,226 @@
+#ifndef _NET_MRP_H
+#define _NET_MRP_H
+
+#define MRP_END_MARK		0x0
+
+struct mrp_pdu_hdr {
+	u8	version;
+};
+
+struct mrp_msg_hdr {
+	u8	attrtype;
+	u8	attrlen;
+};
+
+struct mrp_vecattr_hdr {
+	__be16	lenflags;
+	unsigned char	firstattrvalue[];
+#define MRP_VECATTR_HDR_LEN_MASK cpu_to_be16(0x1FFF)
+#define MRP_VECATTR_HDR_FLAG_LA cpu_to_be16(0x2000)
+};
+
+enum mrp_vecattr_event {
+	MRP_VECATTR_EVENT_NEW,
+	MRP_VECATTR_EVENT_JOIN_IN,
+	MRP_VECATTR_EVENT_IN,
+	MRP_VECATTR_EVENT_JOIN_MT,
+	MRP_VECATTR_EVENT_MT,
+	MRP_VECATTR_EVENT_LV,
+	__MRP_VECATTR_EVENT_MAX
+};
+
+struct mrp_skb_cb {
+	struct mrp_msg_hdr	*mh;
+	struct mrp_vecattr_hdr	*vah;
+	unsigned char		attrvalue[];
+};
+
+static inline struct mrp_skb_cb *mrp_cb(struct sk_buff *skb)
+{
+	BUILD_BUG_ON(sizeof(struct mrp_skb_cb) >
+		     FIELD_SIZEOF(struct sk_buff, cb));
+	return (struct mrp_skb_cb *)skb->cb;
+}
+
+enum mrp_applicant_state {
+	MRP_APPLICANT_INVALID,
+	MRP_APPLICANT_VO,
+	MRP_APPLICANT_VP,
+	MRP_APPLICANT_VN,
+	MRP_APPLICANT_AN,
+	MRP_APPLICANT_AA,
+	MRP_APPLICANT_QA,
+	MRP_APPLICANT_LA,
+	MRP_APPLICANT_AO,
+	MRP_APPLICANT_QO,
+	MRP_APPLICANT_AP,
+	MRP_APPLICANT_QP,
+	MRP_APPLICANT_LO,
+	__MRP_APPLICANT_MAX
+};
+#define MRP_APPLICANT_MAX	(__MRP_APPLICANT_MAX - 1)
+
+enum mrp_registrar_state {
+	MRP_REGISTRAR_INVALID,
+	MRP_REGISTRAR_MT,
+	MRP_REGISTRAR_IN,
+	MRP_REGISTRAR_LV,
+	__MRP_REGISTRAR_MAX
+};
+#define MRP_REGISTRAR_MAX	(__MRP_REGISTRAR_MAX - 1)
+
+enum mrp_event {
+	MRP_EVENT_NEW,
+	MRP_EVENT_JOIN,
+	MRP_EVENT_LV,
+	MRP_EVENT_TX,
+	MRP_EVENT_TX_LA,
+	MRP_EVENT_TX_LAF,
+	MRP_EVENT_R_NEW,
+	MRP_EVENT_R_JOIN_IN,
+	MRP_EVENT_R_IN,
+	MRP_EVENT_R_JOIN_MT,
+	MRP_EVENT_R_MT,
+	MRP_EVENT_R_LV,
+	MRP_EVENT_R_LA,
+	MRP_EVENT_REDECLARE,
+	MRP_EVENT_PERIODIC,
+	MRP_EVENT_PERIODIC_DISABLE,
+	MRP_EVENT_PERIODIC_ENABLE,
+	MRP_EVENT_LV_TIMER,
+	MRP_EVENT_LVA_TIMER,
+	MRP_EVENT_FLUSH,
+	__MRP_EVENT_MAX
+};
+#define MRP_EVENT_MAX		(__MRP_EVENT_MAX - 1)
+
+enum mrp_tx_action {
+	MRP_TX_ACTION_NONE,
+	MRP_TX_ACTION_S_NEW,
+	MRP_TX_ACTION_S_JOIN_IN,
+	MRP_TX_ACTION_S_JOIN_IN_OPTIONAL,
+	MRP_TX_ACTION_S_IN,
+	MRP_TX_ACTION_S_IN_OPTIONAL,
+	MRP_TX_ACTION_S_LV,
+};
+
+enum mrp_notification {
+	MRP_NOTIFY_NONE,
+	MRP_NOTIFY_NEW,
+	MRP_NOTIFY_JOIN,
+	MRP_NOTIFY_LV,
+};
+
+enum mrp_timer_state {
+	MRP_TIMER_PASSIVE,
+	MRP_TIMER_ACTIVE,
+};
+
+struct mrp_attr {
+	struct rb_node			node;
+	enum mrp_applicant_state	state;
+	enum mrp_applicant_state	new_state;
+	enum mrp_tx_action		action;
+	enum mrp_registrar_state	fix_state;
+	enum mrp_registrar_state	reg_state;
+	enum mrp_notification		notify;
+	u8				aging;
+	u8				type;
+	u8				len;
+	u8				changed;
+	unsigned char			value[];
+};
+
+struct mrp_lva {
+	enum mrp_timer_state		state;
+	u8				rx;
+	u8				tx;
+};
+
+struct mrp_periodic {
+	enum mrp_timer_state		state;
+};
+
+enum mrp_applications {
+	MRP_APPLICATION_MVRP,
+	MRP_APPLICATION_MMRP,
+	MRP_APPLICATION_MSRP,
+	__MRP_APPLICATION_MAX
+};
+#define MRP_APPLICATION_MAX	(__MRP_APPLICATION_MAX - 1)
+
+struct mrp_application {
+	enum mrp_applications	type;
+	unsigned int		maxattr;
+	struct packet_type	pkttype;
+	unsigned char		group_address[ETH_ALEN];
+	u8			version;
+};
+
+struct mrp_applicant {
+	struct mrp_application	*app;
+	struct net_device	*dev;
+	struct mrp_lva		lva;
+	struct mrp_periodic	periodic;
+	struct work_struct	join_work;
+	struct work_struct	periodic_work;
+	struct work_struct	leave_work;
+	struct work_struct	lva_work;
+	struct timer_list	join_timer;
+	struct timer_list	periodic_timer;
+	struct timer_list	leave_timer;
+	struct timer_list	lva_timer;
+	struct {
+		u8 join:1;
+		u8 periodic:1;
+		u8 leave:1;
+		u8 lva:1;
+	} timer_arm;
+	u8 p2p_mac:1;
+	u8 rx:1;
+	u8 dry_run:1;
+	struct rb_node		*last_node;
+	void *parent;
+	u8 lva_type;
+	u8 msg_cnt;
+	u8 port;
+	int ver_diff;
+	unsigned long rla_jiffies;
+	u16 normal;
+	u8 *group_address;
+	u8 src_addr[ETH_ALEN];
+
+	void (*attrval_inc)(void *value, u8 len);
+	int (*attr_chk)(u8 attrtype, u8 attrlen);
+	int (*attr_cmp)(const struct mrp_attr *attr, const void *value,
+		u8 len, u8 type);
+	int (*attr_valid)(u8 attrtype, const void *value);
+	u8 (*attr_size)(u8 attrtype, u8 attrlen);
+	u8 (*attr_len)(u8 attrtype, u8 attrlen);
+	u8 (*attr_type)(struct mrp_attr *attr);
+	int (*attr_upd)(const void *value, struct mrp_attr *attr, int tx);
+	int (*rxpdu)(struct mrp_applicant *app, u8 *data, int len);
+	int (*txpdu)(struct mrp_applicant *app);
+	void (*acton)(struct mrp_applicant *app, struct mrp_attr *attr);
+	void (*cleanup)(struct mrp_applicant *app);
+
+	spinlock_t		lock;
+	struct sk_buff_head	queue;
+	struct sk_buff		*pdu;
+	struct rb_root		mad;
+	struct rcu_head		rcu;
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+	struct rnd_state	*rnd;
+#endif
+};
+
+struct mrp_port {
+	struct mrp_applicant __rcu	*applicants[MRP_APPLICATION_MAX + 1];
+	struct rcu_head			rcu;
+};
+
+int mrp_register_application(struct mrp_application *app);
+void mrp_unregister_application(struct mrp_application *app);
+
+
+#endif /* _NET_MRP_H */
diff --git a/drivers/net/ethernet/micrel/ksz9897/msrp.c b/drivers/net/ethernet/micrel/ksz9897/msrp.c
new file mode 100644
index 0000000..c707e29b
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/msrp.c
@@ -0,0 +1,1226 @@
+/*
+ *	IEEE 802.1Qat Multiple Stream Registration Protocol (MSRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/8021q/vlan_gvrp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+
+
+#ifndef ETH_P_MSRP
+#define ETH_P_MSRP		0x22EA
+#endif
+
+
+#define MRP_MSRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x0E }
+
+enum msrp_attributes {
+	MSRP_ATTR_INVALID,
+
+	/* Used in PDU but not in storage. */
+	MSRP_ATTR_TALKER,
+	MSRP_ATTR_TALKER_FAILED,
+	MSRP_ATTR_LISTENER,
+	MSRP_ATTR_DOMAIN,
+	__MSRP_ATTR_MAX
+};
+#define MSRP_ATTR_MAX	(__MSRP_ATTR_MAX - 1)
+
+enum msrp_listener_attributes {
+	MSRP_LISTENER_IGNORE,
+	MSRP_LISTENER_ASKFAILED,
+	MSRP_LISTENER_READY,
+	MSRP_LISTENER_READYFAIL,
+};
+
+/* Attribute length is 25. */
+struct srp_talker {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 max_frame_size;
+	u16 max_interval_frames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 accumulated_latency;
+} __packed;
+
+/* Attribute length is 34. */
+struct srp_talker_failed {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 max_frame_size;
+	u16 max_interval_frames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 accumulated_latency;
+	u8 bridge_id[8];
+	u8 failure_code;
+} __packed;
+
+/* Attribute length is 8. */
+struct srp_listener {
+	u8 id[8];
+	u8 substate;
+	u8 nextval;
+} __packed;
+
+/* Attribute length is 4. */
+struct srp_domain {
+	u8 class_id;
+	u8 class_priority;
+	u16 class_vid;
+} __packed;
+
+
+static struct mrp_application srp_mrp_app __read_mostly = {
+	.type		= MRP_APPLICATION_MSRP,
+	.maxattr	= MSRP_ATTR_MAX,
+	.pkttype.type	= htons(ETH_P_MSRP),
+	.group_address	= MRP_MSRP_ADDRESS,
+	.version	= 0,
+};
+
+static void msrp_attrvalue_inc(void *value, u8 len)
+{
+	if (sizeof(struct srp_talker) == len ||
+	    sizeof(struct srp_talker_failed) == len) {
+		u8 dest_len;
+		struct srp_talker *talker = value;
+
+		len = 8;
+		dest_len = 6;
+		while (len > 0 && !++talker->id[--len])
+			;
+		while (dest_len > 0 && !++talker->dest[--dest_len])
+			;
+	} else if (sizeof(struct srp_listener) == len) {
+		struct srp_listener *listener = value;
+
+		len = 8;
+		while (len > 0 && !++listener->id[--len])
+			;
+	} else if (sizeof(struct srp_domain) == len) {
+		struct srp_domain *domain = value;
+
+		domain->class_id++;
+		domain->class_priority++;
+	}
+else
+dbg_msg(" %s ?\n", __func__);
+}
+
+/* XMOS uses same stream id and a different destination address for sync? */
+#if 0
+#define TALKER_DEST_ADDR_LEN	6
+#else
+#define TALKER_DEST_ADDR_LEN	0
+#endif
+
+static int msrp_attrvalue_cmp(const void *value, const void *attr, u8 len)
+{
+	if (sizeof(struct srp_talker) == len ||
+	    sizeof(struct srp_talker_failed) == len)
+		return memcmp(value, attr, 8 + TALKER_DEST_ADDR_LEN);
+	else if (sizeof(struct srp_listener) == len)
+		return memcmp(value, attr, 8);
+	else if (sizeof(struct srp_domain) == len)
+		return memcmp(value, attr, 2);
+	return 0;
+}
+
+static int msrp_attr_cmp(const struct mrp_attr *attr,
+			 const void *value, u8 len, u8 type)
+{
+	if (MSRP_ATTR_TALKER == type && 25 == len) {
+		type = MSRP_ATTR_TALKER_FAILED;
+		len = 34;
+	}
+	if (attr->type != type)
+		return attr->type - type;
+	if (attr->len != len)
+		return attr->len - len;
+	return msrp_attrvalue_cmp(attr->value, value, len);
+}
+
+static int msrp_attr_chk(u8 attrtype, u8 attrlen)
+{
+	if ((MSRP_ATTR_LISTENER == attrtype &&
+	    sizeof(struct srp_listener) - 2 == attrlen) ||
+	    (MSRP_ATTR_TALKER == attrtype &&
+	    sizeof(struct srp_talker) == attrlen) ||
+	    (MSRP_ATTR_TALKER_FAILED == attrtype &&
+	    sizeof(struct srp_talker_failed) == attrlen) ||
+	    (MSRP_ATTR_DOMAIN == attrtype &&
+	    sizeof(struct srp_domain) == attrlen))
+		return 0;
+	if (MSRP_ATTR_LISTENER != attrtype &&
+	    MSRP_ATTR_TALKER != attrtype &&
+	    MSRP_ATTR_TALKER_FAILED != attrtype &&
+	    MSRP_ATTR_DOMAIN != attrtype)
+		return 1;
+	return -1;
+}
+
+static u8 msrp_attr_size(u8 attrtype, u8 attrlen)
+{
+	if (attrtype == MSRP_ATTR_LISTENER ||
+	    attrtype == MSRP_ATTR_TALKER ||
+	    attrtype == MSRP_ATTR_TALKER_FAILED)
+		return attrlen * 2;
+	return attrlen;
+}
+
+static u8 msrp_attr_len(u8 attrtype, u8 attrlen)
+{
+	if (MSRP_ATTR_LISTENER == attrtype)
+		return 8;
+	if (MSRP_ATTR_TALKER == attrtype)
+		return 8;
+	if (MSRP_ATTR_TALKER_FAILED == attrtype)
+		return 8;
+	if (MSRP_ATTR_DOMAIN == attrtype)
+		return 1;
+	return attrlen;
+}
+
+static u8 msrp_attr_type(struct mrp_attr *attr)
+{
+	u8 type = attr->type;
+
+	if (MSRP_ATTR_TALKER_FAILED == attr->type) {
+		struct srp_talker_failed *talker =
+			(struct srp_talker_failed *) attr->value;
+
+		if (!talker->failure_code)
+			type = MSRP_ATTR_TALKER;
+	}
+	return type;
+}
+
+static int msrp_attr_upd(const void *value, struct mrp_attr *attr, int tx)
+{
+	int ret = false;
+
+	/* Update talker failure code. */
+	if (MSRP_ATTR_TALKER_FAILED == attr->type) {
+		struct srp_talker_failed *old;
+		struct srp_talker_failed *old_rx = (struct srp_talker_failed *)
+			attr->value;
+		struct srp_talker_failed *old_tx = old_rx + 1;
+		const struct srp_talker_failed *new =
+			(struct srp_talker_failed *) value;
+
+		/* Initialize the second half after creation. */
+		if (memcmp(old_tx, old_rx, 8))
+			memcpy(old_tx, old_rx,
+				sizeof(struct srp_talker_failed));
+		old = tx ? old_tx : old_rx;
+		if (memcmp(old->dest, new->dest,
+			sizeof(struct srp_talker_failed) - 8)) {
+#if 0
+u8 *src = (u8 *) old->dest;
+u8 *dst = (u8 *) new->dest;
+int n = sizeof(struct srp_talker_failed) - 8;
+int i;
+for (i = 0; i < n; i++) {
+dbg_msg("%02x:%02x ", src[i], dst[i]);
+if ((i % 10) == 9)
+dbg_msg("\n");
+}
+dbg_msg("\n %p %d\n", value, n);
+#endif
+			if (attr->state == MRP_APPLICANT_QA)
+				attr->state = MRP_APPLICANT_AA;
+			memcpy(old->dest, new->dest,
+				sizeof(struct srp_talker_failed) - 8);
+			ret = true;
+		}
+
+	/* Update listener substate. */
+	} else if (MSRP_ATTR_LISTENER == attr->type) {
+		struct srp_listener *old;
+		struct srp_listener *old_rx = (struct srp_listener *)
+			attr->value;
+		struct srp_listener *old_tx = old_rx + 1;
+		const struct srp_listener *new = (struct srp_listener *)
+			value;
+
+		/* Initialize the second half after creation. */
+		if (memcmp(old_tx, old_rx, 8))
+			memcpy(old_tx, old_rx,
+				sizeof(struct srp_listener));
+		old = tx ? old_tx : old_rx;
+		if (old->substate != new->substate) {
+
+			/* Does changing require a NEW indication? */
+			if (attr->state == MRP_APPLICANT_QA)
+				attr->state = MRP_APPLICANT_AA;
+			old->substate = new->substate;
+			ret = true;
+		}
+
+	/* Update domain VID. */
+	} else if (MSRP_ATTR_DOMAIN == attr->type ) {
+		struct srp_domain *old = (struct srp_domain *) attr->value;
+		const struct srp_domain *new = (struct srp_domain *) value;
+
+		if (old->class_vid != new->class_vid) {
+			if (attr->state == MRP_APPLICANT_QA)
+				attr->state = MRP_APPLICANT_AA;
+			old->class_vid = new->class_vid;
+			ret = true;
+		}
+	}
+	return ret;
+}
+
+static int chk_listen_id(u8 *first, u8 *next)
+{
+	if (!memcmp(first, next, 6) && (first[6] < next[6] ||
+	    (first[6] == next[6] && first[7] <= next[7]))) {
+		int diff;
+
+		diff = next[6];
+		diff -= first[6];
+		diff <<= 8;
+		diff |= next[7];
+		diff -= first[7];
+		return diff;
+	} else
+		return 0x10000;
+}
+
+static int find_used_space(int cnt, int left, int cnt_per_byte)
+{
+	int num;
+
+	num = cnt - left;
+	num += (cnt_per_byte - 1);
+	num /= cnt_per_byte;
+	return num;
+}
+
+static int find_largest_cnt(int cnt)
+{
+	int num;
+	int attr_cnt;
+	int decl_cnt;
+	int attr_left;
+	int decl_left;
+
+	num = 21;
+	attr_left = (3 - (cnt % 3)) % 3;
+	decl_left = (4 - (cnt % 4)) % 4;
+	do {
+		--num;
+		attr_cnt = find_used_space(num, attr_left, 3);
+		decl_cnt = find_used_space(num, decl_left, 4);
+#ifdef DBG_MRP
+dbg_msg(" f: c=%d a=%d d=%d n=%d %d\n",
+cnt, attr_left, decl_left, num, attr_cnt + decl_cnt);
+#endif
+	} while (attr_cnt + decl_cnt > 11);
+	return num;
+}
+
+static void msrp_prepare_tx(struct mrp_applicant *app, u8 attrtype,
+	int firstval_max)
+{
+	u8 firstval[40];
+	int firstval_cnt = 0;
+	int firstval_opt = 0;
+	int opt_cnt = 0;
+	int listen_cnt = 0;
+	struct rb_node *node, *next;
+	struct rb_node *opt = NULL;
+	struct mrp_attr *attr;
+	struct mrp_attr *first = NULL;
+	struct mrp_attr dummy;
+	struct mrp_attr *last = &dummy;
+	enum mrp_tx_action last_action = MRP_TX_ACTION_NONE;
+	u8 attr_type = attrtype;
+	u8 attr_len;
+	struct srp_listener *listener;
+	void *value;
+
+	/* MSRP_ATTR_TALKER_FAILED type is used internally in storage. */
+	if (MSRP_ATTR_TALKER == attrtype)
+		attr_type = MSRP_ATTR_TALKER_FAILED;
+	memset(firstval, 0, sizeof(firstval));
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attr_type != attr->type)
+			continue;
+		attr_len = attr->len;
+		value = attr->value;
+		if (MSRP_ATTR_TALKER_FAILED == attr_type) {
+			struct srp_talker_failed *talker =
+				(struct srp_talker_failed *) attr->value;
+
+			talker++;
+			if ((MSRP_ATTR_TALKER == attrtype &&
+			    talker->failure_code) ||
+			    (MSRP_ATTR_TALKER_FAILED == attrtype &&
+			    !talker->failure_code))
+				continue;
+			value = talker;
+		}
+		if (mrp_10_5_1_hack && attr->action == MRP_TX_ACTION_S_LV) {
+dbg_msg(" no Lv sent\n");
+			attr->action = MRP_TX_ACTION_NONE;
+		}
+		if (MRP_TX_ACTION_NONE == attr->action)
+{
+dbg_msg(" no send %d\n", app->port);
+			continue;
+}
+
+		/* Used only for listener. */
+		listener = NULL;
+		if (MSRP_ATTR_LISTENER == attr_type) {
+			listener = (struct srp_listener *) attr->value;
+
+			listener++;
+			value = listener;
+
+			/* Actual attribute length. */
+			attr_len = 8;
+			listener->nextval = 0;
+		}
+
+		/* At least one attribute that is trying to send something. */
+		if (last) {
+			last = attr;
+
+			/* Rememeber action so that it can be restored. */
+			last_action = attr->action;
+		}
+		if (firstval_cnt) {
+			int firstval_next = 1;
+			int diff = 0;
+
+			if (MSRP_ATTR_LISTENER == attrtype)
+				diff = chk_listen_id(firstval, value);
+
+			app->attrval_inc(firstval, attr->len);
+			if (memcmp(firstval, value, attr_len))
+				firstval_next = 0;
+
+			if (MSRP_ATTR_LISTENER == attrtype) {
+				if (!firstval_next) {
+					int max;
+
+					max = find_largest_cnt(listen_cnt);
+					if (diff <= max) {
+						listen_cnt += diff;
+						listener->nextval = diff;
+						opt = NULL;
+						opt_cnt = 0;
+						firstval_opt = 0;
+						first = attr;
+						memcpy(firstval, value,
+							attr_len);
+						firstval_cnt = 1;
+						continue;
+					}
+				} else
+					++listen_cnt;
+			}
+			if (!firstval_next)
+				first = NULL;
+			else {
+				++firstval_cnt;
+				if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL ==
+				    attr->action ||
+				    MRP_TX_ACTION_S_IN_OPTIONAL ==
+				    attr->action) {
+					if (!opt) {
+						opt = node;
+						opt_cnt = 1;
+						firstval_opt =
+							firstval_cnt % 3;
+						if (!firstval_opt)
+							firstval_opt = 3;
+					} else {
+						++opt_cnt;
+						++firstval_opt;
+					}
+
+					/* Exceed allowed encoding. */
+					if (firstval_opt >= firstval_max)
+						first = NULL;
+				} else {
+					opt = NULL;
+					opt_cnt = 0;
+					firstval_opt = 0;
+				}
+			}
+		}
+		if (!first) {
+			while (opt_cnt) {
+				first = rb_entry(opt, struct mrp_attr, node);
+				first->action = MRP_TX_ACTION_NONE;
+				opt = rb_next(opt);
+				--opt_cnt;
+			}
+			opt = NULL;
+			firstval_opt = 0;
+
+			/* Skip if not needed to send. */
+			if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action ||
+			    MRP_TX_ACTION_S_IN_OPTIONAL == attr->action) {
+				attr->action = MRP_TX_ACTION_NONE;
+				continue;
+			}
+			first = attr;
+			memcpy(firstval, value, attr_len);
+			firstval_cnt = 1;
+			if (MSRP_ATTR_LISTENER == attrtype)
+				listen_cnt = 1;
+
+			/* There is at least one attribute sending. */
+			last = NULL;
+		}
+	}
+
+	/* Need to send at least one for LeaveAll. */
+	if ((app->lva.tx & (1 << (attrtype - 1))) && last &&
+	    MRP_TX_ACTION_NONE == last->action)
+		last->action = last_action;
+}
+
+/* attrlist + mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+#define MSRP_MIN			(2 + 2 + 1 + 4)
+
+#define MSRP_TALKER_MIN			(25 + MRP_MIN)
+#define MSRP_TALKER_FAILED_MIN		(34 + MRP_MIN)
+#define MSRP_LISTENER_MIN		(9 + MRP_MIN)
+#define MSRP_DOMAIN_MIN			(4 + MRP_MIN)
+
+#define MSRP_TALKER_OPT_MAX		MRP_OPT_MAX(25)
+#define MSRP_TALKER_FAILED_OPT_MAX	MRP_OPT_MAX(34)
+#define MSRP_DOMAIN_OPT_MAX		MRP_OPT_MAX(4)
+#define MSRP_LISTENER_OPT_MAX		\
+	((((2 + 8) + 2) * 2 - (2 + 8)) / 2 * 3)
+
+static u8 *msrp_encode_substate(u8 *msg_ptr, u8 *list_decl, int len)
+{
+	int j;
+
+	memset(&list_decl[len], 0, len & 3);
+	for (j = 0; j < len; j += 4)
+		*msg_ptr++ = mrp_4pack_encode(&list_decl[j]);
+	return msg_ptr;
+}
+
+static int msrp_tx(struct mrp_applicant *app, u8 *msg_buf, u8 *msg_eof,
+	int *bytes, u8 attrtype, u8 attrlen, u8 attrmax, u8 msg_min)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	struct mrp_msg_hdr *mh;
+	struct mrp_vecattr_hdr *vah;
+	u8 firstval[40];
+	u8 vectevt[3];
+	int i = 0;
+	int len = 0;
+	int num_break = false;
+	int node_break = false;
+	u8 *msg_ptr = msg_buf;
+	u16 *attr_list;
+	u8 attr_type = attrtype;
+	int nextval;
+	u8 *list_decl;
+	void *value;
+
+	/* MSRP_ATTR_TALKER_FAILED type is used internally in storage. */
+	if (MSRP_ATTR_TALKER == attrtype)
+		attr_type = MSRP_ATTR_TALKER_FAILED;
+	*bytes = 0;
+
+	/* Check if previous attribute types are all sent. */
+	node = app->last_node;
+	if (node) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attr_type != attr->type)
+			return 0;
+	} else
+		node = rb_first(&app->mad);
+
+	/* Need one more byte for listener substate. */
+	if (MSRP_ATTR_LISTENER == attrtype)
+		msg_min++;
+
+	/* mrp_msg_hdr + mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+	if (msg_ptr > (msg_eof - (sizeof(struct mrp_msg_hdr) + 2 + msg_min)))
+		return -1;
+	memset(vectevt, 0, 3);
+
+	msrp_prepare_tx(app, attrtype, attrmax);
+
+	mh = (struct mrp_msg_hdr *) msg_ptr;
+	mh->attrtype = attrtype;
+	mh->attrlen = attrlen;
+	msg_ptr += sizeof(struct mrp_msg_hdr);
+
+	attr_list = (u16 *) msg_ptr;
+	msg_ptr += 2;
+
+	vah = (struct mrp_vecattr_hdr *) msg_ptr;
+	msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	list_decl = kzalloc(1600 / 2, GFP_KERNEL);
+	for (;
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+
+		/* Check if there is enough space for all required data. */
+		if (!len && msg_ptr > (msg_eof - msg_min)) {
+dbg_msg(" no space\n");
+			node_break = true;
+			break;
+		}
+
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attr_type != attr->type)
+			continue;
+
+		value = attr->value;
+		if (MSRP_ATTR_TALKER_FAILED == attr_type) {
+			struct srp_talker_failed *talker =
+				(struct srp_talker_failed *) attr->value;
+
+			talker++;
+			if ((MSRP_ATTR_TALKER == attrtype &&
+			    talker->failure_code) ||
+			    (MSRP_ATTR_TALKER_FAILED == attrtype &&
+			    !talker->failure_code))
+				continue;
+			value = talker;
+		}
+
+		/* See if next value is close enough for efficient encoding. */
+		nextval = 0;
+		if (MSRP_ATTR_LISTENER == attr_type) {
+			struct srp_listener *listener =
+				(struct srp_listener *) attr->value;
+
+			listener++;
+			value = listener;
+			nextval = listener->nextval;
+		}
+
+		if (MRP_TX_ACTION_NONE == attr->action) {
+			if (mrp_applicant_chk(attr))
+				mrp_attr_destroy(app, attr);
+
+#if 0
+if (i)
+dbg_msg(" no snd\n");
+#endif
+			/* In case there are events waiting. */
+			if (i)
+				i += 2;
+			goto val_end;
+		}
+
+		do {
+			if (!len) {
+				memcpy(firstval, value, attrlen);
+				memcpy(vah->firstattrvalue, firstval, attrlen);
+				msg_ptr += attrlen;
+			} else {
+				app->attrval_inc(firstval, attr->len);
+			}
+			if (nextval)
+				--nextval;
+
+			/* Special case for listener. */
+			if (nextval > 0) {
+				/* Fill in dummy attribute. */
+				vectevt[i] = MRP_VECATTR_EVENT_NEW;
+				list_decl[len] = MSRP_LISTENER_IGNORE;
+#if 0
+dbg_msg(" iinc [%02x:%02x] ", firstval[6], firstval[7]);
+#endif
+			} else {
+#if 0
+if (MSRP_ATTR_LISTENER == attr_type)
+dbg_msg(" ne [%02x:%02x] ", firstval[6], firstval[7]);
+#endif
+				if (len && memcmp(firstval, value, attrlen)) {
+					/* Process with current node. */
+					next = node;
+					num_break = true;
+					if (i)
+						i += 2;
+					goto val_end;
+				}
+				mrp_encode_action(attr, &vectevt[i]);
+				if (MSRP_ATTR_LISTENER == attrtype) {
+					struct srp_listener *listener =
+						(struct srp_listener *) value;
+
+					list_decl[len] = listener->substate;
+				}
+				if (mrp_applicant_chk(attr))
+					mrp_attr_destroy(app, attr);
+			}
+
+			++i;
+			++len;
+
+val_end:
+			if (len && i >= 3) {
+				int left = (1 + 4);
+
+				if (MSRP_ATTR_LISTENER == attrtype)
+					left = (1 + (len + 3) / 4 + 4);
+#if 0
+dbg_msg(" vect:%d ", nextval);
+#endif
+				*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+				if (msg_ptr > msg_eof - left) {
+dbg_msg(" node break\n");
+					num_break = true;
+					node_break = true;
+					nextval = -1;
+				}
+			}
+			if (num_break) {
+#if 0
+dbg_msg(" num: %d ", nextval);
+#endif
+				if (MSRP_ATTR_LISTENER == attrtype)
+					msg_ptr =
+						msrp_encode_substate(msg_ptr,
+								     list_decl,
+								     len);
+				put_unaligned(cpu_to_be16(len), &vah->lenflags);
+				if (app->lva.tx & (1 << (attrtype - 1)))
+					vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+				len = 0;
+				num_break = false;
+
+				if (msg_ptr > (msg_eof - msg_min)) {
+					node_break = true;
+					break;
+				}
+
+				vah = (struct mrp_vecattr_hdr *) msg_ptr;
+				msg_ptr += sizeof(struct mrp_vecattr_hdr);
+if (nextval > 0)
+dbg_msg(" ?? next");
+nextval = -1;
+			}
+		} while (nextval > 0);
+		if (node_break)
+			break;
+	}
+	if (len) {
+		if (i)
+			*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+		if (MSRP_ATTR_LISTENER == attrtype)
+			msg_ptr = msrp_encode_substate(msg_ptr, list_decl, len);
+		put_unaligned(cpu_to_be16(len), &vah->lenflags);
+		if (app->lva.tx & (1 << (attrtype - 1)))
+			vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+
+	/* Need to send LeaveAll with zero number of attributes. */
+	} else if (app->lva.tx & (1 << (attrtype - 1)) && !node_break) {
+		memset(vah->firstattrvalue, 0, attrlen);
+		msg_ptr += attrlen;
+		put_unaligned(0, &vah->lenflags);
+		vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+	}
+	app->last_node = node;
+	kfree(list_decl);
+
+	if (msg_ptr == msg_buf +
+	    sizeof(struct mrp_msg_hdr) + 2 +
+	    sizeof(struct mrp_vecattr_hdr)) {
+		return 0;
+	}
+	*msg_ptr++ = MRP_END_MARK;
+	*msg_ptr++ = MRP_END_MARK;
+
+	*bytes = msg_ptr - msg_buf;
+	len = msg_ptr - (u8 *) attr_list;
+	*attr_list = htons(len - 2);
+
+	/* Not all attributes sent. */
+	if (node)
+		return -1;
+	return 0;
+}
+
+static int msrp_parse_events(struct mrp_applicant *app,
+	u8 **msg_ptr, u8 *msg_eof, u8 attrlen, u8 attrtype)
+{
+	struct mrp_vecattr_hdr *vah;
+	u16 valen;
+	u16 LA;
+	u8 firstval[40];
+	u8 vaevent;
+	u8 vaevents;
+	int add_len = 0;
+	u8 *decl_ptr = NULL;
+	u8 decl[4];
+	int i = 4;
+	int ignore;
+
+	vah = (struct mrp_vecattr_hdr *) *msg_ptr;
+	valen = get_unaligned(&vah->lenflags);
+	LA = valen & ~MRP_VECATTR_HDR_LEN_MASK;
+
+	/* Only LeaveAll is understood in this version. */
+	if (app->ver_diff <= 0 && LA && LA != MRP_VECATTR_HDR_FLAG_LA)
+		return -1;
+#if 0
+if (app->port < 3)
+dbg_msg("  rx: %d=%d %x %lu\n", app->port, attrtype, LA, jiffies);
+#endif
+
+	LA &= MRP_VECATTR_HDR_FLAG_LA;
+	valen &= MRP_VECATTR_HDR_LEN_MASK;
+	valen = be16_to_cpu(valen);
+
+	*msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	if (MSRP_ATTR_LISTENER == attrtype)
+		add_len = (valen + 3) / 4;
+
+	/* Listener storage has extra stuff. */
+	if (attrlen < sizeof(struct srp_talker_failed))
+		memset(firstval, 0, sizeof(struct srp_talker_failed));
+	memcpy(firstval, vah->firstattrvalue, attrlen);
+	*msg_ptr += attrlen;
+
+	if (*msg_ptr + (valen + 2) / 3 + add_len > msg_eof)
+		return -1;
+
+	/* Do it once for each attribute type. */
+	if (!app->dry_run && LA && !(app->lva.rx & (1 << attrtype))) {
+#if 0
+app->rla_jiffies = jiffies;
+#endif
+		app->lva_type = attrtype;
+		mrp_mad_event(app, MRP_EVENT_R_LA);
+		app->lva_type = 0;
+		app->lva.rx |= (1 << attrtype);
+	}
+
+	if (!valen)
+		return 0;
+
+	if (MSRP_ATTR_LISTENER == attrtype) {
+		/* For substate. */
+		attrlen += 2;
+		decl_ptr = *msg_ptr + (valen + 2) / 3;
+	} else if (MSRP_ATTR_TALKER == attrtype) {
+		attrtype = MSRP_ATTR_TALKER_FAILED;
+		memset(&firstval[25], 0, 34 - 25);
+		attrlen = 34;
+	}
+
+	/* In a VectorAttribute, the Vector contains events which are packed
+	 * three to a byte. We process one byte of the Vector at a time.
+	 */
+	while (valen > 0) {
+		vaevents = **msg_ptr;
+		*msg_ptr += sizeof(vaevents);
+
+		/* Extract and process the first event. */
+		vaevent = vaevents / (__MRP_VECATTR_EVENT_MAX *
+				      __MRP_VECATTR_EVENT_MAX);
+		if (vaevent >= __MRP_VECATTR_EVENT_MAX) {
+			/* The byte is malformed; stop processing. */
+			return -2;
+		}
+		ignore = false;
+		if (decl_ptr) {
+			if (4 == i)
+				mrp_4pack_decode(*decl_ptr++, decl, &i);
+			if (MSRP_LISTENER_IGNORE == decl[i])
+				ignore = true;
+			firstval[8] = decl[i++];
+		}
+		if (!app->dry_run && !ignore)
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the second event. */
+		if (!--valen)
+			break;
+		vaevents %= (__MRP_VECATTR_EVENT_MAX *
+			     __MRP_VECATTR_EVENT_MAX);
+		vaevent = vaevents / __MRP_VECATTR_EVENT_MAX;
+		app->attrval_inc(firstval, attrlen);
+		ignore = false;
+		if (decl_ptr) {
+			if (4 == i)
+				mrp_4pack_decode(*decl_ptr++, decl, &i);
+			if (MSRP_LISTENER_IGNORE == decl[i])
+				ignore = true;
+			firstval[8] = decl[i++];
+		}
+		if (!app->dry_run && !ignore)
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the third event. */
+		if (!--valen)
+			break;
+		vaevents %= __MRP_VECATTR_EVENT_MAX;
+		vaevent = vaevents;
+		app->attrval_inc(firstval, attrlen);
+		ignore = false;
+		if (decl_ptr) {
+			if (4 == i)
+				mrp_4pack_decode(*decl_ptr++, decl, &i);
+			if (MSRP_LISTENER_IGNORE == decl[i])
+				ignore = true;
+			firstval[8] = decl[i++];
+		}
+		if (!app->dry_run && !ignore)
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+		app->attrval_inc(firstval, attrlen);
+		--valen;
+	}
+#if 0
+dbg_msg(" decl_ptr: %p %p\n", *msg_ptr, decl_ptr);
+#endif
+	if (decl_ptr)
+		*msg_ptr = decl_ptr;
+	return 0;
+}
+
+static int msrp_parse_msg(struct mrp_applicant *app, u8 **msg_ptr, u8 *msg_eof)
+{
+	struct mrp_msg_hdr *mh;
+	u16 *attr_list;
+	int rc;
+	int add_len = 0;
+
+	/*
+	 * End mark terminates the message.  Happen when there are filler
+	 * bytes in the frame.
+	 */
+	mh = (struct mrp_msg_hdr *) *msg_ptr;
+	if (MRP_END_MARK == mh->attrtype &&
+	    MRP_END_MARK == mh->attrlen) {
+		return -1;
+	}
+
+	/* Check whether the attribute type is understood. */
+	rc = app->attr_chk(mh->attrtype, mh->attrlen);
+	if (rc < 0)
+		return -1;
+
+	/* Attribute not understood but not in future protocol. */
+	if (rc && (app->ver_diff <= 0 || !app->msg_cnt))
+		return -1;
+
+	if (MSRP_ATTR_LISTENER == mh->attrtype)
+		add_len++;
+
+	*msg_ptr += sizeof(struct mrp_msg_hdr);
+	attr_list = (u16 *) *msg_ptr;
+	if (ntohs(*attr_list) < mh->attrlen + 4)
+{
+int i;
+u8 *data = *msg_ptr;
+
+for (i = 0; i < msg_eof - *msg_ptr; i++) {
+dbg_msg("%02x ", data[i]);
+if ((i % 16) == 15)
+dbg_msg("\n");
+}
+if ((i % 16))
+dbg_msg("\n");
+dbg_msg(" bad attrlist: %d %d\n", ntohs(*attr_list), mh->attrlen);
+		return -1;
+}
+	*msg_ptr += 2;
+	while (*msg_ptr <= (msg_eof - (2 + 1 + add_len))) {
+		rc = msrp_parse_events(app, msg_ptr, msg_eof,
+			mh->attrlen, mh->attrtype);
+		if (rc < 0)
+			return rc;
+		rc = mrp_parse_end_mark(msg_ptr, msg_eof);
+		if (rc <= 0) {
+			u16 len = *msg_ptr - (u8 *) attr_list;
+			u16 list_len = ntohs(*attr_list) + 2;
+
+			if (len != list_len) {
+if (app->dry_run)
+dbg_msg("attrlist %d:%u %u %d\n", app->port, list_len, len, rc);
+				if (len < list_len)
+					*msg_ptr += list_len - len;
+			}
+		}
+
+		/* End mark terminates the vector list. */
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int msrp_rx(struct mrp_applicant *app, u8 *data, int len)
+{
+	struct mrp_pdu_hdr *ph;
+	u8 *msg_ptr;
+	u8 *msg_eof;
+	int rc;
+
+	ph = (struct mrp_pdu_hdr *) data;
+	app->ver_diff = ph->version - app->app->version;
+	app->msg_cnt = 0;
+
+	app->lva.rx = 0;
+
+	msg_ptr = (u8 *)(ph + 1);
+	msg_eof = msg_ptr + len - sizeof(struct mrp_pdu_hdr);
+
+	while (msg_ptr <= (msg_eof - (2 + 2))) {
+		rc = msrp_parse_msg(app, &msg_ptr, msg_eof);
+		if (rc < 0) {
+			if (app->dry_run && rc == -2)
+				rc = 0;
+			return rc;
+		}
+		app->msg_cnt++;
+
+		/* End mark terminates the frame. */
+		rc = mrp_parse_end_mark(&msg_ptr, msg_eof);
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int msrp_rxpdu(struct mrp_applicant *app, u8 *data, int len)
+{
+	int rc;
+
+	app->dry_run = 1;
+	rc = msrp_rx(app, data, len);
+
+	/* Discard entire PDU if malformed. */
+	if (rc < 0)
+		return rc;
+	app->dry_run = 0;
+	return msrp_rx(app, data, len);
+}
+
+static int msrp_txpdu(struct mrp_applicant *app)
+{
+	int bytes;
+	int err;
+	u8 *msg_buf;
+	u8 *msg_eof;
+	u8 *msg_ptr;
+#if 1
+	int tx_0 = 0;
+	int tx_1 = 0;
+#endif
+
+	if (!app->normal)
+		return 0;
+
+	err = mrp_pdu_init(app);
+	if (err < 0)
+		return err;
+
+	msg_buf = app->pdu->data;
+	msg_eof = msg_buf + app->dev->mtu;
+	msg_buf++;
+	msg_ptr = msg_buf;
+
+	if (app->normal & (1 << MSRP_ATTR_TALKER)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_TALKER, sizeof(struct srp_talker),
+			      MSRP_TALKER_OPT_MAX, MSRP_TALKER_MIN);
+
+		/* LeaveAll for MAC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_TALKER - 1));
+#if 1
+			tx_0 = 1;
+#endif
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+
+	if (app->normal & (1 << MSRP_ATTR_TALKER_FAILED)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_TALKER_FAILED,
+			      sizeof(struct srp_talker_failed),
+			      MSRP_TALKER_FAILED_OPT_MAX,
+			      MSRP_TALKER_FAILED_MIN);
+
+		/* LeaveAll for MAC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_TALKER_FAILED - 1));
+#if 1
+			tx_1 = 1;
+#endif
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+
+	if (app->normal & (1 << MSRP_ATTR_LISTENER)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_LISTENER,
+			      sizeof(struct srp_listener) - 2,
+			      MSRP_LISTENER_OPT_MAX, MSRP_LISTENER_MIN);
+
+		/* LeaveAll for SVC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_LISTENER - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+#if 0
+if (tx_0 && !tx_1)
+goto send;
+#endif
+#if 1
+	if (mrp_10_5_1d_hack && !tx_0 && tx_1)
+		goto send;
+#endif
+
+	if (app->normal & (1 << MSRP_ATTR_DOMAIN)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_DOMAIN, sizeof(struct srp_domain),
+			      MSRP_DOMAIN_OPT_MAX, MSRP_DOMAIN_MIN);
+
+		/* LeaveAll for SVC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_DOMAIN - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+	app->lva.tx = 0;
+
+send:
+
+	return mrp_txpdu(app, msg_buf, msg_ptr, msg_eof);
+}
+
+static int msrp_req_join_domain(struct mrp_applicant *app,
+	struct srp_domain *domain, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, domain, sizeof(struct srp_domain),
+		MSRP_ATTR_DOMAIN, new_decl);
+}
+
+static int msrp_req_new_listener(struct mrp_applicant *app,
+	struct srp_listener *listener, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	listener->nextval = 0;
+	return mrp_req_new(app, listener, sizeof(struct srp_listener),
+		MSRP_ATTR_LISTENER, new_decl);
+}
+
+static int msrp_req_new_talker(struct mrp_applicant *app,
+	struct srp_talker_failed *talker, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, talker, sizeof(struct srp_talker_failed),
+		MSRP_ATTR_TALKER_FAILED, new_decl);
+}
+
+static int msrp_req_leave_domain(struct mrp_applicant *app,
+	struct srp_domain *domain)
+{
+	return mrp_req_leave(app, domain, sizeof(struct srp_domain),
+		MSRP_ATTR_DOMAIN);
+}
+
+static int msrp_req_leave_listener(struct mrp_applicant *app,
+	struct srp_listener *listener)
+{
+	listener->nextval = 0;
+	return mrp_req_leave(app, listener, sizeof(struct srp_listener),
+		MSRP_ATTR_LISTENER);
+}
+
+static int msrp_req_leave_talker(struct mrp_applicant *app,
+	struct srp_talker_failed *talker)
+{
+	return mrp_req_leave(app, talker, sizeof(struct srp_talker_failed),
+		MSRP_ATTR_TALKER_FAILED);
+}
+
+static void msrp_init_application(struct mrp_applicant *app,
+				  void (*acton)(struct mrp_applicant *app,
+				  struct mrp_attr *attr),
+				  void (*cleanup)(struct mrp_applicant *app))
+{
+	app->attrval_inc = msrp_attrvalue_inc;
+	app->attr_chk = msrp_attr_chk;
+	app->attr_cmp = msrp_attr_cmp;
+	app->attr_valid = mrp_attr_valid;
+	app->attr_size = msrp_attr_size;
+	app->attr_len = msrp_attr_len;
+	app->attr_type = msrp_attr_type;
+	app->attr_upd = msrp_attr_upd;
+	app->rxpdu = msrp_rxpdu;
+	app->txpdu = msrp_txpdu;
+	app->acton = acton;
+	app->cleanup = cleanup;
+	app->normal = (1 << MSRP_ATTR_TALKER) |
+		      (1 << MSRP_ATTR_TALKER_FAILED) |
+		      (1 << MSRP_ATTR_LISTENER) |
+		      (1 << MSRP_ATTR_DOMAIN);
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/mvrp.c b/drivers/net/ethernet/micrel/ksz9897/mvrp.c
new file mode 100644
index 0000000..020888d
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/mvrp.c
@@ -0,0 +1,160 @@
+/*
+ *	IEEE 802.1Q Multiple VLAN Registration Protocol (MVRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/8021q/vlan_gvrp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+
+
+#ifndef ETH_P_MVRP
+#define ETH_P_MVRP		0x88F5
+#endif
+
+
+#if 1
+#define MRP_MVRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x21 }
+#else
+/* Switch does not forward frames so hard to debug. */
+#define MRP_MVRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x0E }
+#endif
+
+enum mvrp_attributes {
+	MVRP_ATTR_INVALID,
+	MVRP_ATTR_VID,
+	__MVRP_ATTR_MAX
+};
+#define MVRP_ATTR_MAX	(__MVRP_ATTR_MAX - 1)
+
+static struct mrp_application vlan_mrp_app __read_mostly = {
+	.type		= MRP_APPLICATION_MVRP,
+	.maxattr	= MVRP_ATTR_MAX,
+	.pkttype.type	= htons(ETH_P_MVRP),
+	.group_address	= MRP_MVRP_ADDRESS,
+	.version	= 0,
+};
+
+static int mvrp_attr_chk(u8 attrtype, u8 attrlen)
+{
+	if (MVRP_ATTR_VID == attrtype && 2 == attrlen)
+		return 0;
+	if (MVRP_ATTR_VID != attrtype)
+		return 1;
+	return -1;
+}
+
+static int mvrp_attr_valid(u8 attrtype, const void *value)
+{
+	if (MVRP_ATTR_VID == attrtype) {
+		u16 *ptr = (u16 *) value;
+		u16 vid = ntohs(*ptr);
+
+		if (1 <= vid && vid <= 4094)
+			return true;
+	}
+	return false;
+}
+
+static int mvrp_rxpdu(struct mrp_applicant *app, u8 *data, int len)
+{
+	int rc;
+
+	app->dry_run = 1;
+	rc = mrp_rx(app, data, len);
+
+	/* Discard entire PDU if malformed. */
+	if (rc < 0)
+		return rc;
+	app->dry_run = 0;
+	return mrp_rx(app, data, len);
+}
+
+static int mvrp_txpdu(struct mrp_applicant *app)
+{
+	int bytes;
+	int err;
+	u8 *msg_buf;
+	u8 *msg_eof;
+	u8 *msg_ptr;
+
+	if (!app->normal)
+		return 0;
+
+	err = mrp_pdu_init(app);
+	if (err < 0)
+		return err;
+
+	msg_buf = app->pdu->data;
+	msg_eof = msg_buf + app->dev->mtu;
+	msg_buf++;
+	msg_ptr = msg_buf;
+
+	if (app->normal & (1 << MVRP_ATTR_VID)) {
+		err = mrp_tx(app, msg_ptr, msg_eof, &bytes,
+			     MVRP_ATTR_VID, 2, MVRP_VID_OPT_MAX, MVRP_VID_MIN);
+
+		/* LeaveAll for VLAN attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MVRP_ATTR_VID - 1));
+		}
+	}
+	if (!app->last_node)
+		app->lva.tx = 0;
+
+	return mrp_txpdu(app, msg_buf, msg_ptr, msg_eof);
+}
+
+static int mvrp_req_join(struct mrp_applicant *app, u16 vid, u8 new_decl)
+{
+	__be16 vlan_id = htons(vid);
+
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, &vlan_id, sizeof(vlan_id), MVRP_ATTR_VID,
+		new_decl);
+}
+
+static int mvrp_req_leave(struct mrp_applicant *app, u16 vid)
+{
+	__be16 vlan_id = htons(vid);
+
+	return mrp_req_leave(app, &vlan_id, sizeof(vlan_id), MVRP_ATTR_VID);
+}
+
+static void mvrp_req_set(struct mrp_applicant *app, u16 vid,
+			enum mrp_registrar_state state)
+{
+	__be16 vlan_id = htons(vid);
+
+	mrp_req_set(app, &vlan_id, sizeof(vlan_id), MVRP_ATTR_VID, state);
+}
+
+static void mvrp_init_application(struct mrp_applicant *app,
+				  void (*acton)(struct mrp_applicant *app,
+				  struct mrp_attr *attr),
+				  void (*cleanup)(struct mrp_applicant *app))
+{
+	app->attrval_inc = mrp_attrvalue_inc;
+	app->attr_chk = mvrp_attr_chk;
+	app->attr_cmp = mrp_attr_cmp;
+	app->attr_valid = mvrp_attr_valid;
+	app->attr_size = mrp_attr_size;
+	app->attr_len = mrp_attr_len;
+	app->attr_type = mrp_attr_type;
+	app->rxpdu = mvrp_rxpdu;
+	app->txpdu = mvrp_txpdu;
+	app->acton = acton;
+	app->cleanup = cleanup;
+	app->normal = (1 << MVRP_ATTR_VID);
+}
+
diff --git a/drivers/net/ethernet/micrel/ksz9897/spi-ksz9897.c b/drivers/net/ethernet/micrel/ksz9897/spi-ksz9897.c
new file mode 100644
index 0000000..a262631
--- /dev/null
+++ b/drivers/net/ethernet/micrel/ksz9897/spi-ksz9897.c
@@ -0,0 +1,839 @@
+/**
+ * Microchip KSZ9897 SPI driver
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#if 0
+#define NO_ATTACHED_DEV
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#endif
+
+#define MAX_REQUEST_SIZE		80
+
+#include "ksz_cfg_9897.h"
+
+
+#if 1
+#define NO_EEE
+#endif
+
+
+#ifdef CONFIG_KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+#define CONFIG_HAVE_HSR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+
+#define KS9897MLI_DEV0			"ksz9897"
+#define KS9897MLI_DEV2			"ksz9897_2"
+
+#define SW_DRV_RELDATE			"Feb 12, 2018"
+#define SW_DRV_VERSION			"1.1.9"
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+#define HW_R(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R24(ks, reg)		spi_rdreg24(ks, reg)
+#define HW_W24(ks, reg, val)	spi_wrreg24(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+#else
+#define HW_R(ks, reg)		0
+#define HW_W(ks, reg, val)
+#define HW_R8(ks, reg)		0
+#define HW_W8(ks, reg, val)
+#define HW_R16(ks, reg)		0
+#define HW_W16(ks, reg, val)
+#define HW_R24(ks, reg)		0
+#define HW_W24(ks, reg, val)
+#define HW_R32(ks, reg)		0
+#define HW_W32(ks, reg, val)
+#endif
+
+#include "ksz_sw_phy.h"
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define contain_reg(addr, len, reg)	\
+	(addr <= (reg) && (reg) <= (addr + len - 1))
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+static void spi_chk_regs(struct ksz_sw *sw, u32 addr, u8 *val, size_t txl)
+{
+	int i;
+	u32 port_reg;
+
+	port_reg = REG_PTP_MSG_CONF1 + 1;
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PTP_ENABLE)
+			sw->overrides |= PTP_TAG;
+		else
+			sw->overrides &= ~PTP_TAG;
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+		if (!(sw->features & NEW_CAP) &&
+		    sw->TAIL_TAG_LOOKUP >= 0x100)
+			sw->overrides |= PTP_TAG;
+#endif
+	}
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PORT_TAIL_TAG_ENABLE)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+	}
+}  /* spi_chk_regs */
+
+/* SPI frame opcodes */
+#define KS_SPIOP_RD			3
+#define KS_SPIOP_WR			2
+
+#define SPI_ADDR_SHIFT			24
+#define SPI_ADDR_MASK			((1 << SPI_ADDR_SHIFT) - 1)
+#define SPI_TURNAROUND_SHIFT		5
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @ks:		The switch device structure.
+ * @addr:	The register address.
+ * @val:	The value to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @addr.
+ */
+static void spi_wrreg(struct sw_priv *ks, u32 addr, void *txb, size_t txl)
+{
+	struct spi_hw_priv *hw_priv = ks->hw_dev;
+	struct spi_transfer *xfer = &hw_priv->spi_xfer1;
+	struct spi_message *msg = &hw_priv->spi_msg1;
+	struct spi_device *spi = hw_priv->spidev;
+	u32 *txc = (u32 *) hw_priv->txd;
+	int ret;
+	struct ksz_sw *sw = &ks->sw;
+
+	if (!(sw->features & NEW_CAP)) {
+		u32 len = (addr & 3) + txl;
+
+		if (len >= 4 && (len & 3))
+			pr_alert("W may not be correct: %x %x\n", addr, txl);
+	}
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("W not locked: %x\n", addr);
+	*txc = addr & SPI_ADDR_MASK;
+	*txc |= KS_SPIOP_WR << SPI_ADDR_SHIFT;
+	*txc <<= SPI_TURNAROUND_SHIFT;
+	*txc = cpu_to_be32(*txc);
+	++txc;
+	memcpy(txc, txb, txl);
+
+	xfer->tx_buf = hw_priv->txd;
+	xfer->rx_buf = NULL;
+	xfer->len = txl + 4;
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed: %x %u\n", addr, txl);
+	spi_chk_regs(sw, addr, txb, txl);
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 4;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 4);
+}
+
+/**
+ * spi_wrreg24 - write 24bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg24(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 3;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 3);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 2;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *ks, u32 reg, u32 val)
+{
+	u8 txb[4];
+	int i = 0;
+	size_t cnt = 1;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(ks, reg, txb, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @ks:		The device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *ks)
+{
+	return ks->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @ks:		The device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @op.
+ */
+static void spi_rdreg(struct sw_priv *ks, u32 addr, void *rxb, size_t rxl)
+{
+	struct spi_hw_priv *hw_priv = ks->hw_dev;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	struct spi_device *spi = hw_priv->spidev;
+	u32 *txc = (u32 *) hw_priv->txd;
+	u8 *trx = hw_priv->rxd;
+	int ret;
+
+	if (!mutex_is_locked(&ks->lock))
+		pr_alert("R not locked: %x\n", addr);
+	*txc = addr & SPI_ADDR_MASK;
+	*txc |= KS_SPIOP_RD << SPI_ADDR_SHIFT;
+	*txc <<= SPI_TURNAROUND_SHIFT;
+	*txc = cpu_to_be32(*txc);
+
+	if (ksz_rx_1msg(hw_priv)) {
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl + 4;
+		++txc;
+		memset(txc, 0, rxl);
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		memcpy(hw_priv->rxd, hw_priv->txd, 4 + 1);
+		hw_priv->rxd[4] ^= 0xff;
+#endif
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = 4;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed: %x %u\n", addr, rxl);
+	else if (ksz_rx_1msg(hw_priv))
+		memcpy(rxb, trx + 4, rxl);
+	else
+		memcpy(rxb, trx, rxl);
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *ks, u32 reg)
+{
+	u8 rxb[1];
+
+	spi_rdreg(ks, reg, rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *ks, u32 reg)
+{
+	__le16 rx = 0;
+
+	spi_rdreg(ks, reg, &rx, 2);
+	return be16_to_cpu(rx);
+}
+
+/**
+ * spi_rdreg24 - read 24 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 24bit register from the chip.
+ */
+static u32 spi_rdreg24(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	spi_rdreg(ks, reg, &rx, 3);
+	return be32_to_cpu(rx);
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @ks:		The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ */
+static u32 spi_rdreg32(struct sw_priv *ks, u32 reg)
+{
+	__le32 rx = 0;
+
+	spi_rdreg(ks, reg, &rx, 4);
+	return be32_to_cpu(rx);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * delay_micro - delay in microsecond
+ * @microsec:	Number of microseconds to delay.
+ *
+ * This routine delays in microseconds.
+ */
+static inline void delay_micro(uint microsec)
+{
+	uint millisec = microsec / 1000;
+
+	microsec %= 1000;
+	if (millisec)
+		mdelay(millisec);
+	if (microsec)
+		udelay(microsec);
+}
+
+/**
+ * delay_milli - delay in millisecond
+ * @millisec:	Number of milliseconds to delay.
+ *
+ * This routine delays in milliseconds.
+ */
+static void delay_milli(uint millisec)
+{
+	unsigned long ticks = millisec * HZ / 1000;
+
+	if (!ticks || in_interrupt())
+		mdelay(millisec);
+	else {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(ticks);
+	}
+}
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+static inline void copy_old_skb(struct sk_buff *old, struct sk_buff *skb)
+{
+	skb->dev = old->dev;
+	skb->sk = old->sk;
+	skb->protocol = old->protocol;
+	skb->ip_summed = old->ip_summed;
+	skb->csum = old->csum;
+	skb_shinfo(skb)->tx_flags = skb_shinfo(old)->tx_flags;
+	skb_set_network_header(skb, ETH_HLEN);
+
+	dev_kfree_skb(old);
+}  /* copy_old_skb */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define KSZSW_REGS_SIZE			0x8000
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x0000, 0x7FFF },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->reglock);
+	if (sw->reg != reg_ops) {
+		mutex_unlock(sw->reglock);
+		sw->reg->lock(sw);
+	}
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R24(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+#ifndef CONFIG_KSZ_IBA_ONLY
+	spi_rdreg(sw->dev, reg, buf, cnt);
+#else
+	/* Avoid compiler complaint. */
+	memset(buf, 0, cnt);
+#endif
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+#ifndef CONFIG_KSZ_IBA_ONLY
+	spi_wrreg(sw->dev, reg, buf, cnt);
+#endif
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W24(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+
+static void link_update_work(struct work_struct *work);
+
+static void sw_dis_intr(struct ksz_sw *sw);
+static void sw_ena_intr(struct ksz_sw *sw);
+
+#define USE_DIFF_PORT_PRIORITY
+#include "ksz_sw_9897.c"
+
+static int rx_1msg = 1;
+static int spi_bus;
+
+
+#define MAX_SPI_DEVICES		2
+
+#if !defined(CONFIG_KSZ_IBA_ONLY)
+static int ksz9897_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *ks;
+
+	spi->bits_per_word = 8;
+
+	ks = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!ks)
+		return -ENOMEM;
+
+	ks->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!ks->hw_dev) {
+		kfree(ks);
+		return -ENOMEM;
+	}
+	hw_priv = ks->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	ks->dev = &spi->dev;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	ks->irq = spi->irq;
+
+	return ksz_probe(ks);
+}
+
+static int ksz9897_remove(struct spi_device *spi)
+{
+	struct sw_priv *ks = dev_get_drvdata(&spi->dev);
+
+	return ksz_remove(ks);
+}
+
+static const struct of_device_id ksz9897_dt_ids[] = {
+	{ .compatible = "microchip,ksz9567" },
+	{ .compatible = "microchip,ksz9477" },
+	{ .compatible = "microchip,ksz9893" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz9897_dt_ids);
+
+static struct spi_driver ksz9897_driver = {
+	.driver = {
+		.name = KS9897MLI_DEV0,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz9897_dt_ids),
+	},
+	.probe = ksz9897_probe,
+	.remove = ksz9897_remove,
+};
+
+static int __init ksz9897_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+
+	return spi_register_driver(&ksz9897_driver);
+}
+
+static void __exit ksz9897_exit(void)
+{
+	spi_unregister_driver(&ksz9897_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+#endif
+
+#if !defined(CONFIG_KSZ_SWITCH_EMBEDDED)
+subsys_initcall(ksz9897_init);
+module_exit(ksz9897_exit);
+#endif
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+module_param(avb, int, 0);
+module_param(authen, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+MODULE_PARM_DESC(avb, "AVB support");
+MODULE_PARM_DESC(authen, "802.1X Authentication");
+
+#ifdef CONFIG_KSZ_IBA
+module_param(iba, int, 0);
+MODULE_PARM_DESC(iba, "IBA support");
+#endif
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(1=level low, 2=falling)");
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+module_param(sw_host_port, int, 0);
+MODULE_PARM_DESC(sw_host_port,
+	"Configure switch host port");
+
+module_param(ports, int, 0);
+MODULE_PARM_DESC(ports,
+	"Configure number of ports");
+
+module_param(eth1_ports, int, 0);
+module_param(eth2_ports, int, 0);
+module_param(eth3_ports, int, 0);
+module_param(eth4_ports, int, 0);
+module_param(eth5_ports, int, 0);
+module_param(eth6_ports, int, 0);
+MODULE_PARM_DESC(eth1_ports, "Ports to use on device 1.");
+MODULE_PARM_DESC(eth2_ports, "Ports to use on device 2.");
+MODULE_PARM_DESC(eth3_ports, "Ports to use on device 3.");
+MODULE_PARM_DESC(eth4_ports, "Ports to use on device 4.");
+MODULE_PARM_DESC(eth5_ports, "Ports to use on device 5.");
+MODULE_PARM_DESC(eth6_ports, "Ports to use on device 6.");
+
+module_param(eth1_vlan, int, 0);
+module_param(eth2_vlan, int, 0);
+module_param(eth3_vlan, int, 0);
+module_param(eth4_vlan, int, 0);
+module_param(eth5_vlan, int, 0);
+module_param(eth6_vlan, int, 0);
+MODULE_PARM_DESC(eth1_vlan, "VLAN to use on device 1.");
+MODULE_PARM_DESC(eth2_vlan, "VLAN to use on device 2.");
+MODULE_PARM_DESC(eth3_vlan, "VLAN to use on device 3.");
+MODULE_PARM_DESC(eth4_vlan, "VLAN to use on device 4.");
+MODULE_PARM_DESC(eth5_vlan, "VLAN to use on device 5.");
+MODULE_PARM_DESC(eth6_vlan, "VLAN to use on device 6.");
+
+module_param(eth1_proto, charp, 0);
+module_param(eth2_proto, charp, 0);
+module_param(eth3_proto, charp, 0);
+module_param(eth4_proto, charp, 0);
+module_param(eth5_proto, charp, 0);
+module_param(eth6_proto, charp, 0);
+MODULE_PARM_DESC(eth1_proto, "Protocol to use on device 1.");
+MODULE_PARM_DESC(eth2_proto, "Protocol to use on device 2.");
+MODULE_PARM_DESC(eth3_proto, "Protocol to use on device 3.");
+MODULE_PARM_DESC(eth4_proto, "Protocol to use on device 4.");
+MODULE_PARM_DESC(eth5_proto, "Protocol to use on device 5.");
+MODULE_PARM_DESC(eth6_proto, "Protocol to use on device 6.");
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("Microchip KSZ9897 SPI Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz9897");
+#endif
